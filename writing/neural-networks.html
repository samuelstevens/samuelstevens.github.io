<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="Tips and tricks that don&#x2019;t show up
in papers for training big neural networks." />
  <meta name="keywords" content="neural networks, training tips, batch
size, learning rate scheduling" />
  <title>Behold: Tips &amp; Tricks for Training Neural Networks</title>
  
</head>

<body>
  <aside>
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="https://github.com/samuelstevens">GitHub</a>]
        [<a href="mailto:samuel.robert.stevens@gmail.com">Email</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="tips-tricks-for-training-neural-networks">Tips &amp; Tricks for
Training Neural Networks</h1>
<p>Read <a href="https://karpathy.github.io/2019/04/25/recipe/">Andrej
Karpathy&#x2019;s guide</a>. Do everything he says. Visualize your inputs,
write some code to check that you&#x2019;re not mixing information across the
batch dimension&#x2013;do it all.</p>
<p>These are the other tips and tricks that I picked up over many, many
failed attempts.</p>
<ul>
<li>Batch size matters a lot in pretraining. When pretraining a Swin
Transformer (~89M parameters, supervised image classification), I needed
a global batch size of at least 1024 to see meaningful improvement.</li>
<li>Learning rate decay will make it seem like your model is not
&#x201C;converging&#x201D; because the performance will continue to improve until the
last epoch. But if you had a constant learning rate, it would likely
flatten out.</li>
<li><a
href="https://twitter.com/karpathy/status/1621578354024677377">Your
parameter sizes should be multiples of 64 wherever possible</a>. You
will get massive (10-25%) speedups for free.</li>
</ul>
<h2 id="notes-on-googles-tuning-playbook">Notes on Google&#x2019;s Tuning
Playbook</h2>
<blockquote>
<p>I took these notes when working on applying Swin Transformers to the
iNaturalist 2021 dataset.</p>
</blockquote>
<p>Just use the standard model and optimizer (typically read the model&#x2019;s
original paper to find the standard optimizer; check recent citations of
the model&#x2019;s original paper to find any changes).</p>
<p>Don&#x2019;t treat batch size as a hyperparameter to tune validation
performance because you can just tune optimizer hyperparameters,
regularization hyperparameters and number of training steps.</p>
<blockquote>
<p>But in my experience, you might have a minimum required global batch
size.</p>
</blockquote>
<p>If you can double batch size and training throughput <em>does
not</em> double, then you likely have an I/O or node synchronization
bottleneck. You should fix that first.</p>
<p>If throughput only increases up to batch size N, then there is no
benefit to using batch sizes above N (unless you don&#x2019;t reach that
minimum batch size for stable training).</p>
<p>Most hyperparameters are very sensitive to batch size. This makes
changing batch size very expensive in terms of re-tuning
hyperparameters.</p>
<p>To choose the initial configuration, we need to a find a simple, fast
low-resource baseline that achieves a reasonable result.</p>
<ul>
<li>Simple means no learning rate decay, for example.</li>
<li>Fast and low-resource means starting with a smaller model.</li>
<li>Reasonable means it should be much better than random
performance.</li>
</ul>
<p>You probably also want a lower number of steps so you can tune
faster, then do a final run that&#x2019;s longer.</p>
<p>Incremental tuning has four repeating steps:</p>
<ol type="1">
<li>Identifiy a goal for the next round of experiments. Examples:</li>
<li>Improving our pipeline (new regularizer, preprocessing choice)</li>
<li>Understanding the impact of a specific hyperparameter</li>
<li>Maximizing validation error</li>
<li>Design and run experiments to make progress towards that goal.</li>
<li>Learn from the results.</li>
<li>Decide whether to update the running &#x201C;best&#x201D; configuration.</li>
</ol>
<p>Most of our goals should be to learn more about the problem,
<em>not</em> maximizing validation performance.</p>
<p>Optimizer hyperparameters are typically nuisance parameters (need to
be tuned for every experiment) because their optimal values depend
heavily on all other hyperparameters (architecture, batch size, number
of training steps, etc). We also have no <em>a priori</em> reason to
prefer some given optimizer parameter anyways.</p>
<p>But the <em>choice</em> of optimizer is typically a scientific or
fixed hyperparameter (under study or already chosen).</p>
<p>Hyperparameters introduced by a regularization technique are nuisance
hyperparameters but whether we <em>include</em> the regularization
technique is scientific or fixed. The same applies to architectural
hyperparameters.</p>
<p>A <em>study</em> is a set of hyperparameter configurations to be run.
Each configuration is a <em>trial</em>. Typically we choose the
hyperparameters to vary across trials, choose the search space, choose a
number of trials, and choose an algorithm to sample trials from the
space. Just use quasi-random search since you can run jobs in
parallel.</p>
<p>Questions you need to ask before you can draw insights from a set of
experiments:</p>
<ol type="1">
<li>Is the search space large enough? A search space is likely not large
enough if the best point is near the search space boundary.</li>
<li>Have we sampled enough points from the search space?</li>
<li>What fraction of the trials are <em>infeasible</em> (trials diverge,
get really bad loss, fail to run, etc.)? If there are broad areas of the
search space that are infeasible, we should avoid sampling from these
areas. It might also indicate a bug in the code.</li>
</ol>
<h2 id="tidbits">Tidbits</h2>
<ul>
<li>Weight decay typically depends on model size.</li>
<li>Learning rate typically depends on model architecture.</li>
<li>If you want to use a model on downstream tasks, don&#x2019;t decay the
learning rate.</li>
<li>If you are training a really big model (100B+ parameters), then you
can just skip batches that cause loss spikes. See <a
href="https://twitter.com/rosstaylor90/status/1623779902334853122">this
link about Galactica training</a>.</li>
</ul>
<h2
id="learnings-from-large-vision-transformers-22b-parameters">Learnings
from Large Vision Transformers (22B Parameters)</h2>
<ul>
<li>Just like PaLM, you can do your attention and MLP blocks in <a
href="https://twitter.com/PiotrPadlewski/status/1625189242291863555">parallel</a>.</li>
<li>Get rid of biases in qeury/key/value projections and layer norms,
which improve GPU utilization by 3% while only reducing parameter count
<a
href="https://twitter.com/PiotrPadlewski/status/1625189495984295943">by
0.1%</a>.</li>
<li>Apply layer norm to the query and value outputs. This means you can
use a larger <a
href="https://twitter.com/_basilM/status/1625185484837208082">learning
rate</a>.</li>
</ul>
<h2 id="resources-links">Resources &amp; Links</h2>
<ul>
<li><a href="https://karpathy.github.io/2019/04/25/recipe/">Andrej
Karpathy&#x2019;s Guide to Training Neural Networks</a></li>
<li><a
href="https://github.com/google-research/tuning_playbook#choosing-a-model-architecture">Google&#x2019;s
Deep Learning Tuning Playbook</a></li>
</ul>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2022</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
