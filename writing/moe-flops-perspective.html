<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="How to look at MoE models in terms
of efficient use of FLOPs, GPU-sharding and parameters." />
  <meta name="keywords" content="mixture of
experts, MoE, mixture-of-experts, flops, compute, language model" />
  <title>Behold: A FLOPs Perspective of MoE Models</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="a-flops-perspective-of-mixture-of-expert-moe-models">A FLOPs
Perspective of Mixture-of-Expert (MoE) Models</h1>
<p>Mixture-of-Expert (MoE) models have been getting a lot of attention
in the research community, partially because GPT-4 is rumored to be an
MoE model, and partially because Mistral released <a
href="https://mistral.ai/news/mixtral-of-experts/"><em>Mixtral</em></a>,
a high-quality open-source MoE language model.<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>I think an underrated component of MoE models is their efficient
mapping to hardware during training. I&#x2019;m going to try to convince you
that MoE models are a smarter use of resources than dense models during
training.</p>
<p>Here&#x2019;s a diagram showing a typical dense transformer FFNN layer and
how it&#x2019;s split up over some GPUs for training:</p>
<figure>
<a href="/images/moe/dense.png"><img src="/images/moe/dense.png" alt="Diagram of a dense transformer FFNN layer split over 4 GPUs for training."></a>
<figcaption aria-hidden="true">
Dense transformer&#x2019;s FFNN layer.
</figcaption>
</figure>
<p>On each of the 4 GPUs, there are 6 copies of <span
class="math inline">\(W_i\)</span> and <span
class="math inline">\(W_o\)</span>, the matrices that make up the fully
connected layer in a transformer FFNN layer. <span
class="math inline">\(W_i\)</span> corresponds to <a
href="https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L82"><code>self.c_fc</code>
in Karpathy&#x2019;s nanoGPT</a> and <span class="math inline">\(W_o\)</span>
corresponds to <a
href="https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L84"><code>self.c_proj</code></a>.</p>
<p>We have a batch of 4 sequences with 6 tokens each. Our GPUs have
enough memory for a local batch size of 1 (with a sequence length of 6)
so each sequence goes onto its own GPU.</p>
<p>In a regular training loop, we would do a forward pass, store the
activations, compute the gradients, sync the gradients across GPUs, use
that to compute moments for Adam, then apply the updates and throw
activations and gradients away.</p>
<p><strong>The key point here is that we have a copy of the same FFNN
layer for each token</strong></p>
<p>Then, there are a couple natural questions:</p>
<ol type="1">
<li>What if we didn&#x2019;t have to apply the FFNN to every token?</li>
<li>What if we didn&#x2019;t have to copy the <em>same</em> FFNN, but could use
different FFNNs?</li>
</ol>
<p>The key bit to understand: even if you have different FFNNs, you have
identical FLOP requirements (and not excessive memory requirements; more
on that <a href="#memory-requirements">later</a>).</p>
<p>Here&#x2019;s the same diagram as before, but for a standard MoE FFNN
layer:</p>
<figure>
<a href="/images/moe/moe.png"><img src="/images/moe/moe.png" alt="Diagram of a MoE transformer FFNN layer split over 4 GPUs for training."></a>
<figcaption aria-hidden="true">
MoE transformer&#x2019;s FFNN layer.
</figcaption>
</figure>
<p>We now have 12 experts, split across 4 GPUs. Each GPU carries 2
copies of each expert (so each expert gets to process 2 tokens) What has
changed?</p>
<ol type="1">
<li>[+] We&#x2019;ve added a lot more parameters. Our FFNN layer now has 12
times as many parameters (and in general, more parameters means a better
model).</li>
<li>[-] We need to figure out what GPU gets each token, likely learning
some token-expert affinity matrix.</li>
<li>[-] We need to figure out how to combine the outputs of the
experts.</li>
<li>[-] We need to keep track of 12 times as many moments for Adam (not
great).</li>
<li>[+] We <em>don&#x2019;t</em> need to sync FFNN gradients between GPUs
because each GPU has separate FFNNs.</li>
</ol>
<p>What <em>hasn&#x2019;t</em> changed?</p>
<ol type="1">
<li>We only do 24 tokens worth of FLOPs for the FFNN, even though we
have 12 times as many parameters.</li>
<li>We only need 24 tokens worth of activations.</li>
</ol>
<p>What is still unclear (to me)?</p>
<ul>
<li>How many gradients do we need in memory? We have 12 different
gradients for MoE, but I think gradients in PyTorch are stored on each
tensor so that the optimizer can apply them itself. So I think the dense
model is also storing the same number of gradients (24 copies), but I&#x2019;m
not sure.</li>
</ul>
<p>Another useful thing about MoE models is that they more naturally map
onto GPU-based training. If you&#x2019;re Cerebras, you can train a <a
href="https://www.cerebras.net/blog/introducing-gigagpt-gpt-3-sized-models-in-565-lines-of-code">huge
dense, transformer language model on one chip</a>. Everyone else needs
to split the model up over multiple GPUs. Sometimes that&#x2019;s data
parallelism, sometimes model parallelism, sometimes even tensor
parallelism. <a href="https://www.deepspeed.ai/">DeepSpeed</a> is a
software library designed exactly for this with PyTorch. Deciding how to
split the models up is its own research field. You have to balance
tradeoffs between memory bandwidth, compute speed, how fast you can load
data onto GPUs, how big your model is, your network speed, etc. It&#x2019;s
very complicated.</p>
<p>While MoE models might seem like they introduce more complexity, I
argue that they actually simplify some parts of how you split your
models up over GPUs.</p>
<p>If your experts are small enough (and they might not be), each expert
can fit on a single GPU. If your GPUs are bigger, you can put multiple
copies of an expert on a single GPU (each expert can process more than
one token). You can let your available hardware dictate your optimal
model hyperparameters.</p>
<p>If you consider yourself a scientist, this will make you unhappy!
Hardware is just some crap to deal with so that you can explore how and
why these models work. Your experiments should be reproducible on any
hardware, and there should be code to abstract the hardware away from
the experiment itself.</p>
<p>If you consider yourself an engineer, this is great! Of course your
hardware dictates what models you can train. MoE models actually
<em>reduce</em> the abstraction: each GPU has some parameters, gradients
flow through those parameters, each GPU can update its own parameters.
MoE reduces your total inter-GPU communication and might actually lead
to improved training speeds!</p>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer></script>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>It&#x2019;s not clear if Mixtral is truly a MoE model trained
from scratch or if it was initialized from the original Mistral-7B: <a
href="https://twitter.com/tianle_cai/status/1734188749117153684">tweet
arguing that it was initialized from Mistral-7B</a>, <a
href="https://arxiv.org/abs/2212.05055">referenced paper</a>, <a
href="https://twitter.com/arankomatsuzaki/status/1734261474778919105">tweet
that does not agree</a>. Still unclear, but worth remembering.<a
href="#fnref1" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</section>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
