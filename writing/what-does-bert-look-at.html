<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="My take on &#x201C;What Does BERT Look At?&#x201D;
and what it means for future work with BERT." />
  <meta name="keywords" content="academic paper, interesting
paper, bert, natural language
processing, analysis, attention, transformer" />
  <title>Behold: Papers are Neat: What Does BERT Look At?</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links">Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="papers-are-neat-what-does-bert-look-at">Papers are Neat: What
Does BERT Look At?</h1>
<p>My undergraduate thesis focuses on fine-tuning pretrained language
models on sentence classification. Because of this, I&#x2019;ve been reading a
lot about BERT, its derivatives (RoBERTa, SciBERT) and how BERT actually
works on the inside. One of the cooler papers I&#x2019;ve read is &#x201C;What Does
BERT Look At? An Analysis of BERT&#x2019;s Attention&#x201D; <span class="citation"
data-cites="clark2019">(<a href="#ref-clark2019"
role="doc-biblioref">Clark et al. 2019</a>)</span>.</p>
<blockquote>
<p><em>Note: I&#x2019;ll use this styling to highlight direct quotes from the
paper.</em></p>
</blockquote>
<p>The gist of the paper is: Clark and his co-authors find that the
attention heads within BERT find some grammatical structures with
surprising accuracy, <em>given that they&#x2019;re never trained to do
this</em>.</p>
<blockquote>
<p>For example, we find heads that find direct objects of verbs,
determiners of nouns, objects of prepositions and objects of possessive
pronouns with &gt;75% accuracy.</p>
</blockquote>
<p>Just as Clark says, these results are <em>crazy</em> because BERT&#x2019;s
not ever asked to find these patterns. It&#x2019;s just given a bunch of data
and asked to predict most likely words in a context.</p>
<p>If you&#x2019;re unfamiliar with BERT, attention heads, or transformer
models in general, I&#x2019;ll do my best to make this approachable. If you&#x2019;re
interested in how this all works, I recommend the following.</p>
<ol type="1">
<li>For attention: <a
href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay
Alammar&#x2019;s intro to attention in a neural MT context</a></li>
<li>For transformer models: <a
href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar&#x2019;s
intro to transformer models</a></li>
<li>For BERT (a little more involved):
<ol type="1">
<li><a
href="https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77">Part
1</a></li>
<li><a
href="https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1">Part
2</a></li>
<li><a
href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/">Explore with
the <code>huggingface</code> library</a></li>
</ol></li>
</ol>
<h2 id="attention-transformers-bert-tldr">Attention &amp; Transformers
&amp; BERT TL;DR</h2>
<p>Attention is a mechanism for models to figure out which word is most
important at different points in a sentence. Transformers use attention
exclusively (it used to be a supplemental mechanism) to great success.
BERT is one of the most popular and succesful transformer models,
released by Google in 2019. In BERT (specifically the base model), a
sentence goes through 12 layers of modeling, and in each layer, for each
word, attention <em>heads</em> point to another word in the sentence as
the &#x201C;most important&#x201D;.</p>
<h2 id="the-paper">The Paper</h2>
<p>Clark and his co-authors inspect the overall attention trends of and
find that some of the attention head in the earlier layers put the
majority of their weight on either the next or previous token in the
sentence.</p>
<blockquote>
<p>In particular four attention heads (in layers 2, 4, 7, and 8) on
average put &gt;50% of their attention on the previous token and five
attention heads (in layers 1, 2, 2, 3, and 6) put &gt;50% of their
attention on the next token.</p>
</blockquote>
<p>They also find that &#x201C;over half of BERT&#x2019;s attention in layers 6-10
focuses on [SEP]&#x201D; (the last, artificially added token in a sentence).
They hypothesize that attending to [SEP] is a kind of &#x201C;no-op&#x201D; for when
the &#x201C;attention head&#x2019;s function is not applicable.&#x201D; This finding is
further supported by applying gradient-based measures of feature
importance and finding that in layer 5, &#x201C;the same layer where attention
to [SEP] becomes high&#x201D;, the gradients for attention to [SEP]
dramatically diminish.</p>
<blockquote>
<p>This indicates that attending more or less to [SEP] does not
substantially change BERT&#x2019;s outputs, supporting the theory that
attention to [SEP] is used as a no-op for attention heads.</p>
</blockquote>
<h2 id="the-cool-stuff">The Cool Stuff</h2>
<p>The part of the paper that made me gasp was the analysis of BERT&#x2019;s
ability to parse syntactic structures from raw text. For a variety of
syntactic structures (&#x201C;direct objects of verbs, determiners of nouns,
objects of prepositions and objects of possessive pronouns&#x201D;), Clark and
his co-authors look for a particular attention head that best predicts
these dependencies. That means that for the sentence &#x201C;The cat ate the
mouse&#x201D;, the authors look for an attention head points from &#x201C;mouse&#x201D; to
&#x201C;ate&#x201D;. I won&#x2019;t copy the entire table here, but the results in the paper
are very positive. Bert learns these relationships surprisingly
well.</p>
<p>What <em>really</em> is nuts, however, is BERT learning the structure
of objects of possessive pronouns. Clark and his co-authors find that
BERT does not have an attention head that attends from an object to its
possessive pronoun. For example, for the sentence &#x201C;I found John&#x2019;s
ball!&#x201D;, BERT never attends strongly from &#x201C;ball&#x201D; to &#x201C;John&#x201D;, which is how
we (as humans) would diagram this sentence. Instead, one of BERT&#x2019;s
attention heads attends strongly from &#x201C;ball&#x201D; to the &#x201C;&#x2019;s&#x201D; after &#x201C;John&#x201D;.
<em>BERT learns possessive objects on its own, and differently to
humans</em></p>
<blockquote>
<p>Such disagreements [between the human and machine annotations]
highlight how these syntactic behaviors in BERT are learned as a
by-product of self-supervised training, not by copying a human
design</p>
</blockquote>
<p>Holy. Crap.</p>
<p>BERT learns grammar on its own from just reading a lot of English
text. If that doesn&#x2019;t get you excited about NLP, I don&#x2019;t know what
will.</p>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-clark2019" class="csl-entry" role="listitem">
Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning.
2019. <span>&#x201C;What Does Bert Look at? An Analysis of Bert&#x2019;s
Attention.&#x201D;</span> <a
href="http://arxiv.org/abs/1906.04341">http://arxiv.org/abs/1906.04341</a>.
</div>
</div>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
