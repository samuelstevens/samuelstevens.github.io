<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <meta name="description" content="A guide to using submitit on Slurm
systems." />
  <meta name="keywords" content="submitit, slurm, osc, python, cluster" />
  <title>Submitit Guide</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me3.jpg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Sam Stevens</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/research">Research</a>]
        [<a href="/microblog">Blog</a>]
        [<a href="/contact">Contact</a>]
        [<a href="/cv.pdf">CV</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="how-to-use-submitit-on-slurm-systems.">How to Use <a
href="https://github.com/facebookincubator/submitit"><code>submitit</code></a>
on Slurm Systems.</h1>
<p><a
href="https://github.com/facebookincubator/submitit"><code>submitit</code></a>
is a Python package by Facebook/Meta for submitting one or more Python
jobs to Slurm clusters, like those at <a
href="https://www.osc.edu/">OSC</a> or Facebook.</p>
<p>While it&#x2019;s easy to use, there were a couple gotchas and lessons I had
to learn before I really appreciated how useful it was compared to
writing Bash scripts. This article covers those, with plenty of examples
and explanation.</p>
<p>I have used <code>submitit</code> for several projects, including <a
href="https://github.com/samuelstevens/biobench">BioBench</a>, <a
href="https://github.com/samuelstevens/saev">saev</a>, <a
href="https://github.com/samuelstevens/frx/tree/main">frx</a>
(unmaintained at the moment), and <a
href="https://github.com/samuelstevens/mindthegap">mindthegap</a>. These
repos will contain complete examples of how I use <code>submitit</code>
in a larger context than a blog post can provide.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ol type="1">
<li><a href="#minimal-example">Minimal Example</a></li>
<li><a href="#why-use-submitit">Why Use <code>submitit</code>?</a></li>
<li><a href="#gpus-and-cpus">GPUs and CPUs</a></li>
<li><a href="#multi-gpu-training-in-torch">Multi-GPU Training in
Torch</a></li>
<li><a href="#debugexecutor">DebugExecutor</a></li>
<li><a
href="#stderr_to_stdouttrue"><code>stderr_to_stdout=True</code></a></li>
<li><a href="#global-variables">Global Variables</a></li>
<li><a href="#environment-variables">Environment Variables</a></li>
<li><a href="#submitting-gpu-jax-jobs-on-cpu-only-nodes">Submitting GPU
JAX Jobs on CPU-Only Nodes</a></li>
<li><a href="#cloudpickle-and-equinox"><code>cloudpickle</code> and
Equinox</a></li>
<li><a href="#complete-examples">Complete Examples</a></li>
</ol>
<h2 id="minimal-example">Minimal Example</h2>
<p><em>Minimal example for submitting jobs on a cluster.</em></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add(a, b):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">+</span> b</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.AutoExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    timeout_min<span class="op">=</span><span class="dv">1</span>, slurm_partition<span class="op">=</span><span class="st">&quot;gpu&quot;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> executor.submit(add, <span class="dv">5</span>, <span class="dv">7</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> job.result()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> output <span class="op">==</span> <span class="dv">12</span></span></code></pre></div>
<p>This is directly from the <a
href="https://github.com/facebookincubator/submitit?tab=readme-ov-file#an-example-is-worth-a-thousand-words-performing-an-addition"><code>submitit</code>
README</a>.</p>
<h2 id="why-use-submitit">Why Use <code>submitit</code>?</h2>
<p><em>Understand the motivation for <code>submitit</code>.</em></p>
<p>I learned to interact with Slurm clusters through bash scripts and
<code>sbatch</code>. We used this a lot in <a
href="https://github.com/Imageomics/bioclip">BioCLIP</a>, like <a
href="https://github.com/Imageomics/bioclip/blob/main/slurm/make-dataset-wds.sh">this
launch script</a> to run <code>make_wds.py</code>. I would submit it
with <code>sbatch slurm/make-dataset-wds.sh</code>.</p>
<p>This sucked for a couple reasons.</p>
<ol type="1">
<li>I had to write scripts in another language (bash) when I wanted to
use Python for everything.</li>
<li>I had to edit the bash script to change the Python script arguments.
I had a very nice argument parser in my Python script with
<code>argparse</code> and help text, etc. but I couldn&#x2019;t use it because
I didn&#x2019;t have an argument parser for
<code>make-datset-wds.sh</code>.</li>
<li>I couldn&#x2019;t easily programatically launch many jobs at once.
Sometimes I have a config file that specifies a sweep of jobs, and I
want to launch many jobs with one script. But because I can&#x2019;t write bash
very well, I&#x2019;ve written Python scripts to parse the config files, then
launch the jobs, with code like this:</li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># buggy and error-prone code; use submitit instead.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>command <span class="op">=</span> [</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;sbatch&quot;</span>,</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;--output=./logs/</span><span class="sc">{</span>job_name<span class="sc">}</span><span class="ss">-%j.log&quot;</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;--job-name=</span><span class="sc">{</span>job_name<span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f&quot;--export=CONFIG_FILE=</span><span class="sc">{</span>config_file<span class="sc">}</span><span class="ss">&quot;</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    template_file,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> subprocess.run(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>      command, check<span class="op">=</span><span class="va">True</span>, capture_output<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(output.stdout.decode(<span class="st">&quot;utf-8&quot;</span>), end<span class="op">=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> subprocess.CalledProcessError <span class="im">as</span> e:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e.stderr.decode(<span class="st">&quot;utf-8&quot;</span>), end<span class="op">=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span></code></pre></div>
<ol start="4" type="1">
<li>Re-launching jobs is also a hassle. If an experiment is
checkpointable, I would like to restart jobs when they end without
having to log back in.</li>
<li>Setting up your jobs to run on both Slurm cluster and local clusters
is challenging.</li>
</ol>
<p><code>submitit</code> solves all of these pain points. If you look at
projects like <a
href="https://github.com/samuelstevens/biobench">BioBench</a> or <a
href="https://github.com/samuelstevens/saev">saev</a>, there&#x2019;s no bash
scripts whatsoever, but they run on Slurm clusters and local s</p>
<h2 id="gpus-and-cpus">GPUs and CPUs</h2>
<p><em>Setting number of GPUs and CPUs in your jobs.</em></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    time<span class="op">=</span><span class="dv">120</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    partition<span class="op">=</span><span class="st">&quot;gpu&quot;</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    account<span class="op">=</span><span class="st">&quot;ACCOUNT&quot;</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># These args are important.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    gpus_per_node<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    ntasks_per_node<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    cpus_per_task<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># calls job_fn ntasks_per_node times in parallel.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>executor.submit(job_fn)  </span></code></pre></div>
<p><code>gpus_per_node</code> is GPUs per node,
<code>ntasks_per_node</code> is the number of processes that call your
function, and <code>cpus_per_task</code> is the number of CPUs available
per <em>task</em>. So if you want to run two tasks, each with two GPUs,
and a total of 24 CPUs, you need <code>ntasks_per_node=2</code>,
<code>gpus_per_node=4</code>, and <code>cpus_per_task=12</code>.</p>
<h2 id="multi-gpu-training-in-torch">Multi-GPU Training in Torch</h2>
<p><em>torch.distributed</em></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    time<span class="op">=</span><span class="dv">12</span> <span class="op">*</span> <span class="dv">60</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    partition<span class="op">=</span><span class="st">&quot;gpu&quot;</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    account<span class="op">=</span><span class="st">&quot;ACCOUNT&quot;</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># These args are important.</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    gpus_per_node<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    ntasks_per_node<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    cpus_per_task<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>executor.submit(train).result()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train():</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    dist_env <span class="op">=</span> submitit.helpers.TorchDistributedEnvironment().export()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    torch.distributed.init_process_group(</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        backend<span class="op">=</span><span class="st">&quot;nccl&quot;</span>, world_size<span class="op">=</span>dist_env.world_size</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> dist_env.rank <span class="op">==</span> torch.distributed.get_rank()</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> dist_env.world_size <span class="op">==</span> torch.distributed.get_world_size()</span></code></pre></div>
<p>When setting up your executor, be sure to set
<code>ntasks_per_node</code> to the same number as
<code>gpus_per_node</code> so that every GPU has a task.</p>
<p>The <code>submitit.helpers.TorchDistributedEnvironment</code> class
somehow handles environment variables so that PyTorch can setup the
distributed environment correctly.</p>
<p><strong>Note that if you don&#x2019;t have a CUDA device available, you
cannot call <code>init_process_group</code> so you probably want to
handle that.</strong></p>
<h2 id="debugexecutor">DebugExecutor</h2>
<p><em>Run code in the current process rather than in a Slurm
job.</em></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> debug:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    executor <span class="op">=</span> submitit.DebugExecutor(folder<span class="op">=</span>args.log_to)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    executor.update_parameters(</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        time<span class="op">=</span><span class="dv">30</span>, partition<span class="op">=</span><span class="st">&quot;PARTITION&quot;</span>, account<span class="op">=</span><span class="st">&quot;SLURM_ACCT&quot;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Use executor as normal.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>executor.submit(job_fn, arg1, arg2).result()</span></code></pre></div>
<p>If you are debugging jobs, you likely want to use <code>pdb</code> or
other interactive debuggers. You cannot use <code>pdb</code> in a
&#x201C;headless&#x201D; process like a Slurm job. However, the
<code>submitit.DebugExecutor</code> will run jobs in the same process
that you create the executor from. This is really useful for debugging
jobs, because <code>DebugExecutor</code> has the same API as
<code>SlurmExecutor</code> so you can split up your executor
construction code and then debug your jobs.</p>
<p>This solves problem #5.</p>
<blockquote>
<ol start="5" type="1">
<li>Setting up your jobs to run on both Slurm cluster and local clusters
is challenging.</li>
</ol>
</blockquote>
<h2 id="stderr_to_stdouttrue"><code>stderr_to_stdout=True</code></h2>
<p><em>Don&#x2019;t split up stderr and stdout logs.</em></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    time<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    partition<span class="op">=</span><span class="st">&quot;PARTITION&quot;</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    account<span class="op">=</span><span class="st">&quot;SLURM_ACCT&quot;</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    stderr_to_stdout<span class="op">=</span><span class="va">True</span>,  <span class="co"># &lt;- This line</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Most of the time I&#x2019;m not interested in the distinction between stderr
and stdout because I just care about outputs. <code>print()</code> in
Python goes to stdout, <code>logging.info()</code> goes to stderr. If
you mix them, it can be irritating to try and understand how your
debugging statements are ordered (but you should also use <a
href="#debugexecutor">pdb in a DebugExecutor</a> instead of print
statements). Setting <code>stderr_to_stdout=True</code> in
<code>executor.update_parameters()</code>writes everything to the same
stream.</p>
<h2 id="global-variables">Global Variables</h2>
<p><em>Global variables don&#x2019;t work.</em></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> use_ssl:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> ssl</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># By default do not use HTTPS</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        ssl._create_default_https_context <span class="op">=</span> ssl._create_unverified_context</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    executor.update_parameters(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        time<span class="op">=</span><span class="dv">30</span>, partition<span class="op">=</span><span class="st">&quot;PARTITION&quot;</span>, account<span class="op">=</span><span class="st">&quot;SLURM_ACCT&quot;</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    exeuctor.submit(job_fn)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> job_fn():</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(ssl._create_default_https_context)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Will not be an unverified context</span></span></code></pre></div>
<p>If you want to set global variables after the program is running but
before you submit jobs, these variables will not persist in your jobs.
In the example above, I want to set Python&#x2019;s <a
href="https://docs.python.org/3/library/ssl.html"><code>ssl</code></a>&#x2019;s
module to ignore HTTPS certs by setting
<code>ssl._create_default_https_context</code> to
<code>ssl._create_unverified_context</code>. However, in the job
function <code>job_fn</code>,
<code>ssl._create_default_https_context</code> will not be set
correctly.</p>
<h2 id="environment-variables">Environment Variables</h2>
<p><em>How to set environment variables in Slurm jobs.</em></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;./logs&quot;</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    time<span class="op">=</span><span class="dv">30</span>, partition<span class="op">=</span><span class="st">&quot;PARTITION&quot;</span>, account<span class="op">=</span><span class="st">&quot;SLURM_ACCT&quot;</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> use_ssl:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    executor.update_parameters(setup<span class="op">=</span>[</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;export DISABLE_SSL=1&quot;</span>,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;export HAVE_FUN=2&quot;</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    ])</span></code></pre></div>
<p>If you want to set global variables, you might end up using
environment variables. While using environment variables to manage
program state is almost always a source of bugs, if you absolutely need
to, you can use the <code>setup</code> parameter to set environment
variables in the running Slurm jobs.</p>
<h2 id="submitting-gpu-jax-jobs-on-cpu-only-nodes">Submitting GPU JAX
Jobs on CPU-Only Nodes</h2>
<p><em>Weird JAX issues with GPUs.</em></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;logs&quot;</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    time<span class="op">=</span><span class="dv">12</span> <span class="op">*</span> <span class="dv">60</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    partition<span class="op">=</span><span class="st">&quot;PARTITION&quot;</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    account<span class="op">=</span><span class="st">&quot;ACCOUNT&quot;</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    setup<span class="op">=</span>[<span class="st">&quot;export JAX_PLATFORMS=&#39;&#39;&quot;</span>],</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>For whatever reason, we cannot import JAX without a GPU. That is, if
you run <code>import jax</code> with <code>jax[cuda12]</code> installed
in your environment, you get an exception about &#x201C;No GPUs found&#x201D; or
something like that. But you can use <code>JAX_PLATFORMS=cpu</code>
before <code>uv run python -c "import jax; print(jax)"</code> and it
will work fine. <em>But</em>, if you set <code>JAX_PLATFORMS=cpu</code>
to run this launcher script, then it will be true for the submitted
jobs. This means that your training jobs will run on the CPU instead of
the cluster GPUs.</p>
<p>This extra arg exports an updated <code>JAX_PLATFORMS</code> variable
for the cluster jobs and it will find the GPUs for training.</p>
<h1 id="cloudpickle-and-equinox"><code>cloudpickle</code> and
Equinox</h1>
<p>When using <a href="https://docs.kidger.site/equinox">Equinox</a>
modules with <code>submitit</code>, which uses <code>cloudpickle</code>,
JIT tracing fails if the module class is defined in
<code>__main__</code>.</p>
<p>Define an <code>eqx.Module</code> in your main script and submit it
via <code>submitit</code>. JIT tracing will fail.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># broken.py - THIS FAILS</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> submitit</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(eqx.Module):  <span class="co"># Defined in __main__</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    linear: eqx.nn.Linear</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, key):</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> eqx.nn.Linear(<span class="dv">2</span>, <span class="dv">2</span>, key<span class="op">=</span>key)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear(x)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run():</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(jax.random.PRNGKey(<span class="dv">0</span>))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Eager:&quot;</span>, model(jnp.ones(<span class="dv">2</span>)))  <span class="co"># Works</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">@eqx.filter_jit</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(model, x):</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model(x)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;JIT:&quot;</span>, forward(model, jnp.ones(<span class="dv">2</span>)))  <span class="co"># FAILS</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;logs&quot;</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(gpus_per_node<span class="op">=</span><span class="dv">1</span>, ...)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> executor.submit(run)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>job.result()</span></code></pre></div>
<p><strong>Error:</strong></p>
<pre><code>AttributeError: &#39;Model&#39; object has no attribute &#39;linear&#39;</code></pre>
<p>Eager execution works; only JIT tracing fails. This happens because
<code>cloudpickle</code>serializes <code>__main__</code> classes by
inlining their definition, which breaks Equinox&#x2019;s attribute mechanism
during tracing.</p>
<p><strong>Fix:</strong> Move your <code>eqx.Module</code> classes to a
separate importable file:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># model.py</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(eqx.Module):</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    linear: eqx.nn.Linear</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, key):</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> eqx.nn.Linear(<span class="dv">2</span>, <span class="dv">2</span>, key<span class="op">=</span>key)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear(x)</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># main.py - THIS WORKS</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> submitit</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> model <span class="im">import</span> Model  <span class="co"># Import instead of define</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run():</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Model(jax.random.PRNGKey(<span class="dv">0</span>))</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">@eqx.filter_jit</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(model, x):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model(x)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;JIT:&quot;</span>, forward(model, jnp.ones(<span class="dv">2</span>)))  <span class="co"># Works</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;logs&quot;</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>executor.update_parameters(gpus_per_node<span class="op">=</span><span class="dv">1</span>, ...)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>job <span class="op">=</span> executor.submit(run)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>job.result()</span></code></pre></div>
<p><code>cloudpickle</code> handles classes differently based on where
they&#x2019;re defined:</p>
<ul>
<li><strong><code>__main__</code> classes</strong>: Serialized with full
class definition inlined</li>
<li><strong>Imported classes</strong>: Serialized as module references
(e.g., <code>model.Model</code>)</li>
</ul>
<p>The latter preserves the class identity correctly, allowing Equinox&#x2019;s
attribute mechanism to work during JIT tracing in the subprocess. Thus,
always define <code>eqx.Module</code> classes in importable files, not
in <code>__main__</code>, when using <code>submitit</code> or other
<code>cloudpickle</code>-based job submission.</p>
<h2 id="complete-examples">Complete Examples</h2>
<p><em>Complete PyTorch and JAX example with all these tricks.</em></p>
<p>PyTorch:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode py"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># import ...</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(device: <span class="bu">str</span>):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    dist_env <span class="op">=</span> submitit.helpers.TorchDistributedEnvironment().export()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    is_ddp <span class="op">=</span> <span class="va">False</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    global_rank <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    local_rank <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    is_master <span class="op">=</span> <span class="va">False</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device <span class="op">==</span> <span class="st">&quot;cuda&quot;</span>:</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        torch.distributed.init_process_group(</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            backend<span class="op">=</span><span class="st">&quot;nccl&quot;</span>, world_size<span class="op">=</span>dist_env.world_size</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dist_env.rank <span class="op">==</span> torch.distributed.get_rank()</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> dist_env.world_size <span class="op">==</span> torch.distributed.get_world_size()</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        is_ddp <span class="op">=</span> <span class="va">True</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        global_rank <span class="op">=</span> dist_env.rank</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        local_rank <span class="op">=</span> dist_env.local_rank</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        is_master <span class="op">=</span> dist_env.rank <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    logger <span class="op">=</span> logging.getLogger(<span class="st">&quot;Rank </span><span class="sc">%d</span><span class="st">&quot;</span>, args.global_rank)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ViT()</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_ddp:</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> torch.nn.parallel.DistributedDataParallel(model)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train as normal</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    </span></code></pre></div>
<p>JAX:</p>
<p>JAX multi-GPU training with submitit requires a different approach
than PyTorch. While you <em>can</em> use single-process multi-GPU (one
process sees all GPUs via mesh sharding), this causes NaN/gradient
corruption on some clusters. The stable approach mirrors PyTorch: one
process per GPU with <code>jax.distributed</code>.</p>
<p>First, define your model in a separate file to avoid <a
href="#cloudpickle-and-equinox">cloudpickle issues</a>:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># jax_model.py</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(eqx.Module):</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    layers: <span class="bu">list</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dim<span class="op">=</span><span class="dv">128</span>, hidden_dim<span class="op">=</span><span class="dv">256</span>, out_dim<span class="op">=</span><span class="dv">64</span>, <span class="op">*</span>, key):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        k1, k2, k3 <span class="op">=</span> jax.random.split(key, <span class="dv">3</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> [</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>            eqx.nn.Linear(in_dim, hidden_dim, key<span class="op">=</span>k1),</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>            eqx.nn.Linear(hidden_dim, hidden_dim, key<span class="op">=</span>k2),</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            eqx.nn.Linear(hidden_dim, out_dim, key<span class="op">=</span>k3),</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> jax.nn.relu(layer(x))</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers[<span class="op">-</span><span class="dv">1</span>](x)</span></code></pre></div>
<p>Then your training script:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># jax_submit.py</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.sharding</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optax</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> submitit</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax_model <span class="im">import</span> MLP</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> worker_fn():</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize multi-process JAX (like torch.distributed.init_process_group)</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    jax.distributed.initialize()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> jax.process_index() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>jax<span class="sc">.</span>process_count()<span class="sc">}</span><span class="ss"> processes, </span><span class="sc">{</span>jax<span class="sc">.</span>device_count()<span class="sc">}</span><span class="ss"> devices&quot;</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mesh spans all devices across all processes</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    mesh <span class="op">=</span> jax.make_mesh((jax.device_count(),), (<span class="st">&quot;batch&quot;</span>,))</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    replicated <span class="op">=</span> jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    sharded <span class="op">=</span> jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(<span class="st">&quot;batch&quot;</span>))</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> MLP(key<span class="op">=</span>jax.random.PRNGKey(<span class="dv">0</span>))</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> eqx.filter_shard(model, replicated)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optax.adam(<span class="fl">1e-3</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(eqx.<span class="bu">filter</span>(model, eqx.is_array))</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss_fn(model, x, y):</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.mean((jax.vmap(model)(x) <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">@eqx.filter_jit</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(model, opt_state, x, y):</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> eqx.filter_value_and_grad(loss_fn)(model, x, y)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>            grads, opt_state, eqx.<span class="bu">filter</span>(model, eqx.is_array)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> eqx.apply_updates(model, updates)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model, opt_state, loss</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        key, k1, k2 <span class="op">=</span> jax.random.split(key, <span class="dv">3</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> jax.device_put(jax.random.normal(k1, (<span class="dv">256</span>, <span class="dv">128</span>)), sharded)</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> jax.device_put(jax.random.normal(k2, (<span class="dv">256</span>, <span class="dv">64</span>)), sharded)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        model, opt_state, loss <span class="op">=</span> step(model, opt_state, x, y)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> jax.process_index() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;Step </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span><span class="bu">float</span>(loss)<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> jax.process_index() <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">float</span>(loss)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> main():</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    executor <span class="op">=</span> submitit.SlurmExecutor(folder<span class="op">=</span><span class="st">&quot;logs&quot;</span>)</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>    executor.update_parameters(</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>        account<span class="op">=</span><span class="st">&quot;ACCOUNT&quot;</span>,</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        partition<span class="op">=</span><span class="st">&quot;PARTITION&quot;</span>,</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>        nodes<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        gpus_per_node<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>        ntasks_per_node<span class="op">=</span><span class="dv">2</span>,  <span class="co"># One process per GPU</span></span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>        cpus_per_task<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>    job <span class="op">=</span> executor.submit(worker_fn)</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Submitted: </span><span class="sc">{</span>job<span class="sc">.</span>job_id<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Final loss: </span><span class="sc">{</span>job<span class="sc">.</span>results()[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>    main()</span></code></pre></div>
<p>The <code>jax.distributed.initialize()</code> call reads SLURM
environment variables to coordinate processes, similar to how
<code>TorchDistributedEnvironment</code> works.</p>
<p><strong>Why not single-process multi-GPU?</strong> JAX can run with
<code>ntasks_per_node=1</code> and see all GPUs from one process. This
<em>should</em> work but causes gradient corruption (NaN, exploding
loss) on some clusters due to GPU-to-GPU communication issues. Using one
process per GPU with <code>jax.distributed</code> routes communication
through NCCL properly and is 100% stable.</p>
      <hr />
      <p>[<a href="/links" data-no-instant>Links</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</body>

</html>
