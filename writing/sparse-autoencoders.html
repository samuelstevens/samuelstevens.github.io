<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="Some thoughts on sparse
autoencoders." />
  <meta name="keywords" content="sparse
autoencoders, SAE, SAEs, interpretability, machine learning" />
  <title>Behold: Sparse Autoencoders</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links" data-no-instant>Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<p>When scientists seek to understand a phenomenon, they don&#x2019;t just
observe it&#x2014;they form hypotheses and test them through controlled
experiments. For instance, to understand gravity, we don&#x2019;t just watch
objects fall&#x2014;we deliberately drop different objects, measure their
acceleration, and test our theories by seeing what happens when we
change various factors.</p>
<p>Similarly, to truly understand what a neural network has learned, we
shouldn&#x2019;t just observe its behavior&#x2014;we need to test our hypotheses about
how it works. Traditional interpretability methods like saliency maps
are like pure observation&#x2014;they show us what the model appears to be
looking at, but we can&#x2019;t test if those explanations are correct. It&#x2019;s
like if we could only watch objects fall, but never drop them
ourselves.</p>
<p>Sparse autoencoders (SAEs) offer a unique solution because they let
us both interpret AND test our interpretations. When an SAE suggests
that a model recognizes &#x201C;sand&#x201D; in an image, we can actually test this by
modifying that sand-like feature and seeing how it affects the model&#x2019;s
behavior. If our interpretation is correct, changing the &#x201C;sand&#x201D; feature
should reliably change the model&#x2019;s predictions in semantically
meaningful ways&#x2014;like turning predictions of &#x201C;beach&#x201D; into &#x201C;ocean&#x201D; when we
reduce sand-like features.</p>
<p>This provides a kind of empirical validation that&#x2019;s missing from most
interpretability methods. We&#x2019;re not just making educated guesses about
what the model might be doing; rather, we can actually test our
hypotheses through controlled feature interventions.</p>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
