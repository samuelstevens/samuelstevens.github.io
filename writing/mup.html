<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="Another practical guide on using muP
with transformers." />
  <meta name="keywords" content="maximal update
parameterization, mup, guide, tutorial" />
  <title>Behold: Maximal Update Parameterization</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links" data-no-instant>Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="maximal-update-parameterization-mup-or-&#x3BC;p">Maximal Update
Parameterization (muP or &#x3BC;P)</h1>
<p>Maximal update parameterization (henceforce muP; <a
href="https://arxiv.org/abs/2203.03466">arXiv</a>) is a initialization
and learning rate scheme so you can tune many hyperparameters (but not
all) on much smaller models, then transfer those tuned hyperparameters
to a much larger model with confidence.</p>
<p>I recently read the <a
href="https://blog.eleuther.ai/mutransfer/">joint Eleuther-Cerebras
post</a> on muP and decided I would implement it for some ViT training
runs I had lined up. I had some bumps along the way and decided to
document them for future me and others.</p>
<blockquote>
<p>Note that I am training vision models, with dense inputs (pixels) and
an unchanging number of classes (there is no concept of &#x201C;vocab&#x201D;).</p>
</blockquote>
<p>Given a standard initialization of sampling from <span
class="math inline">\(\mathcal{N}(0, 0.02^2)\)</span>, as transformer
width increases, the mean L1 norm of activations at various points in
the model will increase.</p>
<figure>
<a href="/images/mup/sup.svg"><img src="/images/mup/sup.svg" alt="Standard parameterization leads to increasing activation norms as width increases."></a>
<figcaption aria-hidden="true">
Weights are initialized from <span class="math inline">\(\mathcal{N}(0,
0.02^2)\)</span>; learning rate is constant for all models. Note that
even at timestep 1 (before any optimization), as we increase width, the
activations also increase in norm (except the patch norms, because their
<span class="math inline">\(d_{in}\)</span> doesn&#x2019;t change&#x2013;it&#x2019;s a
function of the image size and the patch size, which is fixed).
</figcaption>
</figure>
<p>We can fix this by sampling from <span
class="math inline">\(\mathcal{N}(0,
\frac{0.02}{\sqrt{m_d}}^2)\)</span>. <span
class="math inline">\(m_d\)</span> is just <span
class="math inline">\(\frac{d_{in}}{d_{in,base}}\)</span>; that is, how
much you scale your model width by from your base model.</p>
<figure>
<a href="/images/mup/mup-init-only.svg"><img src="/images/mup/mup-init-only.svg" alt="muP parameterization leads to constant initial activation norms as width increases."></a>
<figcaption aria-hidden="true">
Weights are initialized from <span class="math inline">\(\mathcal{N}(0,
\frac{0.02}{\sqrt{m_d}}^2)\)</span>; learning rate is constant for all
models. Note that at timestep 1 (before any optimization), as we
increase width, the activations <strong>don&#x2019;t</strong> change norm a
lot.
</figcaption>
</figure>
<p>However, after taking optimizer steps, our activations are no longer
constant with respect to model width. This means we need to tune the
learning rate. For the Adam and AdamW optimizer, muP proposes dividing
your original learning rate <span class="math inline">\(\eta\)</span> by
<span class="math inline">\(m_d\)</span> to get <span
class="math inline">\(\frac{\eta}{m_d}\)</span> (for SGD, muP says no
change is necessary).</p>
<figure>
<a href="/images/mup/mup-init-hidden-lr.svg"><img src="/images/mup/mup-init-hidden-lr.svg" alt="muP parameterization and scaling the learning rate does not fix activations after optimization."></a>
<figcaption aria-hidden="true">
Weights are initialized from <span class="math inline">\(\mathcal{N}(0,
\frac{0.02}{\sqrt{m_d}}^2)\)</span>; learning rate is <span
class="math inline">\(\frac{\eta}{m_d}\)</span>. Note that doesn&#x2019;t fix
the problem! Read on to find out why.
</figcaption>
</figure>
<p>This tripped me up a lot when I first saw it. I had scaled the
learning rate appropriately, and my model wasn&#x2019;t working! Why had this
happened?</p>
<p>My explanation is the patch embedding activations. Because <span
class="math inline">\(d_{in}\)</span> of the patch embedding does not
change as you scale width, the mean norm for the patch activation also
does not change. I think this is why muP suggests <em>not</em> scaling
the learning rate for your embedding layer. If you do not change the
learning rate for your embedding layer, then you get the following
figure.</p>
<figure>
<a href="/images/mup/mup-init-hidden-lr-patch-lr.svg"><img src="/images/mup/mup-init-hidden-lr-patch-lr.svg" alt="muP parameterization and scaling the learning rate for all non-embedding layers fixes activations after optimization."></a>
<figcaption aria-hidden="true">
Weights are initialized from <span class="math inline">\(\mathcal{N}(0,
\frac{0.02}{\sqrt{m_d}}^2)\)</span>; learning rate is <span
class="math inline">\(\frac{\eta}{m_d}\)</span> for all non-embedding
layers. Note that even after several optimization steps, mean activation
norm is constant with respect to model width.
</figcaption>
</figure>
<p>This is pretty good! There are a couple more things to add here:</p>
<ol type="1">
<li>Divide attention logits by <span
class="math inline">\(d_{head}\)</span> instead of <span
class="math inline">\(\sqrt{d_{head}}\)</span> to account for
correlation between <span class="math inline">\(Q\)</span> and <span
class="math inline">\(K\)</span> that emerges later in training. This
doesn&#x2019;t lead to any meaningful difference in the coord check
charts.</li>
<li>Not all hyperparameters transfer using muP. Specifically, weight
decay and dropout do not transfer and need to be tuned for large models.
The proxy (small) model must also be trained with your target model&#x2019;s
batch size, depth (number of layers) and learning rate schedule. See <a
href="https://blog.eleuther.ai/mutransfer/#transferring-optimal-hps-from-a-small-scale-to-a-large-scale">this
section</a> for more details.</li>
</ol>
<h2 id="potential-trip-ups">Potential Trip-Ups</h2>
<ul>
<li>Gradient clipping: with clipping, my muP wasn&#x2019;t working at all. With
muP, you shouldn&#x2019;t need clipping (I think).</li>
<li><span class="math inline">\(d_{in,base}\)</span> should be at least
256 per the original muP paper and Eleuther&#x2019;s reproduction. But I think
128 will be fine and will update this page after trials.</li>
</ul>
<h2 id="other-resources">Other Resources</h2>
<ul>
<li><a href="https://blog.eleuther.ai/mutransfer/">Eleuther
post</a></li>
<li><a href="https://github.com/microsoft/mup">Official muP GitHub
repo</a>; in particular, the explanation of and tips for <a
href="https://github.com/microsoft/mup?tab=readme-ov-file#checking-correctness-of-parametrization">coord
checks</a>.</li>
<li><a
href="https://docs.cerebras.net/en/2.1.1/wsc/how_to_guides/mup_docs.html">Cerebras&#x2019;s
official documentation</a> and <a
href="https://arxiv.org/pdf/2304.03208">their Cerebras-GPT paper</a>;
specifically Appendix G. Note, however, that they divide inits and LRs
by <span class="math inline">\(m_d\)</span> and the Eleuther post
recommends <span class="math inline">\(\sqrt{m_d}\)</span>.</li>
</ul>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer></script>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
