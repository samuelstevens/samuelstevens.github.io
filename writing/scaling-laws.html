<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <title>Behold: </title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="https://github.com/samuelstevens">GitHub</a>]
        [<a href="mailto:samuel.robert.stevens@gmail.com">Email</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="planned-literature-reviews">Planned Literature Reviews</h1>
<h2 id="scaling-laws">Scaling Laws</h2>
<ul>
<li>Scaling Laws vs Model Architectures: How does Inductive Bias
Influence Scaling?</li>
<li>Beyond neural scaling laws: beating power law scaling via data
pruning</li>
<li>Scaling Data-Constrained Language Models</li>
<li>Scaling laws for transfer</li>
<li>Revisiting neural scaling laws in language and vision</li>
<li>Scaling Laws and Interpretability of Learning from Repeated
Data</li>
</ul>
<h2 id="pre-training-data">Pre-Training Data</h2>
<ul>
<li>Doremi: Optimizing data mixtures speeds up language model
pretraining</li>
<li>Scaling Laws and Interpretability of Learning from Repeated
Data.</li>
<li>Beyond neural scaling laws: beating power law scaling via data
pruning</li>
<li>Scaling Data-Constrained Language Models</li>
<li>Scaling Laws and Interpretability of Learning from Repeated
Data</li>
</ul>
<h2 id="curriculum-learning">Curriculum Learning</h2>
<ul>
<li>Provable Advantage of Curriculum Learning on Parity Targets with
Mixed Inputs</li>
</ul>
<h2 id="misc.-paper-reads">Misc. Paper Reads</h2>
<p><a href="">Muennighoff et al.&#xA0;2023</a> (<em>Scaling Data-Constrained
Language Models</em>) propose some new scaling laws based on multiple
epochs on the same data. They think you can get away with training for 4
epochs on your data without any issues. Certainly 2 epochs is fine. So
this will help you better allocate whether, given some data, how large a
model you should train, how many epochs you can/should train it for,
etc.</p>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
