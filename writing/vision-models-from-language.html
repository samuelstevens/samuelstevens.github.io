<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="A summary of recent work on vision
models that learn from natural language supervision." />
  <meta name="keywords" content="CLIP, vision-language models, vision
encoders, computer vision, scaling, laion, open clip" />
  <title>Behold: Learning Vision Representations from Language</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links" data-no-instant>Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="learning-vision-representations-from-language">Learning Vision
Representations from Language</h1>
<p>This page is a literature review, mostly for my own purposes, of how
to train vision models using language supervision, like OpenAI&#x2019;s CLIP
model. I update this page as I have time and energy to read and write
about new papers. If you feel that your work should be mentioned here,
please email me.</p>
<figure>
<a href="https://twitter.com/gabriel_ilharco/status/1717541859093098842"><img src="/images/vision-models-from-language/improved-clip.jpg" alt="Graph showing improved CLIP models in the last several years. Full data at https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv"></a>
<figcaption aria-hidden="true">
Improved CLIP models, chart made by
<a href="https://twitter.com/gabriel_ilharco/status/1717541859093098842">gabrial_ilharco</a>
on Twitter.
</figcaption>
</figure>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#foundational-works">Foundational Works</a></li>
<li><a href="#data">Data</a></li>
<li><a href="#issues">Issues</a></li>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#fine-tuning">Fine-Tuning</a></li>
<li><a href="#domain-specific-models">Domain-Specific Models</a></li>
<li><a href="#contrastive-vs.-captioning">Contrastive
vs.&#xA0;Captioning</a></li>
<li><a href="#further-reading">Further Reading</a></li>
</ul>
<h2 id="foundational-works">Foundational Works</h2>
<ul>
<li>Learning Transferable Visual Models From Natural Language
Supervision (CLIP)</li>
<li>Scaling Up Visual and Vision-Language Representation Learning With
Noisy Text Supervision (ALIGN)* Combined Scaling for Zero-shot Transfer
Learning (BASIC)</li>
</ul>
<h2 id="data">Data</h2>
<ul>
<li>DataComp: In search of the next generation of multimodal
datasets</li>
<li>OBELICS: An Open Web-Scale Filtered Dataset of Interleaved
Image-Text Documents</li>
<li>[Redcaps: Web-curated image-text data created by the people, for the
people]</li>
<li>[WIT: wikipedia-based image text dataset for multimodal multilingual
machine learning]</li>
</ul>
<p><a href="https://arxiv.org/abs/2201.12086">Li et al.&#xA0;2023</a>
(<em>Bootstrapping Language-Image Pre-training for Unified
Vision-Language Understanding and Generation</em>,
<strong>BLIP</strong>)</p>
<p><a href="https://arxiv.org/abs/2202.10261">Pizzi et al.&#xA0;2022</a>
(<em>A Self-Supervised Descriptor for Image Copy Detection</em>,
<strong>SSCD</strong>)</p>
<p><a href="https://arxiv.org/abs/2301.02241">Xu et al.&#xA0;2023</a>
(<em>CiT: Curation in Training for Effective Vision-Language Data</em>)
develop an online data curation loop that picks out image-text pairs
that are most similar to the downstream classnames for whatever
downstream task you want. They use pre-trained vision encoders, so the
main goal is to better align text representations with the image
representations. This doesn&#x2019;t really help with the goal of learning
better vision models from image-text data.</p>
<p><a href="https://arxiv.org/abs/2309.16671">Xu et al.&#xA0;2023</a>
(<em>Demystifying CLIP Data</em>, <a
href="https://github.com/facebookresearch/MetaCLIP#demystifying-clip-data"><strong>MetaCLIP</strong></a>)
develop a method for filtering web-scale data without using an existing
CLIP model (like LAION). They exactly follow OpenAI&#x2019;s CLIP procedure,
which uses pure string matching. This outperforms LAION and OpenAI CLIP
models across ImageNet 0-shot and a wide variety of downstream tasks.
This is the first effort to exactly reproduce OpenAI&#x2019;s WIT-400M without
using existing CLIP models.</p>
<p>Multiple papers propose the use of hard negatives, where they add
either additional images or additional texts to a batch that
deliberately do not match any of the images or text in the batch. This
is supposed to help with fine-grained reasoning. Some examples:</p>
<ul>
<li><a href="https://arxiv.org/abs/2401.07669">FiGCLIP: Fine-Grained
CLIP Adaptation via Densely Annotated Videos</a> creates hard negative
captions by replacing verbs in rule-based captions.</li>
</ul>
<h2 id="issues">Issues</h2>
<p><a href="https://arxiv.org/abs/2210.01936">When and Why
Vision-Language Models Behave Like Bags-of-Words, and What to Do About
It?</a> suggests that</p>
<ul>
<li>Note that (Image Captioners Are Scalable Vision Learners Too) shows
that simply training a language model is enough to do well on ARO, so it
might not be that useful.</li>
</ul>
<h2 id="training">Training</h2>
<p><a href="https://arxiv.org/abs/2205.01917">Yu et al.&#xA0;2022</a>
(<em>CoCa: Contrastive Captioners are Image-Text Foundation Models</em>)
train a combination of unimodal vision and text transformers followed by
a multimodal text decoder with a combination of contrastive loss between
unimodal representations and a language modeling loss on the multimodal
text decoder. See the screenshot below for details.</p>
<figure>
<a href="/images/vision-models-from-language/CoCa.png"><img src="/images/vision-models-from-language/CoCa.png" alt="Screenshot from CoCa: Contrastive Captioners are Image-Text Foundation Models"></a>
<figcaption aria-hidden="true">
Screenshot from Yu et al.&#xA0;2022. Click to see full-size.
</figcaption>
</figure>
<p><a href="http://arxiv.org/abs/2306.07915">Tschannen et al.&#xA0;2023</a>
(<em>Image Captioners Are Scalable Vision Learners Too</em>) show that
vision-encoder+ language-decoder transformer models that predict
captions from images produce high-quality vision models. They add a
parallel decoding trick, where the decoder must predict all caption
tokens from just the image representations, similar to BERT predicting
all language tokens instead of causal masking. They show that captioning
is better than contrastive learning (1) when word order matters (ARO
benchmark, see above) (2) in visual question answering (VQAv2 benchmark)
and (3) on fine-grained classification tasks (improves on Flowers, Cars,
Birds, Pets and Dogs). However, they note that the text decoder is
fairly useless; their approach mostly leads to a strong vision model
that should be used on its own.</p>
<p><a href="https://arxiv.org/abs/2212.00794">Li et al.&#xA0;2023</a>
(<em>Scaling Language-Image Pre-training via Masking</em>,
<strong>FLIP</strong>) propose dropping image patches from the input
before the vision encoder sees them. This dramatically speeds up
training because the sequence length is shorter (50% or even 25% of the
original length) and doesn&#x2019;t reduce encoder quality; it&#x2019;s basically a
free win.</p>
<p>They explore data scaling by moving from LAION-400M to LAION-2B, but
keeping the total number of samples seen during training the same. They
find that &#x201C;for the tasks studied here, <em>data scaling</em> is in
general favored for <em>zero-shot transfer</em>, while <em>model
scaling</em> is in general favored for <em>transfer learning</em>&#x201D;
(emphasis mine). See the screenshot below for more details.</p>
<figure>
<a href="/images/vision-models-from-language/FLIP.png"><img src="/images/vision-models-from-language/FLIP.png" alt="Screenshot from Li et al. 2023"></a>
<figcaption aria-hidden="true">
Screenshot from Li et al.&#xA0;2023. Click to see full-size. If you have
compute to spend, make your model larger (a). If you do not have GPU
compute to spend, get a larger dataset (b). Note that training for more
epochs does not lead to improved performance, despite using more GPU
compute (c).
</figcaption>
</figure>
<p><a href="https://arxiv.org/abs/2303.15389">Sun et al.&#xA0;2023</a>
(<em>EVA-CLIP: Improved Training Techniques for CLIP at Scale</em>)</p>
<h2 id="fine-tuning">Fine-Tuning</h2>
<p><a href="https://arxiv.org/abs/2307.11315">Lewis et al.&#xA0;2023</a>
(<em>Generating Image-Specific Text for Fine-grained Object
Classification</em>, <strong>GIST</strong>) use domain-specific prompts
with GPT-4 to generate fine-grained class-level descriptions. They then
match the descriptions with images using CLIP to a small subset of the
class-level descriptions. Finally, they use GPT-4 to summarize the now
<em>image</em>-level descriptions. Finally, they fine-tune CLIP models
on the new image, text dataset. This method produces fine-grained
image-level texts for arbitrary labeled datasets.</p>
<h2 id="domain-specific-models">Domain-Specific Models</h2>
<p><a href="https://arxiv.org/abs/2308.15670">Christensen et
al.&#xA0;2023</a> (<em>Multimodal Foundation Models For Echocardiogram
Interpretation</em>, <strong>EchoCLIP</strong>) train a CLIP model on 1M
cardiac ultrasound videos and the expert interpretations from 224K
studies across nearly 100K patients. EchoCLIP uses a ConvNeXt image
encoder. The authors use EchoCLIP to measure the similarity between two
echocardiograms and classify whether two echocardiograms are from the
same patient (challenging for human physicians). They use a
regular-expression-based tokenizer to enable more efficient tokenizing,
reducing the average number of text tokens from 530.3 to only 63.8.</p>
<p><a href="http://arxiv.org/abs/2303.00915">Zhang et al.&#xA0;2023</a>
(<em>Large-Scale Domain-Specific Pretraining for Biomedical
Vision-Language Processing</em>, <strong>BiomedCLIP</strong>) train a
CLIP model on 15M figure-caption pairs gathered from PubMed Central
articles. The authors improve upon the base CLIP architecture with a
domain-specific pre-trained text encoder (PubMedBert, <a href="TODO">Gu
et al.&#xA0;2021</a>), a longer context (256 tokens instead of 77) and larger
images with patch dropout (448x488, 50% patch dropout until the last
epoch) leads to 2 points improvement on txt2img and img2txt
recall@1.</p>
<p>Some interesting decisions and findings:</p>
<ol type="1">
<li>They re-initialize the vision transformer rather than start with a
pre-trained ViT-B/16 model (Table 6)</li>
<li>224x224 pretraining does better than 448x448 on 4/5 zero-shot
classification tasks. There&#x2019;s a significant drop in accuracy for the two
LC2500 tasks: 72.17 to 47.96 and 94.65 to 70.66.</li>
</ol>
<p><a href="https://arxiv.org/abs/2307.12914">Lu et al.&#xA0;2023</a>
(<em>Towards a Visual-Language Foundation Model for Computational
Pathology</em>, <strong>CONCH</strong>) train on 1.17M image-caption
pairs from educational sources and PubMed&#x2019;s Open Access dataset. They
tuned and used an object detection model, a language model and an
image-text matching model to gather 1.79M pairs which were then filtered
to 1.17M to create the CONCH dataset. They perform unimodal
self-supervised pre-training on large-scale domain-specific unimodal
data, then do multimodal pretraining on their CONCH dataset.<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a></p>
<p>CONCH-trained models do better than PLIP, BiomedCLIP and OpenAI&#x2019;s
CLIP.</p>
<h2 id="contrastive-vs.-captioning">Contrastive vs.&#xA0;Captioning</h2>
<p>An interesting split has emerged in learning vision models from
language: should you use a contrastive objective, where the fundamental
goal is image-text matching, or a captioning objective, where the
fundamental goal is <em>generating</em> the text for a given image? Both
appear to be quite scalable and lead to strong results. What are the
tradeoffs?</p>
<ul>
<li><a href="http://arxiv.org/abs/2303.17376">A Study of Autoregressive
Decoders for Multi-Tasking in Computer Vision</a></li>
<li><a href="http://arxiv.org/abs/2403.19596">LocCa: Visual Pretraining
with Location-aware Captioners</a></li>
<li><a href="http://arxiv.org/abs/2210.03347">Pix2Struct: Screenshot
Parsing as Pretraining for Visual Language Understanding</a></li>
<li><a href="http://arxiv.org/abs/2306.07915">Image Captioners Are
Scalable Vision Learners Too</a></li>
<li><a href="http://arxiv.org/abs/2205.01917">CoCa: Contrastive
Captioners are Image-Text Foundation Models</a></li>
</ul>
<h2 id="further-reading">Further Reading</h2>
<ul>
<li><a href="https://arxiv.org/abs/2012.06946">MiniVLM: A Smaller and
Faster Vision-Language Model</a> was pre-CLIP, so I haven&#x2019;t read it
yet.</li>
<li><a href="https://arxiv.org/abs/2205.14100">GIT: A Generative
Image-to-text Transformer for Vision and Language</a></li>
<li>[Reproducible scaling laws for contrastive language-image
learning]</li>
<li>[CLIPPO: Image-and-language understanding from pixels only]</li>
<li>[Language in a bottle: Language model guided concept bottlenecks for
interpretable image classification]</li>
<li>[Prompt, generate, then cache: Cascade of foundation models makes
strong few-shot learners]</li>
<li>[Finetune like you pretrain: Improved finetuning of zero-shot vision
models]</li>
<li>[Sus-x: Training-free name-only transfer of vision-language
models]</li>
<li>[Multimodality helps unimodality: Cross-modal few-shot learning with
multimodal models]</li>
<li>[Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as
Prompts]</li>
<li><a href="https://arxiv.org/abs/2112.03857">Grounded Language-Image
Pre-training</a></li>
<li><a href="https://arxiv.org/abs/2306.11207">Ikezogwo et al.&#xA0;2023</a>
(<em>Quilt-1M: One Million Image-Text Pairs for Histopathology</em>,
<strong>QUILT</strong>)</li>
</ul>
<h2 id="appendix-vision-inputs-for-language-models">Appendix: Vision
Inputs for Language Models</h2>
<p>Recently vision-language model has implied that you have a language
model that incorporates vision inputs, like GPT-4 or Flamingo. I&#x2019;m more
interested in training high-quality vision models through image-caption
supervision. Other noteworthy works on this topic:</p>
<ul>
<li><a href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping
Language-Image Pre-training with Frozen Image Encoders and Large
Language Models</a></li>
<li><a href="https://arxiv.org/abs/2308.12966">Qwen-VL: A Frontier Large
Vision-Language Model with Versatile Abilities</a></li>
</ul>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>They train a 24-layer GPT-style causal language model,
then use the first 12 layers for their unimodal text encoder and the
last 12 layers for their multimodal decoder. They cite <a
href="https://arxiv.org/abs/2306.07831">Lu et al.&#xA0;2023</a> for this
idea.<a href="#fnref1" class="footnote-back"
role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</section>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
