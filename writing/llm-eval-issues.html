<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <title>Behold: </title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me3.jpg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links" data-no-instant>Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="llm-eval-issues">LLM Eval Issues</h1>
<p>LLM evaluation is broken. I&#x2019;ve been thinking about it a lot recently.
If you look at the Llama 3.1 paper, they evaluate their models on
benchmarks like MMLU, HumanEval, GSM8K, MATH, and GPQA and compare to
closed models like GPT-4o and Claude Sonnet-3.5. But it&#x2019;s not completely
clear that progress on these benchmarks leads to meaningfully &#x201C;better&#x201D;
models.</p>
<p>So we have benchmarks like LMSYS&#x2019;s Chatbot Arena, which is a live
elo-based leaderboard that is challenging to overfit. But even now,
large pre-training efforts are rumored to be</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
fast takeoff isn't bottlenecked on LLMs being good at math, it's
bottlenecked on being good at math being useful to improve LLMs.
</p>
&#x2014; theseriousadult (<span class="citation"
data-cites="gallabytes">(<strong>gallabytes?</strong>)</span>)
<a href="https://twitter.com/gallabytes/status/1816966565784608866?ref_src=twsrc%5Etfw">July
26, 2024</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
I always complain about the lack of realistic, hard, and uncontaminated
NLP tasks.<br><br>Despite all the hype, we don't know how to make
progress in ML w/o guided hill climbing.<br><br>Rose presents three
extremely rich NLP-for-Education benchmarks in thread&#x2935;&#xFE0F;. DSPy programs
on them soon?
<a href="https://t.co/NZlPY1uZWg">https://t.co/NZlPY1uZWg</a>
</p>
&#x2014; Omar Khattab (<span class="citation"
data-cites="lateinteraction">(<strong>lateinteraction?</strong>)</span>)
<a href="https://twitter.com/lateinteraction/status/1816136048344457675?ref_src=twsrc%5Etfw">July
24, 2024</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
