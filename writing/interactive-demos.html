<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="Some ideas/guidance on how to write
and release interactive demos for research papers." />
  <meta name="keywords" content="interactive, web app, demo, research
demo, guide, tutorial" />
  <title>Behold: Interactive Demos</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me3.jpg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links" data-no-instant>Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="interactive-demos">Interactive Demos</h1>
<p>I love interactive web-based demos for explaining new ideas. I made
<a href="/research/encryption">some for my work applying LLMs to
cryptography</a>, <a
href="https://huggingface.co/spaces/imageomics/bioclip-demo">BioCLIP has
a HuggingFace demo</a> and my (recent, at the time of writing) <a
href="https://osu-nlp-group.github.io/saev/#demos">work on sparse
autoencoders for vision has a bunch of demos</a>.</p>
<p>Beyond demos as a form of scientific communication, I build a lot of
interactive webapps for my own research, to convince myself of ideas or
to provide intuition to collaborators. I wanted to share my process for
developing interactive webapps, with links to specific technologies
based on how much effort you want to invest, how much control over the
experience you want and how much public compute you have. Hopefully this
will help you choose and implement the right type of interactive demo
based on your specific research needs and technical constraints.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ol type="1">
<li><a href="#when-to-build-a-demo">When to Build a Demo</a></li>
<li><a href="#marimo">Marimo</a></li>
<li><a href="#gradio">Gradio</a></li>
<li><a href="#javascript">JavaScript</a></li>
<li><a href="#summary">Summary</a></li>
</ol>
<h2 id="when-to-build-a-demo"><a href="#when-to-build-a-demo">When to
Build a Demo</a></h2>
<p>Not every research project needs a demo. If you are developing a new
optimization algorithm whose claim to fame is needing only half the
optimization steps compared to Adam, a demo is not needed. Or if you
have a novel attention variant that reduces memory costs by 20%, don&#x2019;t
bother with a demo.</p>
<p>Demos, to me, are useful when you introduce a qualitative shift that
is not well-captured by existing quantitative measures. What are
examples of this? Consider these three scenarios where interactive demos
can be particularly impactful:</p>
<p><strong>Novel capabilities</strong> like zero-shot species
classification (like BioCLIP), few-shot prompting (like GPT-3) zero-shot
semantic segmentation (like SAM and SAMv2) or manipulating internal
representations (like sparse autoencoders) often require a demo to
properly express just how amazing the capability is. While static
examples in your hook figure can make a big difference, giving potential
users the chance to play with your new system on their own without any
barrier to entry (downloading code, setting up an environment) can
completely change their perspective.</p>
<blockquote>
<p>This was especially apparent to me while doing field work in Hawaii.
Watching biologists use the BioCLIP and SAMv2 demos to iteratively
develop their understanding of model capabilities in real time and then
using that understanding to shape their study design was phenomenal. I
can only imagine how different their research would have looked without
a working BioCLIP demo. This was a highly impactful experience for me
that shifted the way I think about online demos.</p>
</blockquote>
<p><strong>Qualitative shifts</strong> in quantitative metrics like
inference speed or data efficiency often benefit from a demo, where
possible, to give potential users a concrete feel for how dramatically
improved your method is. For example, Mistral&#x2019;s recent <a
href="https://chat.mistral.ai/chat">Le Chat</a> is <a
href="https://cerebras.ai/blog/mistral-le-chat">powered by Cerebras
chips</a> and gets over 1.1K tokens/sec, which is 10x faster than other
assistants. It&#x2019;s shocking to see how quickly Mistral can write code and
responses and really must be experienced to truly understand.</p>
<blockquote>
<p>See my <a href="writing/personal-software#performance">notes on
performance leading to qualitative shifts in workflows</a> for a litte
discussion on this.</p>
</blockquote>
<p><strong>Hard-to-understand ideas</strong> like <a
href="/research/encryption#symmetric-encryption">symmetric
encryption</a> or <a
href="https://osu-nlp-group.github.io/saev/demos/classification?example=680">manipulating
ViT internal representations</a> can benefit from demos to help
potential users <em>build intuition around your new, poorly-understood
idea</em>. This is often why I build demos, even for myself. I want to
move things around with my hands and see how the model responds. My
internal dialogue normally goes &#x201C;What if I do this?&#x201D; followed by &#x201C;What
if I do that?&#x201D; followed by &#x201C;Ohhhhhh.&#x201D; Demos can help your users do the
same thing (I think).</p>
<h2 id="marimo"><a href="#marimo">Marimo</a></h2>
<p><a href="https://marimo.io/">Marimo</a> is a modern take on Jupyter
notebooks that are <em>reactive</em>: whenever you change a cell, all
other cells that depend on that cell&#x2019;s values automatically update. This
enables you to build webapps really quickly without too much effort. I
used this to explore <a
href="https://github.com/OSU-NLP-Group/saev/blob/298cabdb6b771c76b402d0fdddab6907d1941d7a/saev/interactive/features.py">individual
features in sparse autoencoders</a>. Below you can see a screenshot of
the Marimo editor and what the dashboard looks like while I&#x2019;m using
it.</p>
<figure>
<img src="/images/interactive-demos/features-code.webp" alt="">
<figcaption aria-hidden="true">
The Marimo web-based editor. This is how I develop a dashboard. You can
see individual cells and the outputs, like the &#x201C;Checkpoint&#x201D; dropdown.
</figcaption>
</figure>
<figure>
<img src="/images/interactive-demos/features-ui.webp" alt="">
<figcaption aria-hidden="true">
What the dashboard looks like when the code is hidden. Once the
dashboard is mostly complete, this is the UI I use for actually browsing
SAE features.
</figcaption>
</figure>
<p>One of the downsides of Marimo is that it needs to run Python on a
computer somewhere. This is not great if you want to share a particular
notebook with someone else to use themselves and your notebook uses a
lot of hard-to-install dependencies (like PyTorch) or it needs a lot of
compute like a GPU or a huge hard drive.</p>
<p>This can be mitigated by being able to <a
href="https://docs.marimo.io/guides/publishing/playground/">permalink to
online notebooks</a> but this limits you to only WebAssembly-compatible
packages, and no disk space. I use this technique to make an <a
href="https://samuelstevens.me/biobench/">interactive leaderboard</a>
for <a href="https://github.com/samuelstevens/biobench">BioBench</a>.
You can see the leaderboard code in the leaderboard itself. It just
grabs the <a
href="https://github.com/samuelstevens/biobench/blob/main/reports/results.csv">latest
<code>results.csv</code></a> and shows it as a Pandas dataframe.</p>
<figure>
<a href="https://samuelstevens.me/biobench"><img src="/images/interactive-demos/biobench-leaderboard.webp" alt=""></a>
<figcaption aria-hidden="true">
The interactive leaderboard for BioBench using an embedded web-only
Marimo notebook. This could also be accomplished with JavaScript, but I
prefer writing Python. JavaScript is probably more sustainable in the
long term in case Marimo stops working
(<a href="https://en.wikipedia.org/wiki/Software_rot">software rot</a>).
</figcaption>
</figure>
<h2 id="gradio"><a href="#gradio">Gradio</a></h2>
<p>If you want to (1) run real Python code and (2) share it with someone
and (3) not ask them to install Python, then <a
href="https://www.gradio.app/">Gradio</a> via <a
href="https://huggingface.co/spaces">HuggingFace Spaces</a> is probably
the easiest way to do it. You write a Gradio app using the Gradio Python
library. You can install any Python package and run any Python code that
you write. Then, HuggingFace will host this app for free on a little 1
vCPU VM.</p>
<p>The downsides of this approach are that your app has to fit within
the Gradio library. This can be fine for a lot of ML demos: specify some
combination of text, image and audio input, run it through a deep neural
network, then get some text, image and audio output back. The <a
href="https://huggingface.co/spaces/imageomics/bioclip-demo">BioCLIP
demo</a> is exactly that: give it an image and a list of possible
classes, get a probability distribution over those classes back. All for
free, from any device.</p>
<figure>
<a href="https://huggingface.co/spaces/imageomics/bioclip-demo"><img src="/images/interactive-demos/bioclip-demo.webp" alt=""></a>
<figcaption aria-hidden="true">
The BioCLIP demo as a Gradio app hosted on HuggingFace Spaces. For many
demos, this is perfect, and frees you from having to think about
responsive design, CSS styling, etc.
</figcaption>
</figure>
<h2 id="javascript"><a href="#javascript">JavaScript</a></h2>
<p>If you want to build truly custom web-based interactive
<em>~experiences~</em> then you probably need to write some JavaScript
yourself. If you have not written JavaScript-based frontends before,
this is not necessarily a trivial task. JavaScript mostly sucks and
there are various efforts to make it better, but it mostly sucks.</p>
<p>I use <a href="https://elm-lang.org/">Elm</a> for many web-based
demos (<a
href="https://github.com/OSU-NLP-Group/saev/blob/298cabdb6b771c76b402d0fdddab6907d1941d7a/web/src/Classification.elm">[1]</a>,
<a
href="https://github.com/OSU-NLP-Group/saev/blob/298cabdb6b771c76b402d0fdddab6907d1941d7a/web/src/Comparison.elm">[2]</a>,
<a
href="https://github.com/OSU-NLP-Group/saev/blob/298cabdb6b771c76b402d0fdddab6907d1941d7a/web/src/Comparison.elm">[3]</a>),
but have also written my fair share of vanilla JavaScript (<a
href="https://github.com/samuelstevens/samuelstevens.github.io/blob/d641388ee598d98b5a8c68625491418bc53e1d23/research/encryption/js/indcpa-part1.js">[1]</a>,
<a
href="https://github.com/samuelstevens/samuelstevens.github.io/blob/d641388ee598d98b5a8c68625491418bc53e1d23/research/encryption/js/indcpa-part2.js">[2]</a>,
<a
href="https://github.com/samuelstevens/samuelstevens.github.io/blob/d641388ee598d98b5a8c68625491418bc53e1d23/research/encryption/js/plots.js">[3]</a>).</p>
<p>I do not want to get into &#x201C;how to do web dev&#x201D; because there are
smarter people than me out there for that. The trick I want to describe
in this post is how to use Gradio with HuggingFace Spaces as a free
Python backend API server.</p>
<p>Basically, Gradio apps are queryable via HTTP. There is a <a
href="https://www.gradio.app/guides/getting-started-with-the-js-client">JavaScript
package available with docs</a> but I wrote a custom Elm package that
mimics the functionality using <a
href="https://www.gradio.app/guides/querying-gradio-apps-with-curl">raw
HTTP requests</a>. It&#x2019;s not a great strategy, but you can create a
Python-based backend using the free hardware and hosting provided by
HuggingFace Spaces, then host your HTML+CSS+JS app using GitHub Pages,
and you have an entire stack hosted for free! This is great if you&#x2019;re a
grad student.</p>
<h2 id="summary">Summary</h2>
<table>
<thead>
<tr>
<th></th>
<th>Marimo</th>
<th>Gradio</th>
<th>JavaScript</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Setup Difficulty</strong></td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr>
<td><strong>Customizability</strong></td>
<td>Medium</td>
<td>Low-Medium</td>
<td>High</td>
</tr>
<tr>
<td><strong>Requirements</strong></td>
<td>Python environment, or limited web-only version</td>
<td>Free on HuggingFace Spaces (1 vCPU)</td>
<td>Static hosting (GitHub Pages) + optional backend</td>
</tr>
<tr>
<td><strong>Language</strong></td>
<td>Python</td>
<td>Python</td>
<td>JavaScript/web development</td>
</tr>
<tr>
<td><strong>Best For</strong></td>
<td>Quick prototyping, personal research tools, reactive notebooks</td>
<td>ML model demos with standard input/output patterns</td>
<td>Custom interactive experiences with complete design freedom</td>
</tr>
<tr>
<td><strong>Limitations</strong></td>
<td>Web version limited to WebAssembly-compatible packages</td>
<td>Must fit within Gradio&#x2019;s component system</td>
<td>Requires web development skills, potentially complex setup</td>
</tr>
<tr>
<td><strong>Use with ML Models</strong></td>
<td>Direct Python integration</td>
<td>Built specifically for ML demos</td>
<td>Requires API to connect to models</td>
</tr>
<tr>
<td><strong>Example Use Case</strong></td>
<td>Interactive data exploration, feature visualization</td>
<td>Image classification demo, text generation UI</td>
<td>Complex multi-step visualization, custom interactions</td>
</tr>
<tr>
<td><strong>Maintenance</strong></td>
<td>Dependent on Marimo&#x2019;s development</td>
<td>Dependent on Gradio and HuggingFace</td>
<td>More stable long-term with standard web technologies</td>
</tr>
</tbody>
</table>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
