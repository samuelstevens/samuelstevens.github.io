<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <meta name="description" content="How do we get better data for
machine learning models?" />
  <meta name="keywords" content="machine learning, data quality" />
  <title>Data Quality</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me3.jpg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Sam Stevens</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/research">Research</a>]
        [<a href="/contact">Contact</a>]
        [<a href="/cv.pdf">CV</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="data-quality">Data Quality</h1>
<p>Broadly, I am interested in choosing better data for large-scale
machine learning models. There are lots of heuristics for choosing
better data, and even more modality-specific heuristics. But there are
very few general techniques for choosing better data.</p>
<p>Here are some notes and thoughts from other papers that emphasize
data quality.</p>
<p>Lillian Weng (of course) has a <a
href="https://lilianweng.github.io/posts/2024-02-05-human-data-quality/#data-quality--model-training">great,
high-quality summary</a> of influential papers in this area.</p>
<h2 id="reading-list">Reading List</h2>
<ul>
<li>Studying Large Language Model Generalization with Influence
Functions</li>
<li>Understanding Influence Functions and Datamodels via Harmonic
Analysis</li>
<li>Understanding Black-box Predictions via Influence Functions</li>
<li>Dataset Cartography: Mapping and Diagnosing Datasets with Training
Dynamics</li>
<li>An Empirical Study of Example Forgetting during Deep Neural Network
Learning</li>
<li>Identifying Mislabeled Data using the Area Under the Margin
Ranking</li>
</ul>
<h2 id="thoughts">Thoughts</h2>
<p>Data influence functions enable us to see how a single training data
point influences predictions, cheaply. We basically can simulate
removing a single example from a pre-trained model&#x2019;s training set, then
re-evaluate the model on whatever benchmark we like.</p>
<ul>
<li>Does removing just one example make a meaningful difference on
benchmarks? If not, are influence functions good approximations of
removing multiple examples?</li>
<li>Broadly, influence functions should help us find examples that are
high quality or low quality. But what&#x2019;s the difference between using an
influence function and just measuring high and low loss examples?</li>
</ul>
<h2 id="dinov2">DINOv2</h2>
<ul>
<li>They take some curated datasets, then pick out images that are
similar to those curated images, using some measure of &#x201C;similarity&#x201D;</li>
</ul>
<ol type="1">
<li>Pre-train ViT-H/16 on IN21K</li>
<li>Embed all images</li>
<li>k-means clustering using cosine similarity</li>
<li>For a curated image, retrieve N similar images</li>
</ol>
<p>As a data ablation, they try three different pre-training
datasets:</p>
<ol type="1">
<li>LVD-142M (above)</li>
<li>IN21K</li>
<li>142M randomly sampled images from the 1.2B uncurated images</li>
</ol>
<p>Note that all three of these are unsupervised! There is no notion of
label quality here.</p>
<p>They have details on their pipeline, but few design ablations.</p>
<h2 id="data-filtering-networks-dfn">Data Filtering Networks (DFN)</h2>
<ul>
<li>If you train a small CLIP on small-scale, really-high quality data,
you can use that to filter a larger uncurated image-text dataset and
make a really good image-text dataset</li>
<li>but they use 350M high-quality image-text pairs to train their DFN.
that&#x2019;s almost 400M, which openai used for WIT. why not just train an
image-text model directly on the 350M high-quality pairs?</li>
</ul>
<h2
id="datamodels-predicting-predictions-from-training-data">Datamodels:
Predicting Predictions from Training Data</h2>
      <hr />
      <p>[<a href="/links" data-no-instant>Links</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
