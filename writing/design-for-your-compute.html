<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <meta name="description" content="Design your AI systems for your
current level of compute." />
  <meta name="keywords" content="machine
learning, efficiency, compute, hpc, ml, ai, throughput, performance" />
  <title>Behold: Design for Your Compute.</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me3.jpg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links" data-no-instant>Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="design-for-your-compute">Design for Your Compute</h1>
<p>I would prefer to live in a world where Nvidia sends me, a grad
student, a single GPU with 1TB of VRAM and infinite memory bandwidth.
Then I could just write PyTorch code like they do in the tutorials<a
href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> and it would all work out.
Unfortunately, that&#x2019;s not true. My <a
href="https://x.com/osunlp">lab</a> has some 8xA6000 servers with a
shared NFS drive (slow) and each server has its own NVMe drive (fast).
We also do experiments on <a href="https://www.osc.edu/">OSC</a>, which
is a HPC cluster that uses Slurm, has shared NFS drives and fast
node-specific drives. At Meta, we had a <em>ton</em> of V100s (kind of
old) available on a Slurm cluster with great disk read speeds.</p>
<p>Ideally, your experimental codebase is compute-agnostic. Your
experiments would equal MFU on any system without any code changes. I
tried this for several years in my PhD.</p>
<p><strong>I have abandonded this goal: I design experiments for
specific compute.</strong> It&#x2019;s simply too challenging to get good MFU
on all these different systems. I realized I didn&#x2019;t have to feel bad
about this after seeing other, much better engineers run into the same
problem.</p>
<p>Here are a couple examples.</p>
<h2 id="tokasaurus"><a
href="https://scalingintelligence.stanford.edu/blogs/tokasaurus/">Tokasaurus</a></h2>
<p>Tokasaurus is a high-throughput LLM serving engine. It&#x2019;s explicitly
designed for systems without NVLink:</p>
<blockquote>
<p><strong>One of our original goals with Tokasaurus was to efficiently
run multi-GPU inference on our lab&#x2019;s L40S GPUs, which don&#x2019;t have fast
inter-GPU NVLink connections.</strong> Without NVLink, the communication
costs incurred running TP across a node of eight GPUs are substantial.
Therefore, efficient support for PP (which requires much less inter-GPU
communication) was a high priority. (<em>emphasis mine</em>)</p>
</blockquote>
<p>This serving engine is not relaly useful for huge models like
DeepSeekv3, or massive datacenters that prioritize high-bandwidth
interconnect.</p>
<h2 id="deepseek">DeepSeek</h2>
<p>https://www.dwarkesh.com/p/sholto-trenton-2</p>
<p>On Dwarkesh Patel&#x2019;s second podcast with Sholto Douglas and Trenton
Bricken, Sholto explains why he appreciates DeepSeek&#x2019;s algorithmic
innovations:</p>
<blockquote>
<p>This is manifested in the way that the models give this sense of
being perfectly designed up to their constraints. You can really very
clearly see what constraints they&#x2019;re thinking about as they&#x2019;re
iteratively solving these problems. Let&#x2019;s take the base Transformer and
diff that to DeepSeek v2 and v3. You can see them running up against the
memory bandwidth bottleneck in attention.</p>
</blockquote>
<blockquote>
<p>Initially they do MLA to do this, <strong>they trade flops for memory
bandwidth basically</strong>. Then they do this thing called NSA, where
<strong>they more selectively load memory bandwidth</strong>. You can
see this is because <strong>the model that they trained with MLA was on
H800s, so it has a lot of flops</strong>. So they were like, &#x201C;Okay, we
can freely use the flops.&#x201D; But then the export controls from Biden came
in, or they knew they would have less of those chips going forward, and
so they traded off to a more memory bandwidth-oriented algorithmic
solution there.</p>
</blockquote>
<blockquote>
<p>You see a similar thing with their approach to sparsity, where
they&#x2019;re iteratively working out the best way to do this over multiple
papers. (<em>again, emphasis mine</em>)</p>
</blockquote>
<p>DeepSeek, the compnay, makes algorithm trade-offs based on the
compute available for every model.</p>
<h2 id="my-work">My Work</h2>
<h3 id="saes-for-vision">SAEs for Vision</h3>
<p>I train sparse autoencoders for vision models (<a
href="https://osu-nlp-group.github.io/saev/">project website</a>, <a
href="https://github.com/osu-nlp-group/saev">code</a>, <a
href="https://arxiv.org/abs/2502.06755">preprint</a>) and the very first
design decision was to save activations to disk before training the SAE
rather than computing them on the fly. This is inherently more complex
but it intuitively made sense to me because I expected to spend a lot of
time training SAEs rather than saving activations <em>AND</em> my lab
servers had sufficient storage with good random-access read speed
available, so caching activtions made sense.</p>
<h2 id="reflections">Reflections</h2>
<p>I had a deep desire for simplicity over anything else. I still do;
additional algorithmic complexity must lead to dramatic improvement to
justify its complexity. But I am much more appreciative of complexity to
deal with compute restraints. &#x201C;Scale is king&#x201D; is a guiding mantra for
me: simple learning algorithms with few inductive biases are appealing
to me. The somewhat obvious corollary of &#x201C;scale is king&#x201D; is that
complexity is worth it if it enables bigger models. MLA is more complex
than pure self-attention but enabled much larger model training.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This is what the tutorials tell you to do, which is
fine, until you want to actually do something besides ResNet on an
A100.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyNetwork().to(device)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> imgs, tgts <span class="kw">in</span> dataloader:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> imgs.to(device)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    tgts <span class="op">=</span> tgts.to(device)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> model(imgs, tgts)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    loss.backwards()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
<a href="#fnref1" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></li>
</ol>
</section>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</body>

</html>
