<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="A small literature review on
self-improving language models." />
  <meta name="keywords" content="llm, llms, recursive, self-improvement" />
  <title>Behold: Self-Improving Language Models</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="https://github.com/samuelstevens">GitHub</a>]
        [<a href="mailto:samuel.robert.stevens@gmail.com">Email</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="self-improving-language-models">Self Improving Language
Models</h1>
<p>Large language models like Llama can seemingly improve by training on
data they generate. This is initially surprising because there is no new
data added to the model. It&#x2019;s less surprising if you think about it as a
way to push certain capabilities to the front of the model.</p>
<p><strong>Why is this interesting?</strong> Because it means we might
not need as much human-labeled data as we previously thought, if we can
reliably extract data from the pre-trained model.
<!-- Despite jokes about OpenAI just training bigger models on more data, OpenAI clearly has worked really hard on data quality. --></p>
<h2 id="instruction-following">Instruction Following</h2>
<p><strong>Humpback</strong>(<a
href="https://arxiv.org/abs/2308.06259">Li et al.&#xA0;2023</a>) start by
tuning two Llama models (not Llama 2) on two small datsets:</p>
<ol type="1">
<li>The forward model <span class="math inline">\(M_0\)</span> is tuned
on (instruction, output) pairs.</li>
<li>The backward model <span class="math inline">\(M_{yx}\)</span> is
tuned on (output, instruction) pairs.</li>
</ol>
<p>Both models are tuned on the same 3200 examples from the Open
Assistant dataset (<a href="https://arxiv.org/abs/2304.07327">K&#xF6;pt et
al.&#xA0;2023</a>) with the highest possible rating.<a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>With a large, unlabeled set of ouputs (scraped from web pages), they
sample instructions from the backward model <span
class="math inline">\(M_{yx}\)</span>. This produces a large dataset of
(instruction, output) pairs where the outputs are from scraped web data
(not necessarily seen in Llama&#x2019;s training) and the inputs are sampled
from a fine-tuned Llama model.</p>
<p>The authors then use the forward mode <span
class="math inline">\(M_0\)</span> to automatically score the
(instruction, output) pairs from 1 to 5. Since <span
class="math inline">\(M_0\)</span> is trained on a small, but
high-quality set of instructions, this is feasible. Then they tune <span
class="math inline">\(M_0\)</span> on the set of (instruction, output)
pairs with score 5 to produce <span class="math inline">\(M_1\)</span>.
Finally, they repeat.</p>
<p>This works really well and leads to better instruction following
<em>than any other open-source method that doesn&#x2019;t use GPT-4</em>
(measured by the win-rate over text-davinci-003 evaluated by GPT-4).</p>
<h2 id="other">Other</h2>
<p><strong>ReST</strong> (<a
href="https://arxiv.org/abs/2308.08998">Gulcehre et al.&#xA0;2023</a>)
iteratively grows a dataset of (input, output) pairs by sampling outputs
from an unlabeled dataset of inputs. These new samples are scored by a
reward model, and only the high-quality (input, output) pairs are used
to fine-tune the model, either with supervised loss or an offline RL
loss. However, ReST is only evaluated on a machine translation context:
I assume this means that they did not get good results for
instruction-following, since that&#x2019;s a much hotter topic than machine
translation right now.</p>
<figure>
<img src="/images/self-improving-llms/ReST.png"
alt="ReST samples new data from a model in the Grow step and iteratively tunes the model in the Improve step on the highest quality sampled data." />
<figcaption aria-hidden="true">ReST samples new data from a model in the
<code>Grow</code> step and iteratively tunes the model in the
<code>Improve</code> step on the highest quality sampled
data.</figcaption>
</figure>
<h2 id="vision-language-model">Vision-Language Model</h2>
<p>Salesforce&#x2019;s <strong>BLIP</strong> (<a
href="https://arxiv.org/abs/2201.12086">Li et al.&#xA0;2022</a>) noisily
generates synthetic captions for web images, then filters noisy (bad)
captions. They pretrain an image-conditioned text-decoder and an
image-conditioned text-encoder that&#x2019;s initialized from pre-trained ViT
and BERT models (even the text decoder). The image-conditioned
text-decoder generates synthetic captions and adds them to the web-scale
(image, caption) data. The image-conditioned text-encoder predicts
match/no match for the web-scale and the synthetic (image, caption)
data, which is then added to the human-annotated (image, caption) data
to produce the entire dataset. Surprisingly, this works better than just
training on web-scale and human annotated data.</p>
<p>They perform some ablations to explain why this works:</p>
<ol type="1">
<li>They use nucleus sampling for the synthetic captions instead of beam
search, which they argue produces more noisy, but more diverse captions.
The noise doesn&#x2019;t matter since the filter model will remove bad
captions.</li>
<li>The filter and captioner do not share parameters. They tried
parameter sharing during the caption-filter stage and found downstream
performance of the pre-trained models to be worse. They argue that
parameter sharing would lead to confirmation bias: the filter is likely
the think the synthetic captions are correct: 8% filtered with parameter
sharing vs 25% without.</li>
</ol>
<p>They get good results on lots of datasets but nothing absurd.</p>
<p>Interestingly, they don&#x2019;t mention anywhere how many images their
method produces. (I&#x2019;ve asked the author via email).</p>
<h2 id="other-papers-on-my-reading-list">Other Papers on my Reading
List</h2>
<ul>
<li><a href="https://arxiv.org/abs/2209.11755">Promptagator: Few-shot
Dense Retrieval From 8 Examples</a></li>
<li><a href="https://arxiv.org/abs/2305.16635">Impossible Distillation:
from Low-Quality Model to High-Quality Dataset &amp; Model for
Summarization and Paraphrasing</a></li>
<li><a href="https://arxiv.org/abs/2210.11610">Large Language Models Can
Self-Improve</a></li>
<li><a href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping
Language-Image Pre-training with Frozen Image Encoders and Large
Language Models</a></li>
<li><a href="https://arxiv.org/abs/2203.14465">STaR: Bootstrapping
Reasoning With Reasoning</a></li>
<li><a href="https://arxiv.org/abs/2304.06767">RAFT: Reward rAnked
FineTuning for Generative Foundation Model Alignment</a></li>
<li><a
href="https://dblalock.substack.com/p/models-generating-training-data-huge">Models
generating training data: huge win or fake win?</a></li>
</ul>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Using small, high quality data is consistent with LIMA
(<a href="https://arxiv.org/abs/2305.11206">Zhou et al.&#xA0;2023</a>),
another recent LLM work from Meta. The authors find that regular
language modeling on only 1K &#x201C;carefully curated prompts and responses&#x201D;
can lead to strong instruction-following performance. So only using 3200
examples in Humpback and expecting strong instruction-following
performance is very reasonable.<a href="#fnref1" class="footnote-back"
role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</aside>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2022</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
