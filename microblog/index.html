<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <meta name="description" content="Short thoughts and snippets." />
  <meta name="keywords" content="" />
  <title>Blog</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me3.jpg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Sam Stevens</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/research">Research</a>]
        [<a href="/microblog">Blog</a>]
        [<a href="/contact">Contact</a>]
        [<a href="/cv.pdf">CV</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="blog">Blog</h1>
<details>
<summary>
About
</summary>
Arbitrary thoughts, sometimes with links to particularly interesting
articles. # indicates a permalink to the specific &#x201C;post.&#x201D;
</details>
<hr />
<h2 id="mathacademy-podcast"><a href="#mathacademy-podcast">#</a>
Mathacademy Podcast</h2>
<p>I do 20XP a day on <a href="https://mathacademy.com/">Mathacademy</a>
to improve old math that I&#x2019;ve forgotten. I also listen to their <a
href="https://www.justinmath.com/podcasts/#math-academy">podcast</a> and
enjoyed the first half of <a
href="https://www.justinmath.com/math-academy-podcast-2/">#2</a>.</p>
<p><em>Getting &#x201C;Inside the Trade&#x201D;</em>: Jason describes the process of
needing deep domain expertise to build effective high-frequency trading
systems. This is the opposite of <a
href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">the
Bitter Lesson</a>, and not what I would have expected from an ML-driven
startup.</p>
<p><em>December 29, 2025</em></p>
<h2 id="why-saev"><a href="#why-saev">#</a> Why Saev?</h2>
<p>Why did I build <a
href="https://github.com/OSU-NLP-Group/saev">saev</a> instead of using
existing libraries, like <a
href="https://github.com/KempnerInstitute/overcomplete">Overcomplete</a>?
First, SAE training has different computational bottlenecks compared to
ViT or LLM training. Second, SAE training has different complexity
bottlenecks, which leads to different abstractions in the code.</p>
<p>The big computational bottleneck in SAE training is loading model
activations from disk since SAEs are pretty small compared to other
models. In comparison, most LLM training is bottlenecked by matrix
multiplication rather than dataloading. Because of this, things like
torch.compile, custom kernels, etc are less important than building
efficient dataloaders. Saev includes a bunch of specialized activation
dataloaders for different purposes (<a
href="https://osu-nlp-group.github.io/saev/api/api/data/saev.data/#saev.data.ShuffledDataLoader">ShuffledDataLoader</a>
for training, <a
href="https://osu-nlp-group.github.io/saev/api/api/data/saev.data/#saev.data.OrderedDataLoader">OrderedDataLoader</a>
for inference, <a
href="https://osu-nlp-group.github.io/saev/api/api/data/saev.data/#saev.data.IndexedDataset">IndexedDataset</a>
for random access) to help with this.</p>
<p>The other thing is code organization. SAE papers often follow a fixed
pattern:</p>
<ol type="1">
<li>Record activations from a ViT or other model.</li>
<li>Train lots of SAEs on the activations.</li>
<li>Run SAE inference on the training or validation activations.</li>
<li>Explore the SAE predictions, either with summary statistics or
looking at individual predictions.</li>
</ol>
<p>The <code>saev/framework</code> module has code to do this flexibly
enough for different projects, but quickly and simply enough to speed
you up. The framework really leverages all the components efficiently.
For example, the different dataloaders expect ViT activations on disk in
a particular format; <code>saev/framework/shards.py</code> saves
activations in that format. Another example:
<code>saev/framework/inference.py</code> saves SAE predictions as a
sparse matrix, which makes it much easier to flexibly analyze without
needing to load ViT activations or SAE checkpoints.</p>
<p><em>November 23, 2025</em></p>
<h2 id="positive-and-negative-features-in-saes."><a
href="#positive-and-negative-features-in-saes.">#</a> Positive and
Negative Features in SAEs.</h2>
<p>Two recent papers (<a href="https://arxiv.org/pdf/2510.08638">Fel et
al.</a> and <a href="https://arxiv.org/pdf/2510.00404">Zhu et al.</a>)
imply that models represent &#x201C;opposite&#x201D; features (like vertical vs
horizontal or male vs female) as opposite directions. From Fel: &#x201C;Despite
their opposition, the vectors are nearly colinear with opposite signs,
suggesting the model uses polarity to encode meaning.&#x201D; Zhu et
al.&#xA0;deliberately build this into their SAE architecture by keeping high
magnitude features instead of strictly positive features.</p>
<p>Anthropic discussed this a little bit in their <a
href="https://transformer-circuits.pub/2022/toy_model">Toy Models of
Superposition</a> but not empirically. I think it&#x2019;s also hard to build a
toy model of this phenomenon because the &#x201C;opposite-ness&#x201D; is more
qualitative than quantitative. &#x201C;Horizontal&#x201D; and &#x201C;vertical&#x201D; are used in
many of the same contexts, but they represent different ideas
<em>within</em> that context. So in some sense they are the same, and in
another sense they are opposites.</p>
<p><em>November 18, 2025</em></p>
<h2 id="personal-computing-with-ai."><a
href="#personal-computing-with-ai.">#</a> Personal Computing with
AI.</h2>
<p><a href="https://tinygrad.org">Tiny Corp</a> sells a computer called
the <a href="https://tinygrad.org/#tinybox">tinybox</a> to &#x201C;commoditize
the petaflop and enable AI for everyone.&#x201D; I love the idea of tinygrad
and the tinybox, but I don&#x2019;t think the tinybox will ever be able to run
the best LLMs. I think it will be infeasible for me to afford sufficient
compute (VRAM and FLOP/s) to run the best LLMs for many years. Thus, I
have to outsource that to model providers (OpenAI and Anthropic, but
also third parties like Groq or Cerebras).</p>
<p>In contrast, I <em>can</em> spend compute to provide more parallel
environments for the LLMs. Rather than use Codex Cloud or Google&#x2019;s <a
href="https://jules.google">Jules</a>, I can run coding environments
(Docker images) on my personal compute in parallel. <strong>I will still
be compute bound, but I think it will be more general compute (CPU)
rather than matrix-multiplies.</strong></p>
<p><em>November 15, 2025</em></p>
<style>
h2 {
  display: inline;
  font-size: 1em;      /* match paragraph size */
  line-height: inherit;
  margin: 0;            /* remove default margins */
}
h2 + p { display: inline; }
h2 + p::before { content: " "; }
</style>
      <hr />
      <p>[<a href="/links" data-no-instant>Links</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
