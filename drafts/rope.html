<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="Rotary Positional Embeddings (RoPE)
Explained" />
  <meta name="keywords" content="" />
  <title>Behold: RoPE Explained Visually</title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="https://github.com/samuelstevens">GitHub</a>]
        [<a href="mailto:samuel.robert.stevens@gmail.com">Email</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="rotary-positional-embeddings-rope">Rotary Positional Embeddings
(RoPE)</h1>
<p>This is my notes from reading about RoPE in language models.</p>
<h2 id="sources">Sources</h2>
<ul>
<li><a href="https://blog.eleuther.ai/rotary-embeddings/">Eleuther AI&#x2019;s
blogpost</a></li>
</ul>
<h2 id="goal">Goal</h2>
<p>We want a function <span class="math inline">\(f(\mathbf{x},
l)\)</span> for an item <span class="math inline">\(\mathbf{x}\)</span>
and a position <span class="math inline">\(l\)</span> such that for two
items <span class="math inline">\(\mathbf{a}\)</span> and <span
class="math inline">\(\mathbf{b}\)</span> at positions <span
class="math inline">\(m\)</span> and <span
class="math inline">\(n\)</span>, the inner product of <span
class="math inline">\(f(\mathbf{q}, m)\)</span> and <span
class="math inline">\(f(\mathbf{b}, n)\)</span> depends only on <span
class="math inline">\(\mathbf{a}\)</span>, <span
class="math inline">\(\mathbf{b}\)</span> and <span
class="math inline">\(m - n\)</span>. Note that it should not matter
what <span class="math inline">\(m\)</span> and <span
class="math inline">\(n\)</span> are, only what their difference is.</p>
<p>The blog post makes a connection to the kernel trick used in SVMs: we
want a feature map such that the kernel has certain properties.</p>
<h2 id="derivation">Derivation</h2>
<p>For each token, we know where it is in the sequence. Dot products do
not preserve absolute positional information. So if we encode absolute
positional information in the embeddings, we lose information.</p>
<blockquote>
<p>Can you provide an example of how that happens?</p>
</blockquote>
<blockquote>
<p>We can assume that the models try to learn absolutely positional
embeddings that work around this constraint. Or is that why we add
positional embeddings to every layer of the model?</p>
</blockquote>
<p>Dot products do preserve relative position so &#x201C;if we can encode the
absolute positional information into the token embeddings in a way that
only leverages relative positional information, that will be preserved
by the attention function.&#x201D;</p>
<p>Given query and key vectors <span
class="math inline">\(\mathbf{q}\)</span> and <span
class="math inline">\(\mathbf{k}\)</span> and their absolute positions
<span class="math inline">\(m\)</span> and <span
class="math inline">\(n\)</span>, let&#x2019;s find a function <span
class="math inline">\(f(\mathbf{x}, l)\)</span> that takes a token
embedding in position <span class="math inline">\(l\)</span> and outputs
a new embedding that contains the relative positional information
(because that won&#x2019;t be erased by the dot product). Then we can calculate
<span class="math display">\[\langle f(\mathbf{q}, m), f(\mathbf{k}, n)
\rangle = g(\mathbf{q}, \mathbf{k}, m - n)\]</span></p>
<p>&#x201C;where <span class="math inline">\(g(\mathbf{q}, \mathbf{k}, m -
n)\)</span> represents the pre-softmax logit of the usual attention
equation.&#x201D;</p>
<blockquote>
<p>What is that again?</p>
</blockquote>
<p>Okay, we want to replace the regular dot-product with a new function
that takes into account the relative position. Because a normal dot
product apparently removes the absolute positional embeddings (still
don&#x2019;t believe that).</p>
<p>Let: <span class="math display">\[f(\mathbf{q}, m) = R_f(\mathbf{q},
m)e^{i \Theta_f(\mathbf{q}, m)}\]</span> <span
class="math display">\[f(\mathbf{k}, n) = R_f(\mathbf{k}, n)e^{i
\Theta_f(\mathbf{k}, n)}\]</span> <span
class="math display">\[g(\mathbf{q}, \mathbf{k}, m - n) =
R_g(\mathbf{q}, \mathbf{k}, m - n)e^{i \Theta_g (\mathbf{q}, \mathbf{k},
m - n)}\]</span></p>
<blockquote>
<p>Eleuther AI says this is &#x201C;writing the functions in exponential form&#x201D;
but I don&#x2019;t know what that means.</p>
<p>Euler&#x2019;s formula says for any real number <span
class="math inline">\(x\)</span>, then <span
class="math inline">\(e^{ix} = \cos(x) + i \sin(x)\)</span>.</p>
<p>So <span class="math inline">\(R_f(\mathbf{q}, m)e^{i
\Theta_f(\mathbf{q}, m)}\)</span> can be split into a real-valued
coefficient <span class="math inline">\(R_f(\mathbf{q}, m)\)</span> and
a complex coefficient <span class="math inline">\(\Theta_f(\mathbf{q},
m)\)</span>. Since <span class="math inline">\(e^{ix} = \cos(x) + i
\sin(x)\)</span> <span class="math display">\[R_f(\mathbf{q}, m)e^{i
\Theta_f(\mathbf{q}, m)} = R_f(\mathbf{q}, m) \cdot \left[
\cos(\Theta_f(\mathbf{q}, m)) + i \sin(\Theta_f(\mathbf{q},
m))\right]\]</span></p>
<p>I&#x2019;m not sure why we can transform the old equations for <span
class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> into this form though. I guess since we
don&#x2019;t know what <span class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> are yet, it&#x2019;s arbitrary how we choose
to represent or parameterize them.</p>
</blockquote>
<p>Now compute the inner product:</p>
<p><span class="math display">\[\begin{aligned}
  \langle f(\mathbf{q}, m), f(\mathbf{k}, n) \rangle = R_f(\mathbf{q},
m)e^{i \Theta_f(\mathbf{q}, m)} \cdot R_f(\mathbf{k}, n)e^{i
\Theta_f(\mathbf{k}, n)}
  \langle f(\mathbf{q}, m), f(\mathbf{k}, n) \rangle = R_f(\mathbf{q},
m)e^{i \Theta_f(\mathbf{q}, m)} \cdot R_f(\mathbf{k}, n)e^{i
\Theta_f(\mathbf{k}, n)}
\end{aligned}\]</span></p>
<h2 id="notes-from-the-roformer-paper">Notes from the RoFormer
Paper</h2>
<h3 id="background">Background</h3>
<p>Suppose you have <span class="math inline">\(\mathbf{x}_i\)</span>, a
<span class="math inline">\(d\)</span>-dimensional embedding vector of
token <span class="math inline">\(t_i\)</span> without any positional
information. We normally incorporate positional information in the
self-attention operation. This paper suggests that we calculate query,
key and value representations with functions like</p>
<p><span class="math display">\[\mathbf{q}_m = f_q(\mathbf{x}_m,
m)\]</span> <span class="math display">\[\mathbf{k}_n =
f_k(\mathbf{x}_n, n)\]</span> <span class="math display">\[\mathbf{k}_n
= f_k(\mathbf{x}_n, n)\]</span></p>
<p>Where <span class="math inline">\(f_*\)</span> can incorporate the
positional information <span class="math inline">\(m\)</span> and <span
class="math inline">\(n\)</span> however it wants. A typical choice is
learned position embeddings, and only at the first layer.</p>
<p>Let&#x2019;s decompose <span class="math inline">\(\mathbf{q}^\intercal_m
\mathbf{k}_n\)</span> using a position-less defintion of <span
class="math inline">\(f_*\)</span>: <span
class="math display">\[\mathbf{q}_m = \mathbf{W}_q \mathbf{x}_m\]</span>
<span class="math display">\[\mathbf{q}_m^\intercal =
\mathbf{x}_m^\intercal \mathbf{W}_q^\intercal \]</span> <span
class="math display">\[\mathbf{k}_n = \mathbf{W}_k \mathbf{x}_n\]</span>
<span class="math display">\[\mathbf{q}^\intercal_m \mathbf{k}_n =
\mathbf{x}_m^\intercal \mathbf{W}_q^\intercal \mathbf{W}_k
\mathbf{x}_n\]</span></p>
<p>What if we use absolute learned embeddings? <span
class="math display">\[\mathbf{q}_m = \mathbf{W}_q \mathbf{x}_m +
\mathbf{W}_q \mathbf{p}_m\]</span> <span
class="math display">\[\mathbf{q}_m^\intercal = \mathbf{x}_m^\intercal
\mathbf{W}_q^\intercal + \mathbf{p}_m^\intercal
\mathbf{W}_q^\intercal\]</span> <span
class="math display">\[\mathbf{k}_n = \mathbf{W}_k \mathbf{x}_n +
\mathbf{W}_k \mathbf{p}_n\]</span> <span
class="math display">\[\mathbf{q}^\intercal_m \mathbf{k}_n =
(\mathbf{x}_m^\intercal \mathbf{W}_q^\intercal + \mathbf{p}_m^\intercal
\mathbf{W}_q^\intercal) \cdot (\mathbf{W}_k \mathbf{x}_n + \mathbf{W}_k
\mathbf{p}_n)\]</span> <span
class="math display">\[\mathbf{q}^\intercal_m \mathbf{k}_n =
\mathbf{x}_m^\intercal \mathbf{W}_q^\intercal \mathbf{W}_k \mathbf{x}_n
+ \mathbf{x}_m^\intercal \mathbf{W}_q^\intercal \mathbf{W}_k
\mathbf{p}_n + \mathbf{p}_m^\intercal \mathbf{W}_q^\intercal
\mathbf{W}_k \mathbf{x}_n + \mathbf{p}_m^\intercal
\mathbf{W}_q^\intercal \mathbf{W}_k \mathbf{p}_n\]</span></p>
<p>We can simplify this be remembering that <span
class="math inline">\(\mathbf{W}_q\)</span> and <span
class="math inline">\(\mathbf{W}_k\)</span> are always paired. Let&#x2019;s
replace them with <span class="math inline">\(\mathbf{O}\)</span>. <span
class="math display">\[\mathbf{q}^\intercal_m \mathbf{k}_n =
\mathbf{x}_m^\intercal \mathbf{O} \mathbf{x}_n + \mathbf{x}_m^\intercal
\mathbf{W} \mathbf{p}_n + \mathbf{p}_m^\intercal \mathbf{O} \mathbf{x}_n
+ \mathbf{p}_m^\intercal \mathbf{O} \mathbf{p}_n\]</span></p>
<p>There are also relative position embeddings (which is used a lot):
<span class="math display">\[f_q(\mathbf{x}_m) = \mathbf{W}_q
\mathbf{x}_m\]</span> <span class="math display">\[f_k(\mathbf{x}_n, n)
= \mathbf{W}_k \cdot (\mathbf{x}_n + \tilde{\mathbf{p}}_r^k)\]</span>
<span class="math display">\[f_v(\mathbf{x}_n, n) = \mathbf{W}_v \cdot
(\mathbf{x}_n + \tilde{\mathbf{p}}_r^v)\]</span> Where <span
class="math inline">\(\tilde{\mathbf{p}}_r^k, \tilde{\mathbf{p}}_r^v \in
\mathbb{R}^d\)</span> are trainable embeddings. There is a different
embedding for each value of <span class="math inline">\(r\)</span>, and
<span class="math inline">\(r = \text{clip}(m - n, r_\min,
r_\max)\)</span>. Clipping enables you to have a fixed number of
relative position embeddings.</p>
<h3 id="proposed-approach">Proposed Approach</h3>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
<script id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" defer></script>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
