<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <title>Behold: </title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links">Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h2 id="multimodal-benchmarks">Multimodal Benchmarks</h2>
<ul>
<li>MMMU</li>
<li>MMBench</li>
<li>SEED-Bench</li>
<li>MME</li>
<li>V*Bench: designed to test models&#x2019; detailed visual grounding on
high-resolution images.</li>
</ul>
<h2 id="questions">Questions</h2>
<ul>
<li>Can you beat V*Bench with a Mamba-like model where the patch
resolution is very high and so it doesn&#x2019;t matter how many tokens you
have?</li>
<li>Do you need the vision-language task of CLIP for visual tokens to be
easily adjustable to language model embedding spaces? Or do vision-only
pre-trained models also produce visual tokens that are usable?</li>
</ul>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
