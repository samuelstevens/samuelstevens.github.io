<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <title>Behold: </title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="https://github.com/samuelstevens">GitHub</a>]
        [<a href="mailto:samuel.robert.stevens@gmail.com">Email</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<p>I am going to try and read 100 papers by the end of the week in
AI/ML. For each paper, I need a subject area, one sentence describing
the work and one sentence describing possible future work. Let&#x2019;s
start!</p>
<ol type="1">
<li><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts:
Deliberate Problem Solving with Large Language Models</a> (prompting):
This paper proposes a general prompting strategy where you ask the model
to produce intermediate steps, then evaluate which intermediate step is
best, then continue with that step. Future work: use a general prompt,
instead of hand-crafted prompts and strategies for each task.</li>
<li><a href="http://arxiv.org/abs/2310.02304">Self-Taught Optimizer
(STOP): Recursively Self-Improving Code Generation</a> (prompting): STOP
is a method to have language models write DSPy-like programs for itself,
then ask the improved program to write a better DSPy-like program for
itself. Future work: broader benchmarks, more experiments to understand
it. It&#x2019;s not principled enough right now to take useful insights from
this work.</li>
<li><a href="https://arxiv.org/abs/2401.16420">InternLM-XComposer2:
Mastering Free-form Text-Image Composition and Comprehension in
Vision-Language Large Models</a> (LVLM): Propose a partial LoRA method
where the updated LoRA matrix is only applied to vision tokens with very
good results compared to other open-source models on multimodal
benchmarks. Future work: two separate LoRA matrices, one for vision, one
for language?</li>
<li><a href="http://arxiv.org/abs/2210.13604">The Robustness Limits of
SoTA Vision Models to Natural Variation</a> (vision): Studies vision
model robustness to changes such as pose, position, background, etc and
finds that SOTA vision models (ViT, ResNet, MLPMixer) trained with
different objectives (CLIP, MAE, SimCLR, IN21K) are all not robust.
Future work: explore how to make models generalize robustness to
variations in one class to other classes.</li>
<li><a href="http://arxiv.org/abs/2401.05268">AUTOACT: Automatic Agent
Learning from Scratch via Self-Planning</a> (agent, prompting): Proposes
a DSPy-like program that uses a combination of techniques to extract
synthetic training data and then does multiple fine-tunes of a base LM
for different parts of the problem to solve multihop QA. Future work:
not principled enough. These techniques are likely going to be smashed
by GPT-5 and better LMs.</li>
<li><a href="http://arxiv.org/abs/2401.10891">Depth Anything: Unleashing
the Power of Large-Scale Unlabeled Data</a> (vision): train a
semi-supervised depth estimation model using a teacher trained on gold
labeled data and then a student trained on gold labeled data and
teacher-labeled data, while maintaining preservation as much as possible
between a frozen DINOv2 encoder and the student encoder. Future work:
explore other methods where training on a combination of gold data and
teacher-labeled data (BioCLIP v2?)</li>
<li><a href="http://arxiv.org/abs/2401.16380">Rephrasing the Web: A
Recipe for Compute &amp; Data-Efficient Language Modeling</a> (small
language model, synthetic data, distillation): Rephrase data in
different web styles using an instruction-tuned Mistral 7B, then trained
a 1.3B model that does better than a synthetic-only and a real-data-only
model. Future work: compare directly to Mistral-7B distillation.</li>
<li><a href="http://arxiv.org/abs/2401.12926">DsDm: Model-Aware Dataset
Selection with Datamodels</a> (small language model, pre-training data):
Choose pre-training data for 125M parameter LMs using data models (prior
work) and find that this outperforms both random baselines and other
prior work in this area that uses classifier-based pre-training data
selection. Future work: train bigger models, use more complex data
models, see if this holds.<a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li><a href="http://arxiv.org/abs/2207.10551">Scaling Laws vs Model
Architectures: How does Inductive Bias Influence Scaling?</a> (scaling
laws): Compares different architectures to vanilla transformers at
different scales; very little outperforms vanilla transformers,
espcially once fine-tuned on downstream tasks; this emphasizes that you
need to try different architectures at different scales to justify
replacing a transformer. Future work: stick with vanilla transformers
unless your paper is all about the new architecture.</li>
<li><a href="http://arxiv.org/abs/2306.00989">Hiera: A Hierarchical
Vision Transformer without the Bells-and-Whistles</a> (vision
transformer): Adds spatial bias to ViT without shifted windows or such
using an MAE pretraining objective and includes hierarchical feature
maps. Future work: none, this is a vision architecture that doesn&#x2019;t
dominate ViTs so completely that it&#x2019;s worth using.</li>
<li><a href="http://arxiv.org/abs/2310.07704">Ferret: Refer and Ground
Anything Anywhere at Any Granularity</a> (LVLM): proposes a new visual
prompting method that integrates discrete and continuous regions in the
visual prompt and then tunes a LVLM to use them as input and output.
Future work: simpler, cleaner prompt embedding method. Can we simply
draw on the picture with a red marker and say &#x201C;What&#x2019;s circled in
red?&#x201D;?</li>
<li><a href="http://arxiv.org/abs/2308.06259">Self-Alignment with
Instruction Backtranslation</a> (instruction following): Prompts a
language model to generate realistic instruction-following inputs for a
set of true human outputs; uses these to iteratively instruction-tune a
model and outperforms non-distillation methods. Future work: multiturn
instruction following and alignment in this same way.</li>
<li><a
href="Getting%20ViT%20in%20Shape:%20Scaling%20Laws%20for%20Compute-Optimal%20Model%20Design">Getting
ViT in Shape: Scaling Laws for Compute-Optimal Model Design</a> (vision
transformer): Tries lots of different vision transformer shapes (depth
vs width) and finds scaling laws for these properties, leading us to
believe that transformer shape matters in vision. Future work: use this
method for BioCLIP.</li>
<li><a href="http://arxiv.org/abs/2401.06209">Eyes Wide Shut? Exploring
the Visual Shortcomings of Multimodal LLMs</a> (LVLM): Compares CLIP
models and vision-only models to find images that are difficult for
CLIP; uses these in prompts to LVLMs with CLIP vision encoders and find
that they struggle to answer them correctly; propose a fusion of CLIP
and DINOv2 features. Future work: all vision encoders will have these
blind spots. How can LMs understand that a blind spot might exist and
use different features when necessary?</li>
<li><a href="http://arxiv.org/abs/2401.06751">The Unreasonable
Effectiveness of Easy Training Data for Hard Tasks</a> (language
models): Train LMs on &#x201C;easy&#x201D; data from different tasks then evaluates on
&#x201C;hard&#x201D; data from different tasks and find that this is nearly as good as
training on the hard data. Future work: This implies that just showing
the model true question/answer pairs tells it to produce the true answer
for a given question. Can you experimentally verify that?</li>
<li><a href="http://arxiv.org/abs/2205.01580">Better plain ViT baselines
for ImageNet-1k</a> (vision transformer): Proposes a very simple
baseline for ViTs on ImageNet-1K where the most important differences
are using a global mean pool instead of a CLS token, a batch size of
1024 instead of 4096, and some basic data augmentation. Future work: Why
do we not need more data augmentation for models with less inductive
bias?</li>
<li><a href="http://arxiv.org/abs/2305.13829">Learning from Mistakes via
Cooperative Study Assistant for Large Language Models</a> (prompting,
self-feedback): Gathers LM mistakes, stores them, then uses a &#x201C;study
assistant&#x201D; LM to choose important examples to show to the LM at test
time so it doesn&#x2019;t make the same mistakes. Future work: it would be
great to compare the token-level costs of all these different methods.
This is also another method that can be written in a DSPy-like
program.</li>
<li><a href="http://arxiv.org/abs/2307.14430">Skill-it! A Data-Driven
Skills Framework for Understanding and Training Language Models</a>
(pre-training data): Uses the idea of &#x201C;skills&#x201D; (related to The
Quantization Model of Neural Scaling) to organize training data, then
learns skills in an ordered fashion to do better than random sampling.
Future work: compare with data models? Again, this data-efficient
pre-training stuff only works because we can think about how big models
have learned in the past, and there really isn&#x2019;t any guarantee that
these methods will scale up to larger models.</li>
<li><a href="http://arxiv.org/abs/2401.15947">MoE-LLaVA: Mixture of
Experts for Large Vision-Language Models</a> (MoE, LVLM): Combines Llava
instruction tuning with MoE, using the dense FFNNs as initializations
for the MoE experts. Future work: none. This paper is pretty trash.</li>
<li><a href="https://arxiv.org/abs/2312.14135">V*: Guided Visual Search
as a Core Mechanism in Multimodal LLMs</a> (LVLM): Propose an LLM-guided
visual search algorithm over high-resolution images to answer questions
about visual details. Future work: compare against brute-force patch
search, or using a vision mamba model to process high-resolution images.
Feels too complicated for its own good; likely feels good because it
matches human intution about how to find objects in an image. I think
the Bitter Lesson applies here.</li>
<li><a href="http://arxiv.org/abs/2312.08914">CogAgent: A Visual
Language Model for GUI Agents</a> (LVLM, agent): uses a high-resolution
and a low-resolution image encoder to make a LVLM for GUIs where
high-resolution images with low semantic complexity are popular. Future
work: simplify the framework further. Use self-supervised training
rather than scraped supervised data.</li>
<li><a href="http://arxiv.org/abs/2308.06093">Experts Weights Averaging:
A New General Training Scheme for Vision Transformers</a> (MoE, vision
transformer): Use random token routing for an MoE layer then average the
expert weights after every step; they show that is sort of a historical
exponential weight averaging. Future work: think about weight averaging
for vision&#x2013;why does this work? Should we always be using it for better
accuracy on all tasks?</li>
<li><a href="http://arxiv.org/abs/2312.06585">Beyond Human Data: Scaling
Self-Training for Problem-Solving with Language Models</a>
(self-feedback, LLM): Generate samples, filter their quality using an
LM, then tune the LM on the best samples. Repeat. Future work: Compare
to training on more data: in general, applying more compute is better.
But if you have real data, is LM-based data better or worse?</li>
<li><a href="http://arxiv.org/abs/2306.13394">MME: A Comprehensive
Evaluation Benchmark for Multimodal Large Language Models</a>
(vision-language benchmark): Designs a yes/no question answering
benchmark across different tasks (limited reasoning) with 2800
questions. Future work: use harder questions, pick images that are
likely hard for CLIP.</li>
<li><a href="http://arxiv.org/abs/2307.06281">MMBench: Is Your
Multi-modal Model an All-around Player?</a> (vision-language benchmark):
vision language multiple-choice benchmark that proposes evaluating
models with different orderings of the choices; only marked as correct
if it gets them all right. Also uses ChatGPT-3.5 to extract LLM answers
from their text. Future work: better structured answers from models to
stop relying on ChatGPT.</li>
<li><a href="http://arxiv.org/abs/2307.16125">SEED-Bench: Benchmarking
Multimodal LLMs with Generative Comprehension</a> (vision-language
benchmark): 19K image and video multiple choice questions generated by
ChatGPT-4 with human answers; uses log probs to choose among possible
answers. Future work: how to integrate CoT into log-prob-based
answering?</li>
<li><a href="http://arxiv.org/abs/2312.00784">Making Large Multimodal
Models Understand Arbitrary Visual Prompts</a> (LVLM): Teaches
vision-language models to use red arrows and drawings in combination
with references in language. Future work: how to synthetically generate
more data? How much data is needed to compose these skills with other
instruction-following tasks?</li>
<li><a href="http://arxiv.org/abs/2311.09193">The Role of
Chain-of-Thought in Complex Vision-Language Reasoning Task</a>
(prompting, LVLM): Add &#x2018;First, describe the image information relevant
to the question. Then, provide your answer.&#x2019; to prompts (like chain of
thought) and improves vision-language reasoning. Future work:
incorporate these sort of prompts into RLHF, just like how GPT-4 does
CoT naturally.</li>
<li><a href="http://arxiv.org/abs/2203.16527">Exploring Plain Vision
Transformer Backbones for Object Detection</a> (vision transformer,
object detection): Makes hierarchical feature maps from the last layer
of a vision transformer&#x2019;s hidden states; this is better than other
hierarchical strategies for object detection especially at larger scale
ViTs. Future work: do we even need hierarchical feature maps? Can we
simply build object detection models that use a set of visual
tokens?</li>
<li>[Human-Centered Loss Functions (HALOs)]
(https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
(alignment): Devises a new alignment algorithm that doesn&#x2019;t need paired
preference data (like DPO, RLHF), just binary labels (good/bad). Future
work: How much does the alignment algorithm make a difference compare to
data diversity and scale?</li>
<li><a href="http://arxiv.org/abs/2305.18290">Direct Preference
Optimization: Your Language Model is Secretly a Reward Model</a>
(alignment): proposes an alignment algorithm that uses paired
preferences without RL using a cross-entropy loss objective. Future
work: DPO surely depends a lot on the data distribution, whereas (in
theory) RLHF can generalize to new domains (assuming the reward model
generalizes well). Investigate this?</li>
<li><a href="http://arxiv.org/abs/2012.09812">ViNG: Learning Open-World
Navigation with Visual Goals</a> (robot): Propose an offline-only,
visual goal-directed nagivation algorithm for robots (offline-only is
very hard) and do many real-world experiments. Future work: include
language instructions, handle distribution shift over static
pre-training dataset.</li>
<li><a href="http://arxiv.org/abs/1709.10489">Self-supervised Deep
Reinforcement Learning with Generalized Computation Graphs for Robot
Navigation</a> (robot, RL): Propose a sample-efficient RL algorithm for
learning in a self-supervised manner that composes both model-based and
sample-based (model-free) learning. Future work: handle dynamic
environments rather than fixed indoor environments.</li>
<li><a href="http://arxiv.org/abs/1704.05588">Learning to Fly by
Crashing</a> (robot): Collects a huge dataset of negative trajectories
(crashes) and uses them in supervised learning as a very strong baseline
for not crashing. Future work: think about collecting negative data
synthetically? What other distributions can we collect a lot of negative
data for?</li>
<li><a href="http://arxiv.org/abs/1910.11301">Cross-Lingual
Vision-Language Navigation</a> (robot, vision-language navigation):
Collects, trains and evaluates on multilingual vision-language
navifation. Future work: use with large language models? Kind of a
freebie paper that doesn&#x2019;t help anyone.</li>
<li><a href="http://arxiv.org/abs/1905.12255">Stay on the Path:
Instruction Fidelity in Vision-and-Language Navigation</a> (robot,
vision-language navigation): constructs a dataset to measure how well
agents follow instructions instead of simply achieving the goal. Future
work: consider the idea of instruction following over goal achieving
when doing future work.</li>
<li><a href="http://arxiv.org/abs/2010.07954">Room-Across-Room:
Multilingual Vision-and-Language Navigation with Dense Spatiotemporal
Grounding</a> (robot, vision-language navigation): Room-Across-Room
(RxR) is a VLN dataset with time-alignment, more complex paths,
multilingual instructions, and more examples. Future work: good dataset,
likely subsumed by photo-realistic simulators.</li>
<li><a href="http://arxiv.org/abs/1711.07280">Vision-and-Language
Navigation: Interpreting visually-grounded navigation instructions in
real environments</a> (vision-language navigation): The Room-to-Room
(R2R) dataset is the original vision-language benchmark dataset which
comes with the Matterport3D simulator for training. Future work: All VLN
work is future work on this.</li>
<li>Self-Supervised, Goal-Conditioned Policies for Navigation in
Unstructured Environments (robot, RL): Use behavior cloning, then
hindsight relabelling to produce a goal-conditioned policy. Future work:
none, this appears to be an application paper.</li>
<li>Improving Robot Navigation Through Self-Supervised Online Learning
(robot, RL): Really did not get what they did. Combined overhead
features with local viewpoints to improve robot navigation? Future work:
Read paper more closely.</li>
<li>Learning to Fly by MySelf: A Self-Supervised CNN-based Approach for
Autonomous Navigation (RL): train a CNN to predict depth then use this
to learn a good policy. Future work: how to learn the CNN without
gathering a bunch of depth data?</li>
<li><a href="http://arxiv.org/abs/2207.04429">LM-Nav: Robotic Navigation
with Large Pre-Trained Models of Language, Vision, and Action</a>
(vision-language navigation): convert natural language instructions to
natural language descriptions of viewpoints with GPT-3, match those way
point descriptions to images with CLIP, use the images with ViNG to
navigate to the different waypoints. Future work: add fine-tuning&#x2013;can it
improve the system?</li>
<li><a
href="https://ieeexplore.ieee.org/document/10025435/">Reinforcement
Learning for Collaborative Search and Rescue Using Unmanned Aircraft
System Swarms</a> (multi-agent RL): Use deep q learning to train a drone
swarm and improve over statistical baselines; first work to combine RL
and the subsumption architecture. Future work: add more actions beyond
just &#x201C;behaviors&#x201D;</li>
<li><a href="http://arxiv.org/abs/2204.12181">Collaborative Target
Search with a Visual Drone Swarm: An Adaptive Curriculum Embedded
Multistage Reinforcement Learning Approach</a> (multi-agent RL,
curriculum learning): applies multistage training with curriculum
learning to a deep RL algorithm for drone swarms working on
&#x201C;collaborative target search&#x201D;. Future work: curriculum learning for RL?
very interesting concept.</li>
<li><a href="http://arxiv.org/abs/2105.10605">Programming and Deployment
of Autonomous Swarms using Multi-Agent Reinforcement Learning</a>
(multi-agent RL): defines a system to optimize multi-agent swarms using
programmer-defined primitives. Future work: simplify the framework? Not
sure why it&#x2019;s so complex (didn&#x2019;t read too much).</li>
<li><a href="http://arxiv.org/abs/1709.06011">Guided Deep Reinforcement
Learning for Swarm Systems</a> (multi-agent RL): uses a global critic
and local actor in an actor-critic framework. Future work: do it without
the global critic: better rewards? See the OpenAI paper for more.</li>
<li><a href="http://arxiv.org/abs/1710.03748">Emergent Complexity via
Multi-Agent Competition</a> (multi-agent RL): self-play enables complex
learned behaviors in simple environments; also a nice curriculum
learning effect. Future work: what about team work?</li>
<li><a href="http://arxiv.org/abs/1909.07528">Emergent Tool Use From
Multi-Agent Autocurricula</a> (multi-agent RL): you can also find
emergent behaviors in competive multi-agent self-play (team vs team).
Future work: can you see these emergent behaviors without competition?
Can single-player games be reframed as competitive games?</li>
</ol>
<!-- 32. [LiDAR: Sensing Linear Probing Performance in Joint Embedding SSL Architectures](https://arxiv.org/abs/2312.04000) -->
<!-- 20. [Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning](http://arxiv.org/abs/2312.12379) ( -->
<!-- 21. [Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model](http://arxiv.org/abs/2401.09417) (vision, linear transformer):  -->
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Maybe part of the reason we do big pre-training runs is
we don&#x2019;t want to chance it with these data selection methods. If random
selection is a good baseline, then who cares about optimizing it? Just
train with more data and do cheap alignment afterwards.<a href="#fnref1"
class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</aside>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
