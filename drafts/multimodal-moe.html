<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="stylesheet" href="/css/site.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <meta charset="utf-8" />
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <title>Behold: </title>
  
</head>

<body>
  <aside class="sidebar">
    <div id="sidebar">
      <img src="/images/me2.jpeg" />
      <p>Sam Stevens</p>
    </div>
  </aside>
  <main>
    <header>
      <h1>Behold, My Stuff</h1>
      <p>
        [<a href="/">Home</a>]
        [<a href="/writing">Writing</a>]
        [<a href="/links">Links</a>]
        [<a href="/cv.pdf">CV</a>]
        [<a href="/contact">Contact</a>]
      </p>
    </header>
    <article>
      <!-- Must be unindented to prevent code indentation being broken -->
<h1 id="multimodal-mixture-of-expert-models">Multimodal Mixture of
Expert Models</h1>
<p>Typical multimodal models combine a pre-trained vision encoder (like
a CLIP ViT) with a pre-trained language model (like Llama-2) with some
&#x201C;modality fusion layer&#x201D; (like a single linear projection). They tune the
new parameters, then the vision encoder, then the language model, then
the entire thing end-to-end (or some combination of these steps) on some
vision-language data, then maybe some vision-language instruction tuning
tasks. This leads to an instruction-tuned vision-language model like
LLaVA.</p>
<p>Mixture of Expert (MoE) models replace the FFNN block in transformer
models with a MoE-FFNN layer, where there are <em>N</em> FFNNs in
parallel, and only <em>k</em> of them are chosen to activate for a given
sequence.</p>
<p>MoE models are intuitively appealing for for multimodal tasks for two
reasons:</p>
<ol type="1">
<li>Different experts could specialize for different modalities. At
lower, earlier layers, maybe each experts only understands one modality
(language or vision), and at upper layers, experts more seamlessly blend
modalities and specialize among semantic concepts.</li>
<li>When fusing pre-trained language models with pre-trained vision
models, the entire language model is typically fine-tuned on some
vision-language data. This can reduce performance on language-only
tasks.<a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a> Since (sparse) MoE models only
update a subset of all parameters for each example, they would &#x201C;drift&#x201D;
less than dense models, and could hypothetically only use the updated
parameters for vision-language tasks, reserving the original parameters
for language-only tasks.</li>
</ol>
<p>I don&#x2019;t find either of these explanations compelling or convincing.
I&#x2019;m going to explain my thoughts here, along with experiments or
evidence that would change my mind.</p>
<h2 id="different-experts-specialize-for-different-modalities">Different
Experts Specialize for Different Modalities</h2>
<h2 id="sparse-models-will-drift-less-than-dense-models">Sparse Models
Will &#x201C;Drift&#x201D; Less than Dense Models</h2>
<p>First, I&#x2019;m not aware of any evidence that training on vision-language
data will harm language-only performance. I would not be surprised if
this is the case:</p>
<ul>
<li>Tuning a model on a new data distribution will always harm
performance on the original data distribution.</li>
<li>RLHF on GPT-3 and GPT-4 harms language modeling loss, in exchange
for better zero-shot instruction following.</li>
</ul>
<p>But:</p>
<ul>
<li>The magnitude might be trivial and irrelevant for most cases.</li>
<li>Bigger models (more capacity) almost always solves the lack of
multi-task capabilities.</li>
</ul>
<p>For the sake of the argument, let&#x2019;s assume that dense models
<em>do</em> get noticeable worse on language-only benchmarks after
shallow fusion with vision models. Why would MoE models mitigate this
issue?</p>
<p>One argument is that MoE sparsity will minimize drift between the
original model and the vision-language tuned model. Let&#x2019;s consider some
measures of drift, and what experiments we could do to quantify their
accuracy in measuring task performance.</p>
<h3 id="l2-distance-between-weights">L<sub>2</sub> Distance Between
Weights</h3>
<p>Ideally, we could use some different optimizer hyperparameters, some
different losses and measure a correlation between L<sub>2</sub>
distance and reduction in task performance. We would keep the dataset
the same (vision-language, or even just a random language-modeling
dataset) and then see if things like lower learning rates can minimize
L<sub>2</sub> distance, and then see if that helps with reducing
catastrophic forgetting.</p>
<h3 id="proportion-of-changed-parameters">Proportion of Changed
Parameters</h3>
<p>Maybe the number of changed parameters (normalized by the total
number of parameters) correlates with catastrophic forgetting.</p>
<p>Can we measure</p>
<blockquote>
<p><strong>Criticism of these ideas:</strong> Nothing in ML is very
reliable. It would shocking if doing these experiments produced a clear
correlation, especially given that we would want different models,
different datasets, etc. To make these experiments more compelling, I
would have to increase the experiment specificity and reduce the scope
of the independent variable, or keep much more the same between groups.
Something like &#x201C;Comparing a smaller learning rate for more steps vs a
larger learning rate for fewer steps&#x201D;. But that&#x2019;s actually too weak. I
want something specific that is still general across different language
models and tasks.</p>
</blockquote>
<h2 id="moe-models-improve-training-efficiency">MoE Models Improve
Training Efficiency</h2>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>I&#x2019;m not aware of any concrete evidence that
vision-language models are worse at language-only tasks than their
language backbone. I suspect that if you spent X FLOPs tuning an LM
<em>L</em> with vision-language data to produce <em>L<sub>VL</sub></em>
and X FLOPs tuning an LM on language-only data to produce
<em>L<sub>L+</sub></em>, the language-only LM <em>L<sub>L+</sub></em>
would be better at language-only tasks compared to the vision-language
LM <em>L<sub>VL</sub></em>. But I don&#x2019;t know if <em>L<sub>VL</sub></em>
would underperform the base LM <em>L</em> on language-only tasks.<a
href="#fnref1" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</section>
      <hr />
      <p>[<a href="https://www.youtube-nocookie.com/embed/SHbS9tYFpcQ">Relevant link</a>] [<a href="https://github.com/samuelstevens/personal-website">Source</a>]</p>
      <p>Sam Stevens, 2024</p>
    </article>
  </main>
  <script src="/js/instantclick.min.js" data-no-instant></script>
  <script data-no-instant>
    InstantClick.init();
  </script>
  <style>
    
  </style>
</body>

</html>
