<rss version="2.0"><channel><title>Reading List</title><link>https://samuelstevens.me/readinglist.xml</link><description>My personal reading list</description><language>en-us</language><pubDate>Sat, 6 Feb 2021 15:21:03 UT</pubDate><lastBuildDate>Sat, 6 Feb 2021 15:21:03 UT</lastBuildDate><docs>https://cyber.harvard.edu/rss/rss.html</docs><generator>samuelstevens/racket-rss</generator><managingEditor>samuel.robert.stevens@gmail.com</managingEditor><webMaster>samuel.robert.stevens@gmail.com</webMaster><item><title>Execute Program</title><link>https://www.executeprogram.com/</link><description>https://www.executeprogram.com/</description><pubDate>Tue, 24 Mar 2020 12:18:12 UT</pubDate><guid>https://www.executeprogram.com/</guid></item><item><title>Markov Chains</title><link>https://setosa.io/blog/2014/07/26/markov-chains/index.html</link><description>
Markov chains, named after Andrey Markov, are mathematical systems that hop from one "state" (a situation or set of values) to another. For example, if you made a Markov chain model of a baby's behavior, you might include "playing," "eating", "sleeping," and "crying" as states, which together with other behaviors could form a 'state space': a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or "transitioning," from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first.

A simple, two-state Markov chain is shown below.



With two states (A and B) in our state space, there are 4 possible transitions (not 2, because a state can transition back into itself). If we're at 'A' we could transition to 'B' or stay at 'A'. If we're at 'B' we could transition to 'A' or stay at 'B'. In this two state diagram, the probability of
transitioning from any state to any other state is 0.5.

Of course, real modelers don't always draw out Markov chain diagrams. Instead they use a "transition matrix" to tally the transition probabilities. Every state in the state space is included once as a row and again as a column, and each cell in the matrix tells you the probability of transitioning from its row's state to its column's state. So, in the matrix, the cells do the same job that the arrows do in the diagram.



If the state space adds one state, we add one row and one column, adding one cell to every existing column and row. This means the number of cells grows quadratically as we add states to our Markov chain. Thus, a transition matrix comes in handy pretty quickly, unless you want to draw a jungle gym Markov chain diagram.

One use of Markov chains is to include real-world phenomena in computer simulations. For example, we might want to check how frequently a new dam will overflow, which depends on the number of rainy days in a row. To build this model, we start out with the following pattern of rainy (R) and sunny (S) days:



One way to simulate this weather would be to just say "Half of the days are rainy. Therefore, every day in our simulation will have a fifty percent chance of rain." This rule would generate the following sequence in simulation:



Did you notice how the above sequence doesn't look quite like the original? The second sequence seems to jump around, while the first one (the real data) seems to have a "stickyness". In the real data, if it's sunny (S) one day, then the next day is also much more likely to be sunny.

We can minic this "stickyness" with a two-state Markov chain. When the Markov chain is in state "R", it has a 0.9 probability of staying put and a 0.1 chance of leaving for the "S" state. Likewise, "S" state has 0.9 probability of staying put and a 0.1 chance of transitioning to the "R" state.



In the hands of metereologists, ecologists, computer scientists, financial engineers and other people who need to model big phenomena, Markov chains can get to be quite large and powerful. For example, the algorithm Google uses to determine the order of search results, called PageRank, is a type of Markov chain.



Above, we've included a Markov chain "playground", where you can make your own Markov chains by messing around with a transition matrix. Here's a few to work from as an example: ex1, ex2, ex3 or generate one randomly. The transition matrix text will turn red if the provided matrix isn't a valid transition matrix. The rows of the transition matrix must total to 1. There also has to be the same number of rows as columns.

You can also access a fullscreen version at setosa.io/markov



</description><pubDate>Tue, 24 Mar 2020 12:18:48 UT</pubDate><guid>https://setosa.io/blog/2014/07/26/markov-chains/index.html</guid></item><item><title></title><link>https://www.lesswrong.com/posts/FCXCXigp7byv2dM8D/how-to-make-billions-of-dollars-reducing-loneliness</link><description>Loneliness Is a Big Problem
On Facebook, my friend Tyler writes:

Lately, I've been having an alarming amount of conversations arise about the burdens of loneliness, alienation, rootlessness, and a lack of belonging that many of my peers feel, especially in the Bay Area. I feel it too. Everyone has a gazillion friends and events to attend. But there's a palpable lack of social fabric. I worry that this atomization is becoming a world-wide phenomenon – that we might be some of the first generations without the sort of community that it's in human nature to rely on.


And that the result is a worsening epidemic of mental illness...


Without the framework of a uniting religion, ethnicity, or purpose, it's hard to get people to truly commit to a given community. Especially when it's so easy to swipe left and opt for things that offer the fleeting feeling of community without being the real thing: the parties, the once-a-month lecture series, the Facebook threads, the workshops, the New Age ceremonies. We often use these as "community porn" – they're easier than the real thing and they satisfy enough of the craving. But they don't make you whole.


I've had some thoughts about experiments to try. But then I think about how hard it is (especially in this geographic area) to get people to show up to something on at least a weekly basis. Even if it's for something really great. I see many great attempts at community slowly peter out.

Young people are lonely.  Old people are lonely.
Loneliness is bad for your health.  It's bad for society's health.
Having a smartphone that keeps you entertained all day, and enough money to live by yourself, might sound like first world problems.  But they are likely contributors to loneliness.  And as developing countries get richer, they'll start having first world problems too.  So I think addressing loneliness could be very high-leverage for the world.
People are starting businesses to address loneliness: you can pay someone to call you periodically or take you for a walk.  But I'd argue these services are a band-aid in the same sense that parties, workshops, and ceremonies are.  They don't solve the underlying problem: You're still alone by default instead of together by default.
Roommates Could Be a Great Solution
Sociologists think there are three conditions necessary for making friends: proximity; repeated, unplanned interactions; and a setting that encourages people to let their guard down and confide in each other.  These conditions tend to be present during college for many people, but not afterwards.
Why do people find it easier to make friends in college?  Maybe it's because college students don't usually live alone.
Going to events doesn't work because (a) you don't typically get repeated interactions with the same person and (b) events take place at a scheduled time.  Which may or may not be a time you're feeling lonely.
If you have a lot of roommates, all you have to do is step outside your room and find someone to chat with.  No transportation CO2 emissions needed.  But more important, you know your roommates are always gonna be around.
But I Already Have Roommates
Even if you already have roommates, I think there's a good chance your roommate situation is under-optimized.  Given that you spend so much time with them, there's a lot of value in living with people you really connect with.  (Finding great coworkers makes sense for similar reasons.)
The layout of your house and the number of roommates you have can also make a big difference.  I used to have friends living in a 4-bedroom place where all the bedrooms opened directly into a single large common area.  If anyone else was outside their room, you'd immediately know it and have an opportunity for interaction.  Later I lived in an 8-bedroom place which felt far lonelier, even with every room occupied.  The house was laid out so it was easy to go about your day without ever running into a fellow roommate.  I also lived in a house with over 50 bedrooms for a while, which was wild &amp; a lot of fun.
But I Don't Want Roommates
One reason you might not want roommates is because you're worried you might have conflicting preferences for what living together should be like.  For example, my philosophy towards dirty dishes is to let them pile up on the counter and periodically stuff them all in the dishwasher, to be as time-efficient as possible.  Surprisingly, some people dislike this approach.
RoomieMatch.com is a website which tries to solve the roommate compatibility problem.  You create a profile by answering questions about dishes, food in the fridge, housecleaning, social events, noise, overnight guests, shared household items, walking around in your underwear, TV, etc.  In addition, there are questions to help predict how you well you will connect as people.
You Could Make a Lot of Money
RoomieMatch has two search options: free and cheap.  Cheap costs $20/year.
The problem with RoomieMatch is they're leaving a massive amount of money on the table.
A few years ago, a friend of mine was jobless &amp; struggling financially.  He was living in a 4-bedroom house at the time, and he was the primary contact with the landlord.  My friend took responsibility for vetting folks from Craigslist in order to fill the remaining rooms in the house.  He found that folks from Craigslist were willing to pay enough rent for the remaining 3 rooms that he was able to live rent-free until he found a job.
I acknowledge this is murky ethical territory, and I'm not condoning my friend's actions.  (I don't believe anyone ever found out or got upset, for whatever that's worth.)  The point I'm trying to make is that property management is way more lucrative than roommate matching.  RoomieMatch makes $20 per user per year at best.  My friend was making $100+ per user per month.
What I'm suggesting is that you take the full-stack startup playbook which has been successful in Silicon Valley recently, and apply it to online roommate matching + property management.
The extreme full-stack approach is to own your own properties.  Apparently the US has a surplus of big houses right now.
There are already players in this space such as Roam which are proving that people will pay for community.  (As if people paying extra to live in hip cities like SF &amp; NYC didn't prove that already.  BTW, I found that the awesome community at the Athena Hotel more than made up for the fact that it's in a non-hip city.)  Anyway, I think existing players are mostly pursuing the extreme full-stack option.  I actually think this is the wrong play.  You want to be a marketplace, like Airbnb (valued at over $30 billion).  The more people who are using your tool, the finer-grained roommate matching services you can provide.  It's hard to achieve massive scale if you have to own every property.  You want to be playing matchmaker for individuals with common interests who all happen to be looking for rooms around the same time, plus landlords with empty houses.  Maybe you'll want to undercut RoomieMatch, and provide free matching services for people who live in their properties, in order to achieve the necessary scale.  (RoomieMatch's existing scale is impressive by the way--I quickly got 100+ active, vetted matches in a midsize US city when I tried the tool.  If you have the money you might want to just buy it.)
So instead of buying properties, maybe you just want to contact people selling large homes &amp; see if you can convince them to let you manage their property.
Note that this is a good company to start if a recession happens, since people who currently live alone will be thinking about how to save on rent.
This Could Be Really Great
Most roommate search tools, like Craigslist, don't make it easy to figure out if a future roommate is someone you'd actually want to live with.  Imagine reaching a scale where you could match people based on factors like:


They love to play board games, or pool, or Super Smash Bros.


They want a compost pile and a garden in their backyard.


One has a pet, and the other likes animals but isn't yet ready to make a lifetime commitment.


They want a squat rack in the basement to save time &amp; money going to the gym.


They want to continue partying like college students after graduation.


They want to be part of an intentional community devoted to mutual improvement and life optimization, or spirituality, or whatever.


They want to share childcare responsibilities.


They're all fans of the same sports team.


They enjoy reading and discussing the same genre of novels, or watching the same movies.


They're musicians looking for people to jam with.


They want to live near hiking trails and go on group hikes together.


They want to do independent study of the same topic.


They're trying to eat a healthier diet.


They just moved to a new city and want friends they can explore the city with.


They have the same unusual work schedule.


One needs a caretaker, and the other wants to make extra money.


They like the idea of having a couch or two listed on CouchSurfing.


One knows a language the other wants to learn.


They work close together in the same expensive metropolitan area and want save on housing.  So they live in the outskirts of the city and commute together every day using the diamond lane.  One drives and the other pays for gas.


I also see opportunities to reduce friction in the current roommate matching process:


Automatically find times when everyone is available for a meet &amp; greet video call.


Let people take virtual tours of the houses on offer to minimize driving.


No need to worry about breaking a lease if someone moves to a different house in your company's network.  Let people try out a few communities &amp; see what works for them.  Use machine learning to improve your matching as you gather more data.


Provide external mediation in the event of roommate disputes, and have a reputation system to encourage good behavior.


You aren't providing housing as a service (like Airbnb), or companionship as a service (like the people-walking startup).  You're providing community as a service.  You could even organize mixers across your houses.
Conclusion
Technology has been blamed for the loneliness epidemic, but I think we can use technology to cure the loneliness epidemic as well.
I'm too busy being obsessed with machine learning to start any company which isn't mostly about that.  But I think this is a product the world needs, and I want you to build it.  I encourage you to sign the Founders Pledge and donate the money to effective charities in case you actually end up making billions of dollars as a result of reading this.
I apologize if you found the tone of this post overly sales-y.  My goal was to light a spark in the right person.  (Feel free to steal phrases from this post when pitching investors!)
Some folks in the rationalist community might be a little underwhelmed by this idea, since people in the rationalist community have been living together in group houses for a long time.  The thing is, finding roommates by connecting based on mutual interests via the internet is still kind of weird in the eyes of the general public.  As Paul Graham put it: "Live in the future, then build what's missing."  The existence of so many lonely people proves that this option is still missing for most people.
Anyway, if you're interested in building/investing in this, please comment below, or send me a private message via my user page with the country you're in and I'll put you in contact with others who message me.  (Edit: I might be slow to reply, sorry)
Cross-posted from the Effective Altruism Forum.  See also discussion on Hacker News.
</description><pubDate>Tue, 24 Mar 2020 12:22:01 UT</pubDate><guid>https://www.lesswrong.com/posts/FCXCXigp7byv2dM8D/how-to-make-billions-of-dollars-reducing-loneliness</guid></item><item><title>Understanding SAT by Implementing a Simple SAT Solver in Python</title><link>https://sahandsaba.com/understanding-sat-by-implementing-a-simple-sat-solver-in-python.html</link><description>
    
        
            Understanding SAT by Implementing a Simple SAT Solver in Python
        
    

    

    
      
Introduction
SAT is short for "satisfiability". Chances are you have heard of it or one of
its variants like 3-SAT in passing, especially in discussions of complexity and
NP-completeness. In this post, we will go into details of what it is all about,
why it is of such importance from both a theoretical and practical perspective,
and how to approach solving it by developing a simple Python SAT solver.  By
the end of this post, we will have a working SAT solver with a command-line
interface. The code for it is on GitHub: https://github.com/sahands/simple-sat. Feel free to fork and contribute
improvements. Of course, our implementation will not be anywhere close to more
complicated SAT solvers implemented in C or C++, such as miniSAT. The focus here is on simplicity since
the code is to be an introduction to SAT and SAT solvers.
Sections marked with * are more theoretical and not required for
understanding the algorithm we will use. On the other hand, the rest of the
introduction section below can be skipped if you already know the problem
definition and relevant technical terms.

Non-Technical Definitions &amp; Example
Before we start with the definitions, you might be asking why SAT is written in
all capitals if it is not an acronym.  Well, great question. SAT happens to
fall under what are called decision problems in computer science. What that
means is that the answer to a particular instance of the problem is either
"yes" or "no". Decision problems are often simply identified with the set of
inputs for which the answer is "yes", and that set is given a capitalized name.
For example, SAT is the set of all satisfiable CNF expressions, and PRIMES is
the set of all prime numbers (the decision problem in the latter is that of
primality; i.e. given the binary representation of number nn

, decide if
it is a prime or not). To go on a bit of a tangent, this is also the reason
that the title of the paper that introduced the AKS primality test ("PRIMES is
in P") is
not a silly grammar mistake; PRIMES is a set and the paper shows that it is in
P, which is the set of decision problems solvable in polynomial-time. This
naming style, as far as I know, is mainly due to Garey and Johnson's classic
textbook on complexity theory.
So, back to SAT. So far we mentioned that SAT is a decision problem, and
something about mysterious sounding "CNF expressions". Now, if you happen to
know your Boolean logic and already know all about satisfiability and CNF
expressions, then feel free to skip ahead to next section. The rest of this
section assumes no prior knowledge of logic. Like many other interesting
problems, there are a variety of ways of describing SAT, some more technical
and some less. Here I will provide a very non-technical description of the
problem that nonetheless is an accurate description.
Assume you are in charge of elections in a society. Elections in this society
work as follows: there are nn

 candidates, and any number of them, from
00

 (nobody) to nn

 (everybody) can be elected as the result of the
elections. Each voter provides a list of candidates they want elected and
candidates they want not elected. For example, if we call the candidates A, B
and C, then one vote might be "A, B, not C". We say a voter will be satisfied
with the results of the election if at least one of his/her preferences is met.
For example, the voter with the "A, B, not C" vote will be satisfied if either
A or B is elected, or if C is not elected. To be clear, that voter will be
happy even if nobody is elected (anarchy!) because one of the preferences is
"not C" which is met if we do not pick anyone. It's also possible to receive an
empty vote. We take this to mean that the voter will not be satisfied
regardless of who is elected.
You are given all the votes, and your job is to determine if all the voters can
be satisfied or not, and if yes, provide at least one possible pick of
candidates that would satisfy everybody.
We assume that each candidate is represented by a unique identifier that will
be a string in the input. For the votes, we will write just the string
representing candidate xx

 to indicate the voter wants the candidate
elected, and ∼x\sim{}x

 to indicate the voter wants xx

 not elected.
Let's look at an example. Assume the list of votes is given as follows, one per
line:

Then choosing to elect just candidates AA

 and CC

 but not
BB

 will satisfy all the voters. Take a moment to convince yourself that
no other choice of candidates (there are a total of 23=82^3 = 8


possibilities) can satisfy everyone. It is easy to see that in general
the search-space is of size 2n2^n

 where nn

 is the number of
candidates.


Technical Terminology
Now that the problem makes sense, let's define the technical
vocabulary. First, what we called "candidates" are called variables. The
variables in the above example are AA

, BB

 and CC

. A
variable can be assigned true or false. A literal is a variable or its
negation. For example AA

 and ∼A\sim A

 are literals.  Literals
without the ∼\sim

 are called positive, pure, or unnegated literals.
Literals with ∼\sim

 are called negated literals. A set of literals is
called a clause. An assignment is a mapping of variables to true or false.
For example, the assignment that satisfied the clauses in the previous example
was given by A=trueA = true

, B=falseB = false

 and C=trueC = true

. A clause
is satisfied by an assignment if at least one of its unnegated literals is
assigned true by the assignment, or one of its negated literals is assigned
false in the assignment.  It is customary, in logic notation, to separate the
literals in a clause using the ∨\vee

 symbol, read "or". For example, the
first clause above is written as A∨B ∨∼CA \vee B ~ \vee \sim C

 in mathematical
notation.
So SAT can be summarized as follows: given a list of clauses, determine if
there exists an assignment that satisfies all of them simultaneously.
It is also worthy of mention that there is a variation of SAT called 3-SAT with
the restriction that each clause consists of at most 3 (distinct) literals.
It can be shown with relative ease that SAT is in fact reducible to 3-SAT.



A Simple SAT Solver In Python
Even though SAT is NP-complete and therefore no known polynomial-time algorithm
for it is (yet) known, many improvements over the basic backtracking algorithms
have been made over the last few decades. However, here we will look at one of
the most basic yet relatively efficient algorithms for solving SAT. The encoding
and the algorithm are based on Knuth's SAT0W program which you can download
from his programs page.
The algorithm is a watch-list based backtracking algorithm. What makes the
watch-list based algorithms particularly simple, as we will see, is that very
little (practically nothing) needs to be done to "undo" steps taken when we
need to backtrack.

Parsing &amp; Encoding The Input
Before we can approach solving a SAT instance, we need to be able to represent
the instance in memory. Let's remember that a SAT instance is a set of clauses,
and each clause is a set of literals. Finally, a literal is a variable that is
either negated or not. Of course, we can just store the instance as a list of
clauses, with each clause being a list of strings that are the literals. The
problem with this approach is that we will not be able to quickly look up
variables, and checking to see if a literal is negated or not, and negating it
if not, would be rather slow string operations.
Instead, we will first assign a unique number, starting from 00

 and
counting up, to each variable as we encounter them, using a dictionary to keep
track of the mapping. So variables will be encoded as numbers 00

 to
n−1n-1

 where nn

 is the number of variables.  Then for an unnegated
literal with variable encoded as number xx

 we will encode the literal as
2x2x

, and the negated one will be 2x+12x + 1

. Then a clause will
simply be a list of numbers that are the encoded literals, and
Let's look at an example first. For this, let's see how the code that we will
look at in a minute behaves:
&gt;&gt;&gt; from satinstance import SATInstance
&gt;&gt;&gt; s = SATInstance()
&gt;&gt;&gt; s.parse_and_add_clause('A B ~C')
&gt;&gt;&gt; s.variables
['A', 'B', 'C']
&gt;&gt;&gt; s.variable_table
{'A': 0, 'C': 2, 'B': 1}
&gt;&gt;&gt; s.clauses
[(0, 2, 5)]

So as you see, the clause A∨B∨∼CA \vee B \vee \sim C

 is encoded as the tuple
(0, 2, 5) since variable AA

 is assigned number 00

, and hence
literal AA

 is 2⋅0=02 \cdot 0 =0

. On the other hand, ∼C\sim C

 is
encoded as 55

 since CC

 is assigned 22

 and hence
∼C\sim C

 is encoded as 2⋅2+1=52 \cdot 2 + 1 = 5

.
Why the funny encoding, you ask? Because it has a few advantages:

we can keep track of variables by keeping a list of length nn

, and of
literals by keeping a list of length 2n2n

,
checking to see if a literal is negated or not is simple: just do a bit-wise
AND with 11

, that is x &amp; 1 == 0,
looking up the variable in a literal is a matter of dividing by two, which is
the same as a bit-wise shift to the right, that is v = x &gt;&gt; 1,
switching a literal from negated to unnegated and back can be done by doing a
bit-wise XOR with the number one, that is negate(x) = x ^ 1,
and finally going from a variable to a literal can be done by doing a
bit-wise shift to the right (and a bit-wise OR with 1 if negated), that is
x = v &lt;&lt; 1 or x = v &lt;&lt; 1 | 1.

Notice that all of the above can be done using bit-wise operations which are
generally very fast to do. And since these operations will be happening an
exponential number of times, we will take any performance boost we can get.
With this, we are ready to write the code that takes care of reading an input
file and encoding the clauses. Here it is:
class SATInstance(object):
    def parse_and_add_clause(self, line):
        clause = []
        for literal in line.split():
            negated = 1 if literal.startswith('~') else 0
            variable = literal[negated:]
            if variable not in self.variable_table:
                self.variable_table[variable] = len(self.variables)
                self.variables.append(variable)
            encoded_literal = self.variable_table[variable] &lt;&lt; 1 | negated
            clause.append(encoded_literal)
        self.clauses.append(tuple(set(clause)))

    def __init__(self):
        self.variables = []
        self.variable_table = dict()
        self.clauses = []

    @classmethod
    def from_file(cls, file):
        instance = cls()
        for line in file:
            line = line.strip()
            if len(line) &gt; 0 and not line.startswith('#'):
                instance.parse_and_add_clause(line)
        return instance

    def literal_to_string(self, literal):
        s = '~' if literal &amp; 1 else ''
        return s + self.variables[literal &gt;&gt; 1]

    def clause_to_string(self, clause):
        return ' '.join(self.literal_to_string(l) for l in clause)

    def assignment_to_string(self, assignment, brief=False, starting_with=''):
        literals = []
        for a, v in ((a, v) for a, v in zip(assignment, self.variables)
                     if v.startswith(starting_with)):
            if a == 0 and not brief:
                literals.append('~' + v)
            elif a:
                literals.append(v)
        return ' '.join(literals)

As you can see, we also include methods here to decode variables, literals,
clauses, and assignments. These are used for outputting logging messages as
well as the final solutions.


Keeping Track Of The Assignment
Our algorithm will be a backtracking algorithm, in which we will assign true or
false to all the variables, starting from variable 00

 and going in order to
variable n−1n-1

. Of course, the basic search space is of size 2n2^n

 but by
pruning, we will not explore the whole space (usually anyway). The assignment
will be kept as a list of length nn

, with item at index ii

 being
None if neither true or false has been assigned variable ii

, and
00

 (false) or 11

 (true) otherwise, depending on the assignment.
When we backtrack, we set the corresponding item in the assignment list back to
None to indicate it is no longer assigned.


Watch-lists
Now that we have the encoding in place, and know how to keep track of the
assignment, let's look at the key idea of our algorithm. For each clause to be
satisfied, it needs to have at least one of its literals satisfied. As such, we
can make each clause watch one of its literals, and ensure that the following
invariant is maintained throughout our algorithm:

Invariant
All watched literals are either not assigned yet, or they have been
assigned true.

We then proceed to assign true or false to variables, starting from 00

 to
n−1n-1

. If we successfully assign true or false to every variable while
maintaining the above variant, then we have an assignment that satisfies every
clause.
To maintain this invariant, any time we assign true or false to a variable, we
ensure to update the watch-list accordingly. To do this efficiently, we need to
keep a list of clauses that are currently watching a given literal. This is
done in the code below using a list of length 2n2n

 of double-ended queue
(collections.deque), with each clause initially watching the first literal in
it. The function below takes care of this setting up of the watch-list:
def setup_watchlist(instance):
    watchlist = [deque() for __ in range(2 * len(instance.variables))]
    for clause in instance.clauses:
        # Make the clause watch its first literal
        watchlist[clause[0]].append(clause)
    return watchlist

Why double-ended queues instead of just a list? Short answer is that after
experimenting, I found out that double-ended queues provided the best
performance.
Back to the algorithm, whenever we assign true to a variable xx

 we must
make clauses watching ∼x\sim x

 watch something else. And
similarly, whenever we assign false to a variable xx

 we make clauses watching
xx

 watch something else. If we can not make a clause watch something, which
happens when all the other literals in a clause have already been assigned
false, then we know that the current assignment contradicts the clause, and we
stop and backtrack. We only need one clause to be contradicted to know not to
go any further. As such, the heart of our algorithm will be where we update the
watch-list after an assignment has been made. The Python function below, which
is in (watchlist.py), implements this part of the algorithm:
def update_watchlist(instance,
                     watchlist,
                     false_literal,
                     assignment,
                     verbose):
    """
    Updates the watch list after literal 'false_literal' was just assigned
    False, by making any clause watching false_literal watch something else.
    Returns False it is impossible to do so, meaning a clause is contradicted
    by the current assignment.
    """
    while watchlist[false_literal]:
        clause = watchlist[false_literal][0]
        found_alternative = False
        for alternative in clause:
            v = alternative &gt;&gt; 1
            a = alternative &amp; 1
            if assignment[v] is None or assignment[v] == a ^ 1:
                found_alternative = True
                del watchlist[false_literal][0]
                watchlist[alternative].append(clause)
                break

        if not found_alternative:
            if verbose:
                dump_watchlist(instance, watchlist)
                print('Current assignment: {}'.format(
                      instance.assignment_to_string(assignment)),
                      file=stderr)
                print('Clause {} contradicted.'.format(
                      instance.clause_to_string(clause)),
                      file=stderr)
            return False
    return True

So why the watch-list based approach? The main reason is the simplicity it
affords us. Since during a backtracking step, assignments only go from 00

 or
11

 to None, the watch-list does not need to be updated at all to maintain
the invariant. This means the backtracking step will simply be changing the
assignment of a variable back to None and that's it.


Putting It All Together
We are now ready to put it all together to get a simple recursive algorithm for
solving SAT. The steps are simple: try assigning 00

 to variable
dd

, update the watch-list, if successful, move on to variable
d+1d+1

. If not successful, try assigning 11

 to variable dd


and update the watch-list and continue to variable d+1d+1

. If neither
succeed, assign None to variable dd

 and backtrack. Here is the code:
def solve(instance, watchlist, assignment, d, verbose):
    """
    Recursively solve SAT by assigning to variables d, d+1, ..., n-1. Assumes
    variables 0, ..., d-1 are assigned so far. A generator for all the
    satisfying assignments is returned.
    """
    if d == len(instance.variables):
        yield assignment
        return

    for a in [0, 1]:
        if verbose:
            print('Trying {} = {}'.format(instance.variables[d], a),
                  file=stderr)
        assignment[d] = a
        if update_watchlist(instance,
                            watchlist,
                            (d &lt;&lt; 1) | a,
                            assignment,
                            verbose):
            for a in solve(instance, watchlist, assignment, d + 1, verbose):
                yield a

    assignment[d] = None



Making It Iterative *
For fun, let's see if we can implement the above algorithm without recursion.
This is in fact how Knuth implements the algorithm. (He seems to dislike
recursion, see for example this story on Quora.)
The basic idea here is to manually keep track of the current state of the
backtrack tree. When we use recursion, the state is kept implicitly using the
stack and which instruction is executing in each of the function calls. In the
iterative case, we will store the state using d which is the current depth
of the backtrack tree we are currently in, and also the variable we are to
assign to currently, and the state list which keeps track of which
assignments for each variable have been tried so far. Here is the code:
def solve(instance, watchlist, assignment, d, verbose):
    """
    Iteratively solve SAT by assigning to variables d, d+1, ..., n-1. Assumes
    variables 0, ..., d-1 are assigned so far. A generator for all the
    satisfying assignments is returned.
    """

    # The state list wil keep track of what values for which variables
    # we have tried so far. A value of 0 means nothing has been tried yet,
    # a value of 1 means False has been tried but not True, 2 means True but
    # not False, and 3 means both have been tried.
    n = len(instance.variables)
    state = [0] * n

    while True:
        if d == n:
            yield assignment
            d -= 1
            continue
        # Let's try assigning a value to v. Here would be the place to insert
        # heuristics of which value to try first.
        tried_something = False
        for a in [0, 1]:
            if (state[d] &gt;&gt; a) &amp; 1 == 0:
                if verbose:
                    print('Trying {} = {}'.format(instance.variables[d], a),
                          file=stderr)
                tried_something = True
                # Set the bit indicating a has been tried for d
                state[d] |= 1 &lt;&lt; a
                assignment[d] = a
                if not update_watchlist(instance, watchlist,
                                        d &lt;&lt; 1 | a,
                                        assignment,
                                        verbose):
                    assignment[d] = None
                else:
                    d += 1
                    break

        if not tried_something:
            if d == 0:
                # Can't backtrack further. No solutions.
                return
            else:
                # Backtrack
                state[d] = 0
                assignment[d] = None
                d -= 1




Theoretical and Practical Significance *
All right, so SAT is a cool problem, sure; possibly even useful. But why is
it given so much importance? The short answer is that many other problems,
often "difficult" problems, can be reduced to SAT. Let's consider an example
first, and then look at Stephen Cook's result that established SAT as the first
NP-complete problem, to get a sense of both practical applications of SAT, and
its theoretical importance.

Four Colouring *
You might have heard of the "four colour theorem". In simplest terms, it states
that the regions in any map can be coloured using at most four colours
such that no two neighbouring regions are coloured the same. See the Wikipedia
page on it for more
details.
This lends itself to a simple decision problem: given a map, is it possible to
colour it using 4 or less colours such that no two neighbouring regions are
the same colour? The four colour theorem is then true if and only if the answer
to this decision problem is always true (provided the input map meets the
requirements of a planar graph, a detail we are not too concerned with here).
As input, we will take the number of regions nn

, and assume the regions
are labelled using numbers 11

 to nn

, and a list of neighbouring
regions of the form {i,j}\{i, j\}

 with i̸=ji \ne j

, indicating regions
ii

 and jj

 are neighbours. Let us use colours red (R), blue (B),
green (G), and yellow (Y) to colour the regions. Our variables are going to be
RiR_i

, BiB_i

, GiG_i

 and YiY_i

, for 1≤i≤n1 \le i \le
n

, indicating that region ii

 is coloured red, blue, green, or yellow,
respectively.
Next, we need to construct the right set of clauses such that if all of
them are satisfied, then we have a proper colouring of the map. Specifically, we
need every region to be coloured, and we need no two neighbouring regions to be
the same colour. First, let us construct the clauses that will make sure every
region has one and only one colour assigned to it. For this, we need to make sure
only one of RiR_i

, BiB_i

, GiG_i

 or YiY_i

 is picked for
our assignment at a time. We can express this in terms of  KK

 clauses for
each region ii

. First, we add Ri∨Bi∨Gi∨YiR_i \vee B_i \vee G_i \vee Y_i

 as
a clause, which ensures that region ii

 gets at least one colour assigned
to it. Then for pair of colours, say RR

 and BB

, we add the clause
∼Ri∨∼Bi\sim R_i \vee \sim B_i

 which basically says "not both of RiR_i


and BiB_i

 can be picked at the same time", effectively making sure that
exactly one colour is assigned to each region. Finally, for any two
neighbouring regions, say ii

 and jj

, and each colour, say
RR

, we add the clause ∼Ri∨∼Rj\sim R_i \vee \sim R_j

 which says not both
of ii

 and jj

 can be coloured red.
Let's look at a very simple example. Suppose our map has only two regions,
regions 11

 and 22

 and that they are neighbours. Then our SAT
input would be:
# Assign at least one colour to region 1
R1 B1 G1 Y1

# But no more than one colour
~R1 ~B1
~R1 ~G1
~R1 ~Y1
~B1 ~G1
~B1 ~Y1
~G1 ~Y1

# Similarly for region 2
R2 B2 G2 Y2
~R2 ~B2
~R2 ~G2
~R2 ~Y2
~B2 ~G2
~B2 ~Y2
~G2 ~Y2

# Make sure regions 1 and 2 are not coloured the same since they are neighbours
~R1 ~R2
~B1 ~B2
~G1 ~G2
~Y1 ~Y2

Running this through our SAT solver gives:
$ python sat.py --brief --all &lt; tests/colouring/01.in
Y1 G2
Y1 B2
Y1 R2
G1 Y2
G1 B2
G1 R2
B1 Y2
B1 G2
B1 R2
R1 Y2
R1 G2
R1 B2

As you can see, there are many possible solutions, since in such a simple case
we have a valid colouring as long as we assign a different colour to each
region, which can be done in 4⋅3=124 \cdot 3 = 12

 ways, corresponding
precisely to the 1212

 solutions given by our SAT solver.
In the next section, we see that a much broader set of problems can be reduced
to SAT.
In general, the decision problem of the above example is known as graph
colouring, or GT4 in Garey-Johnson's naming, where given a graph and a number
kk

 the decision problem is to determine if a kk

-colouring for the
graph exists.  In the above, we had k=4.k=4.

 In this more general
definition, with nn

 regions, our reduction to SAT involves introducing
k⋅nk\cdot n

 variables and
1+n⋅(k2)+k⋅e
1 + n \cdot \binom{k}{2} + k \cdot e
clauses, where ee

 is the number of edges.  Since e=O(n2)e = O(n^2)

 (in
fact, e=O(n)e = O(n)

 for planar graphs), the number of variables and clauses
in our construction above are polynomials in nn

 and kk

.  Hence we
have a polynomial-time reduction to SAT.  The significance of this is discussed
further in the next section.


NP-Completeness Of SAT *
In previous section we saw how a problem regarding colouring of regions in a
map can be reduced to SAT. This can be further generalized to much larger class
of problems: any decision problem that can be decided in polynomial time using
a non-deterministic Turing machine can be reduced in polynomial time to
SAT. This was first proved in Stephen Cook's paper "The Complexity of
Theorem-Proving Procedures", which is the paper
that introduced the famous P = NP question as well. Let's go over the basic
idea in the paper very briefly here. If you are interested in more details,
make sure you have a look at the paper, as it is rather short and a pleasure to
read.
But before we go into detail, let us take a moment to discuss why it is of such
importance. First, nobody has yet come up with an efficient (polynomial time)
algorithm to solve SAT in its generality. (SAT with some restrictions, e.g.
2-SAT, can be solved efficiently though.) Showing that a problem can be reduced
to SAT means that if we find an efficient algorithm for SAT then we have found
an efficient algorithm for that problem as well. For example, if we find a
polynomial-time algorithm for SAT then we immediately have a polynomial-time
algorithm for the graph colouring problem given above.
Now, the class of decision problems that can be solved in polynomial-time using
a non-deterministic Turing machine is known as NP (which stands for
Non-deterministic Polynomial). This is a very large class of problems, since
Turing machines are one of the most general computational models we have, and
even though we are limited to polynomial-time Turing machines, the fact that
the Turing machine does not have to be deterministic allows us much more
freedom. Some examples of problems that are in NP are:

all problems in P, e.g. determining if a number is prime or not (PRIMES), and
decision versions of shortest path, network flow, etc.,
integer factorization,
graph colouring,
SAT,
and all NP-complete problems (see here for a rather
large list of examples).

A problem is said to be NP-complete if it, in addition to being in NP, also has
the property that any other problem in NP can be reduced to it in
polynomial-time. Cook's paper proved SAT to be NP-complete. In fact, since that
paper introduced the concept of NP-completeness, SAT was the first problem
to be proved NP-complete. Since then, many other problems have been shown to be
NP-complete, often by showing that SAT (or 3-SAT) can be reduced in
polynomial-time to those problems (converse of what we proved earlier for
graph colouring).
Now, as promised, let's briefly look at why SAT is NP-complete. For this, we
need to know more precisely what a Turing machine is. Unfortunately, this would
involve a bit more detail than I want to include in this section. So instead, I
am going to show that if a problem can be solved using a finite-state machine
(FSM) then it be reduced in polynomial-time to SAT. The case for Turing
machines, which are a generalizations of finite-state machines (Turing machines
are basically FSM's with the addition of a tape that they can read from and
write to), is quite similar, just more complicated. I encourage you to read
Cook's original paper for details of the proof with Turing machines.
First, let's define what an FSM is. In simplest terms, an FSM is a program that
has a finite number of states, and that when fed an input character, moves to
another state (or possibly stays in the same state) based on a fixed set of
rules.  Also, some states are taken as "accepting" states. Given an input
string, we feed the string character by character into the FSM, and if at the
end the FSM is in an accepting state, the answer to our decision problem is
yes. If not, the answer is no.
The below code shows how an FSM could can be implemented in Python. Note that
in this implementation, we are forced to have a deterministic FSM. Let's
ignore this detail for now though. This particular example implements an FSM
that accepts input strings that contain an even number of ones.
from __future__ import print_function


def even_ones(s):
    # Two states:
    # - 0 (even number of ones seen so far)
    # - 1 (odd number of ones seen so far)
    rules = {(0, '0'): 0,
             (0, '1'): 1,
             (1, '0'): 1,
             (1, '1'): 0}
    # There are 0 (which is an even number) ones in the empty
    # string so we start with state = 0.
    state = 0
    for c in s:
        state = rules[state, c]
    return state == 0


# Example usage:
s = "001100110"
print('Output for {} = {}'.format(s, even_ones(s)))

So the core of an FSM is a list of rules of the form (S,c)→T(S, c) \rightarrow
T

 which says if the FSM is in state SS

 and receives input character
cc

 then it goes to state TT

. If for any unique pair of (S,c)(S,
c)

 there is only one rule (S,c)→T(S, c) \rightarrow T

 then the FSM is said to
be deterministic. This is because the FSM will never need to make a "choice" as
to which of the rules to apply. With non-deterministic FSM's, the definition of
acceptance needs to be modified a bit: if any set of choices of rules would
get us to an accepting state given an input then the input is said to be
accepted. It is a well-established result in Automata theory that deterministic
and non-deterministic FSM's are computationally equally powerful, because any
non-deterministic FSM can be translated to an equivalent deterministic one by
the "powerset construction" method. The equivalent
deterministic FSM might have an exponentially larger number of states compared
to the non-deterministic one, however.
It is also well-known that FSM's can solve a class of problems known as
"regular" problems. What this means, in very simple terms, is that if you can
write a regular expression that would accept the "yes" instances of your
decision problem, then you can solve the problem using an FSM. In fact, regular
expressions are often implemented using FSM-like structures. The "compile"
phase of using regular expression is precisely when the regular expression
engine builds the FSM-like structure from your regular expression. (Exercise:
Find a regular expression that accepts the above language, namely binary
strings with an even number of ones.)
All right, so let's say a decision problem can be solved using an FSM with states
numbered 11

 to nn

. For simplicity, let's assume that our input
will be binary (character set is {0,1}\{0, 1\}

). Suppose the FSM has
kk

 rules given by (Si,ci)→Ti(S_i, c_i) \rightarrow T_i

, for 1≤i≤k1 \le i
\le k

. And assume the input characters are given by s1s_1

 to
sms_m

. So our input is of length mm

. Finally, assume that the
initial state is 11

 and accepting states are a1a_1

 to aqa_q

,
where qq

 is the number of accepting states.
Following Cook's footsteps, we will introduce the following variables for our
SAT reduction:

PtP_t

 which is true iff st=1s_t = 1

,
and QtiQ^i_t

 which is true iff the FSM is in state ii

 after input
character sts_t

 has been fed into the FSM, for 1≤i&lt;j≤n1 \le i &lt; j \le
n

 and 0≤t≤m0 \le t \le m

. We will take t=0t=0

 to be the starting step,
before anything has been fed into the FSM.

With these definitions, we proceed to translate the question of whether the
input is accepted by the FSM into an instance of SAT. The goal is to produce a
set of clauses that are satisfiable iff the FSM ends in an accepting state
given the particular input. The clauses that will accomplish this are:

PtP_t

 for 0≤t≤m0 \le t \le m

 such that st=1s_t = 1

 and
∼Pt\sim P_t

 for all other 1≤t≤m1 \le t \le m

. These will be the first
mm

 clauses, each consisting of a single literal.
QmajQ^{a_j}_{m}

 for 1≤j≤q1 \le j \le q

. This says that after the last
character is fed into the FSM, we want to be in one of the accepting states.
∼Qti∨∼Qtj\sim Q^i_t \vee \sim Q^j_t

 for any 1≤i&lt;j≤n1 \le i &lt; j \le n

 and
1≤t≤m1 \le t \le m

, which effectively says that the FSM can not be in both
states ii

 and jj

 at step tt

.  Collectively, these
clauses will ensure that the FSM is not in more than one state at a time.
Qt1∨…∨QtnQ^1_t \vee \ldots \vee Q^n_t

 for 1≤t≤m1 \le t \le m

. This says
that the FSM needs to be in at least one state at any step. Together with the
last set of clauses, we ensure that the FSM is in exactly one state at any
step.
∼Qt−1Si∨Pt∨QtTi\sim Q^{S_i}_{t-1} \vee P_t \vee Q^{T_i}_{t}

 for all rules (Si,0)→Ti(S_i,
0) \rightarrow T_i

 and ∼Qt−1Si∨∼Pt∨QtTi\sim Q^{S_i}_{t-1} \vee \sim P_t \vee
Q^{T_i}_{t}

 for all rules (Si,1)→Ti(S_i, 1) \rightarrow T_i

, for
1≤t≤m1 \le t \le m

. These clauses are logically equivalent to
"Qt−1SiQ^{S_i}_{t-1}

 and PtP_t

 implies QtTiQ^{T_i}_{t}

" which is
equivalent to (Si,1)→Ti(S_i, 1) \rightarrow T_i

.  In other words, they ensure
proper transition between states based on the input.
Finally, we want to start in the initial state so we add the
clause Q01Q^1_0

.

Let's see this in action for the above FSM which accepts strings with an even
number of ones in them. First, we have two states, so n=2n=2

. Let's build the
SAT instance to handle inputs of length tt

. Also note that we can leave out the
first set of clauses (the PtP_t

 and ∼Pt\sim P_t

 ones), in which case any SAT
assignment will give us some accepted input. Which means we can list all the
strings accepted by the FSM by looking at all the satisfying assignments of the
above set of clauses.
Here is an example for t=3t=3

. In this input QtiQ^i_t

 is written as
Qi-t.  The states are also labelled 00

 and 11

 instead of
11

 to nn

 in the above.
# No more than one state at each step
~Q0-0 ~Q1-0
~Q0-1 ~Q1-1
~Q0-2 ~Q1-2
~Q0-3 ~Q1-3

# At least one state in each step
Q0-0 Q1-0
Q0-1 Q1-1
Q0-2 Q1-2
Q0-3 Q1-3

# Add the rules
# (EVEN, 1) -&gt; ODD
~Q0-0  ~P1   Q1-1
~Q0-1  ~P2   Q1-2
~Q0-2  ~P3   Q1-3

# (ODD, 1) -&gt; EVEN
~Q1-0  ~P1   Q0-1
~Q1-1  ~P2   Q0-2
~Q1-2  ~P3   Q0-3

# (EVEN, 0) -&gt; EVEN
~Q0-0   P1   Q0-1
~Q0-1   P2   Q0-2
~Q0-2   P3   Q0-3

# (ODD, 0) -&gt; ODD
~Q1-0   P1   Q1-1
~Q1-1   P2   Q1-2
~Q1-2   P3   Q1-3

# Start in state 0
Q0-0
# End in an accepting state
Q0-3

Let's see the output of running a SAT solver on this, and another file for
t=3t=3

 and t=4t=4

:
$ python sat.py --all --starting_with P --brief &lt; tests/fsm/even-ones-3.in

P1 P3
P1 P2
P2 P3
$ python sat.py --all --starting_with P --brief &lt; tests/fsm/even-ones-4.in

P1 P4
P1 P3
P1 P2 P3 P4
P1 P2
P2 P4
P2 P3
P3 P4

As expected, all the possible ways of picking a subset of {P1,P2,P3}\{P1, P2, P3
\}

 with an even number of elements in them are listed above, and similarly for
{P1,P2,P3,P4}\{P1, P2, P3, P4 \}

, although not necessarily in any meaningful order.
(Notice that the empty lines are the empty subsets, which also have even
numbers of ones.)




    




        








</description><pubDate>Tue, 24 Mar 2020 12:22:09 UT</pubDate><guid>https://sahandsaba.com/understanding-sat-by-implementing-a-simple-sat-solver-in-python.html</guid></item><item><title>Clever Functional Design</title><link>https://ferd.ca/clever-functional-design.html</link><description>
        
            2019/08/24
        
        Clever Functional Design
        

One of the best bits of software design I recall participating in was something done a few years ago at Heroku. It's been long enough since then that I feel comfortable boasting about it here. It's one small piece of data-centred functional design, where thinking a bit harder about the problem at hand greatly simplified what would have been a more straightforward and obvious implementation choice in a mutable language.

The Feature


Heroku's routing stack had one very interesting feature it provided to all users: a router log. The router log contains fields such as the overall request time, the time it took to send the request, the time it took to send the body, heroku-specific error codes and so on. They're broader categories of interesting time span.

At the same time, Heroku engineers had internal logs for all the requests for which users had public logs. These internal logs contained more detailed information that more often made sense to display during specific debugging activities. They included information such as time spent parsing headers from the client, time to first response packet (calculating what would essentially be the time after which the whole request was sent, but before which the back-end application would respond), POSIX statuses detected by the proxy on socket issues, and so on.

During more intricate debugging, other values could be required from the logs, but adding them would need code changes. This information was maintained in what was essentially a monolithic proxy that contained both the business logic (what logs to create, which servers to route to, and so on) and the proxying logic (how to shuttle HTTP from point A to point B).

At some point during my Heroku days, we rewrote the entire routing stack to clearly divide the business concerns (routing and features for Heroku apps) and the proxying logic. The idea was to clean up deeply intertwined code, clarify and properly specify the proxying behaviour, and allow to reason about and change what we offered to customers without having to know all the innards of HTTP proxying logic to do so. This divide was successful, and eventually allowed us to open source the proxying logic: since it was no longer business related and was commodity infrastructure, vegur became public.

This division came with a few challenges though, and one of them was with the logs: how were we to take a Heroku feature, such as the router logs we had, with their own specific needs that could change according to business requirements, and bake them into a generic proxy library, where all the interesting measurements and samplings were to take place?

The Straightforward Design


The approach we took in the original router was to just take all the samples as we needed them, mostly as spans. You essentially just intersperse the logging and observational needs with the actual business end of the code, and do what you must.

One straightforward way to do this is with timestamps and might look something like this:

T1 = stamp(),
do_something(),
T2 = stamp(),
report_duration("some label", T2-T1)



You take a timestamp, do the thing, take a second timestamp, and report the difference as the duration of the operation. Eventually you get tired of doing this, and you might wrap them up in a helper that takes a closure (or wraps some object in OO design) and hides the reporting:

with_timestamp("some label", fun() -&gt; do_something() end)



Both approaches work fine. The latter offers slightly more encapsulation, and also prevents having overlapping timestamps where two measurements intersect. The logic is always set at the call-site, and things can be a bit tricky with error handling, but that's generally how you do it. The same kind of approach is still rather broadly used in distributed tracing with the addition of a context, which lets you define some lineage or nesting of operations:

%% approach 1
Ctx = new_span("some label"),
T1 = stamp(),
NewCtx = do_something(Ctx),
T2 = stamp(),
close_span(NewCtx)

%% approach 2
with_span("some_label", fun(Ctx) -&gt; do_something(Ctx) end)



Of course if you've got mutability going on and some global scope available, you'll cheat a bit and hide the span context within the program's state differently.

In any case, the old approach was based on these kinds of mechanism. When time came to split them up into their business and general parts, the tools used for logging needed to be decoupled as well. The general and straightforward approach to that is to do it through dependency injection. Our new approaches might now look something like this:

f(Logger) -&gt;
    T1 = stamp(),
    do_something(Logger),
    T2 = stamp(),
    Logger("some label", T2-T1)



Kind of similarly to passing the context or a span in the distributed tracing approach, you now parametrize each contract of dependent functions to take some contextual cues that explain how to do things.  It would have become a bit cumbersome to do it through all involved components of a proxying library, but it would have been possible to do it, and even more easily with tool or IDE support.

This, however, was the road we decided not to take.

The Weakness of the Straightforward Approach


The problem with the dependency injection approach, aside from its cumbersomeness, is that it did not sufficiently decouple what was a business concern from what was a generic library. Sure, we would hide and abstract away the tools chosen—which logging or tracing library would be used—but in no way would it really de-couple the design.

It would be tricky, for example, to properly track the concerns of "public user logs" and "internal engineer logs". The biggest design issue was something we uncovered by simply asking ourselves this question: if some other project were to use this library, would their reporting need to change every time Heroku decided to log new metrics?

Sure, the implementation could be independent. But the straightforward design only de-coupled the technical dependencies and which code was used. It did not get rid of the logical coupling that still existed between Heroku's business logic and the proxy's need to just shuttle HTTP. If we went with that approach, there was still a very deep dependency between both code bases. Heroku would unsurprisingly rely on the proxy, but it felt weird that the proxy's instrumentation would have to be defined by the Heroku product requirements.

Another slightly less important issue came from implementation details. I mention it not because it was a huge blocker to logical decoupling, but because this implementation detail ended up providing the key to the nicer design. The Vegur proxy had been written to use passive TCP sockets used in a blocking mode, because those were faster back in the day (implementation changes and optimizations within the BEAM VM have since then made this unnecessary). This, and other earlier design choices, made it so the proxy itself had 3 major moving parts:


an HTTP server parsing module, which would listen to incoming requests from the public Internet and parse them
an HTTP client parsing module, which would forward the traffic to a customer's back-end and listen to the responses
an inner loop that would use both the client and server bits and would handle the data transfer across both of them as a bridge.

This meant that some concerns we had in terms of metrics would sometimes reside all within one bit of code (i.e. the time it takes to parse headers is self-contained to any of the components), but sometimes it would cross boundaries. For example, knowing the time it took to parse the request body required taking measurements in the HTTP server, but which could be intertwined with operations taking place both in the inner loop and the HTTP client. It had no clear structural hierarchy.

Worse, some debugging scenarios required taking some measurements that started in the HTTP server, and finished in the HTTP client. That made it particularly difficult to localize and isolate concerns well, and the overall requirements of Heroku's reporting risked having a huge impact on the structure of the proxy.

Dependency injection would not be enough to fix this, we needed to think about the problem differently.

A Functional Design


Even in today's modern distributed tracing ecosystem, the design of most tracing libraries is deeply centered on the concept of a span: a pre-defined point A to point B period of time, which is written and instrumented as such in the code.

The big Eureka! moment for our approach in Vegur was in realizing that what when debugging, what we care about is picking arbitrary points on a timeline. Spans let you represent things a bit like this:
|------------------- Request --------------------|
 |--- Header Parsing ---|-- Body parsing --| ...
     |-- Cookie --|
        | ... |

Those are contiguous subdivisions of the whole timeline. What we wanted, instead, was a flat timeline on which we could pick arbitrary intervals:
request start                                          end of request
|                                                       |
| start header parsing                                  |
| |                first packet sent           ...      |
| |                |                             |      |
|-x--x--x------x---x----x------------------x-----x---...|
     |  |      |        |                  |
     |  |     ...      start body parsing  |
     | end cookie                     end body parsing
 start cookie

All these things happen in a continuum. The divisions of what we wanted to report were not structural to this timeline, they were views or selections of various points of interests, and a measurement between them. The "first packet sent" event is something that could be useful to multiple metrics:


time between the first packet received and the first packet sent ("how long we took to process the headers"
time between header parsing being done and sending the first packet ("how long we took to make a routing decision")
time between the first packet sent and the last header packet sent ("time to send the request headers")
time between the first packet sent and the last request packet sent ("time to send the full request")

and so on. Being able to report on all of these was context-dependent for the consumer, meaning that's usually a business concern. But the proxying library itself only cared about specific arbitrary points we thought could be useful to its technical users.

That distinction and approach as a timeline was the pivotal point we needed in the design. What the proxying library needed to do was not provide all the metrics and spans Heroku expected. What it needed to provide was the list of all important points on a timeline, whether they represented a singular event, or a duration. It would then be up to the consumer to report things however they wanted, whenever they wanted.

The flat timeline was particularly interesting for this because it is easily representable as an immutable data structure. If all you have is a bunch of local monotonic timestamps, all you need to do is maintain a local data structure that maintains sequences of labelled points in time: [{Label1, T1}, {Label2, T2}, ...]

Since timestamps are generally sortable locally—you need some fancy cheats to make it work in a distributed setting—then all the local timelines between the HTTP client, server, and inner loop modules could be maintained independently, but merged reliably: just sort by timestamp.
|-x----------------x---------------------------------...|

  |--x--x------x--------x------------------|

                   |—---------------------------x---...|

         |               |             |
         v               v             v

|-x--x--x------x---x----x------------------x----x----...|

This would let us write one generic well-defined data structure, use it wherever and whenever we needed it, and just merge them near the end of each timeline. No need to coordinate context-passing around, just a fetch and a merge once per item.

Then, the business end of code in Heroku's router could ask for that timeline once the request was ready to be logged, get one well-known data structure, and do as many selections for as many debugging reports as it required. If you wanted to send 15 logs out of there, it did not matter to the proxy library. Just analyze the timeline, generate what you need, and that's it.

Interestingly, since the final data structure could be represented easily in the base types of the language, Heroku's router was able to create its own compatible timeline that itself could be cast and merged with the proxy's timeline, without having them actually share the implementation (which would also have been fine). This would later let us augment the proxying logs with all the routing and business decisions for all kinds of debugging purposes (how much time would we spend queued to find an available back-end to route to?). This turned into app-specific routing flags that could allow to do deeper introspection of routing logic for specific applications, at nearly no overhead in code.

Lesson Learned


The approach itself here is mildly interesting. It has some intriguing implications in the context of designing implementations of distributed tracing libraries. The implementation is so straightforward in Vegur that I didn't spend the time to describe it here.

The true bigger lesson here is in systems design. It relates to functional, immutable, and declarative approaches to structuring communication flows.

The straightforward answer to our decoupling issue was to respond to the technical concern: just make it so the dependency does not know about the libraries used by its parent. I think it would have strictly speaking solved the most blocking problem in getting the code to build.  This would have been easy to do, and the only thing that made it annoying was the fact we were using a functional programming language with no mutable data structures nor global shared context. But satisfying the compiler is not enough to make for good design. This approach would have made it hard to get maintainable code given implementation details, and did not remove any logical coupling.

Rather than dismissing this challenge as "a bad fit for functional programming", it is what led to a better solution: re-think the data structure, gain better insights in the distinction between how the data is produced and how the data is consumed. Take that separation, and make it explicit. Build around it. Turn it into a data contract. This, in turn, lets you more transparently change either ends. You might need to add new measurement points in the producer-side when the consumer needs it, but the properly declared abstraction makes it so the other consumers will not be effected by the change.

The end result was a purely functional data structure that was mergeable, testable, and in line with functional design, but that's just a technical aspect of the result: it was the structural constraint of already being in an immutable context that prompted the cleaner design. Most challenges of working in a functional, declarative, or immutable language are not necessarily due to the language itself. They come from being thrown in a context where easier shortcuts are not as practical as we are used for them to be, and having to re-think both our problem and our solutions.

    </description><pubDate>Tue, 24 Mar 2020 12:22:15 UT</pubDate><guid>https://ferd.ca/clever-functional-design.html</guid></item><item><title>Natural Language Processing: the age of Transformers</title><link>https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/</link><description>

        

            

            
            

            
                    This article is the first installment of a two-post series on Building a machine reading comprehension system using the latest advances in deep learning for NLP. Stay tuned for the second part, where we'll introduce a pre-trained model called BERT that will take your NLP projects to the next level!In the recent past, if you specialized in natural language processing (NLP), there may have been times when you felt a little jealous of your colleagues working in computer vision. It seemed as if they had all the fun: the annual ImageNet classification challenge, Neural Style Transfer, Generative Adversarial Networks, to name a few. At last, the dry spell is over, and the NLP revolution is well underway! It would be fair to say that the turning point was 2017, when the Transformer network was introduced in Google's Attention is all you need paper. Multiple further advances followed since then, one of the most important ones being BERT - the subject of our next article.To lay the groundwork for the Transformer discussion, let's start by looking at one of the common categories of NLP tasks: the sequence to sequence (seq2seq) problems. They are pretty much exactly what their name suggests: both the inputs and the outputs of a seq2seq task are sequences. In the context of NLP, there are typicaly additional restrictions put in place:

The elements of the sequence are tokens corresponding to some set vocabulary (often including an Unknown token for the out-of-vocabulary words)
The order inside the sequence matters.

Next we shall take a moment to remember the fallen heros, without whom we would not be where we are today. I am, of course, referring to the RNNs - Recurrent Neural Networks, a concept that became almost synonymous with NLP in the deep learning field.
1. The predecessor to Transformers: the RNN Encoder-Decoder
This story takes us all the way back to 2014 (Ref, another Ref), when the idea of approaching seq2seq problems via two Recurrent Neural Networks combined into an Encoder-Decoder model, was born. Let's demonstrate this architecture on a simple example from the Machine Translation task. Take a French-English sentence pair, where the input is "je suis étudiant" and the output "I am a student". First, "je" (or, most likely, a word embedding for the token representing "je"), often accompanied by a constant vector hE0 which could be either learned or fixed, gets fed into the Encoder RNN. This results in the output vector hE1 (hidden state 1), which serves as the next input for the Encoder RNN, together with the second element in the input sequence "suis". The output of this operation, hE2, and "étudiant" are again fed into the Encoder, producing the last Encoded hidden state for this training sample, hE3. The hE3 vector is dependent on all of the tokens inside the input sequence, so the idea is that it should represent the meaning of the entire phrase. For this reason it is also referred to as the context vector. The context vector is the first input to the Decoder RNN, which should then generate the first element of the output sequence "I" (in reality, the last layer of the Decoder is typically a softmax, but for simplicity we can just keep the most likely element at the end of every Decoder step). Additionally, the Decoder RNN produces a hidden state hD1. We feed hD1 and the previous output I back into the Decoder to hopefully get "am" as our second output. This process of generating and feeding outputs back into the Decoder continues until we produce an &lt;EOS&gt; - the end of the sentence token, which signifies that our job here is done.
The RNN Encoder-Decoder model in action. To avoid any confusion, there is something that I would like to draw your attention to. The multiple RNN blocks appear in the Figure because of the multiple elements of the sequence that get fed into / generated by the networks, but make no mistake - there is only one Encoder RNN and one Decoder RNN at play here. It may help to think of the repeated blocks as the same RNN at different timesteps, or as multiple RNNs with shared weights, that are envoked one after another.This architecture may seem simple (especially until we sit down to actually write the code with LSTMs or GRUs thrown in for good measure), but it actually turns out to be remarkably effective for many NLP tasks. In fact, Google Translate has been using it under the hood since 2016. However, the RNN Encoder-Decoder models do suffer from certain drawbacks:1a. First problem with RNNs: Attention to the rescueThe RNN approach as described above does not work particularly well for longer sentences. Think about it: the meaning of the entire input sequence is expected to be captured by a single context vector with fixed dimensionality. This could work well enough for "Je suis étudiant", but what if your input looks more like this:"It was a wrong number that started it, the telephone ringing three times in the dead of night, and the voice on the other end asking for someone he was not."Good luck encoding that into a context vector! However, there turns out to be a solution, known as the Attention mechanism.Schematics of (left) a conventional RNN Encoder-Decoder and (right) an RNN Encoder-Decoder with AttentionThe basic idea behind Attention is simple: instead of passing only the last hidden state (the context vector) to the Decoder, we give it all the hidden states that come out of the Encoder. In our example that would mean hE1, hE2 and hE3. The Decoder will determine which of them gets attended to (i.e., where to pay attention) via a softmax layer. Apart from adding this additional structure, the basic RNN Encoder-Decoder architecture remains the same, yet the resulting model performs much better when it comes to longer input sequences.
1b. Second problem with Recurrent NNs: they are (surprise!) Recurrent
The other problem plaguing RNNs has to do with the R inside the name: the computation in a Recurrent neural network is, by definition, sequential. What does this property entail? A sequential computation cannot be parallelized, since we have to wait for the previous step to finish before we move on to the next one. This lengthens both the training time, and the time it takes to run inference.
One of the ways around the sequential dilemma is to use Convolutional neural networks (CNNs) instead of RNNs. This approach has seen its share of success, until it got outshone by the &lt;drumroll&gt; ...
2. Attention is All You Need (c) Google, 2017
The Transformer architecture was introduced in the paper whose title is worthy of that of a self-help book: Attention is All You Need. Again, another self-descriptive heading: the authors literally take the RNN Encoder-Decoder model with Attention, and throw away the RNN. Attention is all you need! Well, it ends up being quite a bit more complicated than that in practice, but that is the basic premise.
How does this work? To start with, each pre-processed (more on that later) element of the input sequence wi gets fed as input to the Encoder network - this is done in parallel, unlike the RNNs. The Encoder has multiple layers (e.g. in the original Transformer paper their number is six). Let us use hi to label the final hidden state of the last Encoder layer for each wi. The Decoder also contains multiple layers - typically, the number is equal to that of the Encoder. All of the hidden states hi will now be fed as inputs to each of the six layers of the Decoder. If this looks familiar to you, it is for a good reason: this is the Transformer's Encoder-Decoder Attention, which is rather similar in spirit to the Attention mechanism that we discussed above. Before we move on to how the Transformer's Attention is implemented, let's discuss the preprocessing layers (present in both the Encoder and the Decoder as we'll see later).
There are two parts to preprocessing: first, there is the familiar word embedding, a staple in most modern NLP models. These word embeddings could be learned during training, or one could use one of the existing pre-trained embeddings. There is, however, a second part that is specific to the Transformer architecture. So far, no where have we provided any information on the order of the elements inside the sequence. How can this be done in the absence of the sequential RNN architecture? Well, we have the positions, let's encode them inside vectors, just as we embedded the meaning of the word tokens with word embeddings. The resulting post-processed vectors, carrying information about both the word's meaning and its position in the sentence, are passed on to the Encoder and Decoder layers.
An Encoder with two layers, processing a three element input sequence (w1, w2, and w3) in parallel. Each input element's Encoder also receives information about the other elements via its Self-Attention sublayers, allowing the relationships between words in the sentence to be captured.2a. Attention, the linear algebra prospective
I come from a quantum physics background, where vectors are a person's best friend (at times, quite literally), but if you prefer a non linear algebra explanation of the Attention mechanism, I highly recommend checking out The Illustrated Transformer by Jay Alammar.
Let's use X to label the vector space of our inputs to the Attention layer. What we want to learn during training are three embedding matrices, WK, WV and WQ, which will permit us to go from X to three new spaces: K (keys), V (values) and Q (queries):
K = X WK       V = X WV       Q = X WQ
The way that these embedded vectors are then used in the Encoder-Decoder Attention is the following. We take a Q vector (a query, i.e., we specify the kind of information that we want to attend to) from the Decoder. Additionally, we take vectors V (values) that we can think of as something similar to linear combinations of vectors X coming from the Encoder (do not take "linear combination" literally however, as the dimensionality of X and V is, in general, different). Vectors K are also taken from the Encoder: each key Kn indexes the kind of information that is captured by the value Vn.
To determine which values should get the most attention, we take the dot product of the Decoder's query Q with all of the Encoder's keys K. The softmax of the result will give the weights of the respective values V (the larger the weight, the greater the attention). Such mechanism is known as the Dot-product attention, given by the following formula:
where one can optionally divide the dot product of Q and K by the dimensionality of key vectors dk. To give you an idea for the kind of dimensions used in practice, the Transformer introduced in Attention is all you need has dq=dk=dv=64 whereas what I refer to as X is 512-dimensional.
2b. What is new: Self-Attention
In addition to the Encoder-Decoder Attention, the Transformer architecture includes the Encoder Self-Attention and the Decoder Self-Attention. These are calculated in the same dot-product manner as discussed above, with one crucial difference: for self-attention, all three types of vectors (K, V, and Q) come from the same network. This also means that all three are associated with the elements of the same sequence (input for the Encoder and output for the Decoder). The purpose of introducing self-attention is to learn the relationships between different words in the sentence (this function used to be fulfilled by the sequential RNN). One way of looking at it is a representation of each element of the sequence as a weighted sum of the other elements in the sequence. Why bother? Consider the following two phrases:
1. The animal did not cross the road because it was too tired.
2. The animal did not cross the road because it was too wide.
Clearly, it is most closely related to the animal in the first phrase and the road in the second one: information that would be missing if we were to use a uni-directional forward RNN! In fact, the Encoder Self-Attention, that is bi-directional by design, is a crucial part of BERT, the pre-trained contextual word embeddings, that we shall discuss later on.
Where are the calculations for the Encoder Self-Attention carried out? Turns out, inside every Encoder layer. This permits the network to pay attention to relevant parts of the input sequence at different levels of abstraction: the values V of the lower Encoder layers will be closest to the original input tokens, whereas Self-Attention of the deeper layers will involve more abstract constructions.
2c. Putting it all together
By now we have established that Transformers discard the sequential nature of RNNs and process the sequence elements in parallel instead. We saw how the Encoder Self-Attention allows the elements of the input sequence to be processed separately while retaining each other's context, whereas the Encoder-Decoder Attention passes all of them to the next step: generating the output sequence with the Decoder. What happens at this stage may not be so clear. As you recall, the RNN Encoder-Decoder generates the output sequence one element at a time. The previously generated output gets fed into the Decoder at the subsequent timestep. Do Transformers really find a way to free us from the sequential nature of this process and somehow generate the whole output sequence at once? Well - yes and no. More precisely, the answer is [roughly] yes when training, and no at inference time.
The Transformer architecture featuting a two-layer Encoder / Decoder. The Encoder processes all three elements of the input sequence (w1, w2, and w3) in parallel, whereas the Decoder generates each element sequentially (only timesteps 0 and 1, where the output sequence elements v1 and v2 are generated, are depicted). Output token generation continues until an end of the sentence token &lt;EOS&gt; appears.The inputs to the Decoder come in two varieties: the hidden states that are outputs of the Encoder (these are used for the Encoder-Decoder Attention within each Decoder layer) and the previously generated tokens of the output sequence (for the Decoder Self-Attention, also computed at each Decoder layer). Since during the training phase, the output sequences are already available, one can perform all the different timesteps of the Decoding process in parallel by masking (replacing with zeroes) the appropriate parts of the "previously generated" output sequences. This masking results in the Decoder Self-Attention being uni-directional, as opposed to the Encoder one. Finally, at inference time, the output elements are generated one by one in a sequential manner.
Some final remarks before we call it a day:
The part of the Decoder that I refer to as postprocessing in the Figure above is similar to what one would typically find in the RNN Decoder for an NLP task: a fully connected (FC) layer, which follows the RNN that extracted certain features from the network's inputs, and a softmax layer on top of the FC one that will assign probabilities to each of the tokens in the model's vocabularly being the next element in the output sequence. At that point, we could use a beam search algorithm to keep the top few predictions at each step and choose the most likely output sequence at the end, or simply keep the top choice each time.
The Transformer architecture is the driving force behind many of the recent  breakthroughs in the field of NLP. To put some hard numbers on that statement, lets turn to a metric called BLEU, commongly used to evaluate the quality of machine translations. The original Transformer achieved a score of 28.4 BLEU on an English-to-German translation task, and if that does not tell you much, suffices to say that it was better than the exisiting best result by over 2 BLEU!Next, in the coming blog post we will discuss BERT (Bidirectional Encoder Representations from Transformers): contextualized word embeddings based on the Transformer (more precisely, Transformer's Encoder), and how to train a BERT-based machine reading comprehension model on the Scaleway GPU instances.
                


            

        

    </description><pubDate>Tue, 24 Mar 2020 12:22:26 UT</pubDate><guid>https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/</guid></item><item><title>Write Fuzzable Code &amp;#8211; Embedded in Academia</title><link>https://blog.regehr.org/archives/1687</link><description>
		

			

		
	
	
		Fuzzing is sort of a superpower for locating vulnerabilities and other software defects, but it is often used to find problems baked deeply into already-deployed code. Fuzzing should be done earlier, and moreover developers should spend some effort making their code more amenable to being fuzzed. 
This post is a non-comprehensive, non-orthogonal list of ways that you can write code that fuzzes better. Throughout, I’ll use “fuzzer” to refer to basically any kind of randomized test-case generator, whether mutation-based (afl, libFuzzer, etc.) or generative (jsfunfuzz, Csmith, etc.). Not all advice will apply to every situation, but a lot of it is sound software engineering advice in general. I’ve bold-faced a few points that I think are particularly important.
Invest in Oracles
A test oracle decides whether a test case triggered a bug or not. By default, the only oracle available to a fuzzer like afl is provided by the OS’s page protection mechanism. In other words, it detects only crashes. We can do much better than this.
Assertions and their compiler-inserted friends — sanitizer checks — are another excellent kind of oracle. You should fuzz using as many of these checks as possible. Beyond these easy oracles, many more possibilities exist, such as: 

function-inverse pairs: does a parse-print loop, compress-decompress loop, encrypt-decrypt loop, or similar, work as expected?
differential: do two different implementations, or modes of the same implementation, show the same behavior?
metamorphic: does the system show the same behavior when a test case is modified in a semantics-preserving way, such as adding a layer of parentheses to an expression?
resource: does the system consume a reasonable amount of time, memory, etc. when processing an input?
domain specific: for example, is a lossily-compressed image sufficiently visually similar to its uncompressed version?

Strong oracles are worth their weight in gold, since they tend to find application-level logic errors rather than the lower-level bugs that are typically caught by looking for things like array bounds violations.
I wrote a bit more about this topic a few years ago. Finally, a twitter user suggested “If you’re testing a parser, poke at the object it returns, don’t just check if it parses.” This is good advice.
Interpose on I/O and State
Stateless code is easier to fuzz. Beyond that, you will want APIs for taking control of state and for interposing on I/O. For example, if your program asks the OS for the number of cores, the current date, or the amount of disk space remaining, you should provide a documented method for setting these values. It’s not that we necessarily want to randomly change the number of cores, but rather that we might want to fuzz our code when set to single-core mode and then separately fuzz it in 128-core mode. Important special cases of taking control of state and I/O include making it easy to reset the state (to support persistent-mode fuzzing) and avoiding hidden inputs that lead to non-deterministic execution. We want as much determinism as possible while fuzzing our code.
Avoid or Control Fuzzer Blockers
Fuzzer blockers are things that make fuzzing gratuitously difficult. The canonical fuzzer blocker is a checksum included somewhere in the input: the random changes made to the input by a mutation-based fuzzer will tend to cause the checksum to not validate, resulting in very poor code coverage. There are basically two solutions. First, turn off checksum validation in builds intended for fuzzing. Second, have the fuzzer generate inputs with valid checksums. A generation-based fuzzer will have this built in; with a mutation-based fuzzer we would write a little utility to patch up the test case with a valid checksum after it is generated and before it is passed to the program being fuzzed. afl has support for this.
Beyond checksums, hard-to-satisfy validity properties over the input can be a serious problem. For example, if you are fuzzing a compiler for a strongly typed programming language, blind mutation of compiler inputs may not result in valid compiler inputs very often. I like to think of validity constraints as being either soft (invalid inputs waste time, but are otherwise harmless) or hard (the system behaves arbitrarily when processing an invalid input, so they must be avoided for fuzzing to work at all). When we fuzz a C++ compiler to look for wrong code bugs, we face a hard validity constraint because compiler inputs that have UB will look like wrong code bugs. There is no simple, general-purpose solution to this kind of problem, but rather a family of techniques for explicitly taking validity properties into account. The most obvious solution — but often not the right one — is to write a new generational fuzzer. The problem is that if you do this, you cannot take advantage of modern coverage-driven fuzzing techniques, which are amazing. To fit into a coverage-driven fuzzing framework you have a couple of options. First, write a custom mutator that respects your validity constraints. Second, structure-aware fuzzing, which basically means taking the mutated data from the fuzzer and translating it into something like what the program being fuzzed expects to see. There’s a lot of research left to be done in making coverage-driven fuzzers work well in the presence of validity constraints without requiring a lot of manual effort. There are some significant subtleties here, maybe I’ll go into them another time. Putting something like a SAT solver into the fuzzer is not, generally speaking, the answer here, first because some validity constraints like checksums are specifically difficult for solvers, and second because some validity constraints (such as UB-freedom in a C++ program) are implicit and cannot be inferred, even in principle, by looking at the system being fuzzed.
A lot of code in a typical system cannot be fuzzed effectively by feeding input to public APIs because access is blocked by other code in the system. For example, if you use a custom memory allocator or hash table implementation, then fuzzing at the application level probably does not result in especially effective fuzzing of the allocator or hash table. These kinds of APIs should be exposed to direct fuzzing. There is a strong synergy between unit testing and fuzzing: if one of these is possible and desirable, then the other one probably is too. You typically want to do both.
Sanitizers and fuzzers often require tweaks or even significant changes to the build process. To make this easier, keep the build process as clean and simple as possible. Make it easy to switch out the compiler and modify the compiler options. Depend on specific tools (and versions of tools) as little as possible. Routinely build and test your code with multiple compilers. Document special build system requirements.
Finally, some fuzzer blockers are sort of silly and easy to avoid. If your code leaks memory or terminates its process with a deep call stack, it will be painful to test using a persistent-mode fuzzer, so don’t do these things. Avoid handling SIGSEGV or, if you really must do this, have a way to disable the handler for fuzzer builds. If your code is not compatible with ASan or UBSan, then these extremely useful oracles are harder to use. In particular, if your code uses a custom memory allocator you should consider turning it off for fuzzer builds, or else adapting it to work with ASan, or else you’ll miss important bugs.
Unblock Coverage-Driven Fuzzers
Because coverage-driven fuzzers refocus their effort to try to hit uncovered branches, they can be blocked in certain specific ways. For example, if a coverage-driven fuzzer is presented with too many uncoverable branches, it can spend so much time on them that it becomes less likely to hit more interesting branches elsewhere in the program. For example, one time I compared afl’s coverage on a program compiled with and without UBSan, and found that (in whatever time limit I used) it covered quite a lot less of the sanitized program, compared to the unsanitized build. On the other hand, we definitely want our fuzzer to look for sanitizer failures.  My advice is to fuzz both sanitized and unsanitized builds of your program. I don’t know how to budget fuzzing resources for these different activities and don’t know of any principled work on that problem. It may not matter that much, since fuzzing is all about overkill.
Sometimes your program will call branchy, already-heavily-fuzzed code early in its execution. For example, you might decompress or decrypt input before processing it. This is likely to distract the coverage-driven fuzzer, causing it to spend a lot of time trying to fuzz the crypto or compression library. If you don’t want to do this, provide a way to disable crypto or compression during fuzzing.
Any interpreter in your program is likely to make life difficult for a coverage-driven fuzzer, since the relevant program paths are now encoded in the data being interpreted, which is generally opaque to the fuzzer. If you want maximum mileage out of coverage-driven fuzzers, you may want to try to avoid writing interpreters, or at least keep them extremely simple. An obvious way to deal with embedded interpreters — which someone must have thought of and tried, but I don’t have any pointers — would be to have an API for teaching the fuzzer how to see coverage of the language being interpreted.
Enable High Fuzzing Throughput
Fuzzing is most effective when throughput is very high; this seems particularly the case for feedback-driven fuzzers that may take a while to learn how to hit difficult coverage targets. An easy throughput hack is to make it possible to disable slow code (detailed logging, for example) when it is not germane to the fuzzing task. Similarly, interposing on I/O can help us avoid speed hacks such as running the fuzzer in a ramdisk.
“But I Want Fuzzing My Code to be Harder, Not Easier”
I don’t have a lot of sympathy for this point of view. Instead of aiming for security through obscurity, we would do better to:

fuzz early and thoroughly, eliminating fuzzable defects before releasing code into the wild
write code in a programming language with as strong of a type system as we are willing to tolerate — this will statically eliminate classes of bad program behaviors, for example by stopping us from putting the wrong kind of thing into a hashmap
aggressively use assertions and sanitizers to get dynamic checking for properties that the type system can’t enforce statically

Anti-fuzzing techniques are a thing, but I don’t think it represents a useful kind of progress towards better software. 
Conclusion
Randomized testing is incredibly powerful and there’s no point avoiding it: if you don’t fuzz your code, someone else will. This piece has described some ways for you, the software developer, to make fuzzing work better. Of course there are plenty of other aspects, such as choosing a good corpus and writing a good fuzzer driver, that are not covered here.
Acknowledgments: Pascal Cuoq and Alex Groce provided feedback on a draft of this piece, and it also benefited from suggestions I received on Twitter. You can read the conversation here; it contains some suggestions and nuances that I did not manage to capture.
	

	

				


	


		
	</description><pubDate>Tue, 24 Mar 2020 12:24:29 UT</pubDate><guid>https://blog.regehr.org/archives/1687</guid></item><item><title>Full Page Reload</title><link>https://spectrum.ieee.org/the-institute/ieee-member-news/educational-resources-that-get-students-up-to-speed-on-advanced-manufacturing-and-programming-languages</link><description>
		
				
					Join IEEE
					|
					IEEE.org
					|
					IEEE Xplore Digital Library
					|
					IEEE Standards
					|
					IEEE Spectrum
					|
					More Sites
					
				

				
				
					
					
					
						
							
							
								
									
										Create
											Account
									
											
								
								|
								
									
									Sign In
									
									
									
							
							
						
					
					
				
		

             




			

 
	

	




 





	
	
	
	

 
 
 



	






			


</description><pubDate>Tue, 24 Mar 2020 12:27:19 UT</pubDate><guid>https://spectrum.ieee.org/the-institute/ieee-member-news/educational-resources-that-get-students-up-to-speed-on-advanced-manufacturing-and-programming-languages</guid></item><item><title>How we reduced deployment times by 95% | Plaid</title><link>https://blog.plaid.com/how-we-reduced-deployment-times-by-95/</link><description>As Plaid grows, so does the scale of our infrastructure. We currently run over 20 internal services and deploy over 50 code commits per day across our core services. Minimizing deployment time is therefore of vital importance to maximizing our iteration velocity. A fast deployment process allows us to rapidly ship bug fixes and run a smooth continuously deployed system.A couple months ago, we noticed that slow deploys of our bank integration service were affecting our team's ability to ship code. Engineers would spend at least 30 minutes building, deploying, and monitoring their changes through multiple staging and production environments, which consumed a lot of valuable engineering time. This became increasingly unacceptable as the team grew larger and we shipped more code daily.While we had plans to implement long-term improvements like moving our Amazon ECS-based service infrastructure onto Kubernetes, a fix was warranted to increase our iteration speed in the short-term. We set out to score a quick win by implementing a custom "fast deployments" mechanism.High latency in Amazon ECS deploysOur bank integration service consists of 4,000 Node.js processes running on dedicated docker containers managed and deployed on ECS, Amazon’s container orchestration service. After profiling our deployment process, we narrowed down the increased deployment latencies to three distinct components:Starting up tasks incurs latency. In addition to application startup time, there is also latency from the ECS health check, which determines when containers are ready to start handling traffic. The three parameters that control this process are interval, retries, and startPeriod. Without careful health check tuning, containers can be stuck in the "starting" state even after they're ready to serve traffic.Shutting down tasks incurs latency. When we run an ECS Service Update, a SIGTERM signal is sent to all our running containers. To handle this, we have some logic in our application code to drain any extant resources before completely shutting down the service.The rate at which we can start tasks restricts the parallelism of our deploy. Despite us setting the MaximumPercent parameter to 200%, the ECS start-task API call has a hard limit of 10 tasks per call, and it is rate-limited. We need to call it 400 times to place all our containers in production.Approaches exploredWe considered and experimented with a few different potential solutions to chip away at the global objective:Reduce the total number of containers running in production. This was certainly feasible, but it involved a significant overhaul of our service architecture in order for it to handle the same request throughput, and more research needed to be done before such a change could be made.Tweak our ECS configuration by modifying the health check parameters. We experimented with tightening the health check by reducing the interval and startPeriod values, but ECS would then erroneously mark healthy containers as unhealthy when they started, causing our service to never fully stabilize at 100% health. Iterating on these parameters was a slow and arduous process due to the root issue, slow ECS deployments.Spin up more instances in the ECS cluster so that we can start more tasks simultaneously during a deployment. This worked to reduce deploy times, but not by very much. It also isn’t cost-effective in the long run.Optimize service restart time by refactoring initialization and shutdown logic. We were able to shave around 5 seconds per container with a few minor changes.Although these changes improved the overall deploy time by a few minutes, we still needed to improve the timing by at least an order of magnitude for us to consider the problem solved. This would require a fundamentally different solution.Preliminary solution: utilizing the node require cache to “hot reload” application codeThe Node require cache is a JavaScript object that caches modules when they are required. This means that executing require('foo') or import * as foo from 'foo' multiple times will only require the foo module the first time. Magically, deleting an entry in the require cache (which we can access using the global require.cache object) will force Node to re-read the module from disk when it’s next imported.To circumvent the ECS deployment process, we experimented with utilizing Node’s require cache to perform a “hot reload” of application code at runtime. On receiving an external trigger — we implemented this as a gRPC endpoint on the bank integration service — the application would download new code to replace the existing build, clear the require cache, and thereby force all relevant modules to be re-imported. With this approach, we were able to eliminate much of the latency present in ECS deploys and fine-tune our entire deployment process.Over Plaiderdays — our internal hackathon — a group of engineers across various teams got together to implement an end-to-end proof of concept for what we termed "Fast Deploys". As we hacked a prototype together, one thing seemed amiss: if the Node code that downloaded new builds also tried to invalidate the cache, it wasn’t clear how the downloader code itself would be reloaded. (There is a way around this with the Node EventEmitter, but it would add considerable complexity to the code). More importantly, there was also some risk of running versions of code that were not in sync, which could cause our application to fail unexpectedly. As we weren’t willing to compromise on the reliability of our bank integration service, this complication warranted rethinking our “hot reloading” approach.Final solution: reloading the processIn the past, in order to run a series of uniform initialization tasks across all our services, we wrote our own process wrapper, which is aptly named Bootloader. At its core, Bootloader contains logic to setup logging pipes, forward signals, and read ECS metadata. Every service is started by passing the application executable’s path to Bootloader, along with a series of flags, which Bootloader then executes as a subprocess after performing the initialization steps.Instead of clearing Node's require cache, we updated our service to call process.exit with a special exit code after downloading the intended deployment build. We also implemented custom logic in Bootloader to trigger a process reload of any child process that exits with this code. Similar to the “hot reload” approach, this enables us to bypass the cost of ECS deploys and quickly boostrap new code, while avoiding the pitfalls of “hot reloading”. Furthermore, having this "Fast Deploy" logic at the Bootloader layer allows us to generalize it to any other service we run at Plaid.Here's what the final approach looks like:Our Jenkins deployments pipeline sends an RPC request to all instances of our bank integration service, instructing them to "Fast Deploy" a specific commit hashThe application receives a gRPC request for a fast deployment and downloads a tarball of the build from Amazon S3, keyed on the received commit hash. It then replaces the existing build on the file system and exits with the special exit code that Bootloader recognizes.Bootloader sees that the application exited with this special "Reload" exit code, and restarts the application.Lo and Behold, the service now runs new code!Here’s a very simplified diagram of what happens during this process.ResultsWe were able to ship this “Fast Deployments” project within 3 weeks and reduce our deployment times from more than 30 minutes to 1.5 minutes across 90% of our containers in production.The graph above shows the number of deployed containers for our bank integration service, color-coded by their commits. If you focus on the yellow line graph, you can observe a leveling off in the increase at around 12:15, which represents the long tail of our containers which are still draining their resources.This project has greatly increased the velocity of Plaid's integrations work, allowing us to ship features and bug fixes more quickly, and minimize engineering time wasted context switching and monitoring dashboards. It is also a testament to our engineering culture of shipping materially impactful projects, embodied by ideas that come out of hackathons.Want to work on impactful projects like Fast Deployments? We're hiring!</description><pubDate>Tue, 24 Mar 2020 12:27:27 UT</pubDate><guid>https://blog.plaid.com/how-we-reduced-deployment-times-by-95/</guid></item><item><title>Most software companies ignore user behavior</title><link>https://www.reifyworks.com/writing/2019-08-26-a-most-software-companies</link><description>

      


      
        
        
                  One of the most important skills to build as a marketer is follow through. In marketing that means taking every opportunity to gather data in order to understand your users and improve your process.


  Understanding your users doesn’t end when they purchase your product.


One deficiency we’ve consistently seen inside companies small and large alike is that they don’t have ready access to information about what users are doing with their product. Marketers are incentivized to instrument the hell of out lead generation and a section of the funnel, sales religiously updates CRMs to track prospects and potential revenue expansion opportunities, but noone is tying it all together by looking to information that you own about what users are doing with your product.

The reason why this is so important to get right early is that it will deeply inform decisions that you’ll need to make later on … if you’re around that long. Decisions about pricing and packaging, feature development, marketing spending, hiring, strategic investment of all sorts, etc., are more intelligently made when they’re backed by data that is enriched by how users are actually using your product.

There are myriad technical approaches to gathering the data necessary for this method, but an ideal setup should contain the following, regardless of how it is implemented:


  A “User table” which rolls up all useful usage information into one easy to query data source. You likely have some version of this powering either a homegrown “admin dashboard” or feeding into one or more external systems.
  A time-bounded data source which is amenable to more complex queries, including historical queries, and can be used to generate reports. Amazon Redshift is a popular solution for this component.


Here’s an example row:


  
    ID
    name
    score
    feature1
    feature2
    plan
    monthly
    seats
    last_seen
    user_since
  
  
    1
    Reify
    87
    1
    0
    small
    99
    3
    019-08-29
    019-01-02
  




Maintaining and curating this date over time will allow you to answer crucial questions about your trial users, customes, ex-customers, and so on. Recording and maintaining this data is so simple that it’s becoming table stakes for good marketers – don’t get left behind.

Bonus Implementation Tips


  Track feature usage in your user table with columns denoting whether or not a user has or has not used this feature
  Associate plan and payment information with your user table rows so that you can easily segment your user data by important revenue based facets
  Create an “engagement metric” number which takes core features of your product into account and rolls them up into a 0-100 score that can be tracked over time.
  Use the data in your users table to power all kinds of interesting customer communication. Want to communicate with users who have or haven’t used a specific feature and ask them why? Now you can. Want to segment trial users into those who haven’t used that one killer feature and those who have? Go for it.



                  
                  
                

      

      
    </description><pubDate>Tue, 24 Mar 2020 12:27:34 UT</pubDate><guid>https://www.reifyworks.com/writing/2019-08-26-a-most-software-companies</guid></item></channel></rss>