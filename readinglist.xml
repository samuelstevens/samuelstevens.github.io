<rss>
  <channel>
    <title>Read It Later</title>
    <link>https://samuelstevens.me/readinglist.xml</link>
    <description>My personal reading list.</description>
    <language>en-us</language>
    <docs>https://cyber.harvard.edu/rss/rss.html</docs>
    <generator>github.com/samuelstevens/go-read-more</generator>
    <managingEditor>samuel.robert.stevens@gmail.com</managingEditor>
    <webMaster>samuel.robert.stevens@gmail.com</webMaster>
    <pubDate>26 Feb 22 11:47 EST</pubDate>
    <lastBuildDate>26 Feb 22 11:47 EST</lastBuildDate>
    <item>
      <title>Mastering Metacognition: The What, Why, and How</title>
      <link>https://www.activelylearn.com/post/metacognition</link>
      <description>&lt;a href=&#34;https://www.activelylearn.com/post/metacognition&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;You may have come across the term “metacognition” recently and wondered if it was just another instructional buzzword. Metacognition is “thinking about thinking,” but what exactly does that mean, why does it matter to learning, and how can teachers emphasize it in their instruction?What is metacognition:Metacognition is an awareness of one’s own learning. It entails understanding the goals of the learning process, figuring out the best strategies for learning, and assessing whether the learning goals are being met. A metacognitive student sees him or herself as an agent in the learning process and realizes that learning is an active, strategic activity.A useful analogy for understanding metacognition might be mastering an athletic skill: in the early stages, a novice player needs to think carefully about how his or her actions affect performance. For instance, in figuring out the best way to swing a golf club, a novice player might deliberately adjust his or her stance, assess whether the adjustment leads to better performance, and then decide whether to adopt the strategy going forward. This is essentially the active thinking about the learning process that we want to encourage with students.Metacognition can include any of the following elements:Understanding what one already knows about a topicFiguring out what one wants to know about a topicRealizing what one has learned in the course of a lessonMonitoring one’s understanding during the course of an activityChoosing which learning strategies to employ and whenEvaluating whether a particular learning strategy was successful in a given circumstanceWhy is metacognition important: Metacognition has been linked to improved learning outcomes. It makes sense that individuals who are strategic in their learning are more successful than those who do not reflect on the learning process. For instance, metacognitive learners are more likely to notice when what they are studying does not make sense. These are the students who try to clarify their understanding rather than passively continuing on with the assignment. According to researcher John Hattie, the effect size for teaching metacognitive strategies is 0.69, making it one of the most effective teaching interventions.Metacognition is also a significant factor in whether students can transfer their learning to new scenarios. Students who are metacognitive are actively embedding new information in their existing network of knowledge and creating connections among ideas. This is the sort of thinking that gets students beyond surface learning and drives them to deeper understanding.How do students become metacognitive:Though some individuals are naturally more metacognitive than others, metacognition is a skill that can be taught and learned. As with other learning skills, students will initially need explicit instruction, scaffolding, practice, and feedback in order to turn unfamiliar operations into habits of mind. Here are a few ideas on how to incorporate metacognition into instruction, particularly as it pertains to literacy.Metacognition is a skill that can be taught and learned.Model your thought processesTeachers can model metacognition by demonstrating their own thought process while they read. They can let students see how they interact with the text: how they determine the purpose of their reading, where they pause to ask a question, how they connect new ideas to previous knowledge, how they track whether they understand the content, how they decide when to slow down and re-read, and how they assess what they have learned. Teachers can do this through read-aloud or by sharing their annotations with students. It is useful to repeat this modeling process, particularly at the beginning of more challenging assignments.Create simple tasks for students to demonstrate thinkingTeachers can scaffold metacognition by asking students to demonstrate their thinking in writing or during discussion. Instructions on how to do this will need to be explicit; for instance, teachers can guide students to mark one moment in the text they found surprising, confusing, or erroneous and explain their reaction. Reviewing what students have noted is a great way to spur a discussion about the reading.Increase writingWriting is a particularly useful way to make metacognition visible because it enables students to look back on their previous notes and see how their thinking has changed. Some teachers find it effective to create portfolios of student learning, including rough drafts and pre-writing activities, then ask students to look back on the trajectory of their work and note how it has evolved with successive revisions.Pre and post reading pollsTo demonstrate the impact of learning, teachers can record students’ attitudes and preconceived notions about a topic prior to reading a text, then follow up after the reading to gauge whether students have changed their minds based on what they’ve learned. This can be done via a pre- and post-reading poll, with the data ultimately driving a discussion about students&#39; previous misconceptions and how the text informed their thinking.Build in a one question for students to ask themselvesTeachers can encourage students to regularly practice metacognition simply by asking, “How did you figure that out?” after a student offers an answer to a question. This helps students reflect on the learning strategies they employ and gives their classmates an opportunity to see how their peers learn.Peer assessmentStudents can help get in the habit of assessing different learning strategies by judging work: after viewing a detailed answer to a question, students can discuss whether the answer shows an effective use of reasoning and how it can be improved.Make revisions part of assignmentsFinally, students benefit tremendously from opportunities to revise their work and reflect on how their thinking has improved. This gives them the chance to understand what errors they made the first time around and how the learning process has led them to see the problem differently.RELATED:The benefits of deeper learning: retention, transfer, and motivation4 ways to deepen understanding with writing, even if you&#39;re not an English teacherThe content comeback: why knowledge matters to thinking and learning</description>
      <pubDate>24 Feb 22 15:36 EST</pubDate>
      <guid>https://www.activelylearn.com/post/metacognition</guid>
    </item>
    <item>
      <title>I Used to Write for Sports Illustrated. Now I Deliver Packages for Amazon.</title>
      <link>https://www.theatlantic.com/ideas/archive/2018/12/what-its-like-to-deliver-packages-for-amazon/578986/</link>
      <description>&lt;a href=&#34;https://www.theatlantic.com/ideas/archive/2018/12/what-its-like-to-deliver-packages-for-amazon/578986/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;There’s a certain novelty, after decades at a legacy media company, in playing for the team that’s winning big.Lucy Nicholson / ReutersAbout the author: Austin Murphy is an author and longtime Sports Illustrated senior writer who recently delivered 279 parcels in one day Holiday parties were right around the corner, and I needed a cover story. I didn’t feel like admitting to casual acquaintances, or even to some good friends, that I drive a van for Amazon. I decided to tell them, if asked, that I consult for Amazon, which is loosely true: I spend my days consulting a Rabbit, the handheld Android device loaded with the app that tells me where my next stop is, how many packages are coming off the van, and how hopelessly behind I’ve fallen.Let’s face it, when you’re a college-educated 57-year-old slinging parcels for a living, something in your life has not gone according to plan. That said, my moments of chagrin are far outnumbered by the upsides of the job, which include windfall connections with grateful strangers. There’s a certain novelty, after decades at a legacy media company—Time Inc.—in playing for the team that’s winning big, that’s not considered a dinosaur, even if that team is paying me $17 an hour (plus OT!). It’s been healthy for me, a fair-haired Anglo-Saxon with a Roman numeral in my name (John Austin Murphy III), to be a minority in my workplace, and in some of the neighborhoods where I deliver. As Amazon reaches maximum ubiquity in our lives (“Alexa, play Led Zeppelin”), as online shopping turns malls into mausoleums, it’s been illuminating to see exactly how a package makes the final leg of its journey.There’s also a bracing feeling of independence that attends piloting my own van, a tingle of anticipation before finding out my route for the day. Will I be in the hills above El Cerrito with astounding views of the bay, but narrow roads, difficult parking, and lots of steps? Or will my itinerary take me to gritty Richmond, which, despite its profusion of pit bulls, I’m starting to prefer to the oppressive traffic of Berkeley, where I deliver to the brightest young people in the state, some of whom may wonder, if they give me even a passing thought: What hard luck has befallen this man, who appears to be my father’s age but is performing this menial task?Thanks for asking!Derek Thompson: Amazon’s HQ2 spectacle isn’t just shameful. It should be illegal.The hero’s journey, according to Joseph Campbell, features a descent into the belly of the beast: Think of Jonah in the whale, or me locked in the cargo bay of my Ram ProMaster on my second day on the job, until I figured out how to work the latch from the inside. During this phase of the journey, the hero becomes “annihilate to the self”—brought low, his ego shrunk, his horizons expanded. This has definitely been my experience working for Jeff Bezos.During my 33 years at Sports Illustrated, I wrote six books, interviewed five U.S. presidents, and composed thousands of articles for SI and SI.com. Roughly 140 of those stories were for the cover of the magazine, with which I parted ways in May of 2017. Since then, as Jeff Lebowski explains to Maude between hits on a postcoital roach, “my career has slowed down a little bit.”This proved problematic when my wife and I decided to refinance our home. Although Gina, an attorney, earns plenty, we needed a bit more income to persuade lenders to work with us. It quickly became clear that for us to qualify, I would need more than occasional gigs as a freelance writer; I would need a steady job with a W-2. Thus did I find myself, after replying to an indeed.com posting for Amazon delivery drivers, emerging from an office-park lavatory a few miles from my house, feigning nonchalance as I handed a cup of urine to the attendant and bid him good day.Little did I know, while delivering that drug-test sample, that this most basic of human needs—relieving oneself—would emerge as one of the more pressing challenges faced by all “delivery associates,” especially those of us crowding 60. An honest recounting of this job must include my sometimes frantic searches for a place to answer nature’s call.To cut its ballooning delivery costs—money it was shelling out to UPS and FedEx—Amazon recently began contracting out its deliveries to scores of smaller companies, including the one I work for. Amazon trains us, and provides us with uniform shirts and hats, but not with a ride. Before 7 a.m., we report to a parking lot near the warehouse where we select a vehicle from our company’s motley fleet of white and U-Haul vans.I’m an Aries, so it stands to reason that I’m partial to Dodge Ram ProMasters. I like their profile and tight turning radius: That’s key, since we make about 100 U-turns and K-turns a day. Problem is, most of the drivers in our company—there are about 40 of us—share my preference. The best vans go to drivers with seniority, even if they show up after I do. Before it was taken out of service for repairs, I was often stuck with a ProMaster that had issues: Side-view mirrors spiderwebbed; the left mirror held fast to the body of the van by several layers of shrink-wrap. The headlights didn’t work unless flicked into “bright” mode, which means that when delivering after dark, I was blinding and infuriating oncoming motorists.I drove that beast on my worst day so far. After a solid morning and early afternoon, I glanced at the Rabbit and sighed. It was taking me to that fresh hell that is 3400 Richmond Parkway, several hundred apartments set up in a mystifying series of concentric circles. The Rabbit’s GPS doesn’t work there, the apartment numbers are difficult to find, and the lady in the office informed me that I couldn’t leave packages with her. She did, however, hand me a map resembling the labyrinth of ancient Greece. I spent an hour wandering, ascending flights of stairs that took me, usually, to the incorrect apartment. By now deep in the hole, with no shot at completing my appointed rounds for the day, I set a forlorn course for my next stop at the nearby Auto Mall. That’s when I heard a thud-thud-thud from the area of my right front tire, which was so old and bald that it had begun to shed four- and five-inch strips of rubber, which were thumping against the wheel well.Read: Amazon is invading your home with micro-convenienceAlthough it was only 4 p.m., I called it quits. Some days in the delivery biz, the bear eats you. But I got some perspective back at the lot, where a fellow driver named Shawn told me about the low point of his day. A woman had challenged him as he emerged from her side yard—where he’d been dropping a package, as instructed. “What are you stealing?”“That sucks,” I said. “I’m sorry that happened to you.”“It&#39;s cool,” he told me. “I called her a bitch.”For both days of my safety training, I sat next to and befriended Will, who now shows up for work wearing every Amazon-themed article of clothing he can get his hands on: shirt, ball cap, Amazon beanie pulled over Amazon ball cap. I found that odd at first, but it makes good sense. If you’re a black man and your job is to walk up to a stranger’s front door—or, if the customer has provided such instructions, to the side or the back of the property—then yes, rocking Amazon gear is a way to protect yourself, to proclaim, “I’m just a delivery guy!”That safety training, incidentally, is comprehensive and excellent. After two days in the classroom, all of us had to pass a “final exam.” It wasn’t a slam dunk. In my experience, however, some of the guidelines Amazon hammers home to us (seat belts must be worn at all times; the reverse gear is to be used as seldom as possible; driveways are not to be blocked while making deliveries) must be thrown overboard if we’re going to come close to finishing our routes.And there’s the bathroom issue.The Google search Amazon driver urinates summons a cavalcade of caught-in-the-act videos depicting poor saps, since fired, who simply couldn’t hold it any longer. While their decision to pee in the side yard—or on the front porch!—of a customer is not excusable, it is, to those of us in the Order of the Arrow (my made-up name for Amazon delivery associates), understandable.Before sending me out alone, the company assigned me two “ride-alongs” with its top driver, the legendary Marco, who went out with 280 packages the second day I rode shotgun with him, took his full lunch break, did not roll through a single stop sign, and was finished by sundown. Marco taught me to keep a lookout not just for porch pirates—lowlifes who swoop in behind us to pilfer packages—but also for portable toilets. In neighborhoods miles from a service station or any public lavatory, a Port-a-John, or a Honey Pot, can be no less welcome than an oasis in the desert. (The afternoon I leapt from the van and beelined to a Honey Pot, only to find it padlocked, was the closest I’ve come to crying on the job.)Read: The authors who love AmazonDelivering in El Sobrante one day, I popped into a convenience store on San Pablo Avenue. I bought an energy bar, but that was a mere pretext. “I wonder if I might use your lavatory,” I asked the proprietor, a gentleman of Indian descent, judging by his accent, in a dapper beret.A cloud passed over his face. “You make number one or two?”“Just one!” I promised. He inclined his head toward the back of the store, in the direction of the “Employees only” bathroom.After thanking him on my way out, I mentioned that I was new at Amazon, still figuring out restroom strategies.“Amazon drivers, FedEx drivers, UPS, Uber, Lyft—everybody has to go.”But where? When no john can be found, when the delivery associate is denied permission to use the gas-station bathroom, he is sometimes left with no other choice than to repair to the dark interior of the cargo bay—the belly of the beast—with an empty Gatorade bottle.It was late afternoon on a Monday when I may or may not have been forced to such an extreme. I was dispensing packages on Primrose Lane in Pinole, and I remember thinking, afterward: Aside from the fact that my checking account is overdrawn and I’m 30 deliveries behind and the sun will be down in an hour and I’m about to take a furtive whiz in the back of a van, life really is a holiday on Primrose Lane!Pinole, incidentally, is the hometown of the ex–Miami Hurricanes quarterback Gino Torretta, a great guy who won the Heisman Trophy in 1992. I covered him then, and a few years later when he was playing for the Rhein Fire in the NFL’s World League. Gino and I hoisted a stein or two at a beer hall in Düsseldorf. Some of the American players were having trouble enunciating the German farewell, auf Wiedersehen. To solve that problem, they would say these words as rapidly as possible: Our feet are the same!Performing my new job, I’m frequently reminded of my old one, whether it’s driving past Memorial Stadium in Berkeley, where I covered countless Pac-12 games, or listening to NFL contests during Sunday deliveries. I’ve talked and laughed with many of the players and coaches and general managers and owners whose names I hear.Sitting in traffic one damp December morning, I turned on the radio to hear George W. Bush eulogizing his father. His speech was funny, rollicking, loving, and poignant. It was pitch-perfect. In the summer of 2005, after returning from the Tour de France—cycling was my beat during the reign of Lance Armstrong—I was invited, along with five other journalists, to ride mountain bikes with W. on his ranch in Crawford, Texas. The Iraq War was going sideways; 43 needed some positive press. I jumped at the chance, even though I loathed many of his policies. In person, Bush was disarming, charming, funny. (These days, compared with the current POTUS, he seems downright Churchillian.) I wrote two accounts, one for the magazine, another for the website. Got a nice note from him a couple weeks later.Lurching west in stop-and-go traffic on I-80 that morning, bound for Berkeley and a day of delivering in the rain, I had a low moment, dwelling on how far I’d come down in the world. Then I snapped out of it. I haven’t come down in the world. What’s come down in the world is the business model that sustained Time Inc. for decades. I’m pretty much the same writer, the same guy. I haven’t gone anywhere. My feet are the same.When I’m in a rhythm, and my system’s working, and I slide open the side door and the parcel I’m looking for practically jumps into my hand, and the delivery takes 35 seconds and I’m on to the next one, I enjoy this gig. I like that it’s challenging, mentally and physically. As with the athletic contests I covered for my old employer, there’s a resolution, every day. I get to the end of my route, or I don’t. I deliver all the packages, or I don’t.That’s what I ended up sharing with people at the first Christmas party of the season. It felt better, when they asked how I was doing, to just tell the truth.This is also true: Gina and I got approved for that loan last week, meaning that our monthly outlay, while not so minuscule that it can be drowned in Grover Norquist’s figurative bathtub, is now far more manageable, thanks in part to these daily journeys which I consider, in their minor way, heroic.</description>
      <pubDate>02 May 21 11:30 EDT</pubDate>
      <guid>https://www.theatlantic.com/ideas/archive/2018/12/what-its-like-to-deliver-packages-for-amazon/578986/</guid>
    </item>
    <item>
      <title></title>
      <link>https://rkg.blog/desperation-induced-focus.php</link>
      <description>&lt;a href=&#34;https://rkg.blog/desperation-induced-focus.php&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; People ask me about processes all the time. Planning processes, performance review processes, OKRs, et cetera. I think it’s because I was once a COO and people have this vision of COOs as process fiends. People who make the trains run on time. People with lots of Gantt charts all around them. I have nothing against process.1 But I think startups generally implement them way too early and way too often. Every time I’m asked about some newly proposed process, I ask: “Why do you think you need this?” The answer is usually some version of “[Smart Person] said that we need to have a process now that we are getting bigger.” Usually this Smart PersonTM is from a big company. The lesson, as always: big companies suck. Don’t get me wrong—they didn’t always suck. At some point they were just like you. A startup fighting for product market fit or creating predictability in go-to-market or expanding geographically or something else that actually matters. They were focused. Focused on hiring trajectory-changing people and giving them more responsibility than they knew what to do with. They were working their a**es off to create something out of nothing. They had desperation-induced focus. But now they are different. Most big companies aren’t focused on creating things out of nothing. Someone else made the magic money-making machine, and they assume that it will just keep working. With the one thing that actually matters taken care of, they care about luxuries like making sure as high a percentage of the company as possible feels included in the planning process. Or creating performance review frameworks with an ever-increasing number of boxes and categories. Or something else that matters even less than that. This lack of focus is a luxury and a disease. My advice to people when they are thinking about instituting a new process is to go to a whiteboard2 and write down the answer to this question: “If you could only get one thing done this year, what would it be?”. If that answer is “institute some new process”, go for it. But if it’s something like “increase market share from 30% to 60%” or “launch this new product that will 2x our TAM”, don’t waste your time on anything else. Just take your best person (up to and including the CEO), make them responsible for solving that problem, and give them everything and everyone they need to make it happen. In reality, process is not my problem. It’s what discussions around new processes often preview within a company. Lack of focus. Peacetime thinking. Complacency. When I think of my time at Instacart, I remember desperation. It was painful. But it was also powerful. Our best moments as a company came when we shared a singular purpose. Our most fun moments as a team came when our backs were against the wall. In my experience, desperation is the single greatest advantage you have as a startup. It takes you down to the lowest level of detail.3 Desperation inspires creativity and intense focus. It is an essential ingredient to building great products and services. So, the next time you feel desperate, lean in. Embrace it. Use it as the fuel to create the next founding moment4 for your company. And the next time someone tries to tell you to do something because a big company does it, be suspicious. Don’t let them infect your company with the thing that is slowly killing theirs. </description>
      <pubDate>24 Feb 22 09:18 EST</pubDate>
      <guid>https://rkg.blog/desperation-induced-focus.php</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.scientificamerican.com/article/people-literally-dont-know-when-to-shut-up-or-keep-talking-science-confirms/</link>
      <description>&lt;a href=&#34;https://www.scientificamerican.com/article/people-literally-dont-know-when-to-shut-up-or-keep-talking-science-confirms/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Error 403 Forbidden Forbidden Guru Mediation: Details: cache-pwk4934-PWK 1645893964 2272150040 Varnish cache server </description>
      <pubDate>08 Mar 21 09:11 EST</pubDate>
      <guid>https://www.scientificamerican.com/article/people-literally-dont-know-when-to-shut-up-or-keep-talking-science-confirms/</guid>
    </item>
    <item>
      <title>10 questions to ask in a job interview that will really expose a company’s culture</title>
      <link>https://www.fastcompany.com/90622890/10-questions-to-ask-in-a-job-interview-that-will-really-expose-a-companys-culture</link>
      <description>&lt;a href=&#34;https://www.fastcompany.com/90622890/10-questions-to-ask-in-a-job-interview-that-will-really-expose-a-companys-culture&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;You are in the last five minutes of the job interview, and the interviewer asks: “What questions do you have?”Time is limited, so you ask the question you think will be most helpful: “What is the culture like here?”Don’t do this. There are better questions to understand the culture. The interviewer will typically respond by describing the values of the company. Their reply will have some variation of trust, collaboration, transparency, integrity which are the same values that show up in various forms in many companies. These don’t help you understand the day-to-day experience.Culture is felt through the behaviors that are reinforced or discouraged on a day-to-day basis on teams. If you want to get a sense of the story of the leader and team’s culture, use detailed questions. You will get a much better sense based on the responses, especially if the leader struggles to think of what to say. If you are a manager, prepare to answer detailed questions that illustrate your team’s culture. Better questions to ask a hiring manager:Tell me about a time a team member changed your mind?  This lets you know if the leader feels they are the only one who has the answers or if they are open to different opinions. You are going to learn how they prefer to receive information and what they value. Tell me about someone you are proud of. This is going to let you know which behaviors and skills they value. You can also learn their attitude towards developing people and celebrating success along the way. Do you fully disconnect during holidays and vacations? Does this leader believes in boundaries and having time off and space that is protected? Or is this someone that will be calling you on your holiday—and will that work for you?Describe a recent success or win. They should be able to come up with something pretty quickly. If they can’t, that might indicate that they aren’t great about celebrating progress or recognizing people along the way to milestones. They don’t have to describe a huge win. However, they should be able to think of a recent event that demonstrates progress. Tell me about a disagreement or conflict on the team.  Every team is going to have conflict. It is a great way to generate ideas and different thinking when the team has the right tools to navigate constructive conflict. You want to see is if the leader says: “We don’t have conflict.” This could mean that different opinions aren’t welcome, and the team sits in silence. Or the leader is trying to avoid the hard conversations that yield better results. The leader should be able to talk about people having different opinions they had to work through. How did you start your last team meeting? Did they jump right into the agenda?  Did they have an activity or conversation to learn more about each other? You can learn a lot about interactions by how they begin meetings and conversations.What is your ideal person for this role?  This is a great way to understand what the leader values and the knowledge, skills, and behaviors they view as making their work easier. They will probably describe the person’s organization, communications, skill set, or certain outcomes achieved. This response helps you get an idea if you fit with the leader’s ideal candidate.Who have you promoted and why? If the leader has never promoted anyone, probe further to understand what is done to develop people. If they are a newer manager and haven’t had the opportunity, ask what they are doing to help grow and develop their team. It is ok if the leader hasn’t promoted anyone. What you want to hear is the thought around it and how they view their role in developing people on the team.Tell me about the last person you recognized. Recognition can be a thoughtful conversation, an email, an award, or even a mention in an all-hands meeting. You want to see if the leader struggles to come up with an example or easily mentions individual and team recognition. Does the leader have the mindset that development includes helping people see the contributions they are making?How do you focus on your own growth and development? Does the leader mention reading articles, listening to podcasts, reading books, having a mentor, taking courses, or having a coach? Are they actively trying to develop themselves? If they are developing themselves, they are more likely to develop their team. If they aren’t, you want to understand why. If they blame their schedule or struggle to find an answer, then odds are good your opportunity for development will be pushed aside. Don’t waste your opportunity to learn more about your prospective employer in an interview. Ask these questions that help you get to the experience of that leader and that team. Culture is experienced at the team level, and every culture tells a story. Ask them about these specific moments to better understand the experience of the leader and the team. Karen Eber is the CEO and chief storyteller of Eber Leadership Group, a talent development boutique. She is also an international consultant, keynote, and TED speaker. </description>
      <pubDate>18 Apr 21 16:42 EDT</pubDate>
      <guid>https://www.fastcompany.com/90622890/10-questions-to-ask-in-a-job-interview-that-will-really-expose-a-companys-culture</guid>
    </item>
    <item>
      <title></title>
      <link>https://atis.substack.com/p/an-underrated-idea-the-priority-view</link>
      <description>&lt;a href=&#34;https://atis.substack.com/p/an-underrated-idea-the-priority-view&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Derek Parfit[This post discusses the Derek Parfit paper ‘Equality or Priority?’, and honestly, you would benefit more from reading that paper than this post if you want to really get into the weeds of the priority view. That being said, the paper is fairly long, and perhaps slightly unfriendly to those not engaged with much political philosophy, so read on if you don’t have the time to digest a ~40 page political philosophy paper. Further, this post assumes you are somewhat familiar with utilitarianism. If totally uninterested in political/moral philosophy, do not bother reading.]The philosopher Thomas Nagel has a well-known test to check whether a person is someone who, in some situations, values equality over utility. Suppose that you have two children - one, a gifted teenage boy who is excelling at school, the other a disabled boy who goes into the hospital for routine checks and medical interventions. As a parent, you are now making a decision whether you ought to live in the suburbs or the city. The suburbs has benefits for your gifted son - the levels of the crime in the city are fairly high, the cost of living is higher and so your home would be smaller, and so on. The city has one huge benefit for your disabled son - it would be much easier to get to the hospital often, and he would receive more and better medical treatment. There is also a catch in the hypothetical - let’s assume that the utility gain for the gifted son from living in the suburbs would be larger than the utility gain for the disabled son from living in the city. A pure utilitarian, then, must choose the suburbs. Nagel’s view is this: if you say that you would live in the city for the sake of your disabled son, despite it being the case that moving to the city creates more utility in total, you are not a utilitarian (at least in all circumstances), but rather an egalitarian. You value the equality of the boys more than you do maximising the overall levels of well-being.This seems like a pretty good argument - if, like me, you think there is some appeal to choosing the city, you are deciding to forego maximising utility in favour of something else. But what is that ‘something else’? Nagel claims that you have chosen to maximise equality - you favour the wellbeing of the disabled boy rather than the gifted boy because doing so produces some degree of equality. But Parfit’s counterargument here is that your preference is contingent on increasing the utility of the disabled boy, rather than a pure egalitarian view.Let’s introduce another scenario: Imagine that the gifted boy has a total utility of 80, and the disabled boy has a total utility of 40. If you took some action to reduce the utility of the gifted boy (say, removing access to any books, television shows, and so on) that led to his utility decreasing to 60, you have made a step towards equality, even though you have not made the disabled boy better off at all. A pure egalitarian is forced to say that this action is a good one, because it has made the two boys more equal. Someone who values equality to an extent, but is not a pure egalitarian, is forced to say that while this action might be bad on net, there is at least some moral value to increasing equality that is outweighed by the negative utility outcome from this action. So, what other explanations do we have to why someone would prefer, in the first hypothetical, moving to the city to moving to the suburbs? Parfit’s answer is that we might value priority, which is prioritising the well-being of the worst off. The point being made here is that we do not assign any moral value to decreasing the well-being of those who are better off (there is no moral justification for removing the gifted boy’s access to books and so on), but we do assign more moral value to increasing the utility of those who start from a lower base. The implication is that a utility gain of 5 can have more moral significance than a utility gain of 10, if the utility gain of 5 would be going to someone with a much lower base utility than the utility gain of 10 would be going to. Let’s use another intuitive example: suppose you have a friend you know is going through a serious depressive episode, and another friend who is thriving. The first friend asks if you are willing to go to dinner with him, and the second friend proposes going to the concert of his favourite band who are in town for one night only. The catch from the city/suburb example also applies here - the thriving friend will gain more utility from the concert than the depressed friend will from getting dinner. You might still opt to go to dinner with the depressed friend in this instance, because you might consider a small amount of happiness for someone who is depressed to have more moral value than a larger amount of happiness for someone who is already fairly happy. The priority view has a lot going for it over pure utilitarianism - to me at least, it aligns more with my instincts about morality. In reality, I would prioritise the depressed friend over the thriving friend, or the disabled boy over the gifted boy, even if I knew that it would not result in more utility in total. Secondly, it (at least somewhat) deals with a famous objection to utilitarianism - the utility monster. The utility monster is one of Robert Nozick’s brilliant thought experiments that supposes that one person receives much more utility from each unit of resource they consume than anyone else does. For a utilitarian, it follows that every resource ought to be directed towards the utility monster in order to maximise total utility. With the priority view, this situation can often be avoided, because we assign more moral weight to increasing the utility of the worst off - we now have a moral justification to avoid directing more resources to the utility monster. It is fairly surprising to me that the priority view (often called ‘prioritarianism’) hasn’t really caught on outside of moral philosophy, even though I think that lots of people intuitively take this view, and it deals with some of the objections towards pure utilitarianism. </description>
      <pubDate>10 Dec 21 08:52 EST</pubDate>
      <guid>https://atis.substack.com/p/an-underrated-idea-the-priority-view</guid>
    </item>
    <item>
      <title>Why learn Racket? A student&#39;s perspective</title>
      <link>https://www.micahcantor.com/blog/why-learn-racket/</link>
      <description>&lt;a href=&#34;https://www.micahcantor.com/blog/why-learn-racket/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; ~/blog/why-learn-racket Published: 2022-02-18 Updated on Friday, February 25th. I&#39;m a student at Grinnell College, a small liberal arts school in Iowa. Here, the computer science department uses Racket in its intro course, which focuses on functional programming and is aimed at all students (majors, non-majors, and those with and without prior programming experience.) After taking the course myself last year, this semester I am working as a student mentor for the course, where I help students during in-class labs and hold weekly review sessions. Grinnell is far from alone in its choice to use Racket or Scheme in its introductory CS course. Perhaps the most well known Scheme course is MIT&#39;s 6.001, which began in 1980 and taught computer science using the book Structure and Interpretation of Computer Programs. MIT replaced their Scheme curriculum with a Python course in 2008, but other universities like Brown, Northeastern, and Indiana maintain Scheme or Racket in their first courses. Still, perhaps following MIT&#39;s lead, the use of Racket for an intro course is still relatively rare among CS undergraduate programs. Given that Racket isn&#39;t a widely popular language, several students at Grinnell, often those who enter the class with prior experience in more popular languages like Python or Java, ask why it&#39;s chosen for Grinnell&#39;s first course. I&#39;ve also heard frustrated students wonder why they&#39;re wasting their time on a &#34;useless&#34; language, when they could learn a language that will help them get a job instead. Our professor gives an answer to those who ask, and there are other resources I&#39;ve seen such as Matthew Butterick&#39;s excellent piece, Why Racket? Why Lisp?, but here I&#39;d like to share my own opinion on why I think learning Racket is worthwhile. I think Racket is a great first language to learn, but in this post, I&#39;d like to concentrate on programmers, college-bound or otherwise, who have some experience in a mainstream language like Python, JavaScript, or Java, but are hesitant or unsure of the value in learning a functional programming language like Racket. Mental models of computation Racket, and Lisps more generally, are often touted as languages with the simplest syntax. In Racket, every form is an expression, which is either an atomic value like a string or number, or a list beginning with a procedure or a special form. This syntax stands in contrast to a language like Python, which has strict rules for which forms may be used as expressions and which must be statements. I think this advantage is sometimes overstated by Lisp enthusiasts. Although Python&#39;s syntax may not be as simple, it does have its own kind of elegance once a user learns its syntactic rules. Where Lisp-style syntax does have an advantage, however, is in the power of its mental evaluation model. When evaluating Racket code, we always evaluate the innermost parentheses first, replacing procedure calls with their appropriate body, except for a few special forms.1 For example, take the following Racket expression to square every number in a list (map sqr &#39;(2 4 8)) To evaluate this, we replace map with the body of its definition, substituting the arguments in place, and then continue with normal evaluation. We can trace an abbreviated version like this: (map sqr &#39;(2 4 8)) -&gt; (if (null? &#39;(2 4 8)) null (cons (sqr (car &#39;(2 4 8))) (map sqr (cdr &#39;(2 4 8))))) -&gt; (cons 4 (map sqr &#39;(4 8))) -&gt; ... -&gt; (cons 4 (cons 16 (cons 64 null))) -&gt; &#39;(4 16 64) Following the trace, we an see that there is nothing magical about map, it&#39;s just a procedure that evaluates like any other. Compare that now to the equivalent list comprehension in Python: [x*x for x in [2, 4, 8]] How can we mentally evaluate this? For that we need remember the specific &#34;magical&#34; syntax and semantics for evaluating Python&#39;s for-comprehension expressions, which look similar yet act completely differently than for-loop statements. Racket has special forms too like let and if for which evaluation strategies must be learned, but these are generally more limited and intuitive than the syntax found in Python or Java. Furthermore, what&#39;s our mental model for evaluating this Java code? public class HelloWorld { public static void main(String[] args) { System.out.println(&#34;Hello World!&#34;); } } In just a few simple lines of Java&#39;s canonical &#34;hello world&#34; example, we&#39;ve made several declarations with special rules for evaluation (public, class, static, the dot operator, etc.) These all must be understood before a Java programmer can know much of anything about what their program is actually doing. Java was the first programming language that I learned, so I know from first-hand experience that the result is that beginners to the language usually just accept that they can&#39;t fully understand how even a simple program like this is evaluated. They&#39;re forced to accept that learning to program involves memorizing seemingly arbitrary rules for structuring their code. Beginner Racket programmers face no such imposition, as its evaluation is much more intuitive, which I see as a clear advantage in its use as a teaching language. Even if you are an experienced Python or Java programmer that knows exactly how every part of the language can be evaluated, I still think that exposure to Racket&#39;s evaluation model is worthwhile. Modeling how Racket code is transformed builds your intuition as a programmer for how different pieces of code decompose and fit together in a way that users of other languages usually don&#39;t directly consider. I think that strengthening these fundamentals builds intuition for quickly grasping evaluation in other languages. Less is more Learning Racket often involves working with a more limited set of tools than the ones offered in other languages. Loops and mutation, for instance, which are core features in other languages, are usually off-limits to beginners in Racket.2 While there are genuine aesthetic advantages to a language built with a small core, namely in how it can be concisely defined and implemented, I think it&#39;s important not to overstate these in comparison to the material advantages of a small language as well. The focus on minimalism in Racket initially forces even experienced programmers to step out of their comfort zone in how they structure their code, and ultimately, how they solve problems. For experienced programmers, I think there is great value in learning fundamentally different ways to approach a problem. This kind of experience pushes the bounds of one&#39;s knowledge and adds to a &#34;bag of tricks&#34; that allow one to solve more difficult or novel problems. Thinking recursively Let&#39;s talk about recursion, because I think that learning to use recursion as a general-purpose control structure is one of the most important consequences of gaining fluency in Racket. When programmers in other languages are introduced to recursion, it&#39;s usually to display an alternative and niche solution to some problems. Canonical examples like calculating a factorial or the nth Fibonacci number are given, even though these toy examples seem useless to programming at large. Additionally, programmers are often warned not to use recursion at all because of stack overflow concerns. Recursion, however, is a much more powerful tool than these beginner tutorials let on. Indeed, without loops and mutation, recursion is the main tool employed in Racket to manage repetition and state, and learning to use it as such is one of the most challenging aspects of learning the language for beginner and experienced programmers alike. The key to programming with recursion isn&#39;t that it can be used to implement recursive mathematical definitions, but instead that it&#39;s a general tool for breaking down a single difficult problem into several easier ones. This decomposition into sub-problems is, in fact, what allows recursion to work at all. Let&#39;s look at an example, comparing an iterative and recursive design of a sum procedure. Here is a standard iterative approach in Python: def sum(lst): total = 0 for x in lst: total += x return total This approach follows directly from how most people would answer the question &#34;how do you sum a list of numbers?&#34; Well, start with zero, then add each value to the total until we reach the end. Implementing sum recursively, however, requires a different mindset. Instead, our strategy will be to ask two questions: First, &#34;what&#39;s the sum of an empty list?&#34; Okay, obviously zero. Next, &#34;if we know the sum of all but the first element in a list, what&#39;s the total sum?&#34; Here we can find this by adding the value of the first element to the sum of the rest. Putting this strategy into Racket, we would get this: (define (sum lst) (if (null? lst) 0 (+ (car lst) (sum (cdr lst))))) This line of thinking is certainly not intuitive to someone unfamiliar with recursive design. However, it importantly forces us to split our design into two sub-problems: first solving the base case, and then breaking down the recursive step into what can be handled by the recursive call, and what we must do at each stage. Recognizing the value to thinking recursively can be difficult, since it&#39;s not obvious why adopting an inherently less intuitive strategy to problem solving is helpful. But the beauty of recursion lies how it allows us to solve difficult problems and manage state with inherently less powerful tools (no mutation or loops), and how that simplicity rewards experienced programmers with the ability to quickly break down difficult problems. Recursion isn&#39;t a silver-bullet, but it&#39;s a fundamental tool that&#39;s worth understanding, even if functional programmers usually prefer to use more specific and safer tools built on top of recursion like higher-order procedures or comprehensions. Racket and recursion Even if all this waxing about recursion is true, one may still wonder why to choose Racket when recursion is available in every other programming language as well. The answer is that Racket, like most other functional languages, is designed to make recursive design more ergonomic and natural. First, every Racket procedure implicitly returns the final expression, so there is no possibility of writing a foot-gun recursive procedure that doesn&#39;t return a value. Additionally, core forms in the language are designed to discourage imperative programming. For example, an if expression must include truthy and false branches, so it&#39;s more difficult for one to forget to include a base case. Perhaps most important, however, is the core placement of the list data structure. Lists are an inherently recursive data structure, as they&#39;re either null or cons of some value and another list. Recursive design over cons lists is natural, so their central placement in Racket makes using recursion a natural choice. Compare that to using recursion in a language like Python. There the primary list data structure is an indexed dynamic array. Although one can translate the same recursive algorithms one would write with cons and cdr in Racket to Python, doing so would not only be unnatural, but also goes against the ethos of Python. And really, the same would go for a translation to other popular imperative languages. Racket is advantageous for recursive design from the perspective of performance, rather than just pedagogy, too. Importantly, Racket guarantees tail call optimization, which means that recursive functions designed to exploit tail recursion won&#39;t explode the memory on the stack as they would in imperative languages like Python and Java. Languages are less important than you might think Now that I have extolled the virtues of learning Racket, I want to add that the choice of a programming language is less important than you might think. One of the main areas of frustration I see from fellow students essentially boils down to the following question: &#34;Why should I learn Racket when I could spend that time learning a language that people actually use?&#34; On the surface, this question makes some sense. Time and effort are scarce resources that should be allocated wisely, but I think asking this question misses a vital point: that programming is much more difficult to learn than a programming language. I don&#39;t want to trivialize the effort needed to learn a new language, especially one like Racket that looks so different than popular C-like languages. But the syntax and specific features of a given language still only make up a small part of the knowledge required to master programming in general, in that language or any other. Indeed, experienced programmers can learn a new language much faster than novices, since they already know most of the concepts that underlie the new language, so they can focus on picking up the syntax or learning language-specific features instead. When a Racket programmer picks up JavaScript for the first time, they already know how to use anonymous functions, closures and higher-order functions, so they only need to map their existing knowledge onto JavaScript&#39;s syntax and focus on its distinct features. This advantage only increases over time, since the more languages that one learns, the more knowledge bases one has to draw on when picking up a new language. So yes, you will have to invest time into learning the specifics of Racket, but the reward for that effort is that most of the knowledge obtained will pay dividends when you do move on to other languages. And because the core of Racket is so small and so expressive, it allows users to quickly dive into those more difficult yet ultimately more valuable aspects of programming, rather than getting bogged down in the heavy syntax and quirkiness found in languages like Java. Functional programming isn&#39;t useless in practice The benefits of functional programming are often extolled from an intellectual point-of-view. The story goes that functional programming&#39;s focus on purity and statelessness will carry over to produce better results in imperative programming languages that de-emphasize those aspects. I agree with this sentiment, since I feel as though programming in a near-functional style is a useful default in any language, while still reaching for mutation and state when they&#39;re needed. That being said, even if the large majority of programmers and engineers in industry use non-functional languages, to say that functional programming by itself is entirely useless in practice is inaccurate. This summer, for instance, I will be working as a software engineer intern for a company that uses Clojure in its back end, an opportunity that I only could have gotten because of my experience with Racket. I say this not to brag, but to point out that if you want to work with functional programming professionally, you may be able to get by even if Racket is the only language on your resume. The functional programming community is certainly smaller than others in software engineering, but that&#39;s not entirely a bad thing. In my (albeit limited) experience, smaller opportunities can often be more accessible and more rewarding than flagship job or internship programmers that attract a massive amount of applications. In the end, every applicant needs something that makes them stand out, and if functional programming is their passion, that could be it. Additionally, and perhaps more importantly, even if the functional programming community within software engineering is relatively small, the importance of functional tools and techniques in almost every modern language has recently grown considerably. In the last few years, bastions of object-oriented programming like Java and C# have added support for lambda expressions, and Python introduced structural pattern matching -- both important tools lifted from functional languages. Not only that, but React, and its functional approach to managing state with hooks, has become the de-facto JavaScript UI library in the frontend web development community. Even if functional programming languages lag behind imperative or object-oriented ones in adoption, their techniques and features have increasingly bled over to the point that they&#39;re now unavoidable. Final Thoughts Racket is above all, an excellent programming language, and in my view, a valuable learning tool for novice and experienced programmers alike. That&#39;s why it frustrates me when my peers complain about the language for what I see as irrational reasons. Not everyone needs to love or prefer functional programming in Racket, but I hope this post at least helps some of those people appreciate its value. Notes </description>
      <pubDate>21 Feb 22 16:09 EST</pubDate>
      <guid>https://www.micahcantor.com/blog/why-learn-racket/</guid>
    </item>
    <item>
      <title>The Best Time Of Your Life Is Right Now</title>
      <link>https://www.nagekar.com/2021/02/the-best-time-of-your-life-is-right-now.html</link>
      <description>&lt;a href=&#34;https://www.nagekar.com/2021/02/the-best-time-of-your-life-is-right-now.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Humans are fragile creatures. The illusion of stability and control we have in our lives is comical. Our mind seems to have mastered the art of separating itself from the world events. We almost know it. It is hard not to if you spend any time on internet social medias or any form of conventional news sources. Unfortunate things happen, lives end or get ruined for reasons so trivial that’d make you not want to believe it. Yet, when it comes to our own lives, we’re fairly certain about our timelines. There’s career, that promotion, getting a house, getting married and so on. There’s always the end of life to be content with what we have; be grateful. Now’s the time to be at unrest, to complain and wish for more, to hustle. And not like it is our fault. The society is truly designed to make you feel exactly this way, discontent at every single stage of your life. Get good grades or you’d not get into a good university, study hard or you’d not have a good job, work harder or you’d not get that promotion, don’t plateau in your career or you’d not be able to afford a house, keep working to be able to enjoy a happy retirement and so on. And what if you’re still not able to enjoy your life finally at 65? Oh those are just the guidelines, too bad it didn’t work out for you. Guess what, it is called 1% for a reason. Try again in the next one. And of course, thank you for your participation in the rat race. So, what’s my point? The point that I’m trying to make is that if something is important enough to you, do it without waiting for some special phase of life to come by. No one knows how tomorrow will look like. If this entire pandemic has taught us anything, it is that we have no control over the future, not tomorrow and much less months or years in the future. The present is the only thing we have for certain, so why not make the best of it; by treating it like it is the peak of our health, wealth, social skills and so on. And how do we do that? By being grateful for what we have. It is only when we consciously  recognize how lucky we are to have all the things that we do, do we start valuing it. Being able to move around on your own, see, hear, talk, travel, read, write, meet friends, drink coffee, enjoy a sunset or snow; little things that many people might not have the good fortune to experience. The world is like a nasty slot machine. Luck plays a huge role in almost everything we do, and it starts right at the moment you’re born. We are the product of our circumstances. There’s not much we can do about that, except that we recognize our privileges and act accordingly. Have a chance to do something good for someone? Do it. Realize your actions might’ve caused hurt? Apologize. Have people that pull you back? Filter. Time really is the only real currency that we have. While it does seem like a tragedy to not know how much more of it do we have left, I think it is a blessing in disguise. Think of the last time you had a deadline for an assignment. Did you wait for the very last moment to do it? If you’re like most people then probably yes. That’s probably what would happen if we knew exactly how long we have to live. We would procrastinate everything until the last moment, wasting away most of it. Fortunately for us, we don’t, and each day can be lived as if it is the deadline for that life’s assignment, doing the things that matter the most to us. So to summarize, there probably will never be a better time to do certain things, and that’s if you’re lucky to live a full life without many problems. If you’re in your teenage years, you probably have the time to learn something thoroughly, spend time with friends and family, have fun, see clearly if you’re into that. If you’re in your twenties, you have the best balance between intelligence, energy, time and maybe some money too. Later in life you get better with relationships, your emotional intelligence grows and the life experiences you accumulate make you wiser while you’re getting rusty physically. Basically, we live through different interpretations of ‘peak’ throughout our lives, and there’s no one big peak that’s going to solve all your problems and make you happy. So make the most of your now; make memes, draw comics, write code, learn music, dance, sing, make someone’s day, be vulnerable, be nice, prioritize yourself, talk to random strangers and share stories, gift without a reason, sleep, cry, hug, do whatever you have to. But make your now count. Thank you for reading! </description>
      <pubDate>23 Feb 21 17:50 EST</pubDate>
      <guid>https://www.nagekar.com/2021/02/the-best-time-of-your-life-is-right-now.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://yosefk.com/blog/blogging-is-hard.html</link>
      <description>&lt;a href=&#34;https://yosefk.com/blog/blogging-is-hard.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I&#39;ve already written some stuff here. I read it again and wiped it out. It was self-righteous. I hate self-righteous. I could talk about how I hate self-righteous, but I won&#39;t, because that would be self-righteous. See? It&#39;s hard to blog without being self-righteous. I mostly wanted a technical blog, with an occasional sprinkle of life in it, like a picture or something. But mostly technical. Tech blogs I like fall into two overlapping categories: informative and entertaining. Sometimes both. Myself, I sure manage to deliver both in the physical world. &#34;Could you see that bug I bump into?&#34; &#34;Yeah, let&#39;s look at that, aha, oh, not this code, SHIT, this thing sucks, it&#39;s a torrent of shit, man, it&#39;s a ShitTorrent we have here. Ewww, this is so disgusting, wait, what do you mean _next==-1, -1 MY ASS, what the hell… um… get_what?! Here&#39;s that stupid fucking bug! Have a nice day.&#34; See? Informative and entertaining. Sometimes I even gather little audiences looking over my shoulder when I debug, all because profanity is my number one debugging tool. Catches all the bugs in a snap. Trust me. And that is loads of fun. Trouble is, it&#39;s not necessarily the kind of thing people want to get as a response for their next HTTP request. So I think I&#39;ll go for &#34;informative&#34; as first priority. If it works out at all. I have this problem with scaling communication. According to my estimations, the quality of my communication is inversely proportionate to the number of people listening (or people that I think are listening). That is, you get 100% face-to-face with nobody around, 50% if there&#39;s two of you and so on. All the way down to a whopping 1% of my exceptional rhetorical skills when I think I&#39;m talking to an audience. With this kind of personality, blogging is pretty hard. Seems like there&#39;s no reason to bother, then, but I think I will, because there&#39;s stuff in my brain that wants to come out. I recently spoke to a guy with lots of experience, in a broad sense. He&#39;s previously told me a couple of times how it would be wiser to keep my mouth shut in certain contexts. But this time, he said, &#34;sometimes it&#39;s extremely hard to hold it when I hear something stupid&#34;. What I think happens is, our brains are really cells of a larger brain (shaped like the Internet, of course); when stuff wants to come out, they have to let it out. And this is going to be in English, because writing about programming in Russian or Hebrew, which are my other options, is frigging ridiculous. Fellow Russians and/or Israelis, stop doing that! You ought to put so many English words into your text, like &#34;threads&#34;, &#34;namespaces&#34;, &#34;closures&#34;, that you end up switching languages twice per sentence. Or you can use those moronic translations of such words. That still counts as switching between languages – one good one and one stupid one. Just write the whole thing in English; makes it way more machine-readable, too (you know, vi). And if your English, like mine, is really just a first-order approximation rather than the real thing, it&#39;s not your problem – you won&#39;t notice. The native English speakers shouldn&#39;t have had their ancestors conquer that much land; now, they&#39;ll just have to put up with the consequences. </description>
      <pubDate>07 Mar 21 18:05 EST</pubDate>
      <guid>https://yosefk.com/blog/blogging-is-hard.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://utcc.utoronto.ca/~cks/space/blog/web/SafariUserPrivacyWildcard</link>
      <description>&lt;a href=&#34;https://utcc.utoronto.ca/~cks/space/blog/web/SafariUserPrivacyWildcard&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Safari is now probably the influential wild card browser for user privacy March 18, 2021 Today, Chrome is by far the dominant web browser, which gives it significant influence and weight, including in the area of user privacy. But Chrome is beholden to Google and Google is beholden to the torrents of money that pour in from intrusive Internet advertising and the associated consumer surveillance business. This means that there are limits on what Chrome will do; for instance it&#39;s probably not likely to be aggressive about not sending Referer headers. In general, Chrome&#39;s support of user privacy will always be limited and conditional. Firefox isn&#39;t beholden to Google (at least not in the same way), but sadly its overall usage is relatively low. Firefox still matters, for various reasons, but its influence is probably more moral than concrete at this point. People may be swayed by what Firefox does, including in the area of user privacy, but with low usage they&#39;re probably not directly affected by it. Inevitably Firefox generally has to wield its remaining influence carefully, and radical moves to help user privacy don&#39;t actually help all that many people; not all that many people use Firefox, and websites probably won&#39;t change much to accommodate them. (Such moves help me to some extent, but I&#39;m already taking extensive steps there that go well beyond any browser&#39;s normal behavior.) Safari definitely isn&#39;t beholden to Google, and it has enough usage to matter. Partly this is because of absolute numbers, but partly it&#39;s because Safari is the browser for what is generally an important and valuable market segment, namely iPhone users (sure, and iPads). If Safari does something and your website doesn&#39;t go along, you may have just entirely lost the iPhone market, which is generally seen as more willing to spend money (and more upscale) than Android users. This is true in general but especially true in user privacy; Apple has a brand somewhat built on that and it has less business that&#39;s affected by being strict on it (especially in the browser as opposed to apps). If Apple decides to have Safari do something significant for user privacy, it will affect a significant number of people in a valuable market segment. My guess is that this gives it outsized influence and makes it the wild card in this area. If Safari became aggressive about not sending Referer headers, for example, it probably becomes much more likely that Chrome will grumble and follow along in some way. (Conversely, if Safari refuses to implement some alleged &#39;feature&#39;, it becomes much less useful even if Chrome does implement it.) </description>
      <pubDate>19 Mar 21 22:32 EDT</pubDate>
      <guid>https://utcc.utoronto.ca/~cks/space/blog/web/SafariUserPrivacyWildcard</guid>
    </item>
    <item>
      <title></title>
      <link>https://product.hubspot.com/blog/how-to-learn-complex-things-quickly</link>
      <description>&lt;a href=&#34;https://product.hubspot.com/blog/how-to-learn-complex-things-quickly&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In my 20+ years as a software engineer, I’ve constantly been asked or expected to learn complex things. In his book Deep Work, when talking about the knowledge economy, author Cal Newport writes, “If you can’t learn, you can’t thrive.” New languages and platforms are launched, projects get new requirements, libraries and frameworks release new versions ⁠— in software, the only constant is change. A friend who was recently starting a new role asked “Got any tips to help me learn this new landscape quickly?” When I sent him some very rough thoughts he said they were “gold” and encouraged me to publish them for the world to see ⁠— I hope others can find them even a little bit useful. Disclaimer ⁠— I’m a breadth first, experiential learner. I try stuff and when it doesn’t work, I try something else, moving on to the next step as soon as it ‘works.’ This helps me create an ever clearer mental model with each iteration. This works for me, but I recognize there are many learning styles, and it may not work for you wholesale. Define your goals Break whatever you’re trying to learn down into use cases. Start with bite sized chunks that take a few minutes and build on them incrementally. “Learning python” is too broad. Installing python, printing hello world, installing and using a dependency, reading from a file, etc. are more well-defined, and it’s easier to know when you’re done, therefore helping to reinforce progress. Consider how much you need to learn to accomplish each. Bucketing things into &#34;need to know more&#34; and &#34;skip the details for now&#34; can be helpful. For instance, I don&#39;t know how python dependency management works, but I can rely on it working and make progress without that knowledge. Start by just trying stuff. You might be scared that you’ll screw something up, but in my experience computers are pretty hard to break. Use the mental models that you already have to guess at how it might work. If it doesn’t, then you can respond to error messages. Take breaks. If you’re in the groove and cruising, by all means ⁠— keep in the flow. But if you’re finding that you’re losing steam, moving sideways, or retreading old ground, give your brain a rest. If you defined your goals in small increments, you should be able to walk away feeling like you made some progress and give your brain time to process what you’ve learned.  Read the full manual (RTFM), but don’t try to internalize it all Get the gist. If you’re learning something with published documentation, read the intro, the first and last paragraph of each section/chapter, and the first sentence of every paragraph, along with any provided code samples. If you find yourself confused, back up a bit and go deeper as necessary. Heed the callouts. Callouts and tips can be really helpful. They’re often provided in spots where it’s easy to make mistakes. Create a ‘read later’ list. I’ll often find myself curious about how a related part of the system works, but unless it is something I need to know to accomplish the use case I’m focused on, I defer that. I keep a reading list in my todo app, but there are lots of solutions out there.  Use ‘getting started’ guides.This is my exception to the “just skim it” rule. Getting started guides are usually aimed at folks just beginning their learning journey and are designed to give you just enough information to accomplish a single use case. Don’t skim them ⁠— read the whole thing and follow along with the provided examples. Check GitHub for boilerplate projects. There’s more than just docs. Videos, podcasts, and livestreams can each be a valuable way to convey information, if they exist. Experiment with different media and find what works for you. Work your way up to watching / listening at 2x speed, but slow down for the interesting parts. Learn to dig (a little bit) deeper So you’ve trusted your instinct and just started, but you immediately run into an obstacle. Maybe it is command not found, NullPointerException, or a 404 on a web page. How can you get more information out of the system? CLI help. Most Command Line Interfaces (CLIs) will return some content for help, --help, or -h. There might be clues about the arguments and their order that could be useful.  Dive into the logs. Most systems will log some output, either to standard output on the console or somewhere on disk. Reading the logs can be like reading a story of what the system is trying to do. Crank up the verbosity. If you’ve read the logs and aren’t sure where things are going sideways, try increasing the verbosity of the output/logs. You can often find helpful clues and new areas to investigate that will help get you closer to the problem. Search for error messages. Others may have run into the same issue, and you might find a lead on a Q&amp;A site, like Stack Overflow, discussion or support forums, or open issues in GitHub. Make sure you quote the error message and try removing parts that are specific to your use case vs more general. Read the Source Code. If you have access to the source code of the system, reading it can help you understand how the system expects to treat your use case. Don’t try to understand every line, but use class and method names as signals for the system’s nouns and verbs. For internal tools that don’t have good logging, docs, or help, this may be the only option. Not sure if the code is open source? Try searching for the library in your language’s package manager (npmjs.org for Node.js, pypy.org for python, etc) or try “GitHub {programming language} {package name}” in your favorite search engine. Tip: Give yourself a timebox to avoid turning your investigation into a never-ending spelunking excursion. Decrease the size of your timebox as you gain comfort with the system. If you’re brand new to something, give yourself a few hours. If you’ve been living in the system for months, dial that down to 15 minutes. Don’t be afraid to ask for help Do your homework first - chances are someone has run into this before, especially if you are brand new to this topic. All of the following are good resources for you while trying to find answers: Documentation (see RTFM) FAQs Community forums Chat history Search Make it easy for others to help by documenting the issue and some relevant context. What are you trying to accomplish? What behavior were you expecting, what did you observe? What have you tried? What’s your execution context? (OS, versions, plugins, etc) Here’s an (older) example from real life: https://github.com/Kong/kong/issues/1186  Reflect on the process Summarize and share what you’ve learned - that’s a good habit for all knowledge workers Where did you spend time that didn’t help you get closer to your goal? What would have made it easier for you to learn again? Docs clarifications? Interface changes? Can you fix them (internal or open source) or suggest changes to make it easier for future learners? Summary Learning is an iterative process. Start by defining your goals, and break them down into bite-sized chunks. Get some context by reading whatever docs are available, but don’t try to digest them in one bite. Don’t be scared to try to get more information out of the system. Ask for help when you’ve exhausted your budget, but be very clear about what you’re asking. Create feedback loops to improve your learning process and that of future learners.  Now go and learn something new! Interested in working with a team that&#39;s just as invested in your learning and growth as you are? Check out our open positions and apply. </description>
      <pubDate>08 Apr 21 13:55 EDT</pubDate>
      <guid>https://product.hubspot.com/blog/how-to-learn-complex-things-quickly</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.raptitude.com/2013/10/the-elegant-secret-to-self-discipline/</link>
      <description>&lt;a href=&#34;https://www.raptitude.com/2013/10/the-elegant-secret-to-self-discipline/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Despite my lofty ethical and financial aspirations, I developed a tragic ice cream habit during the summer. There are all kinds of long- and short-term problems with this: it’s bad for my health, morally dubious to say the least, and totally anti-frugal — a big no-no for my new career as a tightfisted writer. My justification was always pretty lame. I would explain to myself that I’m about to stop doing this, therefore it doesn’t matter if I do it right now. The Devil on my shoulder would only have to say, “But it’s just for now. Enjoy!” and I would already be on an unstoppable march to Safeway. If I had given the angel on the other shoulder a chance to rebut, she would have explained the foolish tradeoff I was making. I gain twenty minutes or so of low-brow pleasure. All the benefit of this choice is gone after that. I lose, in a more lasting way, some of my money, my dignity, my sense of self-control, and my health. Only a fool would choose the first option, but when faced with certain frozen desserts, or other present-moment incentives I often become a fool, and maybe you do too. The hallmark of the fool is that he borrows fleeting pleasures, at interest, from himself. Self-discipline is time travel I have a beautiful banana sitting beside my laptop right now. No black spots, no green tinge. It’s truly the perfect banana and I know it will fulfill my expectations when I do eat it. It’s sitting about six inches from the edge of my desk and a foot from the front. I could move it to the other side of the desk, to the back of the desk or the front, and it would be the same promising banana. I could also move it in a third dimension by putting it on top of my bookcase, or move it across all three dimensions by walking it back to the fruit bowl at the center of my dining room table, and nothing of value will be lost. I really want to eat this banana, and that desire distracts me from realizing that I could move my banana in a fourth dimension, by eating it in an hour, or four hours, and it would still provide pretty much the same levels of pleasure and dietary potassium. I forget that if I eat it now, Future David will have no banana to eat at all. So I am rewarding Right Now David at the expense of Future David. Depending on the circumstances, Future David might even benefit more from that banana than Right Now David would. If it wasn’t quite ripe right now, there would be more enjoyment to be gained from it tomorrow. Still, Right Now David has a considerable preference for himself, and in fact he is already eating the banana. As I mature, I notice Right Now David getting better at sharing with his Future-based colleague, and I hope one day he is able to treat all other Davids as he treats himself.  A Right Now banana and a Near-future banana will usually have about the same value, so it’s not exactly a pivotal life decision. However, there are some circumstances in which I serve Right Now David in a way that’s small and fleeting, that simultaneously denies Future David something much more significant. Once upon a time, I would occasionally spend more than my paycheck in a given pay period, garnishing poor Future David’s wages for the next period, when obviously even a full paycheck didn’t always feel like enough. Other times, Right Now David would be drunk and would decide that he would have a few more unmemorable drinks, which added very little to his pleasure level, yet invariably sentenced Tomorrow David to severe physical suffering. At around age 30, Future David caught on to this injustice and would tap Right Now David on the shoulder when he was about to do something mean like that. Progress. I still quite often sell out Future David though, leaving him with less so that Right Now David can indulge some Right-Now urge. The reality, I am somehow still gradually learning, is that Future David will actually be Right Now David at some real point in time, and not in an abstract way. At any given present moment, whether I realize it or not, I am the Future David that Past Davids have sold out in all sorts of ways. Right Now David would have a lot more money, as one example, if Past Davids had not indulged their momentary desires for ice cream and booze — or to dredge up some really old baggage — for candy, basketball cards and Super Nintendo Games. Right Now David would be smart to understand how Past Davids have sold him out (and, less often, helped him) as he contemplates what to do with his day, for many Future Davids live at the mercy of Right Now David’s wisdom and discipline, or his shortage thereof. Future David is praying that Right Now David realizes that his future self is just as much a human being with needs and desires as his current self. If he can’t treat other people quite as highly as he treats himself, at least he can treat himself as highly as he treats himself, even if it’s the self he will be later. That’s the elegant secret to discipline: valuing your future self as highly as you value your current self, at least long enough to get your Right Now Self to do the right thing. That moment of choice is where the ants go one way and grasshoppers the other. I’m reminded of the now well-known Marshmallow Experiment, conducted at Stanford in the late 1960s. Researchers sat young children in front of a marshmallow on a plate, told them if they wait fifteen minutes before eating the marshmallow they would get a second one, and then left them alone. A third of the kids waited the full fifteen minutes — an eon to a five-year-old — and earned their second marshmallow. The experiment has been reproduced many times since and the footage is hilarious. When scientists followed up fifteen years later, the kids who waited for the second marshmallow had all become doctors and presidents, or were at least on their way. There is often much more at stake than bananas and marshmallows. This year I made a living experiment out of seeing whether I wouldn’t be just as happy living on half of what I lived on last year. It turns out that This Year David has had a consistently higher quality of life than the comparatively foolish Last Year David, and as a direct result, Right Now David is currently writing at his sunny home office desk on a weekday morning in his pyjamas instead of being told what to do by The Man. Like any other insight, it’s one thing to nod your head while you think about it and something else to turn it into a real advantage in your life. There are two tricks that I can see to doing it in real-time: 1) Recognize that right now already is the future. You are currently experiencing the future of all your Past Selves. Their choices have come to fruition. If you would like better fruits, make your Right Now Self into someone who, as a habit, rolls out the red carpet for Future Self. Imagine if someone had already done that for you. Highly disciplined people are always experiencing advantages inherited from their wise and caring Past Selves. 2) Recognize the moments when you’re about to sell out your Future Self. These moments often happen when you are in retail establishments. They often involve televisions or other gratifying electronic devices, including the snooze button of your alarm clock. They also frequently involve high-fructose corn syrup and disposable packaging. Future Self is totally, absolutely You as much as you’re You right now. It will be living real moments with real advantages and disadvantages, determined mostly by Right Now Self’s behavior. In its confounding ignorance, Right Now Self often blames Past Self for having squandered its opportunities and resources, while simultaneously failing to fulfill its responsibilities to helpless Future Self. When will it see that it already is Future Self? Later, I suppose. *** PHOTO BY Andrés Nieto Porras Need help focusing? The big productivity books are written by people who don&#39;t especially struggle with productivity. The rest of us find other ways. I shared mine in this post. </description>
      <pubDate>08 Nov 21 14:12 EST</pubDate>
      <guid>https://www.raptitude.com/2013/10/the-elegant-secret-to-self-discipline/</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.yossarian.net/2021/12/05/Blockchains-dont-solve-problems-that-are-interesting-to-me</link>
      <description>&lt;a href=&#34;https://blog.yossarian.net/2021/12/05/Blockchains-dont-solve-problems-that-are-interesting-to-me&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;ENOSUCHBLOG Programming, philosophy, pedaling. Home Tags Series Favorites Archive Main Site Blockchains don&#39;t solve problems that are interesting to me Dec 5, 2021     Tags: cryptography, rant     Preword Financial disclosures: I have no direct or derivative positions in any cryptocurrencies, or any investments at all in any distributed ledger companies. Professional disclosures: My professional work is not tied to cryptocurrencies, and I am not paid to work on cryptocurrencies. I am also not paid to work on any competing financial instruments or products. My opinions are also not necessarily the opinions of my employer. This post has been a long time coming: for the past 5+ years, I’ve been asked questions that vary across these templates: Do you like blockchains? Why, or why not? Should I invest in $CRYPTOCURRENCY? Are blockchains doing $GOOD_THING for $GROUP? I’ve had friends, family, professional acquaintances, and random strangers ask me these questions. The blogosphere is already saturated with excellent technical and rhetorical summaries of why the cryptocurrency space is, to save a whole bunch of words, just plain bad1. So I’m going to take a different tack: I’m going to start with the financial issues that I personally want and wish to be solved, and walk backwards to demonstrate why I don’t think blockchains adequately solve them. Paying companies, my friends and my family a little bit of money This is really three problems rolled into one: I read a lot of articles online, and I would like (automatically?) to send a nominal but not meaningless amount of money each time I read them. Something like 10 to 50 cents per read. I would like all or virtually all of that money to end up in the publication or author’s bank account, without having to worry about it being whittled down by transaction fees or minimum withdrawal amounts. I buy dinner and drinks with friends, and I would like to be able to automatically itemize and split either the receipt itself or the debt-settling process afterwards. I pay my family back for things. They don’t know how to use payment apps (and sometimes aren’t on the Internet at all), and I would like to be able to immediately send money to their checking or savings accounts without having to worry about physical checks or setting them up with a service that they don’t know how to withdraw from all while being charged fees. As far as I can tell, no cryptocurrency solves any of these problems for me. The closest might be (1), which companies like Brave purport to solve by offering “attention tokens”. Except not really: instead of allowing me to pay a newspaper for an article, Brave wants pays me in company scrip for the privilege of watching ads that they’ve placed on participating websites2. Similarly, as far as I can tell, nobody in the cryptocurrency space has dedicated any substantial amount of effort to (2) or (3). (2) is mostly being handled by payment apps like Venmo and Cash, which give you either standard (2-3 day) settlement for free or “instant”3 settlement for a percentage-plus fee. But even these apps don’t give me what I think would be really cool, which is a way to create an ephemeral debit pool between my friends and I, one that we could all charge (up to a limit) and then automatically itemize and/or evenly split at the end of the meal or event. Everything else being equal4, this seems like the kind of thing that a smart contract would excel at! But it doesn’t exist. I don’t think (3) will ever exist in the cryptocurrency space. Nobody I’ve talked to in the space seems particularly interested in anyone who doesn’t use the Internet regularly5, much less the unbanked or underbanked6. There’s just no (obscene) profit in it. Giving money to nonprofits without middlemen I try to donate money every year, both because there are things I’d like to support and because I’m fortunate enough to work for a company with a generous matching policy. I try to use a debit card for donations, so that the places I donate to aren’t hit with credit card fees on top of the normal overhead associated with soliciting donations. This, on face value, seems like a reasonable use for a cryptocurrency! And I’ve had people tell me that, when $SETTLEMENT_SOLUTION hits $POPULAR_BLOCKCHAIN7, transaction fees will crater and that both I and the recipient will be able to enter and exit the blockchain with virtually all of the intended donation unscathed. This never seems to materialize. I’ve had a few people respond to this by pointing me to niche cryptocurrencies that boast low transaction fees. It’s not clear to me whether these low fees are a function of the environment (nobody using the coin, so miners/verifiers/whoever will take what they can get), or a function of a novel technical improvement that genuinely solves the problem. But at the end of the day, the distinction does not matter: just about every nonprofit that I care about donating to is not going to waste their time chasing down a few hundred dollars in $RANDOM_COIN for me. Even if they’re using volunteers to handle their finances, it costs them money in the form of time to leave the world of actual money and cater to yet another middleman. Overseas remittance without middlemen This is not a problem that affects me directly, but it’s one that I see raised frequently as an example of something that can be legitimately solved with cryptocurrencies. On face value, this again seems reasonable to me: I’ve had friends explain that companies like Western Union are effectively a racket and a regressive tax on those who send and receive remittances. Cutting out a middleman whose business model dates back to the time of settlement-by-telegraph seems like a good thing. From here, a few things lack explanations: Where does the money exit8? Are we going to litter bitcoin ATMs across Central America? Cryptocurrency advocates point out that Western Unions are an obvious place to rob people of their remittances9; it’s not clear how replacing the brick-and-mortar structure with a freestanding ATM improves the state of things. Again, the transaction fees. Western Union is expensive; is Bitcoin or Ethereum doing any better at the volumes of money that people are regularly remitting? I genuinely don’t know, but the recent ConstitutionDAO fracas suggests that people are paying close to 50% of their transaction in gas and wallet transfer fees. Maybe people are remitting more with the niche cryptocurrencies I mentioned above, and so are “flying under the radar”? But that doesn’t feel especially sustainable. Novel criminal potential. When someone robs a Western Union, it’s Western Union’s problem — they still have to make their customers whole. Similarly for a bank. Nobody is responsible for the contents of a bitcoin wallet (or bearing account on a service) except the credential holder(s), and recourse after some rubber hole cryptanalysis is significantly limited. Clawing my money back when I do something stupid I, like every other fallible being, occasionally make mistakes, have mistakes made upon me, or just do plain old stupid things with my money or money-bearing instruments. In the past couple of years, I’ve: Been accused of stealing a ride shake bike that I rented; Used a debit card on an obviously skimmed ATM because there were no other ATMs around and I needed cash; Bought something from a buggy website that charged me multiple times for shipping; Received obviously fake or counterfeit stand-ins for things I’ve purchased; Sent money to the wrong person, both by check and ACH. In each of these cases, I was saved by human systems: I was able to call a phone number, get a human being on the other end, walk them through my situation, and receive a satisfactory outcome (refunds, replacements, service credits, new payment cards, &amp;c). When my initial contact at the merchant was unable or unwilling to help me, I was able to escalate until I reached an person who (1) was authorized to resolve my problem, and (2) could see engage further systems to convert the problem into an internal one that got resolved without my involvement. All human systems are subject to fraud and abuse. But removing humans from the system does not remove the fraud — it just incentivizes novel forms of fraud automation, and promotes reversible accidents into irreversible ones. I dread to think of a world where a cosmic bitflip sends my paycheck to the wrong blockchain address, either flushing my money into the void or sending it to someone who has no formal reason to help me get it back10. Wrapup All told, the things I want fall into a few very simple categories: Instant settlement to a currency that everybody agrees on (no weird tokens that swing daily, or scrip for the company store) Very, very low transaction fees for settlement options (economies of scale are a wonderful thing in finance — a 0.0001% transaction fee can be sustainable, even profitable, with enough transactions) Financial “customization”: I would love to be able to program more of my financial patterns, without involving a global distributed ledger. Human-proofedness: I would like my finances to not be dependent on mechanical perfection from either me or the machines I use. The machines should speed things up, and then get the hell out of the way when things go wrong. To point out the obvious: cryptocurrencies don’t have to solve all, or even most, or even a small minority of these problems and economic frictions. But these, not others, are the things that are present in my life (or that I care about, because I think they’re net benefits to society). It’s also not a claim that cryptocurrencies can’t, in a formal sense, solve some of these problems. But I’ve yet to see an application that made unique sense for a cryptocurrency, one that justifies burning coal11 or formally enforcing an oligopoly of stakeholders12. It’s easy to be negative. So, what would I actually like to see? I would like to see the reintroduction of a public banking option in the United States. Public banking, more than anything else, stands the best chance at materially improving access to the financial system for the most needy. I would like to see the completion of FedNow, the Fed’s nascent ACH replacement. FedNow promises to deliver 24/7 instant settlement at even lower costs than ACH13, and will put pressure on payment apps to kill their fees. I would like to see countries cooperate on a formal remittance framework: the only things stopping direct international bank-to-bank transfers are inertia and greed. Maybe that’s awfully idealistic of me, but it feels no more delusional (and a whole lot nicer) than some of the things I’ve seen the cryptocurrency predict and propose. Better, faster, more just, and more equitable financial systems are within our reach, without cryptocurrency. I hope that we get there. Discussions: Reddit Twitter </description>
      <pubDate>05 Dec 21 15:47 EST</pubDate>
      <guid>https://blog.yossarian.net/2021/12/05/Blockchains-dont-solve-problems-that-are-interesting-to-me</guid>
    </item>
    <item>
      <title>Science Is Getting Less Bang for Its Buck</title>
      <link>https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/</link>
      <description>&lt;a href=&#34;https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The writer Stewart Brand once wrote that “science is the only news.” While news headlines are dominated by politics, the economy, and gossip, it’s science and technology that underpin much of the advance of human welfare and the long-term progress of our civilization. This is reflected in an extraordinary growth in public investment in science. Today, there are more scientists, more funding for science, and more scientific papers published than ever before:Data from Patrick Collison and Michael NielsenOn the surface, this is encouraging. But for all this increase in effort, are we getting a proportional increase in our scientific understanding?  Or are we investing vastly more merely to sustain (or even see a decline in) the rate of scientific progress?It’s surprisingly difficult to measure scientific progress in meaningful ways. Part of the trouble is that it’s hard to accurately evaluate how important any given scientific discovery is.Consider the early experiments on what we now call electricity. Many of these experiments seemed strange at the time. In one such experiment, scientists noticed that after rubbing amber on a cat’s fur, the amber would mysteriously attract objects such as feathers, for no apparent reason. In another experiment, a scientist noticed that a frog’s leg would unexpectedly twitch when touched by a metal scalpel.Even to the scientists doing these experiments, it wasn’t obvious whether they were unimportant curiosities or a path to something deeper. Today, with the benefit of more than a century of hindsight, they look like epochal experiments, early hints of a new fundamental force of nature.But even though it can be hard to assess the significance of scientific work, it’s necessary to make such assessments. We need these assessments to award science prizes, and to decide which scientists should be hired or receive grants. In each case, the standard approach is to ask independent scientists for their opinion of the work in question. This approach isn’t perfect, but it’s the best system we have.With that in mind, we ran a survey asking scientists to compare Nobel Prize–winning discoveries in their fields. We then used those rankings to determine how scientists think the quality of Nobel Prize–winning discoveries has changed over the decades.As a sample survey question, we might ask a physicist which was a more important contribution to scientific understanding: the discovery of the neutron (the particle that makes up roughly half the ordinary matter in the universe) or the discovery of the cosmic-microwave-background radiation (the afterglow of the Big Bang). Think of the survey as a round-robin tournament, competitively matching discoveries against each other, with expert scientists judging which is better.For the physics prize, we surveyed 93 physicists from the world’s top academic physics departments (according to the Shanghai Rankings of World Universities), and they judged 1,370 pairs of discoveries. The bars in the figure below show the scores for each decade. A decade’s score is the likelihood that a discovery from that decade was judged as more important than discoveries from other decades. Note that work is attributed to the year in which the discovery was made, not when the subsequent prize was awarded.Data from Patrick Collison and Michael NielsenThe first decade has a poor showing. In that decade, the Nobel Committee was still figuring out exactly what the prize was for. There was, for instance, a prize for a better way of illuminating lighthouses and buoys at sea. That’s good news if you’re on a ship, but it scored poorly with modern physicists. But by the 1910s, the prizes were mostly awarded for things that accord with the modern conception of physics.A golden age of physics followed, from the 1910s through the 1930s. This was the time of the invention of quantum mechanics, one of the greatest scientific discoveries of all time, a discovery that radically changed our understanding of reality. It also saw several other revolutions: the invention of X-ray crystallography, which let us probe the atomic world; the discovery of the neutron and of antimatter; and the discovery of many fundamental facts about radioactivity and the nuclear forces. It was one of the great periods in the history of science.Following that period, there was a substantial decline, with a partial revival in the 1960s. That was due to two discoveries: the cosmic-microwave-background radiation, and the standard model of particle physics, our best theory of the fundamental particles and forces making up the universe. Even with those discoveries, physicists judged every decade from the 1940s through the 1980s as worse than the worst decade from the 1910s through 1930s. The very best discoveries in physics, as judged by physicists themselves, became less important.Our graph stops at the end of the 1980s. The reason is that in recent years, the Nobel Committee has preferred to award prizes for work done in the 1980s and 1970s. In fact, just three discoveries made since 1990 have been awarded Nobel Prizes. This is too few to get a good quality estimate for the 1990s, and so we didn’t survey those prizes.However, the paucity of prizes since 1990 is itself suggestive. The 1990s and 2000s have the dubious distinction of being the decades over which the Nobel Committee has most strongly preferred to skip, and instead award prizes for earlier work. Given that the 1980s and 1970s themselves don’t look so good, that’s bad news for physics.Many reasonable objections can be leveled at our survey. Maybe the surveyed physicists are somehow biased, or working with an incomplete understanding of the prizewinning discoveries. As discussed earlier, it’s hard to pin down what it means for one discovery to be more important than another. And yet, scientists’ judgments are still the best way we have to compare discoveries.Even if physics isn’t doing so well, perhaps other fields are doing better? We carried out similar surveys for the Nobel Prize for chemistry and the Nobel Prize for physiology or medicine. Here are the scores:Data from Patrick Collison and Michael NielsenThe results are slightly more encouraging than physics, with perhaps a small improvement in the second half of the 20th century. But it is small. As in physics, the 1990s and 2000s are omitted, because the Nobel Committee has strongly preferred earlier work: Fewer prizes were awarded for work done in the 1990s and 2000s than over any similar window in earlier decades.The picture this survey paints is bleak: Over the past century, we’ve vastly increased the time and money invested in science, but in scientists’ own judgement, we’re producing the most important breakthroughs at a near-constant rate. On a per-dollar or per-person basis, this suggests that science is becoming far less efficient.Now, a critic might respond that the quality of Nobel Prize discoveries isn’t the same as the overall rate of progress in science. There are certainly many limitations of this measure. Parts of science are not covered by the Nobel Prizes, especially newer areas like computer science. The Nobel Committee occasionally misses important work. Perhaps some bias means scientists are more likely to venerate older prizes. And perhaps what matters more is the bulk of scientific work, the ordinary discoveries that make up most of science.We recognize these limitations: The survey results are striking, but provide only a partial picture. However, we’ll soon see supporting evidence suggesting that it’s getting much harder to make important discoveries across the board. It’s requiring larger teams and far more extensive scientific training, and the overall economic impact is getting smaller. Taken together, these results suggest strong diminishing returns to our scientific efforts.When we report these diminishing returns to colleagues, they sometimes tell us that this is nonsense, and insist that science is going through a golden age. They point to amazing recent discoveries, such as the Higgs particle and gravitational waves, as evidence that science is in better shape than ever.These are, indeed, astonishing discoveries. But previous generations also made discoveries that were equally, if not more, remarkable. Compare, for example, the discovery of gravitational waves to Einstein’s 1915 discovery of his general theory of relativity. Not only did general relativity predict gravitational waves, it also radically changed our understanding of space, time, mass, energy, and gravity. The discovery of gravitational waves, while enormously technically impressive, did much less to change our understanding of the universe.And while the discovery of the Higgs particle is remarkable, it pales beside the pantheon of particles discovered in the 1930s, including the neutron, one of the main constituents of our everyday world, and the positron, also known as the antielectron, which first revealed the shadowy world of antimatter. In a sense, the discovery of the Higgs particle is remarkable because it’s a return to a state of affairs common in the first half of the 20th century, but rare in recent decades.Another common response is from people who say science is in better shape than ever because their own field is making great progress. We hear this most often about artificial intelligence (AI) and the CRISPR gene-editing technology in biology. But while AI, CRISPR, and similar fields are certainly moving fast, there have always been fields just as hot or hotter through the entire history of modern science.Consider the progress of physics between 1924 and 1928. Over that time, physicists learned that the fundamental constituents of matter have both a particle and a wave nature; they formulated the laws of quantum mechanics, leading to Heisenberg’s uncertainty principle; they predicted the existence of antimatter; and many other things besides. As one of the leading protagonists, Paul Dirac, said, it was a time when “even second-rate physicists could make first-rate discoveries.”For comparison, major discoveries in AI over the past few years include an improved ability to recognize images and human speech, and the ability to play games such as Go better than any human. These are important results, and we’re optimistic that work in AI will have a huge impact in the decades ahead. But it has taken far more time, money, and effort to generate these results, and it’s not clear they’re more significant breakthroughs than the reordering of reality uncovered in the 1920s.Similarly, CRISPR has seen many breakthroughs over the past few years, including the modification of human embryos to correct a genetic heart disorder, and the creation of mosquitoes that can spread genes for malaria resistance through entire mosquito populations. But while such laboratory proofs-of-principle are remarkable, and the long-run potential of CRISPR is immense, such recent results are no more impressive than those of past periods of rapid progress in biology.Why has science gotten so much more expensive, without producing commensurate gains in our understanding?A partial answer to this question is suggested by work done by the economists Benjamin Jones and Bruce Weinberg. They’ve studied how old scientists are when they make their great discoveries. They found that in the early days of the Nobel Prize, future Nobel scientists were 37 years old, on average, when they made their prizewinning discovery. But in recent times that has risen to an average of 47 years, an increase of about a quarter of a scientist’s working career.Perhaps scientists today need to know far more to make important discoveries. As a result, they need to study longer, and so are older, before they can do their most important work. That is, great discoveries are simply getting harder to make. And if they’re harder to make, that suggests there will be fewer of them, or they will require much more effort.In a similar vein, scientific collaborations now often involve far more people than they did a century ago. When Ernest Rutherford discovered the nucleus of the atom in 1911, he published it in a paper with just a single author: himself. By contrast, the two 2012 papers announcing the discovery of the Higgs particle had roughly a thousand authors each. On average, research teams nearly quadrupled in size over the 20th century, and that increase continues today. For many research questions, it requires far more skills, expensive equipment, and a large team to make progress today.If it’s true that science is becoming harder, why is that the case?Suppose we think of science—the exploration of nature—as similar to the exploration of a new continent. In the early days, little is known. Explorers set out and discover major new features with ease. But gradually they fill in knowledge of the new continent. To make significant discoveries explorers must go to ever-more-remote areas, under ever-more-difficult conditions. Exploration gets harder. In this view, science is a limited frontier, requiring ever more effort to “fill in the map.” One day the map will be near complete, and science will largely be exhausted. In this view, any increase in the difficulty of discovery is intrinsic to the structure of scientific knowledge itself.An archetype for this point of view comes from fundamental physics, where many people have been entranced by the search for a “theory of everything,” a theory explaining all the fundamental particles and forces we see in the world. We can only discover such a theory once. And if you think that’s the primary goal of science, then it is indeed a limited frontier.But there’s a different point of view, a point of view in which science is an endless frontier, where there are always new phenomena to be discovered, and major new questions to be answered. The possibility of an endless frontier is a consequence of an idea known as emergence. Consider, for example, water. It’s one thing to have equations describing the way a single molecule of water behaves. It’s quite another to understand why rainbows form in the sky, or the crashing of ocean waves, or the origins of the dirty snowballs in space that we call comets. All these are “water,” but at different levels of complexity. Each emerges out of the basic equations describing water, but who would ever have suspected from those equations something so intricate as a rainbow or the crashing of waves?The mere fact of emergent levels of behavior doesn’t necessarily imply that there will be a never-ending supply of new phenomena to be discovered, and new questions to be answered. But in some domains it seems likely. Consider, for example, that computer science began in 1936 when Alan Turing developed the mathematical model of computation we now call the Turing machine. That model was extremely rudimentary, almost like a child’s toy. And yet the model is mathematically equivalent to today’s computer: Computer science actually began with its “theory of everything.” Despite that, it has seen many extraordinary discoveries since: ideas such as the cryptographic protocols that underlie internet commerce and cryptocurrencies; the never-ending layers of beautiful ideas that go into programming language design; even, more whimsically, some of the imaginative ideas seen in the very best video games.These are the rainbows and ocean waves and comets of computer science. What’s more, our experience of computing so far suggests that it really is inexhaustible, that it’s always possible to discover beautiful new phenomena, new layers of behavior which pose fundamental new questions and give rise to new fields of inquiry. Computer science appears to be open-ended.In a similar way, it’s possible new frontiers will continue to open up in biology, as we gain the ability to edit genomes, to synthesize new organisms, and to better understand the relationship between an organism’s genome and its form and behavior. Something similar may happen in physics and chemistry too, with ideas such as programmable matter and new designer phases of matter. In each case, new phenomena pose new questions, in what may be an open-ended way.So the optimistic view is that science is an endless frontier, and we will continue to discover and even create entirely new fields, with their own fundamental questions. If we see a slowing today, it is because science has remained too focused on established fields, where it’s becoming ever harder to make progress. We hope the future will see a more rapid proliferation of new fields, giving rise to major new questions. This is an opportunity for science to accelerate.If science is suffering diminishing returns, what does that mean for our long-term future? Will there be fewer new scientific insights to inspire new technologies of the kind which have so reshaped our world over the past century? In fact, economists see evidence this is happening, in what they call the productivity slowdown.When they speak of the productivity slowdown, economists are using “productivity” in a specialized way, though close to the everyday meaning: Roughly speaking, a worker’s productivity is the ingenuity with which things are made. So productivity grows when we develop technologies and make discoveries that make it easier to make things.For instance, in 1909 the German chemist Fritz Haber discovered nitrogen fixation, a way of taking nitrogen from the air and turning it into ammonia. That ammonia could then, in turn, be used to make fertilizer. Those fertilizers allowed the same number of workers to produce far more food, and so productivity rose.Productivity growth is a sign of an economically healthy society, one continually producing ideas that improve its ability to generate wealth. The bad news is that U.S. productivity growth is way down. It’s been dropping since the 1950s, when it was roughly six times higher than today. That means we see about as much change over a decade today as we saw in 18 months in the 1950s.That may sound surprising. Haven’t we seen many inventions over the past decades? Isn’t today a golden age of accelerating technological change?Not so, argue the economists Tyler Cowen and Robert Gordon. In their books The Great Stagnation and The Rise and Fall of American Growth, they point out that the early part of the 20th century saw the large-scale deployment of many powerful general-purpose technologies: electricity, the internal-combustion engine, radio, telephones, air travel, the assembly line, fertilizer, and many more.By contrast, they marshal economic data suggesting that things haven’t changed nearly as much since the 1970s. Yes, we’ve had advances associated to two powerful general-purpose technologies: the computer and the internet. But many other technologies have improved only incrementally.Think, for example, about the way automobiles, air travel, and the space program transformed our society between 1910 and 1970, expanding people’s experience of the world. By 1970 these forms of travel had reached something close to their modern form, and ambitious projects such as the Concorde and the Apollo Program largely failed to expand transportation further. Perhaps technologies like self-driving cars will lead to dramatic changes in transport in the future. But recent progress in transport has been incremental when compared to the progress of the past.What’s causing the productivity slowdown? The subject is controversial among economists, and many different answers have been proposed. Some have argued that it’s merely that existing productivity measures don’t do a good job measuring the impact of new technologies. Our argument here suggests a different explanation, that diminishing returns to spending on science are contributing to a genuine productivity slowdown.We aren’t the first to suggest that scientific discovery is showing diminishing returns. In his 1996 book The End of Science, the science writer John Horgan interviewed many leading scientists and asked them about prospects for progress in their own fields. The distinguished biologist Bentley Glass, who had written a 1971 article in Science arguing that the glory days of science were over, told Horgan:It’s hard to believe, for me, anyway, that anything as comprehensive and earthshaking as Darwin’s view of the evolution of life or Mendel’s understanding of the nature of heredity will be easy to come by again. After all, these have been discovered!*Horgan’s findings were not encouraging. Here is Leo Kadanoff, a leading theoretical physicist, on recent progress in science:The truth is, there is nothing—there is nothing—of the same order of magnitude as the accomplishments of the invention of quantum mechanics or of the double helix or of relativity. Just nothing like that has happened in the last few decades.Horgan asked Kadanoff whether that state of affairs was permanent. Kadanoff was silent, before sighing and replying: “Once you have proven that the world is lawful to the satisfaction of many human beings, you can’t do that again.”But while many individuals have raised concerns about diminishing returns to science, there has been little institutional response. The meteorologist Kelvin Droegemeier, the current nominee to be President Donald Trump’s science adviser, claimed in 2016 that “the pace of discovery is accelerating” in remarks to a U.S. Senate committee. The problem of diminishing returns is mentioned nowhere in the 2018 report of the National Science Foundation, which instead talks optimistically of “potentially transformative research that will generate pioneering discoveries and advance exciting new frontiers in science.” Of course, many scientific institutions—particularly new institutions—do aim to find improved ways of operating in their own fields. But that’s not the same as an organized institutional response to diminishing returns.Perhaps this lack of response is in part because some scientists see acknowledging diminishing returns as betraying scientists’ collective self-interest. Most scientists strongly favor more research funding. They like to portray science in a positive light, emphasizing benefits and minimizing negatives. While understandable, the evidence is that science has slowed enormously per dollar or hour spent. That evidence demands a large-scale institutional response. It should be a major subject in public policy, and at grant agencies and universities. Better understanding the cause of this phenomenon is important, and identifying ways to reverse it is one of the greatest opportunities to improve our future.Methodology and sources: More details on our methodology and sources may be found in this appendix.* This article previously misstated where Bentley Glass’s quote first appeared.</description>
      <pubDate>24 Jan 22 14:29 EST</pubDate>
      <guid>https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/</guid>
    </item>
    <item>
      <title></title>
      <link>https://pivotal.substack.com/p/minsky-moments-in-venture-capital</link>
      <description>&lt;a href=&#34;https://pivotal.substack.com/p/minsky-moments-in-venture-capital&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;IntroductionMarkets rise, and markets fall. This much, at least, is well-known. But why do market cycles occur? What causes the pendulum to swing from euphoria to crisis and back? Hyman Minsky was a 20th-century economist whose ‘financial instability hypothesis’ is probably the best-known explanation for the boom and bust cycles that characterize public financial markets. But there’s far less examination — in fact, there&#39;s almost none — of how Minsky dynamics apply to private markets. We’re currently in the midst of an unprecedented boom in private market activity. Tech entrepreneurship, angel investing, and venture capital have never been so widespread. Can Minsky cycles happen in this realm as well? Let’s find out.The Inevitable Briefness of AlphaWhen I started my career as a bond trader at a quant hedge fund, arbitrage opportunities were relatively plentiful. Not many investors had the knowledge or infrastructure to effectively exploit these opportunities, so the early pioneers in the market made good money. Of course, it didn’t last. Knowledge spreads, technology diffuses, and arbitrages disappear. Markets asymptote towards ever-greater efficiency. This process is extremely well known in capital markets, and even has a name: ‘alpha decay’. Spreads representing 10s or even 100s of basis points of opportunity, common when I started trading, dwindled to mere 1s of basis points less than a decade later.… Wait, You Did What?Now, most individuals, when faced with declining opportunities, will reduce their exposure to those opportunities. If a particular strategy made 10% last year but is only expected to make 1% this year, common sense suggests allocating less capital to it. But institutions … don’t work like that. Trading desks have quarterly P&amp;L targets, and traders have annual bonuses they want to make. There&#39;s a widespread culture of “what have you done for me lately?” — you can&#39;t coast on past success. The implicit call option embedded in most traders’ compensation profiles exacerbates this.As a result, faced with a 1/10 reduction in expected value for an opportunity, many institutional portfolio managers will actually 10x their exposure to the opportunity, so as to maintain their dollar P&amp;L. I saw this multiple times in the early 2000s, and it&#39;s completely rational, given their incentives.Mathematics Made Me Do ItBut that&#39;s not the interesting part. The interesting part is that many risk and compliance models actively encourage investors to do this. Here&#39;s how it works.Most risk models (whether implicit or explicit, quantitative or qualitative) measure the riskiness of a particular investment based on how it and similar investments have behaved in the past. Sounds reasonable, right?Now, suppose that a particular class of investments, that used to be quite risky in the past, has grown less risky in recent years. A model trained on both historical and recent data would say it’s okay to put more capital to work against these investments than in the past; they’re just not that risky any more. Again, sounds eminently reasonable, right? “The market is maturing”, is usually how people describe this. Or “The asset class has become more efficient”.The Illusion Of SafetyAh, but here&#39;s the catch. What if it&#39;s precisely the deployment of all this capital that causes the decrease in volatility and hence in perceived risk?In my own little world of bond arbitrage, spreads were far less volatile in 2006 than in 1999 — because any tiny deviation from ‘fair value’ was quickly met by a flood of arbitrageur dollars pushing the other way. Those dollars simply didn&#39;t exist in 1999. As a result, bond trading in 2006 ‘appeared’ a lot less risky than in 1999. Reward (expected value) declined but risk (realized volatility) declined even more; as a result, Sharpe ratios — roughly speaking, reward divided by risk: a widely used metric of investment performance — went through the roof.This is the Minsky boom. Money entering a market boosts returns and reduces volatility, leading to very strong (realized) performance. This attracts more money, which improves performance even more. A positive feedback loop ensues.And this is perfectly legit! Economies can and do reallocate resources all the time. This is how it works; this is how it’s expected to work. What Goes Up … The problem with feedback loops is that they tend to overshoot. Minsky booms in an asset class attract a constant influx of new money, but they also need that influx to continue marking up the price and marking down the risk of the asset class. And as the wise man said, “If something cannot go on forever, it won’t”. Eventually — and fortunes have been made and lost, trying to predict just when that ‘eventually’ comes — something happens. It could be an exogenous shock like COVID, or a tightening Fed, or an election, or a war; it could be an industry-internal event like a particular firm blowing up or winding down. The new money stops, or maybe just slows down a touch, and prices soften. And that triggers all sorts of nasty consequences.… Must Come DownThe first thing that happens is that risk-management dashboards start flashing red. Even a slight selloff causes returns to drop and risk to rise; portfolios that seemed well-balanced now appear just a little too aggressive. Over-extended investors begin to trim their positions. Unfortunately, this causes more price declines, and more volatility, triggering another round of position-trimming. Even conservative investors suddenly realize that their portfolios are riskier than they thought. They sell as well. The ensuing vicious cycle is sometimes called a risk spiral.Risk spirals are often accompanied by margin spirals. Banks and brokers require leveraged investors to post margins that are proportional to their portfolio risk. As their portfolios become riskier — and remember, the portfolios themselves are often unchanged; all that has changed is the level of volatility in the market — leveraged investors face margin calls. They have to sell assets to service these calls, creating further volatility and downward price pressure; the margin call becomes a self-fulfilling prophecy.(That’s why it’s always best to be the first firm to unwind positions or call margin. Goldman — disclosure: Matt Levine used to work there — was very good at this, Lehman less so.)The final domino is a redemption spiral. Seeing declining performance and increasing volatility, LPs in a fund request their money back. To service these redemption calls, the fund has to sell even more of its positions, triggering yet another feedback loop. Boom quickly turns to bust.Hello Lehman My Old FriendThis is exactly what happened in credit markets in 2007-08. An influx of cash on the way up, accompanied by the sense that it was a can’t-lose trade; everyone was making money on housing and mortgages and credit. And then an equally unstoppable wave on the way down, as value-at-risk and margin calls and redemption spirals all worked to pull capital out of the credit market. And Round And Round We GoIn good times, people are encouraged by past success to place larger and larger bets, thinking they&#39;re less risky than they actually are, when often it&#39;s the very existence of these large bets that drives present success and depresses perceived risk.Investors believe the trend will continue indefinitely, and become complacent. They invest in lower quality instances of the asset, while increasing their leverage. And then the music stops. Markdowns lead to deleveraging which lead to more markdowns; the positive feedback loop now operates in the other direction. Eventually, the asset class overshoots as investors become overly risk averse, setting the stage for the next bull market. Stability breeds instability, and vice versa.Now, all of this is well known. Hyman Minsky fell out of fashion in the 80s and 90s, but his work was rediscovered and widely shared just in time for the GFC. Today it&#39;s part of the toolkit for most macro (and many micro) investors; you can also see it in regulatory ideas like market circuit-breakers and systemic backstops.Is Venture Immune?But credit is a foreign country; they do things differently there. Let’s talk about early stage tech and venture investing.At first glance, venture capital seems an unlikely candidate for Minsky dynamics to take hold. Consider:VCs funds don’t use leverageThey don’t offer redemptions or early liquidity to investorsThere are no counter-parties and no margin callsVolatility is actually good for most VC portfolios (long basket of options)Without mark-to-market, there’s no chance of a risk-reduction spiral. Without leverage and counter-parties, there’s no chance of a margin spiral. And without investor liquidity, there’s no chance of a redemption spiral. What mechanism could force the liquidation of a venture portfolio, or incept a Minsky bust? For that matter, what’s the mechanism for a Minsky boom in venture?It’s A (Confidence) Trick!To answer that question, ask this one: where does confidence come from?The key idea of Minsky cycles isn&#39;t that rising prices attract capital; that&#39;s just standard trend dynamics. The key Minsky idea is that increasing capital inflows reduce perceived risk.In the run-up to the GFC, home prices (and much else) went up, but the underlying confidence of the market was rooted in a belief that advances in securitization (everything from Gaussian copulas to CDO-squareds) had delivered genuine structural innovation to the mortgage market, unlocking a swathe of value. And there was prima facie evidence for this belief in the fact that credit spreads were tighter than ever.Only afterwards did it become clear that those tighter spreads were driven by capital flows into subprime securities, not by an actual reduction in economic risk1.What&#39;s the analogy for venture capital? I suspect that the variable of interest is time.Fast Is In FashionIf you talk to almost anyone in venture today, you’ll hear a few themes again and again.Startups are marked up faster than ever:Rounds are closed faster than ever:Funds are deployed faster than ever:Across every aspect of venture, timelines keep compressing.In Search of Shortened TimeTimelines in venture have compressed dramatically across the board. This is good news for founders. It’s even better news for investors. Why so? The superficial answer is IRR. It typically takes 5-10 years for venture investments to generate cash returns. It’s impractical to wait that long, so LPs judge venture firms on their interim IRR. And fast markups dramatically boost IRR. Firms use these boosted IRRs to aggressively raise new funds, and so they should; if you make 2 and 20 on every dollar you deploy, why deploy slowly? Maximize your lifetime-dollars-deployed. But there’s a deeper answer. Remember the key Minsky idea: it’s not about returns, it’s about risk. The ‘classic’ model of venture assumes that startup outcomes follow a power-law distribution: most startups fail, while a small number of outlier successes generate all the upside. Furthermore, “lemons ripen early”: failed startups fail fast — they don’t show the progress required to raise follow-on financing, and quickly go to zero. Meanwhile the outlier successes take time to grow into their full potential. Venture portfolios therefore exhibit a J-curve.But this is no longer true. Accelerated markups mean the venture J-curve no longer exists!Let’s do the math. In the bad old days, if you invested in 10 startups, then 18 months later maybe 3 or 4 would have raised 1 round each of further financing at say a 2x markup, while the remainder would be dead or doomed. Your portfolio as a whole would be worth 0.6-0.8x what you invested: the negative stage of the J-curve. (Note that this is a portfolio that’s doing well!)Today, if you invest in 10 startups, then 18 months later your 3-4 surviving firms might easily have raised 2-3 more rounds of financing at a 2x markup each time. Thanks to the velocity of financing, your portfolio as a whole could be worth 1.2-2.4x what you invested. Yes, the upper bound is higher, but crucially, so is the lower bound. The fast markups have completely compensated for your lemons! And as a result your risk appears minimal. This is terrific if you’re an investor, and funds know it:Higher returns and lower risk means new money floods into the sector, accelerating the feedback loop. This is the classic template for a Minsky boom, and it’s all driven by compressed time2.Many Things Can Be True At The Same TimeLike all Minsky booms, there are some genuine truths underlying the dynamics of the venture market today.Startups are marked up faster than ever — but startups are also growing faster than ever. 3x year-over-year used to be considered strong; today it’s a bare minimum. The best companies grow at 5x, 10x or even more.Rounds are closed faster than ever — but it’s easier than ever to evaluate the economics of software companies. SaaS diligence is a solved problem. Funds are deployed faster than ever — but that’s what maximizes dollars returned, not some arbitrary investment schedule. Or, in handy graphical form:These arguments are obviously true. But it was also obviously true (and I’m not being sarcastic here) that there were some genuine structural advances in credit markets in the 2000s, broadening access to loans for borrowers while reducing risk for lenders. This did not stop the credit markets from imploding in 2008. So are these arguments strong enough for venture to be immune to Minsky dynamics? Is it different this time?Detour: True Risk and Measured RiskOne way to understand Minsky cycles is that they’re driven by the gap between ‘measured risk’ and ‘true risk’. When you lend money, the ‘true risk’ you take is that the borrower defaults3. But you can’t know this directly; instead you measure it by proxy, using credit spreads. Credit spreads reflect default probabilities, but they also reflect investor demand for credit products. A subprime credit trading at a tight spread doesn’t necessarily imply that subprime loans have become less risky (though that could be true); the tight spread may also be driven by demand for subprime loans. Measured risk has deviated from true risk.Similarly, when you invest in a startup, the ‘true risk’ that you take is that the startup fails. But you can’t know this directly; instead you measure it by proxy, using markups. Markups reflect inverse failure probabilities (the higher and faster the markup, the more successful the company, and hence the less likely it is to fail — at least, so one hopes). But markups also reflect investor demand for startup equity. Once again, measured risk has deviated from true risk.During Minsky booms, measured risks decline. During Minsky busts, measured risks increase. The flip from boom to bust occurs when the market realizes that true risks haven’t gone away.The Destination, Not The JourneySo now let’s rephrase the question. Has the true risk of venture investments changed? More rigorously:Does the compression of timelines in venture change the distribution of terminal outcomes for venture-backed companies? On that question, the jury is still out. It’s not obvious to me that accelerated markups change the power-law dynamics of venture portfolios. Markups change the journey of a business, but do they change the destination? If the answer is yes, then there’s no Minsky dynamic at play; what we’re seeing is a rational evolution of the venture industry. Maybe startups are truly less risky now; maybe the market truly has matured. More capital, lower returns, safer investments4.If the answer is no, then venture is very possibly in a Minsky boom, and we’re just waiting for the moment when it turns into a Minsky bust. What could trigger such a moment?Reasons For Momentary LapsesThis section is necessarily speculative, but I’ll begin with an observation. There is in fact one well-known death spiral in startup land, and it’s the dreaded down round.In a down round, a startup running out of cash is forced to raise capital at a lower valuation than its previous financing. This is bad news. Anti-dilution provisions mean that early investors and common shareholders are wiped out. Recent hires whose options are now underwater begin to leave. The startup is perceived as damaged goods, and has to pay above market comp to replace them, attracting mercenaries instead of missionaries. Customers, not knowing if the startup will survive, churn. Finances worsen, predatory investors circle, and further down rounds loom.A valuation spiral is bad enough. It’s usually accompanied by a talent spiral, which is worse. In a tight labour market, good operators have their choice of where to work. The best startups are able to attract the best talent5. Meanwhile, flailing startups tend to fill up with mediocre employees — the ones who can’t find work elsewhere. This makes it even harder to recruit excellent people 6. The spiral continues.Down rounds are widely considered the harbinger of doom for venture-backed startups. Understandably, founders and investors go to great lengths to avoid them7. How do they do this? The most common approach is to wait it out. Cut costs, squeeze out short-term revenue, raise bridge loans from less-known investors at the best terms you can, and hope to eventually ‘grow into your valuation’. Sometimes it even works. This is — not coincidentally — a perfect mirror image of the logic used by many investors today. “I don’t mind paying up; on the current trajectory, even a doubling in price is easily recouped via just a few months of growth.”But if my hypothesis about time is true, this could be dangerous. If compressed timelines are the driver of Minsky inflows into venture, then anything that delays funding cycles could precipitate a painful reversal. First some startups delay fund-raising because they need to grow into their valuations; then the VCs who invested in those startups have to delay their own fund-raising with LPs because they don’t have the requisite markups; then the LPs reconsider their (hitherto ever-increasing) allocations to venture because the latest returns are uninspiring; and before you know it, there’s an exodus from the asset class. Minsky giveth, and Minsky taketh away.Please, Just Tell Me What To DoIt’s conventional to end this sort of essay with some actionable advice. “Here are 5 things that you, gentle founder / investor / operator [choose as applicable] can do, to protect yourself from the coming catastrophe.” I’m not going to do that. Obvious advice is obvious: while the music plays, it’s best to keep dancing; when the music stops, it’s best to stop. The secret is knowing when the music will stop, and frankly, I have no idea8. That’s the thing about Minsky moments: they’re easy to identify and dissect in retrospect, close to impossible to forecast in advance. All that we know is this: markets rise, and markets fall. Good luck out there! Toronto, 12 Feb 2022Further readingEverett Randle describes how Tiger built “the first structural, non-brand driven competitive advantage and flywheel at scale in venture” — and it’s all based on the effects of accelerated deployment on venture timelines and risk. Possibly my favourite essay of 2021.I wrote that tech financing doesn’t use debt or leverage, but that’s changing fast. Startups like Pipe have already had great success securitizing software cashflows. Alex Danco predicted this in one of my favourite essays from 2020: Debt is coming.Byrne Hobart applies the Minsky framework to supply chains, another topic of some current interest. His focus is more on liquidity and leverage than on risk, but the core ideas remain the same.HousekeepingI’d like to thank Ruben Schreurs and Venkatesh Rao for their suggestions on style and structure, which helped improve this post substantially. Thank you for reading! If you liked this post, please do 3 things straight away:Email it to a friendFollow me on Twitter: @athomasqSubscribe!</description>
      <pubDate>21 Feb 22 12:58 EST</pubDate>
      <guid>https://pivotal.substack.com/p/minsky-moments-in-venture-capital</guid>
    </item>
    <item>
      <title></title>
      <link>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</link>
      <description>&lt;a href=&#34;https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; You might know that I’ve posted funny emails I’ve received on my blog several times in the past. The kind of emails people send me when they experience problems with some device they own (like a car) and they contact me because my email address happens to be visible somewhere. People sometimes say I should get a different email address or use another one in the curl license file, but I’ve truly never had a problem with these emails, as they mostly remind me about the tough challenges the modern technical life bring to people and it gives me insights about what things that run curl. But not all of these emails are “funny”. Category: not funny Today I received the following email From: Al Nocai &lt;[redacted]@icloud.com&gt; Date: Fri, 19 Feb 2021 03:02:24 -0600 Subject: I will slaughter you That subject. As an open source maintainer since over twenty years, I know flame wars and personal attacks and I have a fairly thick skin and I don’t let words get to me easily. It took me a minute to absorb and realize it was actually meant as a direct physical threat. It found its ways through and got to me. This level of aggressiveness is not what I’m prepared for. Attached in this email, there were seven images and no text at all. The images all look like screenshots from a phone and the first one is clearly showing source code I wrote and my copyright line: The other images showed other source code and related build/software info of other components, but I couldn’t spot how they were associated with me in any way. No explanation, just that subject and the seven images and I was left to draw my own conclusions. I presume the name in the email is made up and the email account is probably a throw-away one. The time zone used in the Date: string might imply US central standard time but could of course easily be phony as well. How I responded Normally I don’t respond to these confused emails because the distance between me and the person writing them is usually almost interplanetary. This time though, it was so far beyond what’s acceptable to me and in any decent society I couldn’t just let it slide. After I took a little pause and walked around my house for a few minutes to cool off, I wrote a really angry reply and sent it off. This was a totally and completely utterly unacceptable email and it hurt me deep in my soul. You should be ashamed and seriously reconsider your manners.I have no idea what your screenshots are supposed to show, but clearly something somewhere is using code I wrote. Code I have written runs in virtually every Internet connected device on the planet and in most cases the users download and use it without even telling me, for free.Clearly you don’t deserve my code. I don’t expect that it will be read or make any difference. Update below, added after my initial post. Al Nocai’s response Contrary to my expectations above, he responded. It’s not even worth commenting but for transparency I’ll include it here. I do not care. Your bullshit software was an attack vector that cost me a multimillion dollar defense project. Your bullshit software has been used to root me and multiple others. I lost over $15k in prototyping alone from bullshit rooting to the charge arbitrators. I have now since October been sandboxed because of your bullshit software so dipshit google kids could grift me trying to get out of the sandbox because they are too piss poor to know shat they are doing. You know what I did to deserve that? I tried to develop a trade route in tech and establish project based learning methodologies to make sure kids aren’t left behind. You know who is all over those god damn files? You are. Its sickening. I got breached in Oct 2020 through federal server hijacking, and I owe a great amount of that to you. Ive had to sit and watch as i reported: fireeye Oct/2020Solarwinds Oct/2020Zyxel Modem Breach Oct/2020Multiple Sigover attack vectors utilizing favicon XML injectionJS Stochastic templating utilizing comparison expressions to write to data registersGet strong armed by $50billion companies because i exposed bullshit malware And i was rooted and had my important correspondence all rerouted as some sick fuck dismantled my life with the code you have your name plastered all over. I cant even leave the country because of the situation; qas you have so effectively built a code base to shit all over people, I dont give a shit how you feel about this. You built a formula 1 race car and tossed the keys to kids with ego problems. Now i have to deal with Win10 0-days because this garbage. I lost my family, my country my friends, my home and 6 years of work trying to build a better place for posterity. And it has beginnings in that code. That code is used to root and exploit people. That code is used to blackmail people. So no, I don’t feel bad one bit. You knew exactly the utility of what you were building. And you thought it was all a big joke. Im not laughing. I am so far past that point now. /- Al Al continues Nine hours after I first published this blog post , Al replied again with two additional emails. His third and forth emails to me. Email 3: https://davidkrider.com/i-will-slaughter-you-daniel-haxx-se/Step up. You arent scaring me. What led me here? The 5th violent attempt on my life. Apple terms of service? gtfo, thanks for the platform. Amusingly he has found a blog post about my blog post. Email 4: There is the project: MOUT Ops Risk Analysis through Wide Band Em Spectrum analysis through different fourier transforms.You and whoever the fuck david dick rider is, you are a part of this.Federal server breaches-Accomplice to attempted murder-Fraud-just a few. I have talked to now: FBI FBI Regional, VA, VA OIG, FCC, SEC, NSA, DOH, GSA, DOI, CIA, CFPB, HUD, MS, Convercent, as of today 22 separate local law enforcement agencies calling my ass up and wasting my time. You and dick ridin’ dave are respinsible. I dont give a shit, call the cops. I cuss them out wheb they call and they all go silent. I’ve kept his peculiar formatting and typos. In email 4 there was also a PDF file attached named BustyBabes 4.pdf. It is apparently a 13 page document about the “NERVEBUS NERVOUS SYSTEM” described in the first paragraph as “NerveBus Nervous System aims to be a general utility platform that provides comprehensive and complex analysis to provide the end user with cohesive, coherent and “real-time” information about the environment it monitors.”. There’s no mention of curl or my name in the document. Since I don’t know the status of this document I will not share it publicly, but here’s a screenshot of the front page: Related This topic on hacker news and reddit. I have reported the threat to the Swedish police (where I live). This person would later apologize. </description>
      <pubDate>19 Feb 21 09:08 EST</pubDate>
      <guid>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</guid>
    </item>
    <item>
      <title>Git is my buddy: Effective Git as a solo developer</title>
      <link>https://mikkel.ca/blog/git-is-my-buddy-effective-solo-developer/</link>
      <description>&lt;a href=&#34;https://mikkel.ca/blog/git-is-my-buddy-effective-solo-developer/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; February 23, 2021 • Reading time: 12 minutes At this point, most developers use Git as a tool for collaboration. We have our rote-learned commands to pull, commit, and push. And of course, there&#39;s that one coworker who knows a bit more about Git than everyone else, who helps get us back on track whenever our local repos end up in a strange state. But what if I told you that Git can be a valuable tool without ever setting up a remote repository? I&#39;m not just talking about having a working version of your code base to roll back to if you mess something up, although there&#39;s that too. Used correctly, Git can help to structure your work, identifying gaps in your test coverage and minimizing dead code. There are two subjects I&#39;m going to avoid for the purposes of this blog post: other developers, who are the most compelling but least interesting argument for keeping your commit history clean, and git bisect, which does factor heavily into my workflow but deserves its own blog post. As with any ubiquitous developer tool, the Git user base has a lot of strong and conflicting opinions about the one &#34;correct&#34; way to use it. My goal is simply to introduce a workflow that I&#39;ve been using and refining for much of my career; take from it what you will. And, importantly, it&#39;s a workflow that has become a vital part not just of my collaboration process, but of the way I write code. Ultimately, these principles serve two purposes: they focus my work onto a particular bugfix, feature, or goal, and they ensure that my Git history isn&#39;t set in stone. With proper hygiene, commits can be dropped, rearranged, and split off into other branches painlessly and without merge conflicts. Principle 1: A branch must do one useful thing When I&#39;m managing my own projects, I have a lot of ideas that I want to see happen. If I&#39;m just throwing one commit after another into main, I&#39;ll get halfway through implementing one feature and then jump off to hacking on another. If any of the features get completed, it will be at the expense of a wasteland of half-completed features that are now taking up space in my code base. In a brand-new project, sure, I&#39;ll throw a bunch of garbage commits into main. My rule of thumb for when to stop this is when I can write my first effective integration test. If there is something useful to test, there is now enough substance to my project that I can have distinct tasks on the go. Trying to break into branches too early just results in me throwing my garbage commits into a branch instead of main. In the early stages of a project, articulating the purpose of a branch can be as simple as giving it a descriptive name. If a commit isn&#39;t moving the code base in that direction, it can always get cherry-picked into a different branch. As the project matures, I&#39;ll start using some sort of issue or bug tracking software to flesh out what I&#39;m trying to accomplish in more detail and coordinate the branches for multiple related useful things. I find that descriptive branch names also help to refocus my attention on what I&#39;m trying to accomplish. For instance, my command prompt currently looks like this: 10:02:19 max ~/Projects/mikkel.ca blog-post-git-as-a-solo-developer| R% Principle 2: Every commit must be independent So much for branches, let&#39;s zoom into a commit level. I&#39;ve articulated what concrete thing I want my branch to add, now how do I add it? Usually, there&#39;s some poking around my code base involved in figuring that out. Sometimes I take a wrong turn, sometimes I just get distracted. That&#39;s okay, it&#39;s part of the process. However, that doesn&#39;t mean that every commit I make right now is going to end up getting merged in this branch. By keeping my commits independent from one another, I ensure that I can rearrange or cherry-pick them into new branches if I discover that they really don&#39;t have anything to do with what I&#39;m working on right now. If my commits are not independent, I am essentially stuck with the exact history as it was written. Trying to tease out a commit into a different branch or move it to the beginning of my branch history will become fraught with merge conflicts as later commits that modified code introduced in this commit fall like dominoes. Obviously, I&#39;m still allowed to call code written in one commit from a later commit. That&#39;s the reason I&#39;m doing this particular work in this particular branch, after all. But I never touch the same code multiple times. If I have to go back and fix something, maybe add a validation check or field that I hadn&#39;t thought of, I&#39;ll go back to the commit where it was created rather than amending it in a later commit. Obviously, this could go on forever, which is why the &#34;one useful thing&#34; principle exists. Once I&#39;ve settled on what I want the code to look like for the purposes of this branch, I merge and then start a new commit in the next branch for further changes to the same. Principle 2a: Every commit must include its own tests Here&#39;s where keeping commits small starts to pay dividends. If the code in each commit is small enough for me to reason about, it&#39;s small enough for me to visually ensure that its test coverage is good. And of course, if I do end up rearranging this commit or splitting it off to a different branch, I want its tests to come along with. The exception to this is integration and functional/behavioural tests, which can and should have their own commits. In that case, the tests are really tied to the branch level rather than the commit level, since Principle 1 implies that there should be exactly one new test to add as a result of this branch. Principle 2b: Every commit must pass all tests Again, breaking something in a commit (even if I really definitely intend to fix it in a later commit) locks me into the git history as written. And introducing a breaking change with the intention of fixing things later always carries the risk that I&#39;ll get distracted and end up merging the breaking change. If there&#39;s some prerequisite to get this change to pass tests - say, a preexisting bug that snuck through a hole in my test coverage - that gets its own commit. Speaking of holes in test coverage, there&#39;s another (temporary) exception here. I don&#39;t normally practice strict test-driven development, but if I do fix a long-standing bug, I normally temporarily put its test in a separate commit. I&#39;ll then rebase so that the test appears before the fix, ensure that the test fails without the fix, then complete the rebase and validate that the test now passes. Once the due diligence to validate my test is done, I can go ahead and squash the bugfix with its test. Principle 3: Draft commits are fine If I know that I&#39;ll be coming back to a change later, I&#39;m much more comfortable setting it down and moving on to roughing in the next part of the process, rather than finishing, polishing, and unit testing code that might need to change before my branch gets merged. In fact, I find that I waste much less time on writing tests for things that I&#39;ll later change when I&#39;m following this workflow to the letter than I do when I get &#34;lazy&#34; and start dumping everything into big catch-all commits. Some people favour TODO comments in their code, occasionally supported by automated checks that prevent code containing &#34;TODO&#34; from merging. I prefer to annotate my commit messages and leave my code clean. Normally, this looks something like &#34;add controller class - TODO test me&#34;. (I always put my TODOs on the first line of the commit message, so that they show up even in short log views.) Principle 4: It&#39;s okay to discard commits completely Often I start a task by tidying up the surrounding code, in the same way I might organize my desk before starting work. (I don&#39;t, but I might.) Sometimes that cleanup turns out to be a valuable part of the groundwork for this change, but sometimes it&#39;s just dead weight. Keeping my commits independent makes it easy to discard or cherry-pick out code that turned out to be unnecessary, along with any unit tests that went along with. (I do still consider the tidying to be a valuable part of the process. It clears my mind and refreshes my knowledge of the problem space with some simple rote tasks before I dive into something more complex. And occasionally it results in cleaner code.) I&#39;m not perfect.[citation needed] Obviously, it&#39;s not practical to maintain this level of commit hygiene by making each change sequentially. Instead, I jump around constantly. Doing so requires me to be comfortable in navigating my commit history. (Conversely, it&#39;s also a good way to become comfortable with navigating history.) In that vein, here are some tools beyond your standard checkout/branch/pull/commit/push workflow that come in handy. git commit --amend – A quick and easy way to update the most recent commit. git commit --fixup [hash] – When changing history, I used to find myself making a lot of commits with messages like &#34;merge me with xyz&#34; if I need to revisit commits before the most recent one. It turns out that git commit has flags to help with this: --fixup and --squash will automatically suggest a fixup or squash with another commit during rebase if the --autosquash flag is provided to that command. (To enable this behaviour by default, run git config --global rebase.autosquash true. It won&#39;t behave any differently if there are no commit messages in the history being edited that contain &#34;squash!&#34; or &#34;fixup!&#34;.) A surprise bonus: since the fixup operation inherits the message of the previous commit, you won&#39;t be prompted to enter a new one. git rebase --interactive main – I can also use git rebase --interactive HEAD~5 to edit the last 5 commits, but I find rebasing directly on main (or master, or whatever my upstream branch is) kills two birds with one stone. It will show me all commits since I branched off from main, and will simultaneously bring my branch up to date with my latest local copy of main. git stash – Sometimes I have unrelated changes on the go that I don&#39;t want to commit right now. git stash is an easy way to make them go away, and git stash pop brings them back again. Just use it sparingly, because finding your changes in the stash later is a pain. If I&#39;m not planning to pop it back off again in the near future, I make a progress commit instead. git blame – Okay, this is more valuable in collaboration. When it&#39;s my code base, I already know whose fault it is. Still, despite the name, I use git blame not to find out who to blame, but to find out why something was done. That applies equally whether it was done by another developer or me six months ago. Most commonly, I&#39;ll use it when I see something that looks like a bug, and I want to find out: a) what purpose the thing was supposed to serve, b) if it was successful in serving that purpose, and c) whether or not any related code is still in the code base. It&#39;s the task of finding related code that really puts your project&#39;s commit hygiene to the test. The usual Git/Vim disclaimer applies to my list: if you get five power users in a room and ask them to do a complicated task, they&#39;ll get it done quickly, efficiently, and in seven different ways. The commands I use are not the only ways to accomplish the same results, and are probably not the best way. What else can I do with it? While I think my workflow stands on its own as a way of structuring your thoughts and ensuring that your test coverage is good, this is of course also a workflow that will get you a lot of love from coworkers or collaborators. Well-crafted pull requests are a joy less of a misery to review, and referring back to a well-written commit turned up in git blame makes it much easier to understand what you or another developer was trying to accomplish with a change (and evaluate whether or not it was successful). And of course, there&#39;s git bisect. The short version, for those who haven&#39;t used it, is that git bisect allows you to find when something changed, across all of history, either manually or using automated tests, all in O(log n) time. Ensuring that your commits always pass tests make them friendly to git bisect, and ensuring that they are as small as possible means that when bisect tells you which commit introduced a bug, there is very little code in which that bug could appear. </description>
      <pubDate>23 Feb 21 12:59 EST</pubDate>
      <guid>https://mikkel.ca/blog/git-is-my-buddy-effective-solo-developer/</guid>
    </item>
    <item>
      <title></title>
      <link>https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/</link>
      <description>&lt;a href=&#34;https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Every concurrency API needs a way to run code concurrently. Here&#39;s some examples of what that looks like using different APIs: go myfunc(); // Golang pthread_create(&amp;thread_id, NULL, &amp;myfunc); /* C with POSIX threads */ spawn(modulename, myfuncname, []) % Erlang threading.Thread(target=myfunc).start() # Python with threads asyncio.create_task(myfunc()) # Python with asyncio There are lots of variations in the notation and terminology, but the semantics are the same: these all arrange for myfunc to start running concurrently to the rest of the program, and then return immediately so that the parent can do other things. Another option is to use callbacks: QObject::connect(&amp;emitter, SIGNAL(event()), // C++ with Qt &amp;receiver, SLOT(myfunc())) g_signal_connect(emitter, &#34;event&#34;, myfunc, NULL) /* C with GObject */ document.getElementById(&#34;myid&#34;).onclick = myfunc; // Javascript promise.then(myfunc, errorhandler) // Javascript with Promises deferred.addCallback(myfunc) # Python with Twisted future.add_done_callback(myfunc) # Python with asyncio Again, the notation varies, but these all accomplish the same thing: they arrange that from now on, if and when a certain event occurs, then myfunc will run. Then once they&#39;ve set that up, they immediately return so the caller can do other things. (Sometimes callbacks get dressed up with fancy helpers like promise combinators, or Twisted-style protocols/transports, but the core idea is the same.) And... that&#39;s it. Take any real-world, general-purpose concurrency API, and you&#39;ll probably find that it falls into one or the other of those buckets (or sometimes both, like asyncio). But my new library Trio is weird. It doesn&#39;t use either approach. Instead, if we want to run myfunc and anotherfunc concurrently, we write something like: async with trio.open_nursery() as nursery: nursery.start_soon(myfunc) nursery.start_soon(anotherfunc) When people first encounter this &#34;nursery&#34; construct, they tend to find it confusing. Why is there an indented block? What&#39;s this nursery object, and why do I need one before I can spawn a task? Then they realize that it prevents them from using patterns they&#39;ve gotten used to in other frameworks, and they get really annoyed. It feels quirky and idiosyncratic and too high-level to be a basic primitive. These are understandable reactions! But bear with me. In this post, I want to convince you that nurseries aren&#39;t quirky or idiosyncratic at all, but rather a new control flow primitive that&#39;s just as fundamental as for loops or function calls. And furthermore, the other approaches we saw above – thread spawning and callback registration – should be removed entirely and replaced with nurseries. Sound unlikely? Something similar has actually happened before: the goto statement was once the king of control flow. Now it&#39;s a punchline. A few languages still have something they call goto, but it&#39;s different and far weaker than the original goto. And most languages don&#39;t even have that. What happened? This was so long ago that most people aren&#39;t familiar with the story anymore, but it turns out to be surprisingly relevant. So we&#39;ll start by reminding ourselves what a goto was, exactly, and then see what it can teach us about concurrency APIs. Contents: What is a goto statement anyway? What is a go statement anyway? What happened to goto? goto: the destroyer of abstraction A surprise benefit: removing goto statements enables new features goto statements: not even once go statement considered harmful go statements: not even once Nurseries: a structured replacement for go statements Nurseries preserve the function abstraction. Nurseries support dynamic task spawning. There is an escape. You can define new types that quack like a nursery. No, really, nurseries always wait for the tasks inside to exit. Automatic resource cleanup works. Automated error propagation works. A surprise benefit: removing go statements enables new features Nurseries in practice Conclusion Comments Acknowledgments Footnotes What is a goto statement anyway? Let&#39;s review some history: Early computers were programmed using assembly language, or other even more primitive mechanisms. This kinda sucked. So in the 1950s, people like John Backus at IBM and Grace Hopper at Remington Rand started to develop languages like FORTRAN and FLOW-MATIC (better known for its direct successor COBOL). FLOW-MATIC was very ambitious for its time. You can think of it as Python&#39;s great-great-great-...-grandparent: the first language that was designed for humans first, and computers second. Here&#39;s some FLOW-MATIC code to give you a taste of what it looked like: You&#39;ll notice that unlike modern languages, there&#39;s no if blocks, loop blocks, or function calls here – in fact there&#39;s no block delimiters or indentation at all. It&#39;s just a flat list of statements. That&#39;s not because this program happens to be too short to use fancier control syntax – it&#39;s because block syntax wasn&#39;t invented yet! Sequential flow represented as a vertical arrow pointing down, and goto flow represented as an arrow that starts pointing down and then leaps off to the side. Instead, FLOW-MATIC had two options for flow control. Normally, it was sequential, just like you&#39;d expect: start at the top and move downwards, one statement at a time. But if you execute a special statement like JUMP TO, then it could directly transfer control somewhere else. For example, statement (13) jumps back to statement (2): Just like for our concurrency primitives at the beginning, there was some disagreement about what to call this &#34;do a one-way jump&#34; operation. Here it&#39;s JUMP TO, but the name that stuck was goto (like &#34;go to&#34;, get it?), so that&#39;s what I&#39;ll use here. Here&#39;s the complete set of goto jumps in this little program: If you think this looks confusing, you&#39;re not alone! This style of jump-based programming is something that FLOW-MATIC inherited pretty much directly from assembly language. It&#39;s powerful, and a good fit to how computer hardware actually works, but it&#39;s super confusing to work with directly. That tangle of arrows is why the term &#34;spaghetti code&#34; was invented. Clearly, we needed something better. But... what is it about goto that causes all these problems? Why are some control structures OK, and some not? How do we pick the good ones? At the time, this was really unclear, and it&#39;s hard to fix a problem if you don&#39;t understand it. What is a go statement anyway? But let&#39;s hit pause on the history for a moment – everyone knows goto was bad. What does this have to do with concurrency? Well, consider Golang&#39;s famous go statement, used to spawn a new &#34;goroutine&#34; (lightweight thread): Can we draw a diagram of its control flow? Well, it&#39;s a little different from either of the ones we saw above, because control actually splits. We might draw it like: &#34;Go&#34; flow represented as two arrows: a green arrow pointing down, and a lavender arrow that starts pointing down and then leaps off to the side. Here the colors are intended to indicate that both paths are taken. From the perspective of the parent goroutine (green line), control flows sequentially: it comes in the top, and then immediately comes out the bottom. Meanwhile, from the perspective of the child (lavender line), control comes in the top, and then jumps over to the body of myfunc. Unlike a regular function call, this jump is one-way: when running myfunc we switch to a whole new stack, and the runtime immediately forgets where we came from. But this doesn&#39;t just apply to Golang. This is the flow control diagram for all of the primitives we listed at the beginning of this post: Threading libraries usually provide some sort of handle object that lets you join the thread later – but this is an independent operation that the language doesn&#39;t know anything about. The actual thread spawning primitive has the control flow shown above. Registering a callback is semantically equivalent to starting a background thread that (a) blocks until some event occurs, and then (b) runs the callback. (Though obviously the implementation is different.) So in terms of high-level control flow, registering a callback is essentially a go statement. Futures and promises are the same too: when you call a function and it returns a promise, that means it&#39;s scheduled the work to happen in the background, and then given you a handle object to join the work later (if you want). In terms of control flow semantics, this is just like spawning a thread. Then you register callbacks on the promise, so see the previous bullet point. This same exact pattern shows up in many, many forms: the key similarity is that in all these cases, control flow splits, with one side doing a one-way jump and the other side returning to the caller. Once you know what to look for, you&#39;ll start seeing it all over the place – it&#39;s a fun game! [1] Annoyingly, though, there is no standard name for this category of control flow constructs. So just like &#34;goto statement&#34; became the umbrella term for all the different goto-like constructs, I&#39;m going to use &#34;go statement&#34; as a umbrella term for these. Why go? One reason is that Golang gives us a particularly pure example of the form. And the other is... well, you&#39;ve probably guessed where I&#39;m going with all this. Look at these two diagrams. Notice any similarities? Repeat of earlier diagrams: goto flow represented as an arrow that starts pointing down and then leaps off to the side, and &#34;go&#34; flow represented as two arrows: a green arrow pointing down, and a lavender arrow that starts pointing down and then leaps off to the side. That&#39;s right: go statements are a form of goto statement. Concurrent programs are notoriously difficult to write and reason about. So are goto-based programs. Is it possible that this might be for some of the same reasons? In modern languages, the problems caused by goto are largely solved. If we study how they fixed goto, will it teach us how to make more usable concurrency APIs? Let&#39;s find out. What happened to goto? So what is it about goto that makes it cause so many problems? In the late 1960s, Edsger W. Dijkstra wrote a pair of now-famous papers that helped make this much clearer: Go to statement considered harmful, and Notes on structured programming (PDF). goto: the destroyer of abstraction In these papers, Dijkstra was worried about the problem of how you write non-trivial software and get it correct. I can&#39;t give them due justice here; there&#39;s all kinds of fascinating insights. For example, you may have heard this quote: Yep, that&#39;s from Notes on structured programming. But his major concern was abstraction. He wanted to write programs that are too big to hold in your head all at once. To do this, you need to treat parts of the program like a black box – like when you see a Python program do: then you don&#39;t need to know all the details of how print is implemented (string formatting, buffering, cross-platform differences, ...). You just need to know that it will somehow print the text you give it, and then you can spend your energy thinking about whether that&#39;s what you want to have happen at this point in your code. Dijkstra wanted languages to support this kind of abstraction. By this point, block syntax had been invented, and languages like ALGOL had accumulated ~5 distinct types of control structure: they still had sequential flow and goto: Same picture of sequential flow and goto flow as before. And had also acquired variants on if/else, loops, and function calls: Diagrams with arrows showing the flow control for if statements, loops, and function calls. You can implement these higher-level constructs using goto, and early on, that&#39;s how people thought of them: as a convenient shorthand. But what Dijkstra pointed out is that if you look at these diagrams, there&#39;s a big difference between goto and the rest. For everything except goto, flow control comes in the top → [stuff happens] → flow control comes out the bottom. We might call this the &#34;black box rule&#34;: if a control structure has this shape, then in contexts where you don&#39;t care about the details of what happens internally, you can ignore the [stuff happens] part, and treat the whole thing as regular sequential flow. And even better, this is also true of any code that&#39;s composed out of those pieces. When I look at this code: I don&#39;t have to go read the definition of print and all its transitive dependencies just to figure out how the control flow works. Maybe inside print there&#39;s a loop, and inside the loop there&#39;s an if/else, and inside the if/else there&#39;s another function call... or maybe it&#39;s something else. It doesn&#39;t really matter: I know control will flow into print, the function will do its thing, and then eventually control will come back to the code I&#39;m reading. It may seem like this is obvious, but if you have a language with goto – a language where functions and everything else are built on top of goto, and goto can jump anywhere, at any time – then these control structures aren&#39;t black boxes at all! If you have a function, and inside the function there&#39;s a loop, and inside the loop there&#39;s an if/else, and inside the if/else there&#39;s a goto... then that goto could send the control anywhere it wants. Maybe control will suddenly return from another function entirely, one you haven&#39;t even called yet. You don&#39;t know! And this breaks abstraction: it means that every function call is potentially a goto statement in disguise, and the only way to know is to keep the entire source code of your system in your head at once. As soon as goto is in your language, you stop being able do local reasoning about flow control. That&#39;s why goto leads to spaghetti code. And now that Dijkstra understood the problem, he was able to solve it. Here&#39;s his revolutionary proposal: we should stop thinking of if/loops/function calls as shorthands for goto, but rather as fundamental primitives in their own rights – and we should remove goto entirely from our languages. From here in 2018, this seems obvious enough. But have you seen how programmers react when you try to take away their toys because they&#39;re not smart enough to use them safely? Yeah, some things never change. In 1969, this proposal was incredibly controversial. Donald Knuth defended goto. People who had become experts on writing code with goto quite reasonably resented having to basically learn how to program again in order to express their ideas using the newer, more constraining constructs. And of course it required building a whole new set of languages. Left: A traditional goto. Right: A domesticated goto, as seen in C, C#, Golang, etc. The inability to cross function boundaries means it can still pee on your shoes, but it probably won&#39;t rip your face off. In the end, modern languages are a bit less strict about this than Dijkstra&#39;s original formulation. They&#39;ll let you break out of multiple nested structures at once using constructs like break, continue, or return. But fundamentally, they&#39;re all designed around Dijkstra&#39;s idea; even these constructs that push the boundaries do so only in strictly limited ways. In particular, functions – which are the fundamental tool for wrapping up control flow inside a black box – are considered inviolate. You can&#39;t break out of one function and into another, and a return can take you out of the current function, but no further. Whatever control flow shenanigans a function gets up to internally, other functions don&#39;t have to care. This even extends to goto itself. You&#39;ll find a few languages that still have something they call goto, like C, C#, Golang, ... but they&#39;ve added heavy restrictions. At the very least, they won&#39;t let you jump out of one function body and into another. Unless you&#39;re working in assembly [2], the classic, unrestricted goto is gone. Dijkstra won. A surprise benefit: removing goto statements enables new features And once goto disappeared, something interesting happened: language designers were able to start adding features that depend on control flow being structured. For example, Python has some nice syntax for resource cleanup: the with statement. You can write things like: # Python with open(&#34;my-file&#34;) as file_handle: ... and it guarantees that the file will be open during the ... code, but then closed immediately afterward. Most modern languages have some equivalent (RAII, using, try-with-resource, defer, ...). And they all assume that control flows in an orderly, structured way. If we used goto to jump into the middle of our with block... what would that even do? Is the file open or not? What if we jumped out again, instead of exiting normally? Would the file get closed? This feature just doesn&#39;t work in any coherent way if your language has goto in it. Error handling has a similar problem: when something goes wrong, what should your code do? Often the answer is to pass the buck up the stack to your code&#39;s caller, let them figure out how to deal with it. Modern languages have constructs specifically to make this easier, like exceptions, or other forms of automatic error propagation. But your language can only provide this help if it has a stack, and a reliable concept of &#34;caller&#34;. Look again at the control-flow spaghetti in our FLOW-MATIC program and imagine that in the middle of that it tried to raise an exception. Where would it even go? goto statements: not even once So goto – the traditional kind that ignores function boundaries – isn&#39;t just the regular kind of bad feature, the kind that&#39;s hard to use correctly. If it were, it might have survived – lots of bad features have. But it&#39;s much worse. Even if you don&#39;t use goto yourself, merely having it as an option in your language makes everything harder to use. Whenever you start using a third-party library, you can&#39;t treat it as a black box – you have to go read through it all to find out which functions are regular functions, and which ones are idiosyncratic flow control constructs in disguise. This is a serious obstacle to local reasoning. And you lose powerful language features like reliable resource cleanup and automatic error propagation. Better to remove goto entirely, in favor of control flow constructs that follow the &#34;black box&#34; rule. go statement considered harmful So that&#39;s the history of goto. Now, how much of this applies to go statements? Well... basically, all of it! The analogy turns out to be shockingly exact. Go statements break abstraction. Remember how we said that if our language allows goto, then any function might be a goto in disguise? In most concurrency frameworks, go statements cause the exact same problem: whenever you call a function, it might or might not spawn some background task. The function seemed to return, but is it still running in the background? There&#39;s no way to know without reading all its source code, transitively. When will it finish? Hard to say. If you have go statements, then functions are no longer black boxes with respect to control flow. In my first post on concurrency APIs, I called this &#34;violating causality&#34;, and found that it was the root cause of many common, real-world issues in programs using asyncio and Twisted, like problems with backpressure, problems with shutting down properly, and so forth. Go statements break automatic resource cleanup. Let&#39;s look again at that with statement example: # Python with open(&#34;my-file&#34;) as file_handle: ... Before, we said that we were &#34;guaranteed&#34; that the file will be open while the ... code is running, and then closed afterwards. But what if the ... code spawns a background task? Then our guarantee is lost: the operations that look like they&#39;re inside the with block might actually keep running after the with block ends, and then crash because the file gets closed while they&#39;re still using it. And again, you can&#39;t tell from local inspection; to know if this is happening you have to go read the source code to all the functions called inside the ... code. If we want this code to work properly, we need to somehow keep track of any background tasks, and manually arrange for the file to be closed only when they&#39;re finished. It&#39;s doable – unless we&#39;re using some library that doesn&#39;t provide any way to get notified when the task is finished, which is distressingly common (e.g. because it doesn&#39;t expose any task handle that you can join on). But even in the best case, the unstructured control flow means the language can&#39;t help us. We&#39;re back to implementing resource cleanup by hand, like in the bad old days. Go statements break error handling. Like we discussed above, modern languages provide powerful tools like exceptions to help us make sure that errors are detected and propagated to the right place. But these tools depend on having a reliable concept of &#34;the current code&#39;s caller&#34;. As soon as you spawn a task or register a callback, that concept is broken. As a result, every mainstream concurrency framework I know of simply gives up. If an error occurs in a background task, and you don&#39;t handle it manually, then the runtime just... drops it on the floor and crosses its fingers that it wasn&#39;t too important. If you&#39;re lucky it might print something on the console. (The only other software I&#39;ve used that thinks &#34;print something and keep going&#34; is a good error handling strategy is grotty old Fortran libraries, but here we are.) Even Rust – the language voted Most Obsessed With Threading Correctness by its high school class – is guilty of this. If a background thread panics, Rust discards the error and hopes for the best. Of course you can handle errors properly in these systems, by carefully making sure to join every thread, or by building your own error propagation mechanism like errbacks in Twisted or Promise.catch in Javascript. But now you&#39;re writing an ad-hoc, fragile reimplementation of the features your language already has. You&#39;ve lost useful stuff like &#34;tracebacks&#34; and &#34;debuggers&#34;. All it takes is forgetting to call Promise.catch once and suddenly you&#39;re dropping serious errors on the floor without even realizing. And even if you do somehow solve all these problems, you&#39;ll still end up with two redundant systems for doing the same thing. go statements: not even once Just like goto was the obvious primitive for the first practical high-level languages, go was the obvious primitive for the first practical concurrency frameworks: it matches how the underlying schedulers actually work, and it&#39;s powerful enough to implement any other concurrent flow pattern. But again like goto, it breaks control flow abstractions, so that merely having it as an option in your language makes everything harder to use. The good news, though, is that these problems can all be solved: Dijkstra showed us how! We need to: Find a replacement for go statements that has similar power, but follows the &#34;black box rule&#34;, Build that new construct into our concurrency framework as a primitive, and don&#39;t include any form of go statement. And that&#39;s what Trio did. Nurseries: a structured replacement for go statements Here&#39;s the core idea: every time our control splits into multiple concurrent paths, we want to make sure that they join up again. So for example, if we want to do three things at the same time, our control flow should look something like this: Notice that this has just one arrow going in the top and one coming out the bottom, so it follows Dijkstra&#39;s black box rule. Now, how can we turn this sketch into a concrete language construct? There are some existing constructs that meet this constraint, but (a) my proposal is slightly different than all the ones I&#39;m aware of and has advantages over them (especially in the context of wanting to make this a standalone primitive), and (b) the concurrency literature is vast and complicated, and trying to pick apart all the history and tradeoffs would totally derail the argument, so I&#39;m going to defer that to a separate post. Here, I&#39;ll just focus on explaining my solution. But please be aware that I&#39;m not claiming to have like, invented the idea of concurrency or something, this draws inspiration from many sources, I&#39;m standing on the shoulders of giants, etc. [3] Anyway, here&#39;s how we&#39;re going to do it: first, we declare that a parent task cannot start any child tasks unless it first creates a place for the children to live: a nursery. It does this by opening a nursery block; in Trio, we do this using Python&#39;s async with syntax: Opening a nursery block automatically creates an object representing this nursery, and the as nursery syntax assigns this object to the variable named nursery. Then we can use the nursery object&#39;s start_soon method to start concurrent tasks: in this case, one task calling the function myfunc, and another calling the function anotherfunc. Conceptually, these tasks execute inside the nursery block. In fact, it&#39;s often convenient to think of the code written inside the nursery block as being an initial task that&#39;s automatically started when the block is created. Crucially, the nursery block doesn&#39;t exit until all the tasks inside it have exited – if the parent task reaches the end of the block before all the children are finished, then it pauses there and waits for them. The nursery automatically expands to hold the children. Here&#39;s the control flow: you can see how it matches the basic pattern we showed at the beginning of this section: This design has a number of consequences, not all of which are obvious. Let&#39;s think through some of them. Nurseries preserve the function abstraction. The fundamental problem with go statements is that when you call a function, you don&#39;t know whether it&#39;s going to spawn some background task that keeps running after it&#39;s finished. With nurseries, you don&#39;t have to worry about this: any function can open a nursery and run multiple concurrent tasks, but the function can&#39;t return until they&#39;ve all finished. So when a function does return, you know it&#39;s really done. Nurseries support dynamic task spawning. Here&#39;s a simpler primitive that would also satisfy our flow control diagram above. It takes a list of thunks, and runs them all concurrently: run_concurrently([myfunc, anotherfunc]) But the problem with this is that you have to know up front the complete list of tasks you&#39;re going to run, which isn&#39;t always true. For example, server programs generally have accept loops, that take incoming connections and start a new task to handle each of them. Here&#39;s a minimal accept loop in Trio: async with trio.open_nursery() as nursery: while True: incoming_connection = await server_socket.accept() nursery.start_soon(connection_handler, incoming_connection) With nurseries, this is trivial, but implementing it using run_concurrently would be much more awkward. And if you wanted to, it would be easy to implement run_concurrently on top of nurseries – but it&#39;s not really necessary, since in the simple cases run_concurrently can handle, the nursery notation is just as readable. There is an escape. The nursery object also gives us an escape hatch. What if you really do need to write a function that spawns a background task, where the background task outlives the function itself? Easy: pass the function a nursery object. There&#39;s no rule that only the code directly inside the async with open_nursery() block can call nursery.start_soon – so long as the nursery block remains open [4], then anyone who acquires a reference to the nursery object gets the capability of spawning tasks into that nursery. You can pass it in as a function argument, send it through a queue, whatever. In practice, this means that you can write functions that &#34;break the rules&#34;, but within limits: Since nursery objects have to be passed around explicitly, you can immediately identify which functions violate normal flow control by looking at their call sites, so local reasoning is still possible. Any tasks the function spawns are still bound by the lifetime of the nursery that was passed in. And the calling code can only pass in nursery objects that it itself has access to. So this is still very different from the traditional model where any code can at any moment spawn a background task with unbounded lifetime. One place this is useful is in the proof that nurseries have equivalent expressive power to go statements, but this post is already long enough so I&#39;ll leave that for another day. You can define new types that quack like a nursery. The standard nursery semantics provide a solid foundation, but sometimes you want something different. Perhaps you&#39;re envious of Erlang and its supervisors, and want to define a nursery-like class that handles exceptions by restarting the child task. That&#39;s totally possible, and to your users, it&#39;ll look just like a regular nursery: async with my_supervisor_library.open_supervisor() as nursery_alike: nursery_alike.start_soon(...) If you have a function that takes a nursery as an argument, then you can pass it one of these instead to control the error-handling policy for the tasks it spawns. Pretty nifty. But there is one subtlety here that pushes Trio towards different conventions than asyncio or some other libraries: it means that start_soon has to take a function, not a coroutine object or a Future. (You can call a function multiple times, but there&#39;s no way to restart a coroutine object or a Future.) I think this is the better convention anyway for a number of reasons (especially since Trio doesn&#39;t even have Futures!), but still, worth mentioning. No, really, nurseries always wait for the tasks inside to exit. It&#39;s also worth talking about how task cancellation and task joining interact, since there are some subtleties here that could – if handled incorrectly – break the nursery invariants. In Trio, it&#39;s possible for code to receive a cancellation request at any time. After a cancellation is requested, then the next time the code executes a &#34;checkpoint&#34; operation (details), a Cancelled exception is raised. This means that there&#39;s a gap between when a cancellation is requested and when it actually happens – it might be a while before the task executes a checkpoint, and then after that the exception has to unwind the stack, run cleanup handlers, etc. When this happens, the nursery always waits for the full cleanup to happen. We never terminate a task without giving it a chance to run cleanup handlers, and we never leave a task to run unsupervised outside of the nursery, even if it&#39;s in the process of being cancelled. Automatic resource cleanup works. Because nurseries follow the black box rule, they make with blocks work again. There&#39;s no chance that, say, closing a file at the end of a with block will accidentally break a background task that&#39;s still using that file. Automated error propagation works. As noted above, in most concurrency systems, unhandled errors in background tasks are simply discarded. There&#39;s literally nothing else to do with them. In Trio, since every task lives inside a nursery, and every nursery is part of a parent task, and parent tasks are required to wait for the tasks inside the nursery... we do have something we can do with unhandled errors. If a background task terminates with an exception, we can rethrow it in the parent task. The intuition here is that a nursery is something like a &#34;concurrent call&#34; primitive: we can think of our example above as calling myfunc and anotherfunc at the same time, so our call stack has become a tree. And exceptions propagate up this call tree towards the root, just like they propagate up a regular call stack. There is one subtlety here though: when we re-raise an exception in the parent task, it will start propagating in the parent task. Generally, that means that the parent task will exit the nursery block. But we&#39;ve already said that the parent task cannot leave the nursery block while there are still child tasks running. So what do we do? The answer is that when an unhandled exception occurs in a child, Trio immediately cancels all the other tasks in the same nursery, and then waits for them to finish before re-raising the exception. The intuition here is that exceptions cause the stack to unwind, and if we want to unwind past a branch point in our stack tree, we need to unwind the other branches, by cancelling them. This does mean though that if you want to implement nurseries in your language, you may need some kind of integration between the nursery code and your cancellation system. This might be tricky if you&#39;re using a language like C# or Golang where cancellation is usually managed through manual object passing and convention, or (even worse) one that doesn&#39;t have a generic cancellation mechanism. A surprise benefit: removing go statements enables new features Eliminating goto allowed previous language designers to make stronger assumptions about the structure of programs, which enabled new features like with blocks and exceptions; eliminating go statements has a similar effect. For example: Trio&#39;s cancellation system is easier to use and more reliable than competitors, because it can assume that tasks are nested in a regular tree structure; see Timeouts and cancellation for humans for a full discussion. Trio is the only Python concurrency library where control-C works the way Python developers expect (details). This would be impossible without nurseries providing a reliable mechanism for propagating exceptions. Nurseries in practice So that&#39;s the theory. How&#39;s it work in practice? Well... that&#39;s an empirical question: you should try it and find out! But seriously, we just won&#39;t know for sure until lots of people have pounded on it. At this point I&#39;m pretty confident that the foundation is sound, but maybe we&#39;ll realize we need to make some tweaks, like how the early structured programming advocates eventually backed off from eliminating break and continue. And if you&#39;re an experienced concurrent programmer who&#39;s just learning Trio, then you should expect to find it a bit rocky at times. You&#39;ll have to learn new ways to do things – just like programmers in the 1970s found it challenging to learn how to write code without goto. But of course, that&#39;s the point. As Knuth wrote (Knuth, 1974, p. 275): Probably the worst mistake any one can make with respect to the subject of go to statements is to assume that &#34;structured programming&#34; is achieved by writing programs as we always have and then eliminating the go to&#39;s. Most go to&#39;s shouldn&#39;t be there in the first place! What we really want is to conceive of our program in such a way that we rarely even think about go to statements, because the real need for them hardly ever arises. The language in which we express our ideas has a strong influence on our thought processes. Therefore, Dijkstra asks for more new language features – structures which encourage clear thinking – in order to avoid the go to&#39;s temptations towards complications. And so far, that&#39;s been my experience with using nurseries: they encourage clear thinking. They lead to designs that are more robust, easier to use, and just better all around. And the limitations actually make it easier to solve problems, because you spend less time being tempted towards unnecessary complications. Using Trio has, in a very real sense, taught me to be a better programmer. For example, consider the Happy Eyeballs algorithm (RFC 8305), which is a simple concurrent algorithm for speeding up the establishment of TCP connections. Conceptually, the algorithm isn&#39;t complicated – you race several connection attempts against each other, with a staggered start to avoid overloading the network. But if you look at Twisted&#39;s best implementation, it&#39;s almost 600 lines of Python, and still has at least one logic bug. The equivalent in Trio is more than 15x shorter. More importantly, using Trio I was able to write it in minutes instead of months, and I got the logic correct on my first try. I never could have done this in any other framework, even ones where I have much more experience. For more details, you can watch my talk at Pyninsula last month. Is this typical? Time will tell. But it&#39;s certainly promising. Conclusion The popular concurrency primitives – go statements, thread spawning functions, callbacks, futures, promises, ... they&#39;re all variants on goto, in theory and in practice. And not even the modern domesticated goto, but the old-testament fire-and-brimstone goto, that could leap across function boundaries. These primitives are dangerous even if we don&#39;t use them directly, because they undermine our ability to reason about control flow and compose complex systems out of abstract modular parts, and they interfere with useful language features like automatic resource cleanup and error propagation. Therefore, like goto, they have no place in a modern high-level language. Nurseries provide a safe and convenient alternative that preserves the full power of your language, enables powerful new features (as demonstrated by Trio&#39;s cancellation scopes and control-C handling), and can produce dramatic improvements in readability, productivity, and correctness. Unfortunately, to fully capture these benefits, we do need to remove the old primitives entirely, and this probably requires building new concurrency frameworks from scratch – just like eliminating goto required designing new languages. But as impressive as FLOW-MATIC was for its time, most of us are glad that we&#39;ve upgraded to something better. I don&#39;t think we&#39;ll regret switching to nurseries either, and Trio demonstrates that this is a viable design for practical, general-purpose concurrency frameworks. </description>
      <pubDate>26 Feb 21 07:17 EST</pubDate>
      <guid>https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/</guid>
    </item>
    <item>
      <title></title>
      <link>https://entrepreneurshandbook.co/a-web-designer-turned-his-side-project-into-a-700m-year-revenue-business-without-vc-money-55cd13ee560</link>
      <description>&lt;a href=&#34;https://entrepreneurshandbook.co/a-web-designer-turned-his-side-project-into-a-700m-year-revenue-business-without-vc-money-55cd13ee560&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The Abandoned Side Project That Quietly Turned Into a $700m/year Revenue BusinessThe 20-year journey of Ben Chestnut, founder of MailChimpBen Chestnut was 26 when he was laid off and started a web design agency. Original image by Wikimedia Commons“I had a month’s heads-up about the whole thing. So I had time to kind of plan my life.”After getting laid off in 2000, Ben Chestnut resorted to the trade he knew best — designing websites. Over the years, he’d built “about 2 thousand” banner ads for his former employer, Cox newspapers. He knew exactly how to design clickable things online.“And I thought… Well, this is our chance to start a company. My business partner and I went out and got clients. We went knocking on doors, down the hall from our office. And we got paying projects. We got a $13,000 project and a $32,000 project. Before even getting a business licence.”Unfortunately, running a web design agency proved to be less about your prowess in design and more about how flawless you sounded over the phone. Agencies often attract the most whimsical business, where clients’ opinions replace real KPIs and a soothing voice can be more important than a moving product. Naturally, a group of introverted, caring designers called the Rocket Science Group didn’t thrive in an environment that rewarded appearances over substance.Five years passed, and the hamster wheel of unfulfilling client work was about done soothing Ben into a self-employed existence of a stagnating agency owner. The revenues were locked into a stubborn plateau and the team was exhausted by a potent mix of uncertainty and punishing workload. The very meaning of this masochistic lifestyle was put into question.The question prompted Ben and his team to review the core logic — and the foreseeable future — of their business. A spreadsheet was opened, listing detailed revenues from all of the agency’s projects. It was one of the lines on that spreadsheet that produced the only answer Ben needed to hear. Turns out, one of their internal side-projects was quietly generating more money than all of the agency’s consulting projects, combined.Quotes and facts taken from these five sources.A chimp is bornAt the Rocket Science Group, the creative minds were growing weary of implementing the same feature into their clients’ websites over and over again: an email-list-building tool. The work billed was repetitive and ripe for creative automation. To lose the burden, the team coded a one-size-fits-all, self-serve solution, and charged clients $0.01 per email sent.“Instead of ignoring the problem, Ben and Dan identified this as an opportunity to help their clients solve the problem. They got code from a failed digital greeting card product they had made and tweaked it to launch MailChimp to their web agency client base in 2001.” (source)The word spread, slowly. Old clients who no longer worked with the Rocket Science Group were still using the email tool. Small business owners who never were the agency’s clients started contacting with requests. While Ben was zeroed in on putting meat on the table for the agency, his email tool quietly grew its own little following.The amounts were still negligible. When you’re chasing $30,000 web design projects, a few $50 invoices don’t warrant significant attention. Ironically, it was the increasingly inefficient task of issuing these small invoices that prompted Ben to introduce a monthly subscription model and build a credit card feature for MailChimp — effectively giving birth to one of the first software-as-a-service products ever created.How does an internally used background tool turn into a $4.2 billion, industry-spawning behemoth with zero investment? In short, by using every guerrilla tactic in the book:The semi-viral freemium model is the biggest culprit in the Chimp’s success story, and it propelled its growth from 100,000 users to 1m users in just one year. It was novel at the time to give users free access to the entirety of your platform. The explosive success of the freemium model was two-fold: for one, every ‘free’ email sent contained a MailChimp logo in the footnotes, creating a semi-viral loop of organic referrals; since the users had to pay only once they reached a certain number of emails on their list, getting your first paid MailChimp plan acted as a rite of passage you had to ‘earn’ your way to.Niche social platforms. The company’s adage in the early days was Code, Blog, Tweet, Repeat — simply because this marketing strategy worked. Twitter was a lot less crowded in 2007, and MailChimp was getting real exposure on the network. Similarly, they purchased ads to be played at the beginning of every new episode of a crime podcast called Serial. The podcast is now a hit — having been the first one to hit 5m downloads in the history of podcasts — but buying ads on it was a hipster thing to do back in the day. Hence, cheap.Designer marketing campaigns. In 2014, an announcer on one of the podcast ads accidentally mispronounced MailChimp as MailKimp. The ad was streamed to one million users — but, in character, the company decided to turn the comical mistake into an entire marketing campaign. Soon, entire brands for MailShrimp, FailChips, VeilHymn and a bunch of other Bumblesnuff-Crimpysnitch-esque spinoffs were launched, accompanied by amazingly creative campaigns. Weird? Kind of. Hilarious? Just watch the VeilHymn artist-interview-parody.The Chimp! Theories behind branding as a be-all-end-all are often cheesy, but the Chimp certainly worked out for Ben. Back in his days as a web designer, he learned that adding a monkey to any marketing design increased its effectiveness. Over the years, the Chimp had become a champion of the brand, throwing goofy comments at users when they log in, disable-able in the ‘party-pooper’ mode in Settings. Seriously, how many other email platforms can you name?Ultimately, the Chimp succeeded because it found its way into the hearts of small business owners. Ben’s mom used to run a hairdresser’s salon in their home kitchen, so he knew intimately the kinds of struggles self-employed people run into day-to-day.He knew that small businesses don’t have separate marketing budgets — buying a new TV for your living room or investing into Facebook ads were all funded by the same pocket. He knew that self-employed people are the end users of his product — which makes them emotional as decision makers.Ben: “The content that we’re putting out is like… Second chances in life… How do you know when to hold on and when to give up… Those are struggles [small] entrepreneurs are facing all the time. […] We want to help them scale out of the kitchen.”It sounds corny, but a big part of Ben’s success lies in the fact that he was honest with himself about his own strengths and weaknesses. When Ben’s father bought him a computer, he didn’t teach himself coding — he learned how to draw on a computer program that took 5 floppy disks to run. In fact, he wanted to become a cartoonist as a kid. Is MailChimp an incredible feat of engineering? Perhaps. But its core strength is creativity, and that is, apparently, enough to build a billion-dollar tech company.Shut up and take our money!One of the most interesting aspects of Ben’s journey is that he grew MailChip without any external funding, which makes this story somewhat unique as tech companies go.The main reason why Ben could do it is because MailChimp was a revenue-generating piece of software since day one. Their pricing changed over the years (per email -&gt; monthly subscription -&gt; freemium,) but, unlike products like WhatsApp, it had a very clear revenue model — one that didn’t involve selling your users’ data. Also, it’s crucial to consider that MailChimp was a spinout of the agency Ben used to run, by which it was initially funded.Ben: “For us, it was an interesting time. It was the early days of SaaS, nobody was really solving problems with SaaS — or for small business. So we had a nice opportunity to go at it alone. And we just always made loads of money, because we were the only people willing to do email — very unsexy business — for small business — which is also very unsexy.”It’s not like Ben was against taking investor money. But the world was still recovering from the dot-com burst, and VCs weren’t exactly eager to throw money at internet companies. Many were wary of the SaaS freemium model — which was novel at the time. Most investors Ben had met argued that MailChimp should go after enterprises — because that’s where the big bucks are — and not small businesses.To investors, the math just didn’t check out. Why serve a highly fragmented, emotional, low-budget small business audience when 30% of them will be out of business in 2 years, and 50% of them will fail within the next 5 years? What Ben understood from personal experience was that even when solo-preneurs fail, they keep their email lists alive, and most of them start something new some time down the road. So MailChimp doesn’t necessarily lose a customer, even when they go out of business temporarily.Of course, once the revenues started to grow, investors lined up behind the MailChimp door. Over the years, Ben had seen dozens of competitors take millions in funding with hopes of outgrowing the company. Each VC-backed competitor could mean that Ben had made a terrible mistake staying self-funded. As of 2020, however, MailChimp still sits comfortably with a 60% share of the email industry.Ben: “I’ve been in this business for 19 years. So I’ve had waves of competitors taking in money and I’ve gone through those stages of grief where I say ‘oh my God, they’re gonna kill me now.’ The funding gets bigger and bigger… and nothing seems to happen. We just keep our laser focus […] and we’re good.”Could MailChimp have grown even faster if it had taken VC money? Perhaps. But it’s also likely that the corporate-obsessed investors would have gradually chipped away MailChimp’s culture of creativity and innovation, which made the company special in the first place.7 interesting facts about Ben Chestnut and MailChimpFor Ben, a blend of cigarette smoke and hair spray is the smell of business. It’s because his mom used to run a hair salon in their kitchen, and that’s how Ben was exposed to entrepreneurship as a kid.When hiring, Ben is looking for ‘humble educators’ who are ‘so good they invite criticism.’ He wants people who are confident enough in their own abilities that they don’t have a problem having their views challenged or explaining difficult concepts in a simple way. If a person cannot dumb down their message, Ben says, they’re not a good fit for MailChimp.Ben’s first hire and (supposedly technical) co-founder, Dan Kurzius, lied about knowing how to code on his interview. After he got the job, he put together the prototype for MailChimp using “HTML for Dummies” books. Dan tried so hard he actually produced (or stole) clean, functional code which amazed Ben. Only 10 years later did the co-founder tell Ben about his improvisation.Ben confirmed he’s rejected a billion-dollar acquisition offer from an unspecified company. In his defense, he says that “a billion dollars isn’t that much more than a few hundred million.”The low-profile co-founder, Dan Kurzius, does regular anonymous visits to the small businesses that use MailChimp — from yoga studios to warehouses. This way, the company gets invaluable feedback — like the fact that many businesses use MailChimp as a CRM and not an email tool. It does make things a little weird when the biggest critics later learn they’ve been talking to the co-founder at MailChimp events.One of Ben’s favorite books is Viktor Frankl’s Man’s Search for Meaning. Frankl was a psychiatrist who survived a concentration camp during Holocaust. In the book, he documents how prisoners find meaning and purpose even in the most inhumane conditions.Ben’s motto is love what you do vs. the traditional do what you love. He says that eventually all passions fade away if you turn them into profession, and the only way to keep your sense of purpose is by learning to appreciate the craft you’re good at.“Guys, no one’s coming.”To end things off, I’d just like to leave you with one of my favorite Ben Chestnut quotes:“When things get tough and dark — which is frequent when you’re an entrepreneur — I remember something that I realized growing up in Upson, Georgia. It’s something that I said to a bunch of friends when we were lost roaming around in the woods. I said ‘guys, no one’s coming.’ [laughs] That doesn’t sound very positive or anything, I’m sorry, but it’s something you have to realize: no one’s coming! It’s up to us! When you’re an entrepreneur, no one’s really gonna come to help you. It’s up to you to figure out what you have to do to get out of the mess. But if you do get out of the mess, you will have a lot of people joining you. […] I say it to myself all the time.”</description>
      <pubDate>02 Mar 21 12:50 EST</pubDate>
      <guid>https://entrepreneurshandbook.co/a-web-designer-turned-his-side-project-into-a-700m-year-revenue-business-without-vc-money-55cd13ee560</guid>
    </item>
    <item>
      <title></title>
      <link>http://paulgraham.com/simply.html</link>
      <description>&lt;a href=&#34;http://paulgraham.com/simply.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;March 2021I try to write using ordinary words and simple sentences.That kind of writing is easier to read, and the easier something is to read, the more deeply readers will engage with it. The less energy they expend on your prose, the more they&#39;ll have left for your ideas.And the further they&#39;ll read. Most readers&#39; energy tends to flag part way through an article or essay. If the friction of reading is low enough, more keep going till the end.There&#39;s an Italian dish called saltimbocca, which means &#34;leap into the mouth.&#34; My goal when writing might be called saltintesta: the ideas leap into your head and you barely notice the words that got them there.It&#39;s too much to hope that writing could ever be pure ideas. You might not even want it to be. But for most writers, most of the time, that&#39;s the goal to aim for. The gap between most writing and pure ideas is not filled with poetry.Plus it&#39;s more considerate to write simply. When you write in a fancy way to impress people, you&#39;re making them do extra work just so you can seem cool. It&#39;s like trailing a long train behind you that readers have to carry.And remember, if you&#39;re writing in English, that a lot of your readers won&#39;t be native English speakers. Their understanding of ideas may be way ahead of their understanding of English. So you can&#39;t assume that writing about a difficult topic means you can use difficult words.Of course, fancy writing doesn&#39;t just conceal ideas. It can also conceal the lack of them. That&#39;s why some people write that way, to conceal the fact that they have nothing to say. Whereas writing simply keeps you honest. If you say nothing simply, it will be obvious to everyone, including you.Simple writing also lasts better. People reading your stuff in the future will be in much the same position as people from other countries reading it today. The culture and the language will have changed. It&#39;s not vain to care about that, any more than it&#39;s vain for a woodworker to build a chair to last.Indeed, lasting is not merely an accidental quality of chairs, or writing. It&#39;s a sign you did a good job.But although these are all real advantages of writing simply, none of them are why I do it. The main reason I write simply is that it offends me not to. When I write a sentence that seems too complicated, or that uses unnecessarily intellectual words, it doesn&#39;t seem fancy to me. It seems clumsy.There are of course times when you want to use a complicated sentence or fancy word for effect. But you should never do it by accident.The other reason my writing ends up being simple is the way I do it. I write the first draft fast, then spend days editing it, trying to get everything just right. Much of this editing is cutting, and that makes simple writing even simpler.</description>
      <pubDate>11 Mar 21 09:35 EST</pubDate>
      <guid>http://paulgraham.com/simply.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.filfre.net/2021/03/system-shock/</link>
      <description>&lt;a href=&#34;https://www.filfre.net/2021/03/system-shock/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; We approached games as immersive simulations. We wanted to build game environments that reacted to player’s decisions, that behaved in natural ways, and where players had more verbs than simply “shoot.” DOOM was not an influence on System Shock. We were trying something more difficult and nuanced, [although] we still had a lot of respect for the simplicity and focus of [the id] games. There was, to my recollection, a vague sense of fatalism about the parallel tracks the two companies were taking, since it was clear early on that id’s approach, which needed much less player education and which ran on adrenaline rather than planning and immersion, was more likely to be commercially successful. But we all believed very strongly in Looking Glass’s direction, and were proud that we were taking games to a more cerebral and story-rich place. — Dorian Hart We hope that our toiling now to make things work when it is still very hard to do effectively will mean that when it is easier to do, we can concentrate on the parts of the game that are less ephemeral than polygons per second, and distinguish ourselves by designing detailed and immersive environments which are about more than just the technology. — Doug Church In late 1992, two separate studios began working on two separate games whose descriptions sound weirdly identical to one another. Each was to make you the last human survivor on a besieged space station. You would roam its corridors in real time in an embodied first-person view; both studios prided themselves on their cutting-edge 3D graphics technology. As you explored, you would have to kill or be killed by the monsters swarming the complex. Yet wresting back control of the station would demand more than raw firepower: in the end, you would have to outwit the malevolent intelligence behind it all. Both games were envisioned as unprecedentedly rich interactive experiences, as a visceral new way of living through an interactive story. But in the months that followed, these two projects that had started out so conceptually similar diverged dramatically. The team that was working on DOOM at id Software down in Dallas, Texas, decided that all of the elaborate plotting and puzzles were just getting in the way of the simpler, purer joys of blowing away demons with a shotgun. Lead programmer John Carmack summed up id’s attitude: “Story in a game is like story in a porn movie. It’s expected to be there, but it’s not that important.” id discovered that they weren’t really interested in making an immersive virtual world; they were interested in making an exciting game, one whose “gameyness” they felt no shame in foregrounding. Meanwhile the folks at the Cambridge, Massachusetts-based studio Looking Glass Technologies stuck obstinately to their original vision. They made exactly the uncompromising experience they had first discussed, refusing to trade psychological horror in for cheaper thrills. System Shock would demand far more of its players than DOOM, but would prove in its way an even more rewarding game for those willing to follow it down the moody, disturbing path it unfolded. It was in this moment, then, that the differences between id and Looking Glass, the yin and the yang of 1990s 3D-graphics pioneers, became abundantly clear. Looking Glass arrived at their crossroads moment just as they were completing their second game, Ultima Underworld II. Both it and its predecessor were first-person fantasy dungeon crawls set in and around Britannia, the venerable world of the Ultima CRPG series to which their games served as spinoffs. And both were very successful, so much so that they almost overshadowed Ultima VII, the latest entry in the mainline series. Looking Glass’s publisher Origin Systems would have been happy to let them continue making games in this vein for as long as their customers kept buying them. But Looking Glass, evincing the creative restlessness that would define them throughout their existence, was ready to move on to other challenges. In the months immediately after Ultima Underworld II was completed, the studio’s head Paul Neurath allowed his charges to start three wildly diverse projects on the back of the proceeds from the Underworld games, projects which were unified only by their heavy reliance on 3D graphics. One was a game of squad-level tactical combat called Terra Nova, another a civilian flight simulator called Flight Unlimited. And the third — actually, the first of the trio to be officially initiated — was System Shock. Doug Church, the driving creative force behind Ultima Underworld, longed to create seamless interactive experiences, where you didn’t so much play a game as enter into its world. The Underworld games had been a big step in that direction within the constraints of the CRPG form, thanks to their first-person, free-scrolling perspective, their real-time gameplay, and, not least, the way they cast you in the role of a single embodied dungeon delver rather than that of the disembodied manager of a whole party of them. But Church believed that there was still too much that pulled you out of their worlds. Although the games were played entirely from a single screen, which itself put them far ahead of most CRPGs in terms of immediacy, you were still switching constantly from mode to mode within that screen. “I felt that Underworld was sort of [four] different games that you played in parallel,” says Church. “There was the stats-based game with the experience points, the inventory-collecting-and-managing game, the 3D-moving-around game, and there was the talking game — the conversation-branch game.” What had seemed so fresh and innovative a couple of years earlier now struck Church as clunky. Ironically, much of what he was objecting to is inherent to the CRPG form itself. Aficionados of the genre find it endlessly enjoyable to pore over their characters’ statistics at level-up time, to min-max their equipment and skills. And this is fine: the genre is as legitimate as any other. Yet Church himself found its cool intellectual appeal, derived from its antecedents on the tabletop which had no choice but to reveal all of their numbers to their players, to be antithetical to the sort of game that he wanted to make next: In Underworld, there was all this dice rolling going on off-screen basically, and I’ve always felt it was kind of silly. Dice were invented as a way to simulate swinging your sword to see if you hit or miss. So everyone builds computer games where you move around in 3D and swing your sword and hit or miss, and then if you hit you roll some dice to simulate swinging a sword to decide if you hit or miss. How is anyone supposed to understand unless you print the numbers? Which is why, I think, most of the games that really try to be hardcore RPGs actually print out, “You rolled a 17!” In [the tabletop game of] Warhammer when you get a five-percent increase and the next time you roll your attack you make it by three percent, you’re all excited because you know that five-percent increase is why you hit. In a computer game you have absolutely no idea. And so we really wanted to get rid of all that super opaque, “I have no idea what’s going on” stuff. We wanted to make it so you can watch and play and it’s all happening. So, there would be no numbers in his next game — no character levels, no character statistics, not even quantifiable hit points. There would just be you, right there in the world, without any intervening layers of abstraction. Over the course of extensive discussions involving Doug Church himself, Paul Neurath, Looking Glass designer and writer Austin Grossman, and their Origin Systems producer Warren Spector, it was decided to make a first-person science-fiction game with distinct cyberpunk overtones, pitting you against an insane computer known as SHODAN. Cyberpunk was anything but a novelty in the games of the 1990s, a time when authors like William Gibson, Bruce Sterling, and Neal Stephenson all occupied prominent places on the genre-fiction bestseller charts and the game developers who read their novels rushed to bring their visions to life on monitor screens. Still, cyberpunk would suit Looking Glass’s purposes unusually perfectly by presenting a credible explanation for the diegetic interface Church was envisioning. You would play a character with a neural implant that let you “see” a heads-up display sporting a life bar for yourself, an energy bar for your weapons and other hardware, etc. — all of it a part of the virtual world rather than external to it. When you switched between “modes,” such as when bringing up the auto-map or your inventory display, it would be the embodied you who did so in the virtual world, not that other you who sat in front of the computer telling a puppet what to do next. System Shock‘s commitment to its diegetic presentation is complete. As you discover new software and gadgets, they’re added to the heads-up display provided by your in-world neural implant. This serves the same purpose that leveling up did in Ultima Underworld, but in a more natural, realistic way. Dissatisfied with what he saw as the immersion-killing conversation trees of Ultima Underworld, Church decided to get rid of two-way conversation altogether. When the game began, there would be enticing signs that other humans were still alive somewhere on the space station, but you would be consistently too late to reach them; you would encounter them only as the zombies SHODAN turned them into after death. Of course, all of this too was highly derivative, and on multiple levels at that. Science-fiction fans had been watching their heroes take down out-of-control computers since the original Star Trek television series if not before; “I don’t think if you wrote the novel [of System Shock] it would fly off the shelves,” admits Church. Likewise, computer games had been contriving ways to place you in deserted worlds, or in worlds inhabited only by simple-minded creatures out for your blood, for as long as said games had existed, always in order to avoid the complications of character interaction; the stately isolation of the mega-hit adventure game Myst was only the most recent notable example of the longstanding tradition at the time System Shock was in development. But often it’s not what you do in any form of media, it’s how well you do it. And System Shock does what it sets out to do very, very well indeed. It tells a story of considerable complexity and nuance through the artifacts you find lying about as you explore the station and the emails you receive from time to time, allowing you to piece it all together for yourself in nonlinear fashion. “We wanted to make the plot and story development of System Shock be an exploration as well,” says Church, “and that’s why it’s all in the logs and data, so then it’s very tied into movement through the spaces.” Reading a log entry. The story is conveyed entirely through epistolary means like these, along with occasional direct addresses from SHODAN herself that come booming through the station’s public-address system. Moving through said spaces, picking up bits and pieces of the horrible events which have unfolded there, quickly becomes highly unnerving. The sense of embodied realism that clings to every aspect of the game is key to the sense of genuine, oppressive fear it creates in its player. Tellingly, Looking Glass liked to call System Shock a “simulation,” even though it simulates nothing that has ever existed in the real world. The word is rather shorthand for its absolute commitment to the truth — fictional truth, yes, but truth nevertheless — of the world it drops you into. Story is very important to System Shock — and yet, in marked contrast to works in the more traditionally narrative-oriented genre of the adventure game, its engine also offers heaps and heaps of emergent possibility as you move through the station discovering what has gone wrong here and, finally, how you might be able to fix it. “It wasn’t just, ‘Go do this sequence of four things,&#39;” says Church. “It was, ‘Well, there are going to be twelve cameras here and you gotta take out eight of them. Figure it out.’ We [also] gave you the option [of saying], ‘I don’t want to fight that guy. Okay, maybe I can find another way…&#39;” Thus System Shock manages the neat trick of combining a compelling narrative with a completely coherent environment that never reduces you to choosing from a menu of options, one where just about any solution for any given problem that seems like it ought to work really does work. Just how did Looking Glass achieve this when so many others before and since have failed, or been so daunted by the challenges involved that they never even bothered to try? They did so by combining technical excellence with an aesthetic sophistication to which few of their peers could even imagine aspiring. Just as the 3D engine that powers Ultima Underworld is more advanced than the pseudo-3D of id’s contemporaneous Wolfenstein 3D, the System Shock engine outdoes DOOM in a number of important respects. The enormous environments of System Shock curve over and under and around one another, complete with sloping floors everywhere; lighting is realistically simulated; you can jump and crouch and look up and down and lean around corners; you can take advantage of its surprisingly advanced level of physics simulation in countless ingenious ways. System Shock even boasts perspective-correct texture mapping, a huge advance over Ultima Underworld, and no easy thing to combine with the aforementioned slopes. Each of the ten “levels” of System Shock is really multiple levels in the physical sense, as the corridors often curve over and under one another. Just as in Ultima Underworld, you can annotate the auto-map for yourself. But even with this aid, just finding your way around in these huge, confusing spaces can be a challenge in itself. That said, it’s also abundantly true that a more advanced engine doesn’t automatically make for a better game. Any such comparison must always carry an implied addendum: better for whom? Certainly DOOM succeeded beautifully in realizing its makers’ ambitions, even as its more streamlined engine could run well on many more of the typical computers of the mid-1990s than System Shock‘s could. By no means do the engines’ respective advantages all run one way: in addition to being much faster than the System Shock engine, the DOOM engine allows rooms of arbitrary sizes and non-orthogonal walls, neither of which is true of its counterpart from Looking Glass. In the end, System Shock wants to be a very different experience than DOOM, catering to a different style of play, and its own engine is designed to allow it to realize its own ambitions. It demands a more careful approach from its player, where you must constantly use light and shadow, walls and obstacles, to aid you in your desperate struggle. For you are not a superhuman outer-space marine in System Shock; you’re just, well, you, scared and alone in a space station filled with rampaging monsters. A fine example of the lengths to which Looking Glass’s technologists were willing to go in the service of immersion is provided by the mini-games you can play. Inspired by, of all things, the similarly plot-irrelevant mini-games found in the LucasArts graphic adventure Sam and Max Hit the Road, they contribute much more to the fiction in this case than in that other one. As with everything in System Shock, the mini-games are not external to the world of the main game. It’s rather you playing them through your neural implant right there in the world; it’s you who cowers in a safe corner somewhere, trying to soothe your soul with a quick session of Breakout or Missile Command. You get the chance to collect more and better games as you infiltrate the station’s computer network using the cyberspace jacks you find scattered about — a reward of sorts for a forlorn hacker trying to survive against an all-powerful entity and her horrifying minions. Taking the edge off with a quick game of Pong (in the window at lower left). Sean Barret, a programmer who came to Looking Glass and to the System Shock project well into the game’s development, implemented the most elaborate by far of the mini-games, a gentle satire of Origin Systems’s Wing Commander that goes under the name of Wing 0. The story of its creation is a classic tale of Looking Glass, a demonstration both of the employees’ technical brilliance and their insane levels of commitment to the premises of their games. Newly arrived on the team and wishing to make a good impression, Barrett saw a list of mini-game ideas on a whiteboard; a “Wing Commander clone” was among them. So, he set to work, and some days later presented his creation to his colleagues. They were as shocked as they were delighted; it turned out that the Wing Commander clone had been a joke rather than a serious proposal. In the end, however, System Shock got its very own Wing Commander after all. Still, there were many other technically excellent and crazily dedicated games studios in the 1990s, just as there are today. What truly set Looking Glass apart was their interest in combining the one sort of intelligence with another kind that has not always been in quite so great a supply in the games industry. As Looking Glass grew, Paul Neurath brought some very atypical characters into the fold. Already in late 1991, he placed an advertisement in the Boston Globe for a writer with an English degree. He eventually hired Austin Grossman, who would do a masterful job of scattering the puzzle pieces of Doug Church’s story outline around the System Shock space station. There soon followed another writer with an English degree, this one by the name of Dorian Hart, who would construct some of the station’s more devious internal spaces using the flair for drama which he had picked up from all of the books he had read. He was, as he puts it, “a liberal-arts nobody with no coding skills or direct industry experience, thrown onto arguably the most accomplished and leading-edge videogame production team ever assembled. It’s hard to explain how unlikely that was, and how fish-out-of-water I felt.” Nevertheless, there he was — and System Shock was all the better for his presence. Another, even more unlikely set of game developers arrived in the persons of Greg LoPiccolo and Eric and Terri Brosius, all members of a popular Boston rock band known as Tribe, who had been signed to a major label amidst the Nirvana-fueled indie-rock boom of the early 1990s, only to see the two albums they recorded fail to duplicate their local success on a national scale. They were facing a decidedly uncertain future when Doug Church and Dan Schmidt — the latter being another Looking Glass programmer, designer, and writer — showed up in the audience at a Tribe show. They loved the band’s angular, foreboding songs and arrangements, they explained afterward, and wanted to know if they’d be interested in doing the music for a science-fiction computer game that would have much the same atmosphere. Three members of the band quickly agreed, despite knowing next to nothing about computers or the games they played. “Being young, not knowing what would happen next, that was part of the magic,” remembers Eric Brosius. “We were willing to learn because it was just an exciting time.” Terri Brosius became the voice of SHODAN, a role that fell to her by default: artificial intelligences in science fiction typically spoke in a female voice, and she was the only woman to be found among the Looking Glass creative staff. But however she got the part, she most definitely made it her own. She laughs that “people tend to get freaked out” when they hear her speak today in real life. And small wonder: her glitchy voice ringing through the empty corridors of the station, dripping with sarcastic malice, is one of the indelible memories that every player of System Shock takes away with her. Simply put, SHODAN creeps you the hell out. “You had a recurring, consistent, palpable enemy who mattered to you,” notes Doug Church — all thanks to Austin Grossman’s SHODAN script and Terri Brosius’s unforgettable portrayal of her. As I think about the combination of technical excellence and aesthetic sophistication that was Looking Glass, I find one metaphor all but unavoidable: that of Looking Glass as the Infocom of the 1990s. Certainly Infocom, their predecessors of the previous decade on the Boston-area game-development scene, evinced much the same combination — the same thoroughgoing commitment to excellence and innovation in all of their forms — during their own heyday. If the 3D-graphics engines of Looking Glass seem a long way from the text and parsers of Infocom, let that merely be a sign of just how much gaming itself had changed in a short span of time. Even when we turn to more plebeian matters, there are connections to be found beyond a shared zip code. Both studios were intimately bound up with MIT, sharing in the ideas, personnel, and, perhaps most of all, the culture of the university; both had their offices on the same block of CambridgePark Drive; two of Looking Glass’s programmers, Dan Schmidt and Sean Barrett, later wrote well-received textual interactive fictions of their own. The metaphor isn’t ironclad by any means; Legend Entertainment, founded as it was by former Infocom author Bob Bates and employing the talents of Steve Meretzky, is another, more traditionalist answer to the question of the Infocom of the 1990s. Still, the metaphor does do much to highlight the nature of Looking Glass’s achievements, and their importance to the emerging art form of interactive narrative. Few if any studios were as determined to advance that art form as these two were. But Looking Glass’s ambitions could occasionally outrun even their impressive abilities to implement them, just as could Infocom’s at times. In System Shock, this overreach comes in the form of the sequences that begin when you utilize one of those aforementioned cyberspace jacks that you find scattered about the station. System Shock‘s cyberspace is an unattractive, unwelcoming place — and not in a good way. It’s plagued by clunky controls and graphics that manage to be both too minimalist and too garish, that are in fact almost impossible to make head or tail of. The whole thing is more frustrating than fun, not a patch on the cyberspace sequences to be found in Interplay’s much earlier computer-game adaption of William Gibson’s breakthrough novel Neuromancer. So, it turns out that even the collection of brilliant talent that was assembled at Looking Glass could have one idea too many. Doug Church: We thought [that] it fit from a conceptual standpoint. You’re a hacker; shouldn’t you hack something? We thought it would be fun to throw in a different movement mode that was more free-form, more action. In retrospect, we probably should have either cut it or spent more time on it. There is some fun stuff in it, but it’s not as polished as it should be. But even so, it was nice because it at least reinforced the idea that you were the hacker, in a totally random, arcadey, broken kind of way. But at least it suggested that you’re something other than a guy with a gun. We were looking at ourselves and saying, “Oh, of course we should have cyberspace! We’re a cyberpunk game, we gotta have cyberspace! Well, what can we do without too much time? What if we do this crazy thing?” Off we went… By way of compounding the problem, the final confrontation with SHODAN takes place… in cyberspace. This tortuously difficult and thoroughly unfun finale has proven to be too much for many a player, leaving her to walk away on the verge of victory with a terrible last taste of the game lingering in her mouth. Cyberspace was a nice idea, but its implementation leaves much to be desired. Luckily, it’s possible to work around even this weakness to a large extent, thanks to another of the generous affordances which Looking Glass built into the game. You can decide for yourself how complex and thus how difficult you wish the game to be along four different axes: Combat (the part of the game that is most like DOOM); Mission (the non-combat tasks you have to accomplish to free the station from SHODAN’s grip); Puzzle (the occasional spatial puzzles that crop up when you try to jigger a lock or the like); and Cyber (the cyberspace implementation). All of these can be set to a value between zero and three, allowing you to play System Shock as anything from a straight-up shooter where all you have to do is run and gun to an unusually immersive and emergent pure adventure game populated only by “feeble” enemies who “never attack first.” The default experience sees all of these values set at two, and this is indeed the optimal choice in my opinion for those who don’t have a complete aversion to any one of the game’s aspects — with one exception: I would recommend setting Cyber to one or even zero in order to save yourself at lot of pain, especially at the very end. (The ultimate challenge for System Shock veterans, on the other hand, comes by setting the Mission value to three; this imposes a time limit on the whole game of about seven hours.) If you really, really want to play System Shock as a DOOM clone, that’s okay with Looking Glass. System Shock was released in late 1994, almost two full years after Ultima Underworld II, Looking Glass’s last game. It sold acceptably but not spectacularly well for a studio that was already becoming well-acquainted with the financial worries that would continue to dog them for the rest of their existence. Reviews were quite positive, yet many of the authors of same seemed barely to have noticed the game’s subtler qualities, choosing to lump it in instead with the so-called “DOOM clones” that were beginning to flood the market by this point, almost a year after the original DOOM‘s release. (One advantage of id Software’s more limited ambitions for their game was the fact that it was finished much, much quicker than System Shock; in fact, a DOOM II was already on store shelves by the time System Shock made it there.) Although everyone at Looking Glass took the high road when asked about the DOOM connection, the press and public’s tendency to diminish their own accomplishment in 3D virtual-world-building had to rankle at some level; former employees insist to this day that DOOM had no influence whatsoever on their own creation, that System Shock would have turned out the same even had DOOM never existed. The fact is, Looking Glass’s own claim to the title of 3D-graphics pioneers is every bit as valid as that of id, and their System Shock engine actually was, as we’ve seen, more advanced than that of DOOM in a number of ways. No games studio in history has ever deserved less to be treated as imitators rather than innovators. Alas, mainstream appreciation would be tough to come by throughout the remaining years of Looking Glass’s existence, just as it had sometimes been for Infocom before them. A market that favored the direct, visceral pleasures of id’s DOOM and, soon, Quake didn’t seem to know quite what to do with Looking Glass’s more nuanced 3D worlds. And so, yet again as with Infocom, it would not be until after Looking Glass was no more that the world of gaming would come to fully appreciate everything they had achieved. When asked pointedly about the sales charts which his games so consistently failed to top, Doug Church showed wisdom beyond his years in insisting that the important thing was just to earn enough back to make the next game. id did a great job with [DOOM]. And more power to them. I think you want to do things that connect with the market and you want to do things that people like and you want to do things that get seen. But you also want to do things you actually believe in and you personally want to do. Hey, if you’re going to work twenty hours a day and not get paid much money, you might as well do something you like. We were building the games we were interested in; we had that luxury. We didn’t have spectacular success and a huge win, but we had enough success that we got to do some more. And at some level, at least for me, sure, I’d love to have huge, huge success. But if I get to do another game, that’s pretty hard to complain about. Today, free of the vicissitudes of an inhospitable marketplace, System Shock more than speaks for itself. Few games, past or present, combine so many diverse ideas into such a worthy whole, and few demonstrate such uncompromising commitment to their premise and their fiction. In a catalog filled with remarkable achievements, System Shock still stands out as one of Looking Glass’s most remarkable games of all, an example of what magical things can happen when technical wizardry is placed in the service of real aesthetic sophistication. By all means, go play it now if you haven’t already. Or, perhaps better said, go live it now. (Sources: the books Game Design Theory and Practice, second edition, by Richard Rouse III and System Shock: Strategies and Secrets by Bernie Yee; Origin’s official System Shock hint book; Origin’s internal newsletter Point of Origin from June 3 1994, November 23 1994, January 13 1995, February 10 1995, March 14 1995, and May 5 1995; Electronic Entertainment of December 1994; Computer Gaming World of December 1994; Next Generation of February 1995; Game Developer of April/May 1995. Online sources include “Ahead of Its Time: The History of Looking Glass” and “From Looking Glass to Harmonix: The Tribe,” both by Mike Mahardy of Polygon. Most of all, huge thanks to Dorian Hart, Sean Barrett, and Dan Schmidt for talking with me about their time at Looking Glass. System Shock is available for digital purchase at GOG.com.) </description>
      <pubDate>19 Mar 21 22:31 EDT</pubDate>
      <guid>https://www.filfre.net/2021/03/system-shock/</guid>
    </item>
    <item>
      <title></title>
      <link>https://psyche.co/ideas/self-compassion-is-not-self-indulgence-heres-how-to-try-it</link>
      <description>&lt;a href=&#34;https://psyche.co/ideas/self-compassion-is-not-self-indulgence-heres-how-to-try-it&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I’ll never forget the time I overheard one of my high-school classmates repeatedly calling herself stupid in front of the bathroom mirror. When I recognised her voice, chills ran down my spine. I’d always thought of her as one of the kindest people in the whole school. I was shocked to hear how cruel she was to herself when she thought she was on her own. From a young age, we learn how to be a good friend to others. In kindergarten or nursery school, we’re taught how to share, cooperate and play. Any child who calls other kids dumb, losers or ‘fart face’ is swiftly scolded or given a time out. All in all, we grow up learning to follow the golden rule: ‘Treat others how you want to be treated.’ Yet many of us receive no guidance on how to be a friend to ourselves. In fact, we might even get counterproductive messaging about what it means to treat ourselves with kindness. We might come to believe that being kind towards ourselves is self-indulgent, lazy or weak. As a clinical psychologist in training, I’ve discovered such self-beratement is commonplace. For example, people often judge their bodies, work performance or parenting abilities by standards to which they’d never hold others. Many people call themselves names they’d never dare utter to friends or family members, or even to people they dislike. It’s little surprise that the psychological concept of ‘self-compassion’ is cloaked in controversy. At its core, self-compassion involves treating yourself with the same kindness and consideration with which you’d treat a loved one. Just as compassion begins by recognising another’s pain, self-compassion begins by recognising when you, yourself, are suffering. A self-compassionate response, according to a leading self-compassion researcher, Kirstin Neff at the University of Texas, entails three critical ingredients:self-kindness: offering yourself warmth and understanding rather than self-judgment;common humanity: remembering that all human beings make mistakes and experience pain, rather than feeling isolated in your suffering; andmindfulness: observing your thoughts and emotions in a balanced way, without becoming consumed by them.I’ve found that the idea of self-compassion elicits reactions ranging from an enthusiastic ‘Sign me up!’ to suspicion or even fear. Upon the mere mention of self-compassion, a host of thoughts can bubble up: ‘Self-compassion is just not for me.’ ‘Aren’t people too soft on themselves these days?’ ‘I need self-criticism to motivate me to achieve my goals.’ Or, ‘If I’m self-compassionate, won’t I just sit on the couch and eat Ben and Jerry’s all day?’ Without the heavy baggage of self-criticism and shame, it’s easier for self-compassionate people to grow, improve and move forward These beliefs have consequences, including affecting how people respond to life’s challenges. For instance, in one study, my colleagues Patricia Chen, Jamil Zaki and I looked into the coping strategies used by people who were disappointed and upset following the outcome of the 2016 US presidential election. Those who viewed self-compassion positively were more likely than others to draw upon self-compassion in a beneficial way to help them get through the difficult times – for instance, they reported using more active strategies to manage their emotions, such as seeking support from others, and relied less upon unhelpful strategies, such as distraction or self-blame. This not only helped them feel better, it worked better too – our participants who practised more self-compassion reported having more intentions to improve themselves and the situation, such as by committing to become more politically active. Our work echoes what research finds time and time again – self-compassion is a healthy response to suffering. It is critical not only to our wellbeing but also helps us rise to challenges. For example, other researchers have found that self-compassion helps people take personal responsibility for transgressions and persist following obstacles, such as a disappointing test grade. Contrary to assumptions that self-compassion is selfish, self-compassion even helps us to be kinder towards others. All of this might sound counterintuitive: how can something as unassuming as self-compassion help us become better, more resilient versions of ourselves? An interesting thing happens when we’re self-compassionate – it becomes safe for us to admit our missteps to ourselves. Think about it this way: would you rather share an embarrassing mistake with someone with a track record of responding kindly – or with someone who might fly off the handle with harsh criticism? In this way, when mistakes or perceived failures arise, self-compassionate people are able to recognise them for what they are: normal human happenings. Then, without the heavy baggage of self-criticism and shame, it’s easier for self-compassionate people to grow, improve and move forward bravely. In her popular TEDx talk from 2013, Neff offered a helpful analogy for understanding why self-compassion works so well. Imagine that a child returned home from school upset, having received a failing grade in mathematics. A parent could respond with harsh criticism, expressing disappointment, anger or even shame. They could yell and question the intellect of the child. For a short while, the child might study harder. But over time, the child could become depressed and quit mathematics altogether, as the consequences of failing again are too high. Alternatively, a parent could respond to the child with compassion, recognising and validating the child’s feelings of disappointment (eg, ‘I can tell how upset you are. That sounds really tough’), reminding them that everyone struggles occasionally, and helping them maintain a balanced perspective (eg, ‘There are still more quizzes ahead of you. Let’s figure out together how we can help you feel prepared and ready for the next one’). Notice that the compassionate response didn’t involve turning a blind eye to the test grade. Nor did it entail stroking the child’s ego. Instead, it involved creating a safe and nurturing environment, where mistakes are OK for the child to confront. Put another way, your words of self-talk are the fuel: you can choose to fill your tank with either criticism or compassion. Both will get you moving, but the self-compassionate variety lasts longer and causes less harm to the engine in the end. When you’re kind towards yourself, you’ll find it easier to confront life’s many challenges, whether that be studying after receiving a failing test grade, apologising to someone after losing your cool, or returning to the gym even when you feel weak. Importantly, self-compassion enables us to confront these hurdles head-on, without becoming consumed by feelings of inadequacy. Self-compassion is not an elusive trait that only some people can possess. There are concrete ways for us all to cultivate compassion, both for others and for ourselves. Researchers have created programmes (eg, the Mindful Self-Compassion programme), workbooks and resources to help people build greater self-compassion. We can train our self-compassion muscle in various ways, for example, through writing exercises (eg, writing a letter to oneself from the perspective of an unconditionally caring friend), imagery or meditations. These exercises train us to respond to our own pain or perceived inadequacies just like we’d respond to those of a friend – with encouragement and caring. Self-compassion is about relating to yourself in a more constructive, nurturing way. It’s not about feeling good all the time And yet, if you’re like most people, getting to the self-compassion gym is the hardest part. If you have doubts that the gym will bring any benefits, you’re unlikely to visit! Encouragingly, in our work, we found that just changing participants’ beliefs about the usefulness of self-compassion helped them cope better with challenges. When we told people that the research shows that self-compassion actually improves motivation rather than harming it, they were subsequently more likely to practise self-compassion during difficulties. This, in turn, helped them to cope better and seek self-improvement. Our work thus underscores the importance of taking the time to understand and gently correct your assumptions about self-compassion. Doing so could help you respond more effectively to the inevitable bumps in the road ahead. First, notice what beliefs you have about self-compassion. Ask yourself: what have other people told you, either through words or actions, about self-compassion? Did parental figures in your life practise compassion? If so, did they include themselves within their sphere of compassion? What do you believe would happen if you were self-compassionate? What do you think would happen if you let go of harsh self-criticism? Next, notice how you talk to yourself. If you’re like most people, your mind is filled with a steady stream of chatter and yet, just like when you mindlessly consume popcorn during a movie without noticing its texture or flavour, you might not pause to reflect on your self-talk. Does it tend to be negative? Do you hold yourself to impossible standards? You’ll be spending the rest of your life with this voice, so take the time to truly get to know it and consider making conscious adjustments if necessary. Finally, check your assumptions about self-compassion. Remember that, time and time again, researchers find that self-compassion not only helps us feel better but has positive practical consequences too. Self-compassion is a powerful motivational tool that can help you persist, even in the face of challenges. At first, self-compassion might feel foreign, scary or difficult. Be patient with yourself. Remember that self-compassion is about relating to yourself in a more constructive and nurturing way, and that it might take time to develop. It’s not about feeling good all the time. I’ve seen how, just like beginning a new physical workout regimen, the journey to relate to yourself with compassion can be difficult, even painful at the start. For many people, self-compassion is a radically different approach than they’re used to – it means having compassion for yourself that’s unconditional, regardless of your circumstances or achievements. This stands at odds with a culture that often rewards us for accomplishments, capital and accolades. Where the ego whispers a siren’s call (achieve more, do better, and you will be worthy), self-compassion is the reliable friend that we all deserve (I believe in you, I’m here for you, no matter what). Thinking back to my high-school classmate berating herself in front of the bathroom mirror, I wish she could have known that she didn’t have to be her own bully. If she’d believed in the power of self-compassion, I might have instead overheard a self-compassionate pep talk: ‘Receiving that bad test score really hurt, but it doesn’t say anything bad about me as a person. I know that other people in class are struggling, too, and that I’m not alone in this. I’ll ask for help on how to study more effectively, and get the support I need and deserve.’ In my clinical work and research, I’ve seen that self-compassion is a resiliency supercharger. If my classmate could have befriended herself, I bet she’d have found school would improve, and her life down the road would have grown much richer. Know that this applies to you, too. While the journey towards cultivating greater self-compassion might seem daunting, it’s worthwhile. With you by your own side, you will be unstoppable.</description>
      <pubDate>25 Mar 21 21:29 EDT</pubDate>
      <guid>https://psyche.co/ideas/self-compassion-is-not-self-indulgence-heres-how-to-try-it</guid>
    </item>
    <item>
      <title></title>
      <link>https://dorinlazar.ro/2021-02-programming-is-hard/</link>
      <description>&lt;a href=&#34;https://dorinlazar.ro/2021-02-programming-is-hard/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Most beginners in programming eventually end up with the same ingratiating message: „Programming is easy, everyone can do it”, with some threatening message that people doing the gatekeeping should stop doing that. I’m here to tell you that that is not true. Programming is hard, programming is not for everyone, and for the time being everyone might be able to do it, but most definitely most should not. Like most of the empty, shallow, positive messages coming with an automatic defense against refute, in this case talking about this gatekeeping thing that simply doesn’t happen. Well, until now. I’m the gatekeeper now.Programming is accessibleWhen silly people say that programming is easy, everyone can do it, it’s really actually about how accessible the programming field is. If you have the basic utility of access to Internet resolved, it’s quite easy to access resources, to learn things. No other science is as accessible as computer science, and most of its proceeds happen out in the open - computer science thrives on the internet, and although there are corners that do actual gatekeeping, most of content is freely accessible and thrives on this accessibility. You can download compilers, editors, IDEs, even, you can access documentation, discuss with people about issues, and so on. It’s a big community, welcoming and open in ways that no other science ever was in the history of mankind.And indeed, you don’t need to know a lot, there are things that can be done in the literal first 30 minutes of your contact with the programming field. Programming is accessible in ways that no other science is.But just because one can access something easily that doesn’t make it easy. Just because I can mix three things in an omelette that doesn’t make me a top chef. I can cook a few things but I never said that cooking is easy - cooking is hard work, patience to details, and requires a lot of knowledge and experience.Programming is a peculiar combination of creativity and hard science; and nobody ever said that creativity is „easy”. Some things come easy to some people, some things don’t.Some programming things are easyNow, there’s no doubt about it: there are some things that are easy. There are things that you can do and at the end of a day, you might have a skeleton of a blog application, for example. One can make (with guidance) a professional looking web-page in the first hours of learning HTML. It’s easy to ask something and find a solution on StackOverflow, one can easily copy and paste a solution into their webpages.There is a ton of tutorials for beginners - even for complete beginners, some of them created by beginners. Programming is nothing if not a welcoming place for beginners. Beginners walk a very fertile ground.What is easy, however, is different from person to person. Things that I find incredibly difficult, others find incredibly easy. Things that I find easy, others are overwhelmed by. There’s no consensus of what is easy to whom, but there are, for everyone, easy things, depending on the learning path they followed. However.Most programming things are hardHere’s the truth: if one starts doing anything more complex than a simple web page, or a simple demo program, things become difficult, and the more pieces are put together, the more complex and overwhelming the task becomes. And that’s what every „programming is easy” bullshitter doesn’t say. Solving programming puzzles are hard especially when they’re not meant to be puzzles.In fact, if treated properly, most programming things are hard, even things that might seem simple. That is because you have complex pieces that you have to put together and to make them work. And the hardest part is when one has to write the complex pieces from scratch. Things only seem easy because you have people with 5, 10, 20 years of experience doing things that are easy to them because they did them many times before, because they made all the possible mistakes or thought about them and made sure they don’t fall in those traps.But there is no programmer that hasn’t spent a full day on a bug that was fixed by something silly, like adding a ; character, or changing a &lt; to a &lt;=. Nobody tells you, as a beginner, about those times when the world is unfair, and you hit a library bug, or a limiting hardware bug. That being said…Imposture and the positivity racketHow many times have you heard about the imposter syndrome? If you’re hearing about the „programming is easy” speech, you probably heard about it, a lot of times from the very same people. The imposter syndrome is, theoretically, when one feels not competent enough to do the job they are being paid for, and their success is undeserved; but you hear about it when people actually hit those seemingly unproductive bottlenecks, when they have to fix or polish their own work, or they cannot find a fast answer on Stack Overflow. Mostly, I hear about imposters in the programming field when the balloon pops, when they can no longer keep up with the untennable standards they created for themselves, when they cannot rush at the speed they are used to. (I have my own beef with the imposter syndrome, but there’s another time for that)The positivity, the „it’s easy” racket doesn’t tell you a thing about the moments when things are incredibly hard, when diligent work and steady effort is required. When there are no answers that others can give you, and it’s all between you and other people’s code, or, worse, your code. It’s those moments where you have to rethink entire architectures because some things just will not hold the test of water.I said that the programming field is a fertile ground for beginners, and it is. But what’s fertile for grain is fertile for weeds too, even moreso. And we need to talk about these people, taking advantage of the fertile ground. And where there’s plenty of beginners, there’s plenty of people taking advantage of them.There are many ways this happens; some of them are not even aware that they do it, they are just instinctive hustler that oversell their own skills. Usually you see them: two years of experience in software development, writing books and giving advice, sometimes at a hefty price. You see them at conferences, or with articles promoted, or with other types of media, sometimes playing the diversity card, at other times the beginner card, pushing their way in and taking advantage of credulous mass of beginners.And one doesn’t need to be a complete liar to present a deceitful image of the field. All you need to do is play with people’s sentiments. For example, you tell them something wrong that people want to hear. You have to find the keywords that will flatter the most your audience - „programming” is one of them because programming is quite a fuzzy thing, but the label of „programmer” is one quite sought for, just like the label „senior” is sought for by people who want to be seniors in their early 20s. So you need to find the label that flatters the audience, and apply it in an unmistakeable way to your audience. And that’s how you end up with „programming is easy” (it’s not), or „HTML is a programming language” (it’s not), or „programming is all about mastering Google/StackOverflow searching”.To emphasize your point, you then expose the enemy. „Don’t let anybody tell you otherwise” is a simple, yet effective way of enlisting the audience in a you-against-the-world mindset. Gatekeepers is also a good word to use, as the discourse around privilege and people keeping things away from the audience is catchy. They are keeping you down, they are injust, they are lying to you about difficulty, they are making you do the hard, boring work. This is a technique that applies to more than just programming, it’s a simple technique for manipulating mobs, as long as there are enemies there’s an easy thing to rally the audience against them. But regardless of this technique, the main point is to be vague in the content of your communication, so that it’s complicated to refute that communication.Let’s look at my examples. „Programming is easy” took me so far 1385 words, and it’s not done yet. „HTML is a programming language” is somewhat more complicated, because of the shifting meaning of „programming”. The pure technical truth about HTML is that it’s not programming in the real sense, but it’s a markup presentation language. It’s an incomplete description of how a webpage looks like and what it contains. HTML is a necessary tool in any web-related project, but HTML is not used in isolation, and it’s not where you do the programming. But if someone tells you that HTML is a programming language, their defense is usually about you gatekeeping the meaning of „programming”, but they are rarely interested in diving in the real meaning of their own words. As I said, „programming” is a label, and like any glory-labels, this one makes people, especially the incompetent, act very defensively (see my own defense here).The illusion of „I can do what you do”Now, the trickier example is the „programming is all about mastering Google/SO searching”. Because that is true and wrong at the same time. Mastering searching the web is a very important skill of a modern developer. I come from a world where this was not the case, where Google did not exist, and, instead, one had to look through documentation and read sometimes hundreds of pages to get an answer on how something should behave. If the documentation existed; if not, there was the only solution of trying, failing, and trying again. Now, however, there is no try, in a very Yoda-esque change of circumstances. There are full toolkits, libraries, or programming languages (and even commercial offerings of that) that rely strictly on you searching the answer for their problem. The documentation is not meant for users to read, it’s for search engines to index and for people to use a search engine on that. But most of the current day tools rely heavily on the answers site Stack Overflow.In fact, the public’s reliance on Stack Overflow is probably the scariest thing that happened to the programming community in the past 10 years. Stack Overflow is a huge crutch that stops you from walking on your own, because it’s too darn easy to look for the answer there. And when people stop thinking for themselves, they end up writing things that don’t make sense.But mastering Google/SO searching is a must. I, for one, find myself often searching for very basic things, like how to write a for loop in &lt;insert language here&gt;. But that’s not because I don’t know how to write a for loop, but because I’m switching between many languages in a very short timeframe, and I’m looking for the best option to iterate through a collection (not always a for loop, mind you), or the proper syntax (which each language changes only slightly, making things confusing). So while a beginner might say: sure, not even an experienced developer can remember all these things, the reasons why I look for the same things that a beginner looks for are quite different.If, as a beginner, you look at what experienced developers do, it looks easy. It looks like anyone can do what they do. It looks unimpressive. There is a myth of the super-programmers that do things in a different way. Hollywood paints them as hyper-fast typists breaking the speed of light, because the only way to easily portray how good a character is at something is to make them look faster than any human could possibly be (as an aside, Hollywood would portray the best person to count ten seconds as the person who finishes in five). The point is that it looks easy, but it isn’t. Because what is missing on the beginner’s side is the experience, the perspective over things, the focus on the real important things. For a beginner, the syntax of a loop is the main obstacle in achieving the ability to count to ten, for an experienced developer the syntax is the thing that slows them from applying an operation to an collection of filtered data.„Programming is easy” holds people downThe reason this posts exist is the following tweet (and this article written in Romanian, but I’ll focus on the English source):I understand the intent of this post and know it’s not malicious, however, learning to code is difficult and this “anyone can code” // “coding isn’t hard” dialogue made me feel dumb as hell when I was new. https://t.co/l0ajMn4SG8— Ashley Willis (@ashleymcnamara) February 15, 2021„Anyone can code” // „coding isn’t hard” dialogue made me feel dumb as hell when I was new.People feel dumb when they have to trace back on their own creation and fix it, improve it, make it work against real-world scenarios that sometimes you didn’t envision when first writing your code. Positivity doesn’t help you when you actually need help. The only thing it can do is offer you courage, but what do you do with unsubstantiated courage? How fast will the positivity and unsubstantiated claims will succumb to the grim realities of the programming job.The original poster, that this tweet was a response to, said something on the line of „people think I’m smart because I’m a programmer” and „it’s about googling and fixing our own bugs”. But programming is really not about that, and this exaggerated, limited view tells me that her experience in software development is quite reduced. The scary part? she is building a site called „thecodinginterview.com”, making her a top source for advice for beginners. And don’t get me started on how immoral this „coding interview” racket is.Positivity is infallible because positivity was never put to test; and it’s completely absent when shit really hits the fan. The mistakes are all yours, because „anyone can code” and „coding isn’t hard”. When you have to face your mistakes, you’re alone, no positivity bullshit can fix that.„Programming is hard” doesn’t have to be scarySome people insist that lying to beginners about how hard programming really is would scare them. I’m not sure when the fact that something is hard stopped people really interested in pursuing something. After all, most of our popular culture is about some heroes doing the hard thing. I feel that this is part of the same culture that coddles the minds of people making anything that is not immediate not worth the effort. But when I started programming nobody told me that programming is easy. Instead, my expectation was that it’s hard.I know that there is an obsession with overachieving, with mastering programming, with being a senior at 22. I refrained from pursuing such fool’s errands. Instead, I knew that programming was hard, and achieving what I want might be impossible. In a sense, it is; I learned programming so I can write my own games and I haven’t done that more than a quarter of century later. But that gave me clarity about the scope of my undertaking.What I’m saying should not discourage beginners. It should just prepare them for what’s ahead. So if you want to tell a beginner something like this: „when it comes to programming, some things are easy, and some things are hard. If you’re patient enough, in time, the hard things will be exciting, and the easy things will become harder”.But don’t tell them that programming is easy. It’s not.</description>
      <pubDate>06 Apr 21 11:23 EDT</pubDate>
      <guid>https://dorinlazar.ro/2021-02-programming-is-hard/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.nateliason.com/blog/decomplication</link>
      <description>&lt;a href=&#34;https://www.nateliason.com/blog/decomplication&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;This article is long and will make you think. I recommend saving it to pocket and reading it in a comfy chair with a cup of earl grey tea.How do you lose weight?If we were to plug that question into Google, we’d be barraged by millions of pages explaining tactics for weight loss. Foods to eat, foods to avoid, when to eat, how to eat, what speed to eat at, where to eat, who to eat with, what kind of utensils to use, what pills to take, what exercise to do, what procedures to do, what plans to follow, what coaches to hire, and that list will barely get us past the first 10 results.What if we asked: “how do you manage your finances?”Down the rabbit hole again. What to save money on, where to save money, how to coupon, what to invest in, where to live, how to live, where to work, what kind of car to get, how to do your groceries, what countries to live in, what to spend money on, what credit cards to get, what banks to join, and again we’re likely not past the first page.The amount of information available for both of these problems would seem to indicate that they’re complicated. There are tons of variables you need to manipulate in order to get it right, and if you don’t understand each variable, then you won’t hit your goal. You won’t lose weight, you won’t save money, you won’t be productive, you won’t start a business, and you won’t solve any other popular problem either. These problems are complex and you need a monumental amount of information to get them right.Bullshit.The core solutions to many problems, maybe most problems, are extremely simple. In one paragraph each, you can explain how to lose weight, how to gain muscle, how to save money, how to be productive, how to sleep better, how to grow a website, and just about any other popular problem. The finishing touches near perfection aren’t so simple, but the effective amount for the vast majority of our purposes? Certainly.But, we don’t want to hear this. Through a combination of psychological biases, willpower depletion, and effective marketing, we’ve begun to believe that the simple things are difficult and complex and that we need swaths of information and expertise to solve them.We’ve created and been sucked into a world of artificial complexity – one where topics are made more complex than they need to be in order to appeal to our biases and frustrations, and to help companies make more money.But here’s the good news.Once you recognize this world of artificial complexity, you can turn any problem back into its simple solution through decomplication. Weight loss, strength gain, productivity, skill enhancement, sleep, they’re all incredibly simple once you decomplicate them.We’ll get to how to do that soon, but first, why does artificial complexity exist in the first place?Why and How We Create Artificial ComplexityArtificial complexity occurs when a commonly encountered problem has a simple solution, but that solution is made more complex to appeal to the solver’s lack of willpower, past failures, or to benefit the interests of a third party (usually a company selling something).Let’s take sleep as an example.Getting a good night’s sleep is important. If you don’t do it regularly, you’re going to die much sooner than you need to, which will make me and, presumably, many of your friends sad.But how do you get a good night’s sleep?The market for sleep aids is on track to reach $76.7 billion dollars by the end of the decade, from special mattresses and pillows to tech, supplements, sound machines, and anything else that can help you get that full night’s rest.At first blush, this isn’t that surprising. Almost everyone uses some sort of sleep aid, whether that’s a sleep tracking app, white noise machine, memory foam pillow, or eye mask. But when you dig in, do you really need, in a biological sense, a fancy iPhone sleep tracker, Valerian Root supplement, “delta wave inducing” music, and cup of chamomile tea to sleep well?No, of course not. All you need is 8 hours with minimal stimulation or interruption (i.e. no light, sounds, movements, discomforts, etc.).So why is there such a big industry around sleep?As humans, we’re not good at making tradeoffs. We’re tired, but we also want to sleep less than 8 hours. We want to be thin, but we also want to eat Oreos. We want to save money, but we also want to go out drinking.The solution to not being tired is extremely simple: sleep 8 hours with minimal stimulation and don’t take too many stimulants. But we don’t want that to be the answer. We want to throw back our venti lattes and watch late night TV in bed and live an 18-20 hour day, so we look for magical sleep aids, stimulants, and other silver bullets to compress our comatose period from 8 hours to 6 or 4.This is where complexity starts to seep in. Since we don’t want to get the full 8 hours, don’t want to give up our coffee, and don’t want to get the screens out of the bedroom, we look for more complex answers. Our desire to have our cake and eat it too makes us look for methods to get both at once, adding significant complexity to what is, at root, a simple biological process.And lucky for us, there is an emerging $76.7 billion dollar industry willing to help us in our search for complex silver bullets.This cycle plays out again and again in almost every area of our modern lives. We become frustrated by what should be simple problems, we look for more complex solutions to address our frustrations, and then we buy things that promise to make that complex problem easy again.This cycle can be broken, but to do it, we need to better understand the three forces that create artificial complexity.Three Forces that Create Artificial ComplexityArtificial complexity follows a predictable cycle.Problems start by having a simple solution that’s easy to execute, or a simple solution that’s hard to execute.Our frustration with following through on the simple solution causes us to challenge or ditch it, leading us to imagine more complex solutions.We buy things that promise to address the complexity and make it easy again.Before moving on to each force, though, I need to make an important terminological distinction to avoid any confusion.Simple here means straightforward and containing few steps or moving pieces. Running a marathon is simple because you just run, but it is not easy.Easy here means requiring little effort or willpower to follow through on. Taking Hydroxycut is easy, but Hydroxycut is not a simple solution to weight loss. If you don’t believe me, look at its ingredients.From those definitions, their opposites, “complex” and “hard” should be obvious. Your job as you continue reading is to not assume ease when I say something is simple, and to not assume simplicity when I say something is easy.On to the three forces.Force 1: Failure and ChallengeNothing becomes artificially complex without first becoming a problem. If I dumped you in a wilderness lodge with all of your necessities provided and with no technology, work, or meetings, you would sleep perfectly well. With time, you would also end up in good shape, not be stressed, and never even begin to have many of the “problems” that plague modern humans.But if I pull you out of that wilderness utopia and return you to the modern world, the easy things aren’t so easy any more. You’ll struggle to sleep enough, struggle to eat well, struggle to exercise.You know that you need to sleep 8 hours, but it’s getting harder and harder to fit it into your schedule. You’ll start sleeping poorly, a laughable problem to anyone outside of modernity, but a problem nonetheless. With this challenge, you’ve started down the path of artificial complexity by going from simple and easy, to simple and hard.This isn’t to say that sleeping in modern society is secretly easy to do. It isn’t, if you don’t completely control your environment, but we have to recognize that at root sleeping is a very simple and easy thing. It’s our environments that make it difficult, not some aspect of sleep.It’s worth noting, too, that not everything starts out as easy to do. Growing a popular website is simple, but hard to do from the get go. It’s never easy to do. It still goes through the “failure and challenge” process, but that process moves it deeper into the “hard to do” box instead of from easy to hard.If a problem is physiological, philosophical, psychological, otherwise human or “old,” though, it’s likely that the solution can be distilled to being simple, and even easy if you were removed from modern society.Force 2: Cognitive DissonanceOnce you’ve gone from “this is easy” to “this is hard,” or “this is hard” to “this is impossible,” artificial complexity begins to kick in.The process for losing weight is simple, but doing it is difficult when faced with all of the junk food we have available to us. When that difficulty leads to failure, cognitive dissonance kicks in, and our desire to not feel responsible for that failure causes us to seek out alternative explanations.We first create artificial complexity in our minds to explain our failures or shortcomings. It’s not that your diet sucks, you don’t exercise, you open yourself up to distractions, or that you’re not setting aside 8 hours a night, it’s that you haven’t found the right trick yet.If you accept that the solution is simple (which it truly is), then you have no one to blame for failure but yourself. But, if you can convince yourself that the solution is complex, well, then you just haven’t found the right trick yet. Fuck eating healthy, let’s all do coolsculpting and take Hydroxycut!With sleep, this second force is our desire to not do the hard work of cutting our day down to 16 hours. We know that we’re tired when we only sleep 6 hours, so how do we solve it? We could adjust our schedules so that we get a full 8 hours (simple), but that’s hard and we don’t want to do it. Instead, we look for supplements, tools, tactics, tricks, and whatever else we can find to make up for those 2 hours (complex).For now, we’ve only made the problem worse. It’s still hard, because we don’t know what tricks and tactics to use. Our cognitive dissonance has taken the simple but hard to do solution, and turned it into a complex solution that’s equally hard to do.Luckily, we aren’t stuck in this box for long.Force 3: Money and MarketingBusinesses thrive on artificial complexity. Recognizing the human desire for a complex solution to a simple problem is a fantastic way to make money, and has driven the absurd wealth in industries like fitness, productivity, and entrepreneurship (that is, telling people how to be entrepreneurs).In many cases, if someone is selling something (product, training, course, etc), it benefits them to make the problem their product solves seem more complex than it is, while also making their solution easy, so that you feel like you need to buy what they’re selling. Your mind is stuck in the box of “this is hard and extremely complex,” and they move you to “this is complex, but thankfully I can pay someone to make it easy.”Now, this is not necessarily a bad thing. A good personal trainer can take the complex world of fitness, explain its simplicity, and then teach you how to keep working out on your own. But there are 10 times as many bad trainers who will inject artificial complexity into fitness to keep you paying them and buying their products. It’s the latter you need to watch out for.This deception is especially common when a company has a mediocre product. If the product can’t sell itself, the job of marketers, salespeople, or product designers is to create artificial complexity and aggressively push the consumer into making a purchase.Our cognitive dissonances takes problems that are simple but hard and makes them complex and hard. Marketing and products take our belief that something is hard and complex, and convinces us that, with their solution, it can be easy and complex.For sleep, our frustrations move us from believing it’s simple and hard to complex and hard. Once we’re there, all companies and entrepreneurs need to do is provide an easy solution to what appears to be a complex problem. And, voilà, $76.7 billion.No one makes money by saying “hey just sleep 8 hours with minimal stimulation,” even though that’s the best solution. They make money by convincing you that you don’t need just any sleep mask, you need their sleep mask, because you have no idea how complicated and nuanced the world of sleep mask purchasing truly is (but don’t worry, here’s a list of “10 things you didn’t know about choosing a sleep mask”).By recognizing these three forces, we can see how simple problems quickly get turned hard, then complex, then made easy again, so long as you buy into someone’s complex solution.Most people end the cycle here. They hit a problem, make it complex in their heads, then buy into someone’s “easy” solution to the artificial complexity.But we don’t have to. Instead, we can decomplicate the problem, and return it to a simple solution.Decomplication: How to Undo Artificial ComplexityArtificial complexity is bad. It’ll make you spend money and time on solutions you don’t need, cause you to waste hours reading and hunting for the “perfect answer,” and leave you strung out and depressed not getting the results you want from solutions you shouldn’t have been trying in the first place.Not all problems can be reduced to being simple and easy, but anything that people commonly encounter can at least be reduced to being simple and hard. Losing weight is simple and hard, sleeping well is simple and maybe hard depending on your situation, being productive is simple with variable difficulty.But the only way we can bring these challenges back to the simple side of the graph is through decomplication. Not simplification, which involves taking something truly complex and conveying a simpler version of it, but decomplication, undoing the complication that’s unnecessarily been added to it.Unfortunately, this is not easy to do. It requires two steps:Realistically assessing the complexity of the problem, by exposing how it may have been made artificially complex.Realistically assessing the difficulty of the problem, by figuring out how you may have convinced yourself of artificial difficulty.It’s only once you’ve gone through these two steps and decomplicated the problem that you can easily see the simple solution.Realistically Assessing ComplexityThe first step in putting any problem back in its correct box is to assess the true complexity of it. Some problems, like rocket science, are truly complex. You’ll never get them into the simple category.But for most of the problems we encounter in daily life, we can decomplicate them by exposing how they were turned artificially complex. The best way to do that is to ask good questions about the problem, in order to assess if there may be factors that are making it look more complex than it needs to be.Question 1: “Does anyone profit from this being complex?”The first place to look when assessing complexity is if anyone makes money or has built their business on making something seem complex. And more importantly, are you listening to this person on how complex it is?For example, health and fitness magazines make money by making health and fitness seem complex. Losing weight is simple, but they make a lot more money by making you think you need to follow their diet, supplement regimen, exercise routine, latest list of 10 superfoods, or whatever else they’re selling.Conversely, no one makes money from quantum physics being complex. You don’t see people selling 10 week quantum physics bootcamps (yours at 50% off for a limited time). That’s a legitimately complex field.If the problem you’re trying to solve has been monetized through complexity, then odds are that there’s a simple solution hidden deep down.Question 2: “Do I secretly know the simple solution?”In many cases of artificial complexity, we secretly know what the simple solution is, but we desperately want there to be some other, easier, more complex answer.Most smokers know the solution to their smoking habit is to stop smoking. Fat people know the solution is to eat better and less. Weak people know the solution is to exercise. Unproductive people know the solution is to get to work.But these simple solutions are hard, and we don’t want hard. We want easy. So we claw at the easy, complex, and frequently expensive solutions, hoping desperately that one of them will save us from having to do the hard work of quitting smoking, eating well, working out, or focusing.It’s difficult, but we need to train our ability to honestly assess whether we’re creating complexity despite knowing the answer is simple.Question 3: “Am I assigning value to complexity?”Part of why artificial complexity thrives is that we treat complex things as more valuable. We want a crazy complicated workout routine because we believe it must be more complicated to work. If a trainer told you to go to the gym just once a week, do five sets of five deadlifts, and then leave, you’d probably (wrongly) fire them.We’ve been sold complexity our entire lives, and that’s made us undervalue the simple. As a result of the “monetization through complexity” problem, we no longer trust that simple solutions could be valid.To get to the root of a problem’s solution, you need to honestly ask yourself if you’re seeking out complexity simply because you trust complexity more than you trust simplicity.Question 4: “Is this something I’ve failed at?”Failing can be the first step towards artificial complexity. When something doesn’t go our way, or when we put in effort and don’t get the results we want, we tell ourselves a story of complexity to explain the shortcoming, even though the failure more likely came from randomness, lying to ourselves, or not trying hard enough.If you failed at losing weight, getting strong, being productive, sleeping better, managing your money, or any other artificial-complexity-prone area in the past, and now think it’s complex, now you know why you believe that.By running through these four questions, you’ll get an idea of how artificial complexity may have seeped into the problem you’re trying to solve. If you said “yes” to any of them, there’s a good chance that the problem you’re contemplating is simpler than you think it is.Realistically Assessing DifficultyDifficulty is significantly less objective than complexity. Eating well is simple for everyone, but it’s not necessarily easy for everyone.If you earn a good amount of money and live in a major city with access to Instacart, a nutritionist, and a chef, it’s very easy to eat well. Living in the middle of nowhere Arkansas with a minimum wage income, not so easy.That said, you can still ask a few good guiding questions to see if something is truly difficult, or artificially difficult.Question 1: “Do I control the variables that make this seem difficult?”If you think sleeping well is difficult, but you go out drinking every night until 2am, then it’s not actually difficult. You’re making other choices that cause it to be difficult, but you want to have your cake and eat it too.Or, maybe you think that it’s difficult to not snack on things that are bad for you, but you keep buying snacks when you go to the grocery store. Not snacking is easy if you don’t have the option, but you’ve made it difficult by putting the option in front of you.Question 2: “Am I treating this as difficult as an excuse for inaction, or to prevent cognitive dissonance?”Even if you acknowledge that, say, eating well is not complex, you might tell yourself “yeah but it’s harrrrrd” as you bite into your fifth OREO.In this situation you don’t want to admit to yourself that it’s easy, since that would mean there is little excuse for eating the OREO. The cognitive dissonance from admitting that you might be neglecting something good for you is painful, and it’s easier to imagine it being difficult.Question 3: “Have I failed at this before?”Past failures can create artificial difficulty just as they can create artificial complexity. If you failed at something in the past, you might have written it off as “too hard” and kept that mentality towards it ever since.Worse, you might have developed the belief that it isn’t possible at all, telling yourself the story that you’re “not someone who sleeps well,” or that you’re “meant to be fat.”If you can start bringing these questions into your life when you run into problems that you think are hard or complex, odds are, you’ll start to discover much simpler solutions.Finding the Simple Answers: A Priori ReasoningAfter you’ve gone through this questioning process, you could be left wondering “what is the simple solution, though?”While it’s easy to recognize that most health information online is purely artificial complexity, some of it must be relevant, right?Unfortunately, the more artificially complex a field has become, the harder it is to find the simple answers. Worse, you can end up thinking you’ve found the simple answer, be completely wrong, and have to go through the cycle again later.The solution is a priori reasoning, or as it’s commonly referred to, “reasoning from first principles.”  A priori reasoning is when you take premises, rules, axioms, fundamental truths, mental models, and other principles that are inarguable, or very certain truths, and reason out a solution based on logical deduction. It requires building conclusions off of what you know to be true, instead of relying on opinions or assumptions.Here are some examples of how we might use a priori reasoning to find simple solutions to common problems, particularly ones discussed in this article.Losing weightPeople without access to food get very skinny. If I eat less, I will lose weight.Building muscleThe body responds to stress by making itself stronger for the next time that stressor appears, which is why vaccines work. If I lift weights close to my point of failure, I’ll get stronger.NetworkingFamous people tend to hang out with other famous people, or people as accomplished in tangential fields. If I want to get to know someone I respect, then I should do something that puts me on a level where I could be friends with them.ProductivityThe goal of productivity is to get more done, and the biggest reason you don’t get things done is that you’re doing other things. If I remove the ability to do other things, I’ll do the thing I’m trying to be productive on.Search Engine OptimizationGoogle has an amazing team of data scientists working on its search engine, and the goal of the search engine is to return the best answers possible to questions. If I want to rank on Google, my primary goal should be to answer questions really, really well.SleepRemoved from modern society, sleep is not a problem. If I can create a sleep environment as if I wasn’t in modernity, I should sleep fine.Personal FinanceDebt and money problems happen when you spend more than you make. If I create systems to spend less than I make, I’ll be fine.Now, maybe you read these solutions and went “well yeah, duh,” and that’s the point. Cognitively, we know these problems have easy solutions, but we look for harder ones in reaction to forces 1 and 2.You also could have looked at them and said “okay, I buy that, but I need information on how to do the next step.” Not necessarily. We can use the same type of reasoning to figure most of the pragmatic next steps out, too.SleepIf I create a sleep environment as if I wasn’t in modernity, I’ll sleep fine.Therefore, I should sleep somewhere quiet, dark, undisturbed, and until I wake up naturally.Therefore, I should dampen noise in my room, get a white noise machine, blackout my shades, turn off phone notifications, cover any lights, and have a comfy bed.Did you need a book on sleep to figure that out? No, you could have figured it out a priori.Two Laws for ComplexityThere are certainly topics of knowledge that are complex: particle physics, epistemology, organic chemistry, but are there practical problems we run into daily that are truly complex?I now believe that the answer is no. I can’t find any solvable problems that could be reasonably experienced by a person in modern society that have truly complex solutions. Complexity is reserved for rocket science, not for challenges we encounter on a day to day basis.Despite that underlying simplicity, it’s the problems the greatest number of people experience that tend to have the most artificial complexity. Everyone has some trouble with sleep, weight management, feeling fulfilled, staying productive, and you’ll notice that those kinds of topics have the most artificial complexity.Which brings us to the simplicity of this concept. Two simple mental models you can put in your pocket and take with you into this world of artificial complexity.The Law of Artificial Complexity: As the number of people experiencing a problem increases, so will the artificial complexity of the solution.The law of artificial complexity tells us that as more people experience a problem, more artificial complexity will be added to the solution. The most common problems have the most artificial complexity added to them, since these common problems provide the most business opportunities, and have the most people struggling with them.Some of these problems are ones that you may have never even thought you had, but then started to believe you had simply because you stumbled across artificial complexity based marketing.And once we recognize the law of artificial complexity, we can take its converse, and create a Law of Decomplication:The Law of Decomplication: The more people that are experiencing a problem, the simpler the solution should be.Common human problems have simple solutions. Our errors in judging complexity come when we treat daily human problems as tail end knowledge work problems, believing tweaking our diets to be as complex as building a Falcon 9 rocket.It’s on us to recognize when we’re being over-influenced by artificial complexity, to go through the decomplication process, and then to use our a priori reasoning to arrive at the better, simpler, solution.</description>
      <pubDate>06 Oct 21 09:39 EDT</pubDate>
      <guid>https://www.nateliason.com/blog/decomplication</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.raptitude.com/2013/11/the-four-horsemen-of-writers-block-and-how-to-defeat-them/</link>
      <description>&lt;a href=&#34;https://www.raptitude.com/2013/11/the-four-horsemen-of-writers-block-and-how-to-defeat-them/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This article was originally about writer’s block — a particular kind of procrastination — but as some readers have pointed out, it applies to anything you’ve been avoiding. Writing is my example here; you know better what it is you’re avoiding right now. Getting myself writing used to feel something like trying to start an old lawn mower. Occasionally I’d get it running right away, but most of the time it would take at least a few rips at the cord, and I was always aware that I might not get it to turn over at all that day. This made it feel like there were days I could write and couldn’t write, and I could never do much more than hope it was the right kind of day. Some time in the last year I lost most of my fear of writing, or at least by now I’ve experienced enough of that fear that I can see it has a rather simple and predictable structure. I’m not saying I’ve defeated it, only that I understand it enough that I can always get myself to the point where I actually write something. I still encounter creative fear every day, but it arrives in only a few predictable forms and I know what to do for each one. There are four forms, and almost every day they ride out to confront me in the same order. I call them the Four Horsemen of Writer’s Block, but they are undoubtedly the same evil forces that stifle creators of all types. Initially they come in disguise, seducing travelers away from the creative path. Often they defeat you without your even knowing it. Once you know their names and their strategies, you begin to see your encounters with them as an everyday part of your job that need cause you little trouble. But be careful. Even if you’ve defeated them a hundred times they will still be capable of tricking you — in fact, my overconfidence allowed the first one to outsmart me yesterday on the piece you’re reading. Know which you’re dealing with and what to do for each.  1. Tomorrow The first horseman is responsible for the greatest number of casualties. Usually he alone is enough to defeat a given person. His persistence is the primary reason there are people who believe they have no creative ability at all. He keeps the majority of the population from ever even beginning to do their best work. His name is Tomorrow and his first arrow is so sudden and penetrating that it can slay your creative spirit for the day before you even notice his arrival. Every subsequent day he attacks from farther away, until each assault can kill weeks, months and years of your creative life. Across his breastplate his mantra is etched: “Now is not the time.” His strategy: He wants today to look spoiled to you, so that tomorrow, next week, or next year seems like a vastly better time to get to work. When you notice it’s 11:17 and you’ve got nothing down, you begin to think that today’s energies might be better invested in laundry or errand-running. This is his deathblow, and it is so insidious it feels good. His weakness: Tomorrow needs you to regard future days as your most fertile creative periods, making today look comparatively unsuitable for working. When you recognize that it is actually impossible to do work tomorrow, then you know to stay with your work until something starts to take form. Today is the only day you can ever work, and once you see this truth, he is defeated. 2. Later The second horseman arrives in the quiet hours of the morning, when you still feel the abundance of a whole day ahead of you. He’s most effective when Tomorrow defeated you yesterday and you’re determined to work today, but still rattled. The prospect of creating is slightly scarier to you today than it was yesterday and it is this scent of blood that attracts your next enemy. Later is well-dressed and generous, and the inexperienced traveler is drawn to him. He flatters you for your commitment and industriousness, and extols the principles of emotional preparation and the rejuvenating effect of play. The moment he senses your anxiousness about getting to work, he reassures you of the abundance of time. After lunch, after dinner, after the next episode of Orange is the New Black, there is a clear stretch of time to work, and you’ll be more energized and balanced then. Sometimes he will offer you cannabis. His strategy: Later offers you gifts but they must be accepted immediately. He sells you on what appears to be perfect compromise — do whatever you like now, as long as you get to work right after after lunch, or right after dinner. It’s a nearly irresistible deal: at the time you accept the gift you believe are losing nothing, because you’ll simply do the same amount of work later in the day, and you get to enjoy a lovely treat right now. His weakness: He can defeat you only if you never learn that work only gets harder throughout the day. Accepting his morning gifts weakens you in several ways simultaneously. Firstly, you’re using the day’s freshest hours to do its least demanding activities; secondly, you’re training yourself to expect the easy and fun part of the day to come before you begin working; and thirdly, some part of you knows that you have already sold out on your high expectations for the day, and the day becomes tinged with shame. Tomorrow will be upon you in an instant. If you do begin to work later, you’ll expect less of yourself and you’ll quit early. If you follow a policy of never accepting gifts of gratification before you’ve done enough work to be proud that day, he cannot win. 3. Distraction The third horseman waits until you’re at your desk, having thwarted the first two opponents. Rather than sneak up to you, he rides in to the sound of trumpets and pyrotechnics. He wears a great blue cape with a white lower case “f” on it. On his tunic are embroidered his emblems of power: a coffeemaker, a sudoku grid, a banana nut muffin and a Reddit alien. Though Distraction has been antagonizing writers and artists for centuries, his power has grown a hundredfold in the past few decades. In fact, he is threatening to dominate an entire generation of youth, who worship him by absently fondling a black or silver rectangle they carry in their pockets. Those who write for the web are particularly vulnerable to his power, because he lurks in the very tools the writer uses. His strategy: He wants to reduce your output by diluting your writing time with social media time, second breakfasts and daydreaming, so that you start to believe you need enormous blocks of time to produce anything. When you begin to despair at your inability to get anywhere, you will stop working for the day, leaving you ripe for all four horsemen to descend upon you when they please. His weakness: Distraction works by enchantment. He doesn’t want you know you’re distracted until you’re too hooked on the distraction to quit immediately when you do realize. You have to learn what the in-the-moment sensation of becoming distracted feels like, and when you notice it, return your attention immediately to what you were doing, without “resolving” the distraction. It is easier to learn to do this in small stretches. Set a timer and declare the next thirty minutes distraction free. Snap back to the task at hand the moment you notice you’re not doing it anymore. This is a muscle you have to work. 4. Self-doubt The final enemy often waits until you’ve actually begun to get somewhere. You may even be almost done a day’s work by the time you notice his long shadow creeping across your workspace. Self-doubt stalks every creative and will appear at some point during every project. His figure is indistinguishable from Death — dressed in black with a bare skull for a face. Sometimes you just spot his silhouette on a distant hilltop, and then he will disappear, allowing you to finish. But you know you saw him and you are left unsettled about your work. Other times he may ride right up to you, unfurling a great black banner that says, “Everything you write is shit.” Unlike his predecessors, who are satisfied with merely ruining your day, his aim is to get you to stop forever. His strategy: To get you to give up on your projects out of the belief that you’re missing a crucial ingredient, typically talent or inspiration. Creatives who believe they’re missing something must either wait for it to come to them, or quit the pursuit altogether. His weakness: Primarily, he needs you to believe that bad work is avoidable and that it threatens your good work. Self-doubt has trouble gaining traction with the writer who is unfazed by producing something he knows is bad. If you embrace your shitty work as a necessary component of getting to your good work, he begins to doubt his own effectiveness. Joel Saltzman’s analogy of writing as panning for gold is helpful — the gold is only ever found amongst many times as much sand. If you see the sand as being in the way of your gold production, you stop producing. Good writing needs bad writing, and the less resistance you have to one, the more easily the other comes. Self-doubt is the most complex of the enemies to creativity, and it can come from a lot of different places. But having a name for the stifling force goes a long way towards continuing to work regardless of its presence. *** Essentially, if you know which foe is stifling you at a given moment, it’s not difficult to defeat him. Expect them to come in this order, but be aware that they never really die. When you have trouble with one of them, often the others will reappear, even if you’ve already handled them that day. I’m convinced now that these four enemies make up the entirety of everyday resistance to creative work, that they are predictable and that anyone can defeat any one of them on any given day. There’s really not much that can stop you if you decide you’ll keep working no matter who shows up. *** Photo by Hartwig HKD Need help focusing? The big productivity books are written by people who don&#39;t especially struggle with productivity. The rest of us find other ways. I shared mine in this post. </description>
      <pubDate>08 Nov 21 14:12 EST</pubDate>
      <guid>https://www.raptitude.com/2013/11/the-four-horsemen-of-writers-block-and-how-to-defeat-them/</guid>
    </item>
    <item>
      <title></title>
      <link>https://codeascraft.com/2021/06/15/improving-the-deployment-experience-of-a-ten-year-old-application/</link>
      <description>&lt;a href=&#34;https://codeascraft.com/2021/06/15/improving-the-deployment-experience-of-a-ten-year-old-application/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Improving the Deployment Experience of a Ten-Year Old Application In 2018, Etsy migrated its service infrastructure from self-managed data centers to cloud provisioning. (We blogged about it at the time.) The change opened up opportunities for improvements to technical processes across the company. For the Search team, the flexible scaling that comes with a cloud environment allowed us to completely reevaluate a somewhat cumbersome deployment process. Inspired by the existing architectural pattern of canary rollouts, we wrote a new custom tool to supplement our existing deployment infrastructure.  What we ended up with, after three months of effort, was a more scalable, more developer-friendly, and ultimately a more robust way to roll out changes to Search. The Blue and the Green Historically, we deployed our stack on two separate sets of hosts, in what’s known as a blue-green strategy. At any one time, only one set of hosts is live; the other set, or “side,” is dark. Both sides were always fully scaled and ready to serve traffic, but only the live side was accessible to the public internet.  While simple, a blue-green deployment strategy has some very useful features: We can perform major changes to our search application, which is stateful, on one side while continually serving traffic using the other side.We have somewhere to test changes manually before sending production traffic to it.We always have a previous version of Search that we can easily revert back to in an emergency.We have a built-in mechanism for testing and productionizing other breaking changes like software version upgrades.  We refer to the two sets of hosts as “flip” and “flop”, named after the circuit that is a fundamental building block of modern computers. We point our monolithic PHP web application to whichever side should be active via some lines of code in a configuration file.  A diagram of what our infrastructure previously looked like. One side (flop, in this example) was always live, and during a deployment we’d move all traffic at once to the other side (flip in this example). This blue-green deployment method was “lifted and shifted” during Etsy’s migration to the cloud three years ago. The Search team moved the search application to Google Kubernetes Engine (GKE), and flip and flop became two separate production Kubernetes namespaces.  That change aside, things worked as they always had: deploying Search instantly triggered a redirect of all traffic from one namespace—the live side—to the same services running in the other namespace. To ensure the dark side would always be ready to go, we continued maintaining 200% capacity at all times (100% in each production namespace), just as we had done when we were on-premises. This original deployment strategy was immensely useful to the team, especially for giving us a secure place to test and prepare major software updates and infrastructural changes. But it wasn’t without its painful aspects. Abruptly shifting all traffic between sides gave us no room to test changes on small amounts of production traffic before going all in. Even when things went  right, deploying was stressful. And when things went wrong, engineers had to triage to decide whether to fully revert to the former side. On top of all that, having to permanently maintain at double capacity was expensive and inefficient. Thanks to the flexibility provided by the cloud, once we were safely on GKE we had an opening to rethink our blue-green strategy and address these longstanding issues. The Canary (Lite) Our first thought was to adopt a canary deployment strategy. During a “canary rollout”, a small subset of traffic is sent to the new version of a service to determine it is “safe” before switching over all traffic to the new service.  Why the name? Coal miners used to use canaries to detect carbon monoxide at a level that could hurt a small bird, but not yet hurt a human. Software engineers have adopted a similar—albeit more humane—model to build confidence that new software is safe to serve traffic. Although Kubernetes’ architecture and flexible scaling mean canary rollouts are a very popular deployment solution, the design of Etsy’s search system meant we couldn’t use any off-the-shelf canary release solutions. We had to build something new for ourselves, a sort of Canary Lite. We had two key limitations when looking to re-architect our deployment process to incorporate a canary component.  First, we had no single load balancer or API endpoint where we could control the amount of incoming traffic going to flip versus flop. This made it impossible to do basic canary rollouts using Kubernetes labels on a single Kubernetes deployment for any search service, because Search is made of many disparate Kubernetes deployments. There was no place we could put routing logic to check the labels and route to the canary pods accordingly.  However, Etsy’s PHP web application is the search application’s only client. This is a common pattern at Etsy, and as a result, configuration load balancing is often managed directly within the web application itself. Any new deployment solution would either have to manage traffic from the web application to Search from within the web application, or implement some sort of entirely new mesh network (like Istio) to catch and direct all traffic from the web application to Search. Neither of these options were viable in the time frame allotted for this project. The second limitation was that the search application assumes any single web request will be served by the same version of all search services in the request path. As a result, deployment of any new solution would need to ensure that in-flight search requests would finish being served by the old version of all search services. Even sophisticated canary rollout solutions like Istio require your application to handle version mismatches between different services, which we couldn’t guarantee. So how could we create a gradual rollout for a new version of all search services, while simultaneously managing load-balancing from the web application to all parts of the rollout AND guaranteeing search services only ever talked to other search services of the same version? There were no off-the-shelf solutions that could solve such an Etsy-specific problem. So we built an entirely new tool, called Switchboard. Enter Switchboard Switchboard’s primary function is to manage traffic: it rolls a deployment out to production by gradually increasing the percentage served to the new live side, and proportionally decreasing the amount going to the old one.  Deployment stages with predefined traffic ratios are hardcoded into the system, and when all pods added during the current rollout stage are fully created and healthy, Switchboard transitions to the next stage. It does this by editing and committing the new traffic percentages to a configuration file within the web application. The web app re-checks this file on every new request and uses the information to load balance search traffic between two different production Kubernetes namespaces, both still called flip and flop. Example of a side switch using Switchboard. Smoke tests are running at 16:57 and 17:07. Switchboard largely automates the migration of traffic from one search side to the other during a deployment. Smoke tests run at different phases of the deployment, sending both artificially-created and real historical search query requests to the new side. Developers just need to monitor the graphs to make sure the rollout went smoothly.  The engineer driving the deploy manages Switchboard through a user interface that shows the current state of the rollout and also provides options for pausing or rolling back the deployment. With Switchboard, we largely rely on Kubernetes’ built-in autoscaling to scale the new cluster during the deployment. We have found that we only need to prescale the cluster to serve 25% of our current capacity before we start sending production traffic to it. Kubernetes’ built-in autoscaling is reactive, and therefore necessarily slower than if we force Search to scale before it needs the extra capacity. As a result, it helps to prescale the new live side so it responds faster to the initial shift as that side goes live and starts to receive traffic. From there, Switchboard lets Kubernetes manage its own autoscaling, simply monitoring the Kubernetes rollout to make sure all services are healthy at the current stage before making the decision to ramp up. Results We designed Switchboard to improve the resource consumption of our Search system, and it has done that. But the stepped deployment approach has also resulted in a number of nice workflow improvements for developers.  Switchboard allows us to keep our overall search VM capacity at or close to 100% rather than the 200% capacity we’d been supporting before. We no longer need to provision double capacity when traffic to Search increases. It’s now much easier to adapt to variations in traffic, since any additional reactive (automatic) or proactive (manual) scaling only needs to reserve compute services for our true capacity instead of double. As a result, there was a noticeable improvement in our cloud VM utilization during the period in which we released Switchboard. Cloud costs per search request (cloud billing total/number of requests) over several months showing our improved utilization efficiency post-Switchboard. The second big win from Switchboard is that it has made deploys to our staging environment consistently faster. Our first attempt to move away from the legacy double provisioning approach was to fully scale down the unused search cluster between deploys and and then preemptively rescale it as the first step in the next deploy. One problem with this approach was that developers had to wait for all the services inside our Search system to be scaled up enough to accept traffic before they could test in our staging environment.  As you can see in the graph below, deploys to staging have become less bursty since we adopted Switchboard. Switchboard’s stepped scaling means we can send staging traffic to the new live side much faster. In the worst-case scenarios, provisioning a completely new fleet of nodes in the cloud was taking 20 minutes or more. That is 20 minutes that a developer needs to wait before being able to see their changes in staging.  Time elapsed per staging environment deploy. Each individual line is a single deploy. Overall, after implementing Switchboard we saw similar increased utilization to our intermediate solution, but without having to compromise on slower deploy times. Switchboard even improved on the utilization efficiency of the intermediate solution. It’s also easier now to spot and respond to issues during a deploy. Search deploys technically take longer than they did when we maintained two fully scaled clusters, but that additional time is caused by the gradualness of the automated traffic rollout process. A human search deployer typically passively monitors rollout stages without interacting at all. But if they need to, they can and will pause a deploy to examine current results. Search deployers use Switchboard at least once a month to pause a rollout. This is an option that simply wasn’t available to us before. Due to Switchboard’s gradual rollouts and its ability to pause, individual deploys have become safer and more reliable. In the end, rearchitecting our blue-green deployment process to include a canary-like gradual traffic ramp-up via Switchboard allowed us to make our system more scalable and efficient while also designing for a better developer experience. We were able to successfully adapt our search application’s architecture to take advantage of the flexibility of our Kubernetes and cloud environment. Related Posts </description>
      <pubDate>18 Nov 21 12:28 EST</pubDate>
      <guid>https://codeascraft.com/2021/06/15/improving-the-deployment-experience-of-a-ten-year-old-application/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.stephendiehl.com/blog/nothing-burger.html</link>
      <description>&lt;a href=&#34;https://www.stephendiehl.com/blog/nothing-burger.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; The Handwavy Technobabble Nothingburger At this point pretty much every economist worth their weight in salt has given the public fair warning about the financial absurdity of crypto assets using the well-known basic economic arguments against the faux currencies. However economic crypto scepticism has to go hand in hand with a deeper understanding of why the technology doesn’t work as its advocates claim, in addition to the legal and regulatory arguments against its existence. There’s a simple inescapable truth at the heart of technical crypto scepticism that almost all software engineers intuit at some level: Any application that could be done on a blockchain could be better done on a centralized database. Except crime. The essence of the financial arguments against crypto assets are quite easily summarized. As I previously described, crypto assets have no claim to be currencies because their deflationary properties and volatility don’t fulfill the theoretical or even practical function of money. They aren’t commodities because they have no non-circular economic use case. There is a somewhat coherent proposition that crypto assets are effectively unregistered securities contracts, basically like stock in an empty company that doesn’t do anything except promote the sale of its own stock. Historically these investments would have been called “Blue Sky Contracts” in the era before the Uniform Securities Act of 1956 outlawed such things. And then there’s the claim that crypto assets are a piece of performance art about libertarian politics, but this is an unfalsifiable proposition. Despite their financial incoherence there are effectively three technology buckets you can put most of these crypto asset schemes into: Memecoins - Investment schemes that exist to perpetuate some narrative based on an internet or political meme and whose purpose is “number go up”. Examples: Doge, Shibu Inu, Bitcoin, Litecoin, Hex, etc Progcoins - Investment schemes which host other investment schemes and typically allow the execution of so-called smart contracts. Examples: Ethereum, Solana, etc Stablecoins - Investment schemes which maintain value against real world assets and used as a medium of exchange for extra-legal transactions. Examples: USDC, USDT, etc Memecoins are pure greater fool investments, they’re basically a hot potato that people trade hoping to offload it on someone dumber than them who will pay more for it. And the implicit assumption behind the terminal value of these assets is that there’s an infinite chain of fools who will keep doing this forever. Nassim Taleb deconstructed this concept from a quantitative finance perspective in his whitepaper but nevertheless these assets persist because people behave economically irrationally and like lighting money on fire and dumping it into memes regardless of financial sanity. Meme coins like dogecoin exist simply for people to gamble on a fantasy about talking dogs, and bitcoin is a meme token for gambling on a fantasy about living in a cyberpunk dystopia. At the end of the day, memecoins are not that economically distinguishable from Ponzi schemes. Progcoins are manifestations of what some of us programmers call decentralized woo woo, these projects claim to build all manner of programmatic applications. Yet when you dig into the details of such claims they’re very hand-wavy appeals to things that either don’t exist yet or are thinly veiled gambling schemes and outright scams. After twelve years of these technologies existing (roughly the same age as the iPhone) there is basically only one type of successful crypto business: exchanges which exist to trade more crypto. But the heart of this issue, and why there’s no other success stories, is because smart contracts tenuously look like a good idea until you actually try to build anything real that has to interact with the non-blockchain outside world. At which point they become too brittle, insecure, or strictly inferior to a centralized alternative. In database terminology smart contracts are stored procedures that run one of the various incarnations of distributed databases these technologies are built on. In theory they act somewhat like self-automated vending machines but for more complex user interactions. In practice they act more like self-automated bug bounties which typically explode violently when certain exploits are issued against the coded logic, and at which point they spill all of the coins locked up in the contract. These disasters happen about two or three times a week now because coding at that level of correctness required in a Javascript-like language with loose and ill-defined semantics is near impossible. When a contract does finally meet its end, the only recourse is begging or threats to return the stolen tokens. However it’s unclear that “stolen” is the right word because the contract was simply behaving exactly as instructed and therein lies the core reason why “code is law” is an absolutely rubbish idea. The second absurdity at the heart of smart contracts is their dependence on external data sources to function, the so-called “Oracle problem” is an intractable issue whereby these blockchain stored procedures must depend on data external to a blockchain in order to allegedly perform some business function. If a contract is modeling some sort of derivative contract then it depends on the price of the underlying asset, which it will have to pull from a price feed from Bloomberg. To check if the counterparty to the derivative has posted collateral it will have to pull out to query the balance of an account at a high street bank for one of the counterparties. To check if the counterparties are allowed to trade with each other they have to check whether either of them is on a sanctions list. So then by the time you’ve folded Bloomberg, Barclays and Uncle Sam into the trust boundary of your smart contract there’s very little point to saying this process is decentralized anymore, and begs the question why even construct this Rube Goldberg machine when it could be better done as a simple program running on a centralized server. It would be far more sensible and efficient to just build a web app that uses Stripe for payments. That is unless your business model fundamentally depends on selling unregistered securities or breaking the law. And then that leads us into the third class of tokens: stablecoins. Stablecoins at face value might have some claim to have moneyness property. They are in essence a derivative of a national currency, usually a US dollar derivative that is issued on a blockchain and maintains a fixed stable value rather than being a speculative investment. Stablecoins are allegedly backed up one-to-one by reserves which should equal the total amount issued. You buy a stablecoin dollar effectively in the same way one buys chips at a casino, except stablecoins are used to gamble at offshore crypto exchanges who can’t get stable banking access because no regulated entity will touch these jurisdiction-hopping externational scofflaw casino boats. Stablecoins thus fulfill the customer “need” to arbitrage money transmitter regulation and move money to entities that exist outside the normal regulatory perimeter. The casino chip analogy is accurate however unlike a casino, stablecoin issuers are not required to redeem tokens for real money and have no legal requirements to maintain reserves or even report on their contents. It’s a pretty good racket printing your own counterfeit dollar derivatives, and in practice many investigative financial journalists allege that some of these issuers are simply absconding with customer money and lying about their reserves. Stablecoin issuers are some of shadiest operators in an already rather shady ecosystem and many are widely believed to be outright scams that may meet the same fate as offshore Caribbean wildcat banks like Liberty Reserve. Some people in technology think that stablecoins could be used to innovate in the banking sector and expedite retail payments. This almost makes sense, until you think about it for more than 10 seconds. Even if you had a completely legal and above-board stablecoin (which doesn’t exist today) you effectively have an institution which is for all intents and purposes basically acting as a bank, they take and custody customer funds and have enough liquid reserves to prevent a run on the coin and honor withdrawals whenever the customer needs. The Biden administration looked at this problem and came to the same conclusion, they should be regulated exactly like banks and be required to have FDIC protection on customer money, post collateral and be plugged into the Federal Reserve like any other bank would. At that point, yes, most of the consumer protection problems are mitigated for this kind of business but it begs the fundamental question: Why even bother? A stablecoin bank would be subject to exactly the same FinCEN and OFAC money movement restrictions and compliance checks as banks; so know your customer gating, counter-terrorism financing, sanctions enforcement, and anti-money laundering enforcement. And these compliance requirements are the almost always the bottleneck consumers may encounter when doing cross-border transactions, and it’s not a technology issue. Nothing about stablecoins is either necessary nor desirable, and any alleged improvement these systems may offer at the moment are purely illusory and derived only from the unstable situation that they temporarily inhabit a yet-unregulated shadow banking system that is either non-compliant or entirely scofflawing. A regulated stablecoin bank is just a bank, but with a core ledger built on a terribly inefficient and bizarre piece of software not built for that purpose. All this while guzzling entire nation states worth of energy for no reason. Using inefficient blockchain as core banking software makes old legacy core banking solutions like Jack Henry look like a Ferrari by comparison. Our European allies all built extremely reliable real time payments like SEPA that work marvelously and they didn’t need any stablecoins. Yet all of these technical arguments circle around a deeper truth: a technology which is purpose built to circumvent and arbitrage the regulatory perimeter cannot be brought within the perimeter without destroying its core claim to value or irreparably crippling it. Until proven otherwise it seems like the goal of the crypto ecosystem is to build an enormous unregulated casino with a crazy party scene. Along with a large lobbying arm to keep the musical chairs party going long enough with the hope of a government bailout through empty appeals to “American innovation” when the pyramid inevitably collapses. I’m not alone in believing in the fundamental technical uselessness of blockchains. There are tens of thousands of other people in the largest tech companies in the world that thanklessly push their organizations away from crypto adoption every day. The crypto asset bubble is perhaps the most divisive topic in tech of our era and possibly ever to exist in our field. It’s a scary but essential truth to realise that normal software engineers like us are an integral part of society’s immune system against the enormous moral hazard of technology-hyped asset bubbles metastasizing into systemic risk. </description>
      <pubDate>05 Dec 21 15:47 EST</pubDate>
      <guid>https://www.stephendiehl.com/blog/nothing-burger.html</guid>
    </item>
    <item>
      <title>Is internet addiction eradicating the habit of reading?</title>
      <link>https://benwajdi.com/2021/12/18/is-internet-addiction-eradicating-the-habit-of-reading/</link>
      <description>&lt;a href=&#34;https://benwajdi.com/2021/12/18/is-internet-addiction-eradicating-the-habit-of-reading/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; (Last Updated On: 30 January 2022)In the early 1990s, e-mail was spreading like wildfire. Among its early adopters, the most religious ones were from the academic community. In 1997, John Sutherland 1wrote that then an “e-mail address was for professors what the cellular phone was for their students: a sign that they were ahead of the game.”2.These days, in 2021, and as far as tech gadgets possessed as signals of status and progressiveness go, Sutherland’s comparison remains valid if we replace cellular phones by an Apple iPhone, and e-mail by a blog or personal website.In an essay he published in The Guardian (which is by now removed from their website, following a massive Twitter campaign against the novelist mainly because of his opinion on techno-consumerism), titled “What’s Wrong with the Modern World?”, American novelist Jonathan Franzen wrote:“Isn’t the essence of the Apple product that you achieve coolness simply by virtue of owning it? It doesn’t even matter what you’re creating on your Mac Air. Simply using a Mac Air, experiencing the elegant design of its hardware and software, is a pleasure in itself, like walking down a street in Paris. Whereas, when you’re working on some clunky, utilitarian PC, the only thing to enjoy is the quality of your work itself.”Franzen, like Karl Kraus3‘s dichotomy between Romantic and Germanic cultures, presents us the “Mac versus PC” dichotomy, and like Kraus before him he chooses content over form. “I’d still rather live among PCs” he wrote.When Philip Roth said that “the novel’s day has come and gone” many members of the literati thought he is yet another anti-tech novelist opening up about his distaste for the age of screens. Literature “is one of the great lost human causes” Roth said, “Oh I don’t think in twenty or twenty five years people will read these things [books] at all”4 And when Jonathan Franzen dubbed Twitter as “dumb”, writers and Twitter users alike tweet-stormed him in an outrage.5 Yet these critics of Roth and Franzen, and who couldn’t relate to neither the former’s austere lifestyle, nor to the latter’s obsession for watching birds, are now willing to spend up to five hundred dollars for smart typewriters, like Astrohaus’ Freewrite Traveler, so they can write without any distraction.Not all book worms are as pessimists about the future of books and reading as Philip Roth. The German writer Günter Grass6, whom Mario Vargas Llosa7 described as “one of the most significant writers of the 20th century”8, had always believed in the private intimate connection with a book. During the 2001’s World Book Day celebrations in Berlin, and when asked about how he sees the future of books, Grass said:“There is no surrogate for books. The act of reading, the private company of a book which you can carry around, to the loo, to your bed, on your travels—no computer can replace that. This intimacy provided by reading is irreplaceable. In many respects, I am a pessimist, but as far as books are concerned, I am sure they will survive.”Günter Grass 9More than fifty years ago, on September 2nd 1969, an experiment at a UCLA lab had been conducted “to watch two computers transmit data from one to the other through a 15-foot cable”. Three months later, four nodes of the new “packet switching” network, then named, ARPANET “were up and running.” ARPANET was named after the US government’s defense agency department, which “conceived and paid” for it. ARPANET later became DARPA—the “D” stands for Defense.Technically, the internet as we now know it “was born when ARPANET was converted to TCP/IP in 1983.” Eight years later, Tim Berners-Lee and Robert Caillian, of the European Center for Nuclear Research, invented the World Wide web.10Recent official reports about internet usage from governments worldwide are alarming. In Malaysia, a country of 32 million people11, 88.7% was the percentage of internet users in 2020. More than a quarter of them have been using the internet for more than a decade as of 2020. Half of them spend “5 to 12 hours a day on the Internet”, while 21% spend more than 12 hours a day on the Internet.12I don’t blame people who read paperbacks and hardcovers instead of PDFs and Epubs anymore. I myself have tried a similar experiment: instead of consuming PDFs, I print them, and actively read them (taking notes while thinking about the material). As a result, I was able to concentrate for longer blocks of time, read more, and grasp more; however, like any first act of rebel against an addiction, I was met with withdrawal symptoms, mainly the urge to open the laptop, and search for something on Google.How can we gain back our ability to focus? Is there anything that can be done to reverse the damage our brains has accrued since they had been stuck to the internet?The fact that Astrohaus had sold out its Freewrite Traveler model of its smart typewriters is nothing but a proof for Roth, Franzen, and Günter Grass’s case, that computers and the internet are only tools in the writing process, and that an anti-tech attitude is sometimes necessary for those who only can get things done when they get tough on themselves. The British journalist Kirsty Wark was once shocked at Philip Roth’s daily routine and lifestyle, and how much loneliness he could endure; Roth explained “That’s the only way I could get my work done.”13For Franzen, the only way to get his work done is to not to have any access to the internet. His ten rules for writing has gained him a group of fans and haters; specifically the “I doubt that anyone with an internet connection is writing good fiction” rule. I myself couldn’t see why people would disagree with this one rule. Most people would agree that leaving the internet open while writing is, for many (if not most of us) is like writing in front of a window. In fact, it is a digital window to a world wide web, where access to information is instant and free, and where notifications and apps only intensify the lack of focus, and impulsivity, and the curiosity to google that thing. In his 2006 introduction to On Writing Well, American author William Zinsser wrote: “I don’t know what still newer marvels will make writing twice as easy in the next 30 years. But I do know they won’t make writing twice as good.”14On the one hand, I resonate with Franzen’s take on the internet, and on corporations, yet on the other hand, I can’t deny that for someone like me, a marginalized North African kid whose first interaction with any part of the internet dates back to circa 2005, the internet was the only way I could’ve accessed the body of knowledge that could fulfill my curiosity, and my eternal search for a way out of the “shithole”. Without the internet, I would have been a very different man. Without it, I would have succumbed to all the currents of local nationalism, religious fanaticism, and the currents of elite leftists running the “shithole” and confining everyone with them in eternal misery.Perhaps, the first step is becoming conscious. Consciousness about how much time are we spending emailing and texting, listening and watching, filling forms and closing popup ads. When we track the hours, we become shocked—not at the numbers per see—at what could have been learned, grasped, or done during these wasted hours.After consciousness about the amount of time we waste online, the next move is to transition towards being a pragmatic user of the internet. This might not work for everyone, but becoming pragmatic users means planning our online tasks before we even open the internet—that’s how serious we must become towards the addiction: treat it as an addiction, a serious threat to our minds and to our lives. And we should forget about throwing our phones, deleting all of our accounts, and isolating ourselves from the rest of the digital world, all at once. Almost all cold turkey attempts to break free from internet addiction have failed, and ours will probably fail too. Instead, we should progressively attempt to limit and plan the time we spend online—in advance.“A restlessness has seized hold of many of us,” wrote the American author Rebecca Solnit, “a sense that we should be doing something else, no matter what we are doing”.15 It’s the same restlessness that reminded Franzen of “what Marx famously identified as the ‘restless’ nature of capitalism.”Personally I’d been deprived of access to computers and the internet for several straight months. It was a tough experience—not by choice, but by obligation. Going months straight without access to the internet forced me to go slower, to think deeper, to concentrate better, to make rational choices, and to prepare better plans. Not to mention the discipline to have a routine that works for me, not to jump into using electronics first thing in the morning, and instead, do the sort of things that propel me to get my work done early during the day. It also taught me not to skim through PDFs—and instead read them, and take detailed notes of the material I read. These notes have accumulated into an archive of my notes; it is one of my most valuable assets.I think that all it takes to realize the dangers of internet addiction is to watch people around you. Watch how they hold their phones, how they look at their screens, how they scroll down through their feeds, how they jump into watching random stories and videos. If these people are close enough to you, you will begin to notice the influence of their digital habits on their self-image, their life choices, and their goals.I sometimes wonder what will happen if the internet gets suddenly shut down worldwide? Will hundreds of millions of internet addicts go down the streets protesting, and possibly overthrowing their governments?The way I see it now, is that the excess of these “informational floods” will only make the top one percent shrink in numbers, and grow in assets owned. For those in the Bottom Billion, most people won’t use the internet to climb the socioeconomic ladder; the amount of stories and “TikTok influencers” occupying their time and attentions won’t leave them a moment for that.That’s the paradox: the internet is here thanks to governments. Its structure resembles a democracy—to some extent. Yet it is often threatened by governments (under totalitarian regimes), and indirectly leads to more inequality (as those who will reap its benefits are few individuals and corporations).Instead of the slow and deep thinker that reading requires, the constant use of the Internet could turn one into a quick responder, an attendant who receives information and instantly demands more. Reading requires a genuine interest, a commitment of energy and cognitive capacity into the text at hand. When reading, one stops at the middle of a paragraph that has struck him, and think it through twice. The reader might even argue with the author, or even laugh at him and his superficiality. In short, voracious readers ultimately become critical thinkers. The Spanish essayist Miguel de Unamuno is attributed this quote: “Fascism is cured by reading, racism is cured by traveling.” I spent hours searching when or where did he say or wrote that to no avail. Whether it is Unamuno who wrote this or not, I am not sure about his latter claim, but I would like to think that the former is true. Ben Wajdi’s Blog is a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for sites to earn advertising fees by advertising and linking to Amazon. If you enjoyed reading this post, you can support my work by buying me a book (one time donation) or by becoming a patron. </description>
      <pubDate>18 Dec 21 02:21 EST</pubDate>
      <guid>https://benwajdi.com/2021/12/18/is-internet-addiction-eradicating-the-habit-of-reading/</guid>
    </item>
    <item>
      <title></title>
      <link>http://neugierig.org/software/blog/2022/01/rethinking-errors.html</link>
      <description>&lt;a href=&#34;http://neugierig.org/software/blog/2022/01/rethinking-errors.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Programming language ecosystems often involve tools that process code and produce errors or warnings, such as compilers and linters. Different programmers have widely different opinions on how to treat these systems. This culminates in disagreements around questions like: Should I treat all warnings as errors? Why is X an error and not a warning? Can I ignore lint warnings? In this post I aim to give you a framework for how to reason about these systematically, with lessons learned from my recent years of work on programming language tooling for thousands of engineers at Google. To begin with, we need to reset your biases, and to do so let&#39;s make up new words just for the duration of this post. Let&#39;s say a compiler/linter/static analysis/etc executes checks which can identify problems in code. Here are some examples of some checks I&#39;ve seen, just to ensure we&#39;re more or less on the same page: does the code have a type error? does the code have an unused import, variable, function parameter, ...? does the code use a dangerous language feature? does the code use a deprecated API? does the code use a string without localizing it? does the code use a suspicious pattern, such as calling a function without using the return value? does the code change something that needs an accompanying change elsewhere? (e.g. keeping two lists in sync) does the code need more comments? is the commit message properly formatted? The development cycle At a high level there are two purposes of checks: identify problems earlier in the development cycle; maintain invariants, such as &#34;the code never contains unused imports&#34;. (There&#39;s potentially a third case of checks — what to do with problems like syntax errors that make the program completely uninterpretable. But more on that in a second.) To elaborate on (1), the process of software development is taking code through a series of phases: edit build run locally commit deploy (e.g. release to users) execute remotely (e.g. on server or on user&#39;s machine) We programmers loop through each of these — we edit many more times than build, build more often than we run, run more often than we commit, and so on. At each phase, discovering a problem is more costly than discovering it at an earlier one, up to orders of magnitude: contrast the cost of spotting and fixing a squiggly underline in a code editor vs the cost of tracking down the cause of a user&#39;s bug report. So here&#39;s an easy goal: when possible, discover problems in earlier phases rather than later. For example, one way to view static typing is that it shifts type problems from the execute phases to the earlier build phase; a reason programmers like IDEs is that they can surface problems as you type rather than needing to wait for a build first. Invariants, and inform versus block How we maintain invariants is more interesting. What do you do when a check finds a problem? There are two basic options: either inform the programmer and move on, or block forward progress. Here&#39;s a warm up to make this more concrete. In most statically typed languages, a type error stops compilation, so you could say that for a type problem the compiler must block at build time. But even something as apparently fundamental as this is not fixed! For one counterexample, TypeScript can be configured to generate code regardless in the presence of type errors (which never affect runtime behavior anyway). You could imagine even a C compiler could choose &#34;inform&#34; on a type problem and generate a runtime failure in its place, to allow you to run the resulting program. (I&#39;ve heard some tools in the Java ecosystem can already do this, perhaps?) This behavior is plausibly useful when developing, where you want to iterate on module A of a program while you ignore type problems in irrelevant module B that you know you won&#39;t execute while you are working on A. In principle then at least, for any problem you could choose between blocking and informing, and informing lets the programmer choose whether to stop rather than forcing it. At any given moment there are surely hundreds of different ways the code could be improved, some of them not even detectable by your tools; in my experience there&#39;s never a shortage of known flaws in a given codebase, and the hard problem is instead how to prioritize these problems. When we block on one problem what we&#39;re effectively saying is that of all the different things that could be worked on, the problem you just identified is absolutely the highest priority to fix, so much so that you cannot be allowed to work anything else before fixing it. So, you might ask, if we were starting over again, why would we ever choose to block for any problem? It&#39;s a pretty deep question, really! In practice it comes down to human nature and how we sometimes won&#39;t do work unless we&#39;re forced. For example, consider warning blindness. Warning blindness You often find programmers recommending to configure C compilers with -Wall, which turns all warnings (which typically &#34;inform&#34;) into errors (&#34;block&#34;). But at that point, why do languages even have a mechanism for warnings at all? Again consider TypeScript: the problems it finds (&#34;diagnostics&#34;, in TypeScript lingo) have a severity field that is an enum that has a possible level of &#34;warning&#34;, which then is never actually used by the compiler! It ends up feeling pretty circular, where warnings are never used so everything is an error, but then there&#39;s also an option to not block on errors. The underlying reason for this is that in my (and likely your!) experience, informing about a problem without blocking on it at some point often leads to a situation where sometimes problems are ignored. This then leads to &#34;warning blindness&#34;, which is when checks find so many problems, it gets hard to spot the important checks amidst the ignored ones, which then means the checks stop actually finding problems. The treat-warnings-as-errors approach fixes this using the only lever we normally have available — blocking compilation — which forces fixing the warnings. Choosing to block at any phase is to enforce an invariant, e.g. &#34;code that passes this phase never contains a type error&#34;. Invariants are powerful tools for reasoning because they let you relax the part of your brain that normally needs to worry about an invariant. For example, imagine a system that verifies (perhaps using a type system) code doesn&#39;t contain XSS or SQL injection, choosing to block before deploy. Now code reviewers don&#39;t need to check for that problem as closely. But importantly, it&#39;s also useful to be able to relax an invariant. While developing, you might wanna sprinkle in some printfs while debugging something and you don&#39;t want to worry about appeasing the XSS checker as you do it. It&#39;s valuable both to be able to sprinkle those prints (not blocking on error at the run phase) while also guaranteeing that you never deploy such code (blocking at the commit or deploy phase). The pattern of informing in one phase followed by blocking in a later one is a nice way to softly introduce an invariant while still giving the programmer some wiggle room while developing. For example, most problems that block at a later phase (such as a blocking build problem) can usefully be informed during the editing phase (as a highlight in the editor) as a way of shortening the development loop. Another example of this pattern in practice is in projects that provide a warning-only linter during development, but then require code to be lint-clean before submitting it. The solution to warning blindness here can be generalized as this principle: informing without blocking can be useful to allow rapid progress, but if any phase ever decides to inform, some later phase must block on the same thing to ensure the problem doesn&#39;t stick around. And that is to my mind the best application of informing, as a marker for effectively time-shifting when blocking happens to a later phase. Unused import hygiene and and lint It&#39;s time for another case study: how unused imports are handled in Go. Go wanted to enforce the invariant that code never has unused imports, so the compiler refuses to build code that has unused imports. This has a nice outcome — you never see Go code with unused imports — but it can be frustrating to work with while developing, where you might need to repeatedly add and remove a temporary import like the &#39;log&#39; module while debugging. As a user of Go and as a person who prefers more checks rather than less, I still think it would be better to allow building and running code with extra imports. What I think we actually care about is that you don&#39;t commit such code, which is a later phase in the development cycle. In my experience there are a lot of &#34;hygiene&#34; like problems, such as preferred whitespacing or whether imports are in the proper order, that better belong as checks during commit rather than while developing. I imagine one big reason Go doesn&#39;t behave this way is that they don&#39;t have the adequate hooks into the developer workflow to enforce such a thing. The lack of adequate hooks in the other direction, from the developer into the toolchain, is also an explanation for the existence of &#34;lint&#34; tools. In my experience tools named &#34;lint&#34; often end up being a grab-bag of &#34;all the checks we wanted but that weren&#39;t enforced by the language already&#34;, which can vary from trivial guidelines all the way up to really important invariants. Switching your mental framing from &#34;which program does the check execute as part of&#34; to &#34;which development phase does the check execute during&#34; makes it obvious to me that programming language tooling like compilers instead ought to provide hooks for programmer-defined checks, such that those checks behave just like language builtins. TypeScript does something almost like this, with an option for providing language service addons in the tsconfig (which can then surface problems in the editor), but those addons aren&#39;t used in the compilation step itself. So in TypeScript and in many other languages, introducing additional checks often involves a separate build step that has to re-parse (or even re-type check) the input source. For another consequence of this layering, note that code editors build compiler integrations (to show compiler checks) and then end up separately building lint integrations (to show lint checks), when it&#39;d be more coherent if they only needed to integrate with one &#34;tooling that checks for problems&#34; system. One objection you might have at this point is something about how compilers typically check &#34;important&#34; problems and linters check &#34;unimportant&#34; problems. This is true to an extent but it&#39;s also often kind of a historical accident; there&#39;s plenty of unimportant things a compiler can complain about, and there are plenty of lint-like tools that discover real problems. And this also explains why different compilers might treat different problems as either a warning (inform) or error (block) kind of problem. Confidence Another axis to analyze checks along are how confident they are about the problem. At one extreme, a compiler might say &#34;if this code is ever executed I guarantee it will explode&#34;; at the other, a linter might say &#34;this code pattern looks kinda fishy but I&#39;m not sure&#34;. At the low confidence end of things this means that the check may misfire and complain about a problem that doesn&#39;t exist. For this reason, in the past I have believed the right approach is to just let programmers ignore (bypass) low-confidence checks; for example, you could decide that some lint check is purely a guess and that you should feel free to commit code even when it fires. I have since come around to seeing this is a bad idea, because it means that the next person who edits the code will need to reevaluate the same question, and a system where you regularly need to disregard warnings leads to warning blindness. (Within Google we had a system that attempts to identify whether a given change actually introduced a problem, or whether it was preexisting in the code, but it ends up fragile when things happen like code is moved or variables get renamed.) Instead, I think a better mechanism for responding to checks is for programmers to annotate code directly. In some cases that can mean modifying the code itself. A common check across languages is to say a statement like if (x = 3) ... is suspicious because it assigns a value to x rather than testing it, and in some languages the fix is to write it if ((x = 3)) ... as a way of more or less saying &#34;no I really meant that&#34;. But in some cases, there isn&#39;t an easy syntactic modification available, in which case I think an explicit acknowledgement of the check is both a great fix for the check and for other programmers. For example, an explicit comment that turns off the check is also exactly the right place to explain to the next programmer why the code looks suspicious but is actually ok: // The unsafe loader here is ok because bar() already sanitizes the input. // ignore:no-dangerous-api dangerouslyLoad(bar(input)); (This example isn&#39;t itself great in that the above case can likely be better addressed with types, but that is also another demonstration of how modifying the code itself can be better than annotations.) One tempting solution that doesn&#39;t work well is to put these annotations somewhere other than the code, like in a file on the side or in a config file. This doesn&#39;t work out because it results in action at a distance: if someone moves the code, suddenly a check starts firing, and it&#39;s not obvious to discover that it was caused by a reference to the old location of code in another file. The above mechanism, with the annotation directly on the code, means that anyone who touches the code (or copy-pastes it) will preserve its behavior. Fixing In some cases the fix for a problem can be autogenerated by tooling. For example, imagine a check that enforces imports are sorted. (Enforcing this invariant is often useful for reducing unmeaningful churn when imports are added and removed over time, as well as making it easier for tools to automatically modify imports.) Computers are better than humans at sorting, so it would be helpful if the imports-are-sorted check was accompanied by a way to automatically apply the fix. Again, such fixes are usefully plumbed throughout the stack of phases: even if there&#39;s a lint-like problem that is blocking at submit time, it would be ideal if you could click a button in your editor to accept the suggested fix, or even apply the fix automatically on save. Frequently, the code to identify a problem is the best place to also identify the fix. A tool might say &#34;you wrote foobra right here but you probably meant foobar&#34;, and when it does so it knows exactly the byte offsets of those symbols that other tools would use to both display or execute the fix. So to make this work well, what you need is a uniform serializable representation of problems along with their accompanying fixes throughout the cycle, which allows tools at any layer to both produce and consume these fixes. (Nothing fancy, JSON is adequate.) For example, an editor consumes fixes by offering the user to apply those fixes, like the light bulb in VSCode (scroll up from here to see it); an offline tool could consume fixes by applying them automatically to code. Evolving checks One reason we used tools to apply fixes offline at Google is to roll out a new check. Suppose that we historically didn&#39;t sort imports but wanted to start doing so. A bad approach is to just turn on the check without changing any code. Doing that means the next person to build or modify an affected file suddenly is interrupted with some demand unrelated to the change they were planning to make, and the resulting commits mix unrelated concerns. Much better is to programmatically execute the check and apply the fix up front, as its own independent change. A common task I did over my last years at Google was such a thing, deploying automated fixes like these to thousands of files. Representing these &#34;code migrations&#34; as a problem check with an attached fix, rather than a specific one-off code migration program, has the additional benefit that you can use the same code to maintain the invariant going forward — the code that implements the offline import sorting is the same code that offers to sort your imports on new commits, sorts your imports within the editor, or comments on code reviews. In the case where a new suppressible check cannot be automatically fixed, it&#39;s counterintuitively still valuable to insert a suppression comment at every current instance of it in the code base as part of rolling it out, because this (1) still allows the new check to fire on newly introduced code, and (2) clearly marks all the locations that are still failing the check, making it easy to grep for (how many remaining places are using APIs that we have marked as dangerous? Just grep for the dangerous-api suppression) and track in a burndown chart. In my experience, sending a code review that inserts a suppression is often a good prompt to the reviewer to either say &#34;this code is fine as is&#34; and accept the change, or push back with &#34;this actually discovered a bug&#34; and send a counterproposal. For fixes that have a high confidence, a better option than bothering humans about them is to just apply them immediately. This, for example, is how whitespace is handled in any humane programming language these days — it&#39;s never mentioned to the programmer, and never brought up in any code review, but it&#39;s just fixed as you hit save in the editor. Meanwhile, other fixes have low confidence, where you definitely need a programmer involved before applying them; sometimes a given problem might even have multiple plausible different generated fix options. For this reason it&#39;s useful to model potential fixes as an array. In VSCode, for example, it can sometimes pop up a menu to let the programmer choose between the fix alternatives. Conclusion In this post I jumped around between how things actually work and how things ought to work. It was my intent not to describe any system in particular, but rather gather the patterns that I have seen work well and also suggest how programming language designers can usefully think about them in the future. I hope it was helpful! </description>
      <pubDate>14 Jan 22 12:32 EST</pubDate>
      <guid>http://neugierig.org/software/blog/2022/01/rethinking-errors.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://graydon2.dreamwidth.org/253769.html</link>
      <description>&lt;a href=&#34;https://graydon2.dreamwidth.org/253769.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Warning: this has turned out to be a .. long post.Recently, on the twitters, Stephanie Hurlburt suggested that it&#39;d be healthy for people who have been around the computering industry for a while (*cough cough*) to take some &#34;audience questions&#34; from strangers. I obliged, and someone asked me an interesting one: &#34;After memory safety, what do you think is the next big step for compiled languages to take?&#34;Setting aside the fact that &#34;compiled&#34; languages have had various more-or-less credible forms of &#34;memory safety&#34; for quite a long time, I agree (obviously!) that cementing memory safety as table stakes in all niches of language design -- especially systems languages -- continues to be an important goal; but also that there&#39;s also lots more to do! So I figured I&#39;d take a moment to elaborate on some areas that we&#39;re still well short of ideal in; maybe some future language engineers can find inspiration in some of these notes. Before proceeding, I should emphasize: these are personal and subjective beliefs, about which I&#39;m not especially interested in arguing (so will not entertain debate in comments unless you have something actually-constructive to add); people in the internet are Very Passionate about these topics and I am frankly a bit tired of the level of Passion that often accompanies the matter. Furthermore these opinions do not in any way represent the opinions of my employer. This is a personal blog I write in my off-hours. Apple has a nice, solid language that I&#39;m very happy to be working on, and this musing doesn&#39;t relate to that. I believe Swift represents significant progress in the mainstream state of the art, as I said back when it was released.That all said, what might the future hold in other languages?Broadly applicable problem areasThese are either ubiquitous abstractions or problems that still need work, that will need work in basically any mainstream language that comes next, as far as I can tell.ModulesThis might come as a surprise to hear, but most languages have module systems with serious weaknesses. And I agree with Bob Harper in his assessment that Modules Matter Most. Many languages have no module system at all, and many that have one have it only as a way of (say) managing namespaces or compilation order. More-powerful module systems exist -- you&#39;ll have run into some of the components if you&#39;ve worked with dependent types, type classes, traits, signatures, functors -- but there&#39;s a bewildering array of design constraints to navigate (generativity, opacity, stratification, coherence, subtyping, higher-order-ness, first-class-ness, separate compilation, extensibility, recursion) when arriving at a practical, usable module system. Few if any languages have &#34;done this right&#34; in a convincing enough way that I&#39;d say the problem is solved. The leading research in the field, at the moment, is probably Andreas Rossberg&#39;s work on 1ML. But there are decades of ground-work that you should really, really read basically all of if you&#39;re going to explore this space.(Writing this makes me think it deserves a footnote / warning: if while reading these remarks, you feel that modules -- or anything else I&#39;m going to mention here -- are a &#34;simple thing&#34; that&#39;s easy to get right, with obvious right answers, I&#39;m going to suggest you&#39;re likely suffering some mixture of Stockholm syndrome induced by your current favourite language, Engineer syndrome, and/or Dunning–Kruger effect. Literally thousands of extremely skilled people have spent their lives banging their heads against these problems, and every shipping system has Serious Issues they simply don&#39;t deal with right.)ErrorsThis too might feel like a surprise, but I&#39;m not convinced that we&#39;ve &#34;solved&#34; error management, in general, in any language. We have a few credible schools of design-philosophy mostly hold together well enough to ship a language: algebraic effects and handlers, checked and unchecked exceptions, crash-failure with partitions and supervision trees, monads, result sum-types, condition / restart systems, transactions with rollbacks; but none of them completely solves the design space well enough that the problem feels &#34;done&#34;. Even committing to one or another such regime -- and it really does involve not just a few mechanisms, but a whole set of interlocking protocols that support error management -- there are serious abstraction leakages and design tradeoffs in nearly every known approach (modularity, compositionality, locality, synchronicity, soundness, cognitive load, implementation costs, interface costs with different regimes). Errors are absurdly hard to get right.Daira Hopwood has some notes and design material in hir work on the Noether language, and there&#39;s (again) a lot to read before deciding how to proceed. Doing it well takes you on a long journey through both a lot of fussy technical material and a lot of highly subjective, downright philosophical topics too, like &#34;how to defend yourself against your own mistakes&#34; and &#34;what does it mean for a value to be right or wrong&#34;.Coroutines, async/await, &#34;user-visible&#34; asynchronicityIt&#39;s in vogue at the moment for new languages to have something like async/await. This does not mean it&#39;s a done deal: lots has been done, but lots is still messy. The boundary between synchronous-world and asynchronous world -- in terms of types, control flow, correctness, errors, modularity, composition -- is still very awkward. Whether and how to mitigate between different synchronicity regimes, especially across FFIs or differing runtimes, is hard. Integration with effects is hard. Integration with parallelism is hard. Which parts need to be supported by the language and which parts surface to the user is hard. Cognitive load is still very high.Effect systems, more generallyThere&#39;s still not a strong consensus on where and how to integrate effects into mainstream compiled languages. The type-systems research world seems to have blown past this point, and now speaks breezily of &#34;type and effect systems&#34; as though they&#39;re a solved problem; but most working programmers in most languages have no access to any meaningful effect system, much less a state of the art, extensible, inference-heavy one. Languages like Eff or Koka are leading in promising directions, but it&#39;s still far from mainstream or solved: modularity, polymorphism and encoding issues abound, as does the general cognitive load of the feature.Extended static checking (ESC), refinement types, general dependent-typed languagesThis has been revisited over the decades of language design more often than Godzilla movies have been remade, and it&#39;s because it&#39;s a fundamentally good idea, it&#39;s just very hard, in terms of the design space.The idea is to embed a &#34;logic&#34; in your type system (the boundary is formally not really there, but notationally and cognitively it sure is!) such that users regularly, freely mix their use of types that assert the in-memory shapes of data and functions, with other &#34;logical types&#34; that assert some classes of more-general, (semi-)computable predicates about those data and functions. In other words, let the user write &#34;general&#34; correctness conditions about their programs in a full-blown (but say, primitive-recursive) expression language, and have the &#34;type system&#34; statically check (prove) those conditions always hold (or emit an error message showing a counterexample, just like a type error).These systems are usually a few steps back from &#34;full blown&#34; higher-order dependent type theory proof assistants, a la Isabelle, Coq, Lean or such; though in some cases the type systems inch into the same territory. The design problem hinges on the annotation burden being low enough that the user doesn&#39;t give up in frustration: like most type systems, the user always has to provide some information for the system to infer the rest, but the balance can tilt pretty dramatically towards &#34;way too much annotation&#34; when more-expressive logics enter the game. In many cases, the scale of the annotations overwhelm the program being annotated, and the maintenance burden of those annotations (as the user edits the program) are greater than the coding burden. Which, as a language designer, is Really Not A Good User Experience.So far, most exercises in this space have ended in frustration, or have had limited application to areas with much higher costs for failure (safety-critical embedded systems, etc.) Most mainstream languages have decidedly more .. uh, decidable type systems than ESC, refinement typing or dependent-typed language projects have proposed. But this does not mean it&#39;s a dead end. It means that (if you&#39;re among the faithful, which I am) it&#39;s a design space that&#39;s not yet had its breakthrough product! Cell phones weren&#39;t a thing either, until they were. Maybe in another decade or two, we&#39;ll be looking back on these as the dark ages of boring types.Projects in this space run a bit of a spectrum of levels of expressivity in their type systems (and degree to which they unify &#34;logic&#34; and &#34;types&#34;), as well as the spectrum from research testbed to attempt at mainstream viability; I don&#39;t feel qualified (and this isn&#39;t the place) to do a detailed compare-and-contrast job, so I&#39;ll just dump some links. If you want to play in this space, you ought to study at least Sage, Stardust, Whiley, Frama-C, SPARK-2014, Dafny, F*, ATS, Xanadu, Idris, Zombie-Trellys, Dependent Haskell, and Liquid Haskell.Be prepared if you venture into this area: the complexity wall here can make other areas of computering look .. a bit like child&#39;s play. It gets very dense, very fast. The comic about how computer people would write math books? This stuff all reads like that.Actual Formalization / MetatheorySpeaking of formal logic: an unfortunate fact about most languages -- even simple ones with very pedestrian type systems -- is that they usually ship with next-to-no formalization of their semantics, nor proofs that any such formalizations have any interesting metatheoretic properties (eg. soundness). We tend to just make do with testing and informal, whiteboard-and-email level reasoning about the systems, hoping that&#39;ll get us close enough to correct that it&#39;ll be ok.This approach is, to say the least, wearing thin. Compilers still have serious bugs decades after they ship, failing to implement the languages they&#39;re supposed to. Worse, languages in the field still have serious design flaws decades after they ship, failing to uphold safety properties when subject to formal analysis. Long term, we have to get to the point where we ship languages -- and implementations -- with strong, proven foundations. There are promising moves in this direction, both in designed-for-language-designer tools like K framework or Redex, and in the general set of libraries and projects being undertaken in general proof assistants like Isabelle and Coq.Grab bag of potential extras for mainstream languagesThese are issues with (IMO) potential applicability to mainstream languages, but I think a little less clear of a foundational role in structuring the language; a little more like &#34;features it&#39;d be nice to include in the design&#34;. Some have come and gone before, others are still in research form.Session types, behavioural types, choreographiesThere&#39;s a family of work called Behavioural Types, Session Types or Choreographies, that involve describing the possible interaction traces of both sides, or all sides, of a multi-party interaction, into a single &#34;global type&#34; -- the type of the interaction itself -- and then systematically refining / decomposing that type into a set of separate but dual endpoint-types, that each enforce only-legal-transitions (in sending, recieving and state-changing senses) on the participants.There are various ways to encode this idea in a language, and lots of components; people have done encodings and experiments, even some prototype languages (eg. Links and Scribble), but nothing that&#39;s broken through to the mainstream yet. But I think it&#39;s a very promising area that&#39;s not too hard to understand. Lots of minable papers, good potential return on investment for taking the research mainstream.Parametric mutability, permission / state polymorphism, ???Here&#39;s a small but tasty question that&#39;s unfortunately awkward to answer in most languages with mutability control: how often do you find yourself having to write two copies of a function, that differ only in the mutability qualifiers associated with its parameters or return values? Or, further: does the system of function types and effects scale to expressing polymorphism over the mutable-ness of a type, such that you can fold (say) a higher-order mutating function over a mutable value with the same higher-order function that you&#39;d use to fold a non-mutating function over an immutable value?(Or I guess if you&#39;re a functional nerd: how often do you find yourself writing lifts between monads?)Newer languages try to enforce fancier reader/writer regimes (fractional permissions, borrow checkers, etc.) but often at some cost to parametricity / polymorphism. And this is not new! Open your average C++ file and look at the number of different ways you have to const-qualify a library&#39;s methods to make it compose correcly with all of its users. There&#39;s a form of abstraction and polymorphism seemingly missing in here.Richer patternsAnother small, tasty question: what&#39;s the most complicated pattern you can write in a match expression in your language? We have plenty of automata theory for compiling-down all sorts of complicated patterns to efficient matchers, and we have tons of evidence that users enjoy writing pattern-based programs, that users write shorter, simpler and less failure-prone programs when they can express a task in pattern/action form, and that pattern-sets are amenable to extremely helpful static analysis / diagnosis along the lines of exhaustiveness checking, equivalence checking, emptiness checking and so forth. So why not push the patterns in your language to the limit?I think this is a very fruitful area to explore, especially building out from tree-patterns and visibly-pushdown patterns. Some languages make the pattern system extensible, eg. F# active patterns at the cost of formal reasoning about it at compile time; others push a specific extended pattern-automaton theory into the type system. Many languages in this space, eg. CDuce or FAST, are (unfortunately) &#34;XML oriented&#34; which turns people off the technology lately, since XML is out of fashion, but they&#39;re worth looking into!Unit-of-measure typesPhysical &#34;units&#34; attached to scalars, with arithmetic operators distributing through to the units: a little language support supposedly goes a long way. Frink pioneered some techniques here, F# picked it up (as with many language experiments!) and I do not know if it&#39;s &#34;done&#34; yet or even if users consider it a universally welcome idea, but it seems to me that it has potential. It&#39;s &#34;small&#34; but rather important not to mix units!Cost-model type systemsThis is a family of features for a type system wherein a complexity bound (a la Big-O notation) is calculated on the resources consumed by performing a computation, letting you &#34;typecheck&#34; the cost-model of a program. The language RAML is one of the more promising general-purpose works in this space, though there&#39;s also a long line of work in WCET analysis of embedded systems, timed automata and so forth.Heterogeneous memory and parallelismThese are languages that try to provide abstract &#34;levels&#34; of control flow and data batching/locality, into which a program can cast itself, to permit exploitation of heterogeneous computers (systems with multiple CPUs, or mixed CPU/GPUs, or coprocessors, clusters, etc.)Languages in this space -- Chapel, Manticore, Legion -- haven&#39;t caught on much yet, and seem to be largely overshadowed by manual, not-as-abstract or not-as-language-integrated systems: either cluster-specific tech (like MPI) or GPU-specific tech like OpenCL/CUDA. But these still feel clunky, and I think there&#39;s a potential for the language-supported approaches to come out ahead in the long run.Open implementationsMany older and more &#34;dynamic&#34; high-level languages (Lisps, Smalltalks, Forths) were designed around a kind of uniform programs-as-data model, and the presence / presupposition that the compiler would always be in-process with your program: programs thus more commonly invoked (and extended) the implementation significantly at runtime, did a lot of dynamic metaprogramming, reflection, and so forth. This was maybe a kinder, gentler time when &#34;arbitrary code execution&#34; wasn&#39;t quite so synonymous with &#34;security nightmare&#34;; but it also had a sort of internal logic, represents a design aesthetic that puts pressure on the language machinery itself to be programmable: pressure to keep the language, its syntax, its type system, its compilation model and so forth all simple, uniform, programmable.We&#39;ve since been through a few different eras of language sensibilities around this sort of thing, including some imaginary mobile-code stories like Telescript, Obliq, and eventually JVM/CLR. These latter were weird since they tried to be mobile (which rarely worked), and tried to have semi-open implementations (at least compilers-as-libraries and some access to the bytecode loaders) but didn&#39;t quite make it to the point where it was easy or obvious to do source-level metaprogramming (with the notable exceptions of F# quotations and F# type providers). But through all this, in the background there&#39;s been this somewhat unfortunate, competing &#34;grown-up&#34; model that tends to dominate mainstream languages (everything from FORTRAN to C++): a pretty complex grammar and AST, a pretty hairy compilation model, a very heavy batch-compiler that&#39;s definitely not part of the normal process runtime, and programs that seldom do any metaprogramming, even in cases where it&#39;d be appropriate. Recent &#34;compiled&#34; languages have adopted this style, I suspect in part because LLVM is simply shaped that way, and I suspect also in part as a response to negative experiences with both JVM/CLR environments and overzealous use of metaprogramming in scripting languages.I don&#39;t think, however, that the baby ought to be thrown out with the bathwater. I don&#39;t think a few bad open implementations invalidates the idea, any more than a few bad static type systems invalidates that idea. They can be done well. Julia for example has quite a nice static type system and compiler, but also a uniform syntax that&#39;s friendly to dynamic metaprogramming and JIT&#39;ing. There are also several static metaprogramming and staged-programming systems: MetaOcaml, Template Haskell, ScalaMeta and so forth. So .. there&#39;s a spectrum, a design space.I&#39;m not sure exactly where to go with this topic, except to say I&#39;m a bit dissatisfied with how hard it is to do tooling for current languages, how large the feedback cycle is between a language and its own (meta)programming tools, how distant the tools are from the users, and perhaps to point out that dynamic compilation is not entirely dead: we appear to be entering an era with a new high-integrity universal bytecode sandbox, designed for mobile code and dynamic JIT&#39;ing, and with a lot of industrial support. It might be an interesting time to consider projects (even &#34;static&#34; ones) that take a slightly more nuanced view of the code/data relationship, the program/metaprogram/compiler relationship, and make the whole compilation model a little more .. pliant (yes that was a Pliant reference and if you remember what that was, congratulations you&#39;ve been on the internet too long, here&#39;s your TUNES badge of merit).The rest...I had some extended notes here about &#34;less-mainstream paradigms&#34; and/or &#34;things I wouldn&#39;t even recommend pursuing&#34;, but on reflection, I think it&#39;s kinda a bummer to draw too much attention to them. So I&#39;ll just leave it at a short list: actors, software transactional memory, lazy evaluation, backtracking, memoizing, &#34;graphical&#34; and/or two-dimensional languages, and user-extensible syntax. If someone&#39;s considering basing a language on those, I&#39;d .. somewhat warn against it. Not because I didn&#39;t want them to work -- heck, I&#39;ve tried to make a few work quite hard! -- but in practice, the cost:benefit ratio doesn&#39;t seem to turn out really well. Or hasn&#39;t when I&#39;ve tried, or in (most) languages I&#39;ve seen. But who knows? There are always exceptions, and I&#39;m wrong far more often than I&#39;m right, so please don&#39;t let me stop you if your heart is set on it!</description>
      <pubDate>15 Feb 22 17:17 EST</pubDate>
      <guid>https://graydon2.dreamwidth.org/253769.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.smashingmagazine.com/2022/02/thoughts-on-markdown/</link>
      <description>&lt;a href=&#34;https://www.smashingmagazine.com/2022/02/thoughts-on-markdown/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 30 min read Tools, Coding, Workflow, Opinion Column Quick summary ↬ Markdown in all its flavors, interpretations, and forks won’t go away. However, it’s important to look at emerging content formats that try to encompass modern needs. In this article, Knut shares his advice against Markdown by looking back on why it was introduced in the first place, and by going through some of the major developments of content on the web. Markdown is second nature for many of us. Looking back, I remember starting typing in Markdown not long after John Gruber released his first Perl-based parser back in 2004 after collaborating on the language with Aaron Swartz. Markdown’s syntax is intended for one purpose: to be used as a format for writing for the web.— John Gruber That’s almost 20 years ago — yikes! What started as a more writer- and reader-friendly syntax for HTML has become a darling for how to write and store technical prose for programmers and tech-savvy people. Markdown is a signifier for the developer and text-tinkerer culture. But since its introduction, the world of digital content has also changed. While Markdown is still fine for some things, I don’t believe it’s should be the go-to for content anymore. There are two main reasons for this: Markdown wasn’t designed to meet today’s needs of content. Markdown holds editorial experience back. Of course, this stance is influenced by working for a platform for structured content. At Sanity.io, we spend most of our days thinking about how content as data unlocks a lot of value, and we spend a lot of time thinking deeply about editor experiences, and how to save people time, and make working with digital content delightful. So, there’s skin in the game, but I hope I’m able to portray that even though I’ll argue against Markdown as the go-to format for content, I still have a deep appreciation for its significance, application, and legacy. Before my current gig, I worked as a technology consultant at an agency where we had to literally fight CMSes that locked our client’s content down by embedding it in presentation and complex data models (yes, even the open-source ones). I have observed people struggle with Markdown syntax, and be demotivated in their jobs as editors and content creators. We have spent hours (and client’s money) on building custom tag-renderers that were never used because people don’t have time or motivation to use the syntax. Even I, when highly motivated, have given up contributing to open-source documentation because the component-based Markdown implementation introduced too much friction. But I also see the other side of the coin. Markdown comes with an impressive ecosystem and from a developer’s standpoint, there is an elegant simplicity to plain-text files and easy-to-parse syntax for people who are used to reading code. I once spent days building an impressive MultiMarkdown-&gt;LaTeX-&gt;real-time-PDF-preview-pipeline in Sublime Text for my academic writing. And it makes sense that a README.md file can be opened and edited in a code editor and rendered nicely on GitHub. There’s little doubt that Markdown brings convenience for developers in some use cases. That is also why I want to build my advice against Markdown by looking back on why it was introduced in the first place, and by going through some of the major developments of content on the web. For many of us, I suspect Markdown is something we just take for granted as a “thing that exists.” But all technology has a history and is a product of human interaction. This is important to remember when you, the reader, develop technology for others to use. Flavors And Specifications Markdown was designed to make it easier for web writers to work with articles in an age where web publishing required writing HTML. So, the intent was to make it simpler to interface with text formatting in HTML. It wasn’t the first simplified syntax on the planet, but it was the one that gained the most traction over the years. Today, the usage of Markdown has grown far beyond its design intent to be a simpler way to read and write HTML, to become an approach of marking up plain text in a lot of different contexts. Sure, technologies and ideas can evolve beyond their intent, but the tension in today’s use of Markdown can be traced to this origin and the constraints put into its design. For those who aren’t familiar with the syntax, take the following HTML content: &lt;p&gt;The &lt;a href=”https://daringfireball.net/projects/markdown/syntax#philosophy”&gt;Markdown syntax&lt;/a&gt; is designed to be &lt;em&gt;easy-to-read&lt;/em&gt; and &lt;em&gt;easy-to.write&lt;/em&gt;.&lt;/p&gt; With Markdown, you can express the same formatting as: The [Markdown syntax](https://daringfireball.net/projects/markdown/syntax#philosophy) is designed to be _easy-to-read_ and _easy-to-write_. It’s like a law of nature that technology adoption comes with the pressure to evolve and add features to it. Markdown’s increasing popularity meant that people wanted to adapt it for their use cases. They wanted more features like support for footnotes and tables. The original implementation came with an opinionated stance, which at the time were reasonable for what the design intent was: For any markup that is not covered by Markdown’s syntax, you simply use HTML itself. There’s no need to preface it or delimit it to indicate that you’re switching from Markdown to HTML; you just use the tags.— John Gruber In other words, if you want a table, then use &lt;table&gt;&lt;/table&gt;. You’ll find that this is still the case for the original implementation. One of Markdown’s spiritual successors, MDX, has taken the same principle but extended it to JSX, a JS-based templating language. From Markdown To Markdown? It can look like Markdown’s appeal for many wasn’t so much its tie-in to HTML, but the ergonomics of plaintext and simple syntax for formatting. Some content creators wanted to use Markdown for other use cases than simple articles on the web. Implementations like MultiMarkdown introduced affordances for academic writers who wanted to use plain text files but needed more features. Soon you would have a range of writing apps that accepted Markdown syntax, without necessarily turning it into HTML or even using the markdown syntax as a storage format. In a lot of apps, you’ll find editors that give you a limited set of formatting options, and some of them are more “inspired” by the original syntax. In fact, one of the feedbacks I got on a draft of this article was that by now, “Markdown” should be lower-cased, since it has become so common, and to make it distinct from the original implementation. Because what we recognize as markdown has also become very diverse. CommonMark: An Attempt To Tame Markdown Like ice cream, Markdown comes in a lot of flavors, some more popular than others. When people started to fork the original implementation and add features to it, two things happened: It became more unpredictable what you as a writer could and couldn’t do with Markdown. Software developers had to make decisions of what implementation to adopt for their software. The original implementation also contained some inconsistencies that added friction for people who wanted to use it programmatically. This started conversations about formalizing Markdown into a specification proper. Something that Gruber resisted, and still does, interestingly, because he recognized that people wanted to use Markdown for different purposes and “No one syntax would make all happy.” It’s an interesting stance considering that Markdown translates to HTML, which is a specification that evolves to accommodate different needs. Even though the original implementation of Markdown is covered by a “BSD-like” license, it also reads “Neither the name Markdown nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.” We can safely assume that most products that use “Markdown” as part of their marketing materials haven’t acquired this written permission. The most successful attempt to bring Markdown into a shared specification is what is today known as CommonMark. It was headed by Jeff Atwood (known for co-founding Stack Overflow and Discourse) and John McFarlane (a professor of philosophy at Berkely who’s behind Babelmark and pandoc). They initially launched it as “Standard Markdown,” but changed it to “CommonMark” after receiving criticism from Gruber. Whose stance was consistent, the intent of Markdown is to be a simple authoring syntax that translates to HTML: @davewiner And that’s what’s flawed with CommonMark. They want to make things easier for programmers as a primary goal. They miss the point.— John Gruber (@gruber) September 8, 2014 I think this also marked the point where Markdown had entered the public domain. Even though CommonMark isn’t branded as “Markdown,” (as per licensing) this specification is recognized and referred to as “markdown”. Today, you’ll find CommonMark as the underlying implementation for software like Discourse, GitHub, GitLab, Reddit, Qt, Stack Overflow, and Swift. Projects like unified.js bridges syntaxes by translating them into Abstract Syntax Trees, also rely on CommonMark for their markdown support. CommonMark has brought a lot of unification around how markdown is implemented, and in a lot of ways has made it simpler for programmers to integrate markdown support in software. But it hasn’t brought the same unification to how markdown is written and used. Take GitHub Flavored Markdown (GFM). It’s based on CommonMark but extends it with more features (like tables, task lists, and strikethrough). Reddit describes its “Reddit Flavored Markdown” as “a variation of GFM,” and introduces features like syntax for marking up spoilers. I think we can safely conclude that both the group behind CommonMark and Gruber were right: it certainly helps with shared specifications, but yes, people want to use Markdown for different specific things. Markdown As A Formatting Shortcut Gruber resisted formalizing Markdown into a shared specification because he assumed it would make it less a tool for writers and more a tool for programmers. We have already seen that even with the broad adoption of a specification, we don’t automatically get a syntax that predictably works the same across different contexts. And specifications like CommonMark, popular as it is, also have limited success. An obvious example is Slack’s markdown implementation (called mrkdown) that translates *this* to strong/bold, and not emphasis/italic, and doesn’t support the [link](https://slack.com) syntax, but uses &lt;link|https://slack.com&gt; instead. You’ll also find that you can use Markdown-like syntax to initialize formatting in rich text editors in software like Notion, Dropbox Paper, Craft, and to a degree, Google Docs (e.g. asterisk + space on a new line will transform to a bulleted list). What’s supported and what’s translated to what varies. So, you can’t necessarily take your muscle memory with you across these applications. For some people, this is fine, and they can adapt. For others, this is a papercut and it keeps them from using these features. Which asks the question, who was Markdown designed for, and who are its users today? More after jump! Continue reading below ↓ Who Are The Users Of Markdown Supposed To Be? We have seen markdown exist in a tension between different use cases, audiences, and notions of whom its users are. What started as a markup language for HTML-proficient web writers specifically, became a darling for developer types. In 2014, web writers started to move away from moving files through parsers in Perl and FTP. Content Management Systems (CMSs) like WordPress, Drupal, and Moveable Type (which I believe Gruber still uses) had steadily grown to become the go-to tools for web publishing. They offered affordances like rich text editors that web writers could use in their browsers. These rich text editors still assumed HTML and Markdown as the underlying rich text syntax, but they took away some of the cognitive overhead by adding buttons to insert this syntax in the editor. And increasingly, writers weren’t and didn’t have to be versed in HTML. I bet if you did web development with CMSs in the 2010s, you probably had to deal with “junk HTML” that came through these editors when people pasted directly from Word. Today, I will argue that Markdown’s primary users are developers and people who are interested in code. It’s not a coincidence that Slack made the WYSIWYG the default input mode once their software was used by more people outside of technical departments. And the fact that this was a controversial decision, so much that they had to bring it back as an option, shows how deep the love for markdown is in the developer community. There wasn’t much celebration of Slack trying to make it easier and more accessible for everyone. And this is the crux of the matter. (Large preview) The Ideology Of Markdown The fact that markdown has become the lingua franca writing style, and what most website frameworks cater to, is also the main reason I’ve been a bit skittish about publishing this. It’s often talked about as an inherent and undeniable good. Markdown has become a hallmark of being developer-friendly. Smart and skilled people have sunk a lot of collective hours in enabling markdown in all sorts of contexts. So, challenging its hegemony will surely annoy some. But hopefully, it can spawn some fruitful discussion about a thing that’s often taken for granted. My impression is that the developer friendliness that people relate to Markdown has mostly to do with 3 factors: The comfortable abstraction of a plain text file. There is an ecosystem of tooling. You can keep your content close to your development workflow. I’m not saying that these stances are wrong, but I’ll suggest that they come with trade-offs and some unreasonable assumptions. The Simple Mental Model Of A Plain Text File Databases are amazing things. But they have also had an earned reputation of being hard and inaccessible for frontend developers. I’ve known a lot of great developers who shy away from backend code and databases, because they represent complexity they don’t want to spend time on. Even with WordPress, which does a lot out of the box to keep you from having to deal with its database after setup, it was overhead of getting up and running. Plain text files, however, are more tangible and are fairly simple to reason about (as long as you’re used to file management that is). Especially compared to a system that will break your content into multiple tables in a relational database with some proprietary structure. For limited use cases, like blog posts of simple rich text with images and links, markdown will get the job done. You can copy the file and stick it in a folder or check it into git. The content feels yours because of the tangibility of files. Even if they’re hosted on GitHub, which is a for-profit Software as a Service owned by Microsoft, and thus covered by their terms of service. In the era where you actually had to spin up a local database to get your local development going and deal with syncing it with remote, the appeal of plain text files is understandable. But that era is pretty much gone with the emergence of backends as a service. Services and tools like Fauna, Firestore, Hasura, Prisma, PlanetScale, and Sanity’s Content Lake, invest heavily in developer experience. Even operating traditional databases on local development has become less of a hassle compared to just 10 years ago. If you think about it, do you own your content less if it’s hosted in a database? And hasn’t the developer experience of dealing with databases become significantly simpler with the advent of SaaS tools? And is it fair to say that proprietary database technology impinges on the portability of your content? Today you can launch what’s essentially a Postgres database with no sysadmin skills, make your tables and columns, put your content inside of it, and at any time export it as a .sql dump. The portability of content has much more to do with how you structure that content in the first place. Take WordPress, it’s fully open-source, you can host your own DB. It even has a standardized export format in XML. But anyone who has tried to move out of a mature WordPress install knows how little this helps if you’re trying to get away from WordPress. A Vast Ecosystem… For Developers We already touched on the vast markdown ecosystem. If you look at contemporary website frameworks, most of them assume markdown as a primary content format, some of them, the only format. For example, Hugo, the static site generator used by Smashing Magazine, still requires markdown files for paginated publishing. Meaning that if Smashing Magazine wants to use a CMS to store articles, it has to interact with markdown files, or convert all the content to markdown files. If you look in the documentation for Next.js, Nuxt.js, VuePress, Gatsby.js, and so on, markdown will figure prominently. It’s also the default syntax for README-files on GitHub, which also uses it for formatting in Pull Request notes and comments. There are some honorable mentions of initiatives to bring the ergonomics of markdown to the masses. Netlify CMS and TinaCMS (the spiritual descendant of Forestry) will give you user interfaces where the markdown syntax is mostly abstracted away for editors. You will commonly find that markdown-based editors in CMSes give you preview functionality for the formatting. Some editors, like Notion’s, will let you paste markdown syntax, and they will translate it to their native formatting. But I think it’s safe to say, that the energy that has gone to innovate for markdown hasn’t favored people who aren’t into writing its syntax. It hasn’t trickled up the stack, as it were. Content Workflows Or Developer Workflows? For a developer who makes their blog, using markdown files reduces some of the overhead of getting it up and running, since frameworks often come with built-in parsing or commonly offer it as part of starter code. And there is nothing extra to sign up for. You can use git to commit these files alongside your code. If you are comfortable with git diffs, you’ll even have revision control like you’re used to with programming. In other words, since markdown files are in plain text, they can be integrated with your developer workflow. But beyond this, the developer experience soon gets more complex. And you end up compromising on your team’s user experience as content creators, and our own developer experience being stuck with markdown to solve problems that are way beyond its design intent. Yes, it might be cool if you get your content team to use git and check in their changes, but at the same time, is this the best use of their time? Do you really want your editors to bump against merge conflicts or how to rebase branches? Git is hard enough for developers who use it every day. And does this setup really represent the best workflow for people who are primarily working with content? Isn’t this a case where developer experience has trumped editor experience, and isn’t the cost, the time and effort that could go into making something better for users? Because the expectations and needs from content and editing environments have evolved, I don’t think markdown will do it for us. I don’t see how some of the developer ergonomics end up favoring non-developers, and I think even for developers, markdown is holding our own content creation and needs back. Because content on the web has significantly changed since the early 2000s. From Paragraphs To Blocks Markdown has always had the option of opting out to HTML if you wanted more complex things. This worked well when the author was also the webmaster, or at least knew HTML. It also worked well because websites usually were mostly HTML and CSS. The way you designed websites was mostly by creating whole page layouts. You could transform Markdown to the HTML markup and put it up alongside your style.css file. Of course, we had CMSes and static site generators in the 2000s too, but they mostly worked the same, by inserting the HTML content inside of templates without any passing of “props” between the components. But most of us don’t really author HTML like in the old days anymore. Content on the web has evolved from mostly being articles with simple rich text formatting to composed multimedia and specialized components often with user interactivity (which is a fancy way of saying “newsletter signup call to actions”). From Articles To Apps In the early 2010s, Web 2.0 was in its heyday, and Software as a Service-companies began to use the web for data-heavy applications. HTML, CSS, and JavaScript were increasingly used to drive interactive UIs. Twitter open-sourced Bootstrap, their framework for building more consistent and resilient user interfaces. This drove what we can call the “componentization” of web design. It shifted the way we build for the web in a fundamental way. The various CSS frameworks that emerged in this era (e.g. Bootstrap and Foundation) tended to use standardized class names and assumed specific HTML structures to make it less hard to make resilient and responsive user interfaces. With the web design philosophy of Atomic Design and class-name conventions like Block-Element-Modifier (BEM) the default was shifted from thinking page-layout first, to seeing pages as a collection of repeatable and compatible design elements. Whatever content you have inside of markdown is not compatible with this. Unless you down the rabbit hole of interjecting the markdown parsers, and tweaked it to output the syntax you wanted (more on this later). No wonder, Markdown was designed to be simple rich text articles of native HTML elements that you would target with a stylesheet. This is still an issue for people who use Markdown to drive content for their sites. The Embeddable Web But something also happened to our content as well. Not only could we start finding it outside of the semantic &lt;article&gt; HTML-tags, but it started to contain more… stuff. A lot of our content moved out from our LiveJournals and blogs and into social media: Facebook, Twitter, tumblr, YouTube. To get the snippets of content back into our articles, we needed to be able to embed them. The HTML convention started using the &lt;iframe&gt; tag to channel the video player from YouTube or even insert a tweet-box in between your paragraphs of text. Some systems started abstracting this into “short-codes”, most often brackets containing some keyword to identify what block of content it should represent, and some key-value attributes. For example, dev.to have enabled syntax from the templating language liquid to be inserted into their Markdown editor: {% youtube dQw4w9WgXcQ %} Of course, this requires you to use a customized Markdown parser, and have special logic to make sure the right HTML was inserted when the syntax was turned into HTML. And your content creators will have to remember these codes (unless there was some kind of toolbar to automatically insert them). And if a bracket gets deleted or messed up, that might break the site. But what about MDX? An attempt to solve the need for block content is MDX, presented with the tagline “Markdown for the component era.” MDX lets you use the JSX templating language, as well as JavaScript, interlaced in markdown syntax. There is a lot of impressive engineering in the community around MDX, including Unified.js, which specializes in parsing various syntaxes into Abstract Syntax Trees (ASTs), so that they are more accessible to be used programmatically. Note, that the standardization of markdown would make the work for the folks behind Unified.js and its users simpler, because there are fewer edge cases to cater for. MDX certainly brings better developer experience in integrating components into Markdown. But it doesn’t bring better editor experience, because it adds a lot of cognitive overhead to content production and editing: import {Chart} from &#39;./snowfall.js&#39; export const year = 2018 # Last year’s snowfall In {year}, the snowfall was above average. It was followed by a warm spring which caused flood conditions in many of the nearby rivers. &lt;Chart year={year} color=&#34;#fcb32c&#34; /&gt; The amount of assumed knowledge just for this simple example is substantial. You need to know about ES6 modules, JavaScript variables, JSX templating syntax, and how to use props, hex codes, and data types, and you need to be familiar with what components you can use, and how to use them. And you need to type it correctly and in an environment that gives you some kind of feedback. I have no doubt that there will be more accessible authoring tools on top of MDX, it feels like solving for something that doesn’t need to be a problem in the first place. Unless you are extremely diligent in how you compose and name your MDX components, it also ties your content to a specific presentation. Just take the example above brought from the MDX front page. You’ll find a hard-coded color hex for the chart. When you redesign your site, that color might not be compatible with your new design system. Of course, there’s nothing keeping you from abstracting this and using the prop color=”primary”, but there’s also nothing in the tool that nudges you to make wise decisions like this. Embedding specific presentation concerns in your content has increasingly become a liability and something that will get in the way of adapting, iterating, and moving quickly with your content. It locks it down in ways that are much more subtle than having content in a database. You risk ending up in the same place as moving out of a mature WordPress install with plugins. It is cumbersome to unmix structure and presentation. The Demand For Structured Content With more complex sites and user journeys, we also see the need to present the same pieces of content throughout a website. If you’re running an e-commerce site, you want to embed product information in many places outside a single product page. If you run a modern marketing site, you want to be able to share the same copy across multiple personalized views. To do this efficiently and reliable you will need to adapt structured content. That means your content needs to be embedded with metadata and chunked up in ways that make it possible to parse for intent. If a developer just sees “page” with “content,” that makes it very difficult to include the right things in the right places. If they can get to all “product descriptions” with an API or a query, that makes everything easier. With markdown, you’re limited to expressing taxonomies and structured content either to some sort of folder organization (making it hard to put the same piece of content in multiple taxonomies) or you need to augment the syntax with something else. Jekyll, an early Static Site Generator (SSG) built for markdown files, introduced “Front Matter” as a way to add metadata to posts using YAML (a simple key-value format that uses spaces to create scope) between three dashes at the top of the file. So, now you’ll have two syntaxes to deal with. YAML also has a reputation for being mischievous (especially if you’re from Norway). Nevertheless, other SSGs have adopted this convention, as well as git-based CMSes that use markdown as their content format. When you have to add additional syntax to your plain files to get some of the affordances of structured content, you may start to wonder if it’s really worth it. And who the format is for and who it excludes. If you think about it, a lot of what we do on the web is not only consuming content, we’re creating it! I’m currently writing this lengthy article in an advanced word processor in my browser. There’s a growing expectation that you should also be able to author block content in modern content applications. People have started to get used to delightful user experiences that works and looks nice, and where you aren’t expected to have to learn specialized syntax. Medium popularized the notion that you could have delightful and intuitive content creation on the web. And speaking of “notion”, the popular note app has gone all in on block content, and lets users mix max from a wide range of different types. Most of these blocks goes beyond markdown, and the native elements of HTML. (Large preview) It’s notable that Notion, describing their process to make their content accessible through their highly anticipated API, makes a point out of chosing their content format, that: Documents from one Markdown editor will often parse and render differently in another application. The inconsistency tends to be manageable for simple documents, but it’s a big problem for Notion’s rich library of blocks and inline formatting options, many of which are simply not supported in any widely-used Markdown implementation. Notion went with a JSON based format that let them express as structured data. Their argument is that it makes it easier and more predictable to interact with for developers who want to build their own presentation of the block content that comes out of Notion’s APIs. If Not Markdown, Then What? I suspect that the prominence of Markdown has held back innovation and progress for digital content. So, when I argue that we should stop choosing it as a primary way to store content, it’s hard to give a straight answer to what should replace it. What we do know, however, is what we should expect from modern content formats and authoring tools. Let’s Invest In Accessible Authoring Experiences Using markdown requires you to learn syntax, and often multiple syntaxes and bespoke tags to be practical with modern expectations. Today, that feels like a completely unnecessary expectation to put on most people. I wish we could direct more energy into making accessible and delightful editorial experiences that produces modern portable content formats. Even though it’s notoriously difficult to build great block content editors, there are a couple of viable options out there that can be extended and customized for your use case (for example Slate.js, Quill.js, or Prosemirror). Then again, investing in the communities around these tools might also help their development further. Increasingly, people will expect authoring tools to be accessible, real-time, and collaborative. Why should one have to push a save button on the web in 2021? Why shouldn’t it be possible to make a change in a document without risking a race condition, because your colleague happened to have the document open in a tab? Should we expect authors to have to deal with merge conflicts? And shouldn’t we make it easy for content creators to work with structured content with visual affordances that make sense? To be a bit polemical: the last decade’s innovations in reactive JavaScript frameworks and UI components are perfect for creating awesome authoring tools. Instead of using them to transpile Markdown to HTML and into an abstract syntax tree to then integrate it in a JavaScript template language that outputs HTML. Block Content Should Follow A Specification I haven’t mentioned WYSIWYG editors for HTML. Because they are the wrong thing. Modern block content editors should preferably interoperate with a specified format. The aforementioned editors do at least have a sensible internal document model that can be transformed into something more portable. If you look at the content management system landscape, you start to see various JSON-based block content formats emerge. Some of them are still tied to HTML assumptions or overly concerned with character positions. And none of them aren’t really offered as a generic specification. At Sanity.io, we decided early that the block content format should never assume HTML as neither input nor output, and that we could use algorithms to synchronize text strings. More importantly, was it that block content and rich text should be deeply typed and queryable. The result was the open specification Portable Text. Its structure not only makes it flexible enough to accommodate custom data structures as blocks and inline spans; it’s also fully queryable with open-source query languages like GROQ. Portable Text isn’t design to be written or be easily readable in its raw form; it’s designed to be produced by an user interface, manipulated by code, and to be serialized and rendered where ever it needs to go. For example, you can use it to express content for voice assistants. { &#34;style&#34;: &#34;normal&#34;, &#34;_type&#34;: &#34;block&#34;, &#34;children&#34;: [ { &#34;_type&#34;: &#34;span&#34;, &#34;marks&#34;: [&#34;a-key&#34;, &#34;emphasis&#34;], &#34;text&#34;: &#34;some text&#34; } ], &#34;markDefs&#34;: [ { &#34;_key&#34;: &#34;a-key&#34;, &#34;_type&#34;: &#34;markType&#34;, &#34;extraData&#34;: &#34;some data&#34; } ] } An interesting side-effect of turning block content into structured data is exactly that: It becomes data! And data can be queried and processed. That can be highly useful and practical, and it lets you ask your content repository questions that would be otherwise harder and more errorprone in formats like Markdown. For example, if I for some reason wanted to know what programming languages we’ve covered in examples on Sanity’s blog, that’s within reach with a short query. You can imagine how trivial it is to build specialized tools and views on top of this that can be helpful for content editors: distinct( *[&#34;code&#34; in body[]._type] .body[_type == &#34;code&#34;] .language ) // output [ &#34;text&#34;, &#34;javascript&#34;, &#34;json&#34;, &#34;html&#34;, &#34;markdown&#34;, &#34;sh&#34;, &#34;groq&#34;, &#34;jsx&#34;, &#34;bash&#34;, &#34;css&#34;, &#34;typescript&#34;, &#34;tsx&#34;, &#34;scss&#34; ] Example: Get a distinct list of all programming languages that you have code blocks of. Portable Text is also serializable, meaning that you can recursively loop through it, and make an API that exposes its nodes in callback functions mapped to block types, marked-up spans, and so on. We have spent the last years learning a lot about how it works and how it can be improved, and plan to take it to 1.0 in the near future. The next step is to offer an editor experience outside of Sanity Studio. As we have learned from Markdown, the design intent is important. Of course, whatever the alternative to markdown is, it doesn’t need to be Portable Text, but it needs to be portable text. And it needs to share a lot of its characteristics. There have been a couple of other JSON-based block content format popping up the last few years, but a lot of them seem to bring with them a lot of “HTMLism.” The convenience is understandable, since a lot of content still ends up on the web serialized into HTML, but the convenience limits the portability and the potential for reuse. You can disregard my short pitch for something we made at Sanity, as long as you embrace the idea of structured content and formats that let you move between systems in a fundamental manner. For example, a goal for Portable Text will be improved compatibility with Unified.js, so it’s easier to travel between formats. Embracing The Legacy Of Markdown Markdown in all its flavors, interpretations, and forks won’t go away. I suspect that plain text files will always have a place in developers’ note apps, blogs, docs, and digital gardens. As a writer who has used markdown for almost two decades, I’ve become accustomed to “markdown shortcuts” that are available in many rich text editors and am frequently stumped from Google Docs’ lack of markdownisms. But I’m not sure if the next generation of content creators and even developers will be as bought in on markdown, and nor should they have to be. I also think that markdown captured a culture of savvy tinkerers who love text, markup, and automation. I’d love to see that creative energy expand and move into collectively figuring out how we can make better and more accessible block content editors, and building out an ecosystem around specifications that can express block content that’s agnostic to HTML. Structured data formats for block content might not have the same plain text ergonomics, but they are highly “tinkerable” and open for a lot of creativity of expression and authoring. If you are a developer, product owner, or a decision-maker, I really want you to be circumspect of how you want to store and format your content going forward. If you’re going for markdown, at least consider the following trade-offs: Markdown is not great for the developer experience in modern stacks: It can be a hassle to parse and validate, even with great tooling. Even if you adopt CommonMark, you aren’t guaranteed compatibility with tooling or people’s expectations. It’s not great for structured content, YAML frontmatter only takes you so far. Markdown is not great for editorial experience: Most content creators don’t want to learn syntax, their time is better spent on other things. Most markdown systems are brittle, especially when people get syntax wrong (which they will). It’s hard to accommodate great collaborative user experiences for block content on top of markdown. Markdown is not great in block content age, and shouldn’t be forced into it. Block content needs to: Be untangled from HTMLisms and presentation agnostic. Accommodate structured content, so it can be easily used wherever it needs to be used. Have stable specification(s), so it’s possible to build on. Support real-time collaborative systems. What’s common for people like me who challenge the prevalence of markdown, and those who are really into the simple way of expressing text formating is an appreciation of how we transcribe intent into code. That’s where I think we can all meet. But I do think it’s time to look at the landscape and the emerging content formats that try to encompass modern needs, and ask how we can make sure that we build something that truly caters to editorial experience, and that can speak to developer experience as well. I want to express my gratitude to Titus Wormer (@wooorm) for his insightful feedback on my first draft of this post, and for the great work he and the Unified.js team have done for the web community. (vf, yk, il) </description>
      <pubDate>21 Feb 22 10:54 EST</pubDate>
      <guid>https://www.smashingmagazine.com/2022/02/thoughts-on-markdown/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.amazon.com/Anthony-Thomas-Chocolate-Deliciously-Delightful/dp/B076B15VRJ/ref=sr_1_2?crid=1V9DVYCKTSDNI&amp;dchild=1&amp;keywords=ohio+state+buckeyes+chocolate&amp;qid=1607894720&amp;sprefix=ohio+state+buckeyes+choc%2Caps%2C176&amp;sr=8-2</link>
      <description>&lt;a href=&#34;https://www.amazon.com/Anthony-Thomas-Chocolate-Deliciously-Delightful/dp/B076B15VRJ/ref=sr_1_2?crid=1V9DVYCKTSDNI&amp;dchild=1&amp;keywords=ohio+state+buckeyes+chocolate&amp;qid=1607894720&amp;sprefix=ohio+state+buckeyes+choc%2Caps%2C176&amp;sr=8-2&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; FREE delivery Friday, March 4 if you spend $25 on items shipped by Amazon Or fastest delivery Tomorrow, February 27. Order within 2 hrs 29 mins [{&#34;displayPrice&#34;:&#34;$24.99&#34;,&#34;priceAmount&#34;:24.99,&#34;currencySymbol&#34;:&#34;$&#34;,&#34;integerValue&#34;:&#34;24&#34;,&#34;decimalSeparator&#34;:&#34;.&#34;,&#34;fractionalValue&#34;:&#34;99&#34;,&#34;symbolPosition&#34;:&#34;left&#34;,&#34;hasSpace&#34;:false,&#34;showFractionalPartIfEmpty&#34;:true,&#34;offerListingId&#34;:&#34;5%2FNlynYtH80y1DdtU00bo3H%2FHsM2Ho12Uya1%2FDPJg1VaKrmhcbUl8QM6KQCbr%2BlwmMCNFFdgF%2BMYWDXtym8S4niYP7mQoBDlrS7tgR8oU5quqN96pcb%2FA2oTFgWERawQzuezj1RAhJoeDD6Imgzi%2FA6JjvLuVXJ2nxRJEUj7DEnX3yWj0RX2aydU3%2F5gquXA&#34;,&#34;locale&#34;:&#34;en-US&#34;,&#34;buyingOptionType&#34;:&#34;NEW&#34;},{&#34;displayPrice&#34;:&#34;$22.49&#34;,&#34;priceAmount&#34;:22.49,&#34;currencySymbol&#34;:&#34;$&#34;,&#34;integerValue&#34;:&#34;22&#34;,&#34;decimalSeparator&#34;:&#34;.&#34;,&#34;fractionalValue&#34;:&#34;49&#34;,&#34;symbolPosition&#34;:&#34;left&#34;,&#34;hasSpace&#34;:false,&#34;showFractionalPartIfEmpty&#34;:true,&#34;offerListingId&#34;:null,&#34;locale&#34;:&#34;en-US&#34;,&#34;buyingOptionType&#34;:&#34;SNS&#34;}] $$24.99 () Includes selected options. Includes initial monthly payment and selected options. Details Initial payment breakdown Shipping cost, delivery date, and order total (including tax) shown at checkout. Enhancements you chose aren&#39;t available for this seller. Details To add the following enhancements to your purchase, choose a different seller. %cardName% ${cardName} not available for the seller you chose Your transaction is secure We work hard to protect your security and privacy. Our payment security system encrypts your information during transmission. We don’t share your credit card details with third-party sellers, and we don’t sell your information to others. Learn more Anthony Thomas, Award Win... has been added to your Cart Add a gift receipt for easy returns </description>
      <pubDate>13 Dec 20 16:45 EST</pubDate>
      <guid>https://www.amazon.com/Anthony-Thomas-Chocolate-Deliciously-Delightful/dp/B076B15VRJ/ref=sr_1_2?crid=1V9DVYCKTSDNI&amp;dchild=1&amp;keywords=ohio+state+buckeyes+chocolate&amp;qid=1607894720&amp;sprefix=ohio+state+buckeyes+choc%2Caps%2C176&amp;sr=8-2</guid>
    </item>
    <item>
      <title>The Fantasy of Opting Out</title>
      <link>https://thereader.mitpress.mit.edu/the-fantasy-of-opting-out/</link>
      <description>&lt;a href=&#34;https://thereader.mitpress.mit.edu/the-fantasy-of-opting-out/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Those who know about us have power over us. Obfuscation may be our best digital weapon.There are still ways to carve out spaces of resistance, counterargument, and autonomy. Source image: Lianhao Qu, via UnsplashConsider a day in the life of a fairly ordinary person in a large city in a stable, democratically governed country. She is not in prison or institutionalized, nor is she a dissident or an enemy of the state, yet she lives in a condition of permanent and total surveillance unprecedented in its precision and intimacy.As soon as she leaves her apartment, she is on camera: while in the hallway and the elevator of her building, when using the ATM outside her bank, while passing shops and waiting at crosswalks, while in the subway station and on the train — and all that before lunch. A montage of nearly every move of her life in the city outside her apartment could be assembled, and each step accounted for. But that montage would hardly be necessary: Her mobile phone, in the course of its ordinary operation of seeking base stations and antennas to keep her connected as she walks, provides a constant log of her position and movements. Her apps are keeping tabs, too.Any time she spends in “dead zones” without phone reception can also be accounted for: Her subway pass logs her entry into the subway, and her radio-frequency identification badge produces a record of her entry into the building in which she works. (If she drives a car, her electronic toll-collection pass serves a similar purpose, as does automatic license-plate imaging.) If her apartment is part of a smart-grid program, spikes in her electricity usage can reveal exactly when she is up and around, turning on lights and ventilation fans and using the microwave oven and the coffee maker.Surely some of the fault must lie with this individual for using services or engaging with institutions that offer unfavorable terms of service and are known to misbehave. Isn’t putting all the blame on government institutions and private services unfair, when they are trying to maintain security and capture some of the valuable data produced by their users? Can’t we users just opt out of systems with which we disagree?Before we return to the question of opting out, consider how thoroughly the systems mentioned are embedded in our hypothetical ordinary person’s everyday life, far more invasively than mere logs of her daily comings and goings. Someone observing her could assemble in forensic detail her social and familial connections, her struggles and interests, and her beliefs and commitments. From Amazon purchases and Kindle highlights, from purchase records linked with her loyalty cards at the drugstore and the supermarket, from Gmail metadata and chat logs, from search history and checkout records from the public library, from Netflix-streamed movies, and from activity on Facebook and Twitter, dating sites, and other social networks, a very specific and personal narrative is clear.If the apparatus of total surveillance that we have described here were deliberate, centralized, and explicit, a Big Brother machine toggling between cameras, it would demand revolt, and we could conceive of a life outside the totalitarian microscope.If the apparatus of total surveillance that we have described here were deliberate, centralized, and explicit, a Big Brother machine toggling between cameras, it would demand revolt, and we could conceive of a life outside the totalitarian microscope. But if we are nearly as observed and documented as any person in history, our situation is a prison that, although it has no walls, bars, or wardens, is difficult to escape.Which brings us back to the problem of “opting out.” For all the dramatic language about prisons and panopticons, the sorts of data collection we describe here are, in democratic countries, still theoretically voluntary. But the costs of refusal are high and getting higher: A life lived in social isolation means living far from centers of business and commerce, without access to many forms of credit, insurance, or other significant financial instruments, not to mention the minor inconveniences and disadvantages — long waits at road toll cash lines, higher prices at grocery stores, inferior seating on airline flights.It isn’t possible for everyone to live on principle; as a practical matter, many of us must make compromises in asymmetrical relationships, without the control or consent for which we might wish. In those situations — everyday 21st-century life — there are still ways to carve out spaces of resistance, counterargument, and autonomy.We are surrounded by examples of obfuscation that we do not yet think of under that name. Lawyers engage in overdisclosure, sending mountains of vaguely related client documents in hopes of burying a pertinent detail. Teenagers on social media — surveilled by their parents — will conceal a meaningful communication to a friend in a throwaway line or a song title surrounded by banal chatter. Literature and history provide many instances of “collective names,” where a population took a single identifier to make attributing any action or identity to a particular person impossible, from the fictional “I am Spartacus” to the real “Poor Conrad” and “Captain Swing” in prior centuries — and “Anonymous,” of course, in ours.We can apply obfuscation in our own lives by using practices and technologies that make use of it, including:The secure browser Tor, which (among other anti-surveillance technologies) muddles our Internet activity with that of other Tor users, concealing our trail in that of many others.The browser plugins TrackMeNot and AdNauseam, which explore obfuscation techniques by issuing many fake search requests and loading and clicking every ad, respectively.The browser extension Go Rando, which randomly chooses your emotional “reactions” on Facebook, interfering with their emotional profiling and analysis.Playful experiments like Adam Harvey’s “HyperFace” project, finding patterns on textiles that fool facial recognition systems – not by hiding your face, but by creating the illusion of many faces.If obfuscation has an emblematic animal, it is the family of orb-weaving spiders, Cyclosa mulmeinensis, which fill their webs with decoys of themselves. The decoys are far from perfect copies, but when a wasp strikes they work well enough to give the orb-weaver a second or two to scramble to safety. At its most abstract, obfuscation is the production of noise modeled on an existing signal in order to make a collection of data more ambiguous, confusing, harder to exploit, more difficult to act on, and therefore less valuable. Obfuscation assumes that the signal can be spotted in some way and adds a plethora of related, similar, and pertinent signals — a crowd which an individual can mix, mingle, and, if only for a short time, hide.There is no simple solution to the problem of privacy, because privacy itself is a solution to societal challenges that are in constant flux.There is real utility in an obfuscation approach, whether that utility lies in bolstering an existing strong privacy system, in covering up some specific action, in making things marginally harder for an adversary, or even in the “mere gesture” of registering our discontent and refusal. After all, those who know about us have power over us. They can deny us employment, deprive us of credit, restrict our movements, refuse us shelter, membership, or education, manipulate our thinking, suppress our autonomy, and limit our access to the good life.There is no simple solution to the problem of privacy, because privacy itself is a solution to societal challenges that are in constant flux. Some are natural and beyond our control; others are technological and should be within our control but are shaped by a panoply of complex social and material forces with indeterminate effects. Privacy does not mean stopping the flow of data; it means channeling it wisely and justly to serve societal ends and values and the individuals who are its subjects, particularly the vulnerable and the disadvantaged. Innumerable customs, concepts, tools, laws, mechanisms, and protocols have evolved to achieve privacy, so conceived, and it is to that collection that we add obfuscation to sustain it — as an active conversation, a struggle, and a choice.Finn Brunton is assistant professor in the Department of Media, Culture, and Communication at New York University. He is the author of “Spam: A Shadow History of the Internet” and coauthor (with Helen Nissenbaum) of “Obfuscation: A User’s Guide for Privacy and Protest,” from which this excerpt is adapted. Helen Nissenbaum is professor of information science at Cornell Tech and the author or coauthor of several books, including “Privacy in Context” and “Values at Play in Digital Games.” She is one of the developers of TrackMeNot, a browser extension used to foil the profiling of users through their searches.</description>
      <pubDate>18 Feb 21 10:08 EST</pubDate>
      <guid>https://thereader.mitpress.mit.edu/the-fantasy-of-opting-out/</guid>
    </item>
    <item>
      <title></title>
      <link>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</link>
      <description>&lt;a href=&#34;https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; You might know that I’ve posted funny emails I’ve received on my blog several times in the past. The kind of emails people send me when they experience problems with some device they own (like a car) and they contact me because my email address happens to be visible somewhere. People sometimes say I should get a different email address or use another one in the curl license file, but I’ve truly never had a problem with these emails, as they mostly remind me about the tough challenges the modern technical life bring to people and it gives me insights about what things that run curl. But not all of these emails are “funny”. Category: not funny Today I received the following email From: Al Nocai &lt;[redacted]@icloud.com&gt; Date: Fri, 19 Feb 2021 03:02:24 -0600 Subject: I will slaughter you That subject. As an open source maintainer since over twenty years, I know flame wars and personal attacks and I have a fairly thick skin and I don’t let words get to me easily. It took me a minute to absorb and realize it was actually meant as a direct physical threat. It found its ways through and got to me. This level of aggressiveness is not what I’m prepared for. Attached in this email, there were seven images and no text at all. The images all look like screenshots from a phone and the first one is clearly showing source code I wrote and my copyright line: The other images showed other source code and related build/software info of other components, but I couldn’t spot how they were associated with me in any way. No explanation, just that subject and the seven images and I was left to draw my own conclusions. I presume the name in the email is made up and the email account is probably a throw-away one. The time zone used in the Date: string might imply US central standard time but could of course easily be phony as well. How I responded Normally I don’t respond to these confused emails because the distance between me and the person writing them is usually almost interplanetary. This time though, it was so far beyond what’s acceptable to me and in any decent society I couldn’t just let it slide. After I took a little pause and walked around my house for a few minutes to cool off, I wrote a really angry reply and sent it off. This was a totally and completely utterly unacceptable email and it hurt me deep in my soul. You should be ashamed and seriously reconsider your manners.I have no idea what your screenshots are supposed to show, but clearly something somewhere is using code I wrote. Code I have written runs in virtually every Internet connected device on the planet and in most cases the users download and use it without even telling me, for free.Clearly you don’t deserve my code. I don’t expect that it will be read or make any difference. Update below, added after my initial post. Al Nocai’s response Contrary to my expectations above, he responded. It’s not even worth commenting but for transparency I’ll include it here. I do not care. Your bullshit software was an attack vector that cost me a multimillion dollar defense project. Your bullshit software has been used to root me and multiple others. I lost over $15k in prototyping alone from bullshit rooting to the charge arbitrators. I have now since October been sandboxed because of your bullshit software so dipshit google kids could grift me trying to get out of the sandbox because they are too piss poor to know shat they are doing. You know what I did to deserve that? I tried to develop a trade route in tech and establish project based learning methodologies to make sure kids aren’t left behind. You know who is all over those god damn files? You are. Its sickening. I got breached in Oct 2020 through federal server hijacking, and I owe a great amount of that to you. Ive had to sit and watch as i reported: fireeye Oct/2020Solarwinds Oct/2020Zyxel Modem Breach Oct/2020Multiple Sigover attack vectors utilizing favicon XML injectionJS Stochastic templating utilizing comparison expressions to write to data registersGet strong armed by $50billion companies because i exposed bullshit malware And i was rooted and had my important correspondence all rerouted as some sick fuck dismantled my life with the code you have your name plastered all over. I cant even leave the country because of the situation; qas you have so effectively built a code base to shit all over people, I dont give a shit how you feel about this. You built a formula 1 race car and tossed the keys to kids with ego problems. Now i have to deal with Win10 0-days because this garbage. I lost my family, my country my friends, my home and 6 years of work trying to build a better place for posterity. And it has beginnings in that code. That code is used to root and exploit people. That code is used to blackmail people. So no, I don’t feel bad one bit. You knew exactly the utility of what you were building. And you thought it was all a big joke. Im not laughing. I am so far past that point now. /- Al Al continues Nine hours after I first published this blog post , Al replied again with two additional emails. His third and forth emails to me. Email 3: https://davidkrider.com/i-will-slaughter-you-daniel-haxx-se/Step up. You arent scaring me. What led me here? The 5th violent attempt on my life. Apple terms of service? gtfo, thanks for the platform. Amusingly he has found a blog post about my blog post. Email 4: There is the project: MOUT Ops Risk Analysis through Wide Band Em Spectrum analysis through different fourier transforms.You and whoever the fuck david dick rider is, you are a part of this.Federal server breaches-Accomplice to attempted murder-Fraud-just a few. I have talked to now: FBI FBI Regional, VA, VA OIG, FCC, SEC, NSA, DOH, GSA, DOI, CIA, CFPB, HUD, MS, Convercent, as of today 22 separate local law enforcement agencies calling my ass up and wasting my time. You and dick ridin’ dave are respinsible. I dont give a shit, call the cops. I cuss them out wheb they call and they all go silent. I’ve kept his peculiar formatting and typos. In email 4 there was also a PDF file attached named BustyBabes 4.pdf. It is apparently a 13 page document about the “NERVEBUS NERVOUS SYSTEM” described in the first paragraph as “NerveBus Nervous System aims to be a general utility platform that provides comprehensive and complex analysis to provide the end user with cohesive, coherent and “real-time” information about the environment it monitors.”. There’s no mention of curl or my name in the document. Since I don’t know the status of this document I will not share it publicly, but here’s a screenshot of the front page: Related This topic on hacker news and reddit. I have reported the threat to the Swedish police (where I live). This person would later apologize. </description>
      <pubDate>19 Feb 21 09:08 EST</pubDate>
      <guid>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</guid>
    </item>
    <item>
      <title></title>
      <link>https://qntm.org/clean</link>
      <description>&lt;a href=&#34;https://qntm.org/clean&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; It may not be possible for us to ever reach empirical definitions of &#34;good code&#34; or &#34;clean code&#34;, which means that any one person&#39;s opinions about another person&#39;s opinions about &#34;clean code&#34; are necessarily highly subjective. I cannot review Robert C. Martin&#39;s 2008 book Clean Code from your perspective, only mine. That said, the major problem I have with Clean Code is that a lot of the example code in the book is just dreadful. * In chapter 3, &#34;Functions&#34;, Martin gives a variety of advice for writing functions well. Probably the strongest single piece of advice in this chapter is that functions should not mix levels of abstraction; they should not perform both high-level and low-level tasks, because this is confusing and muddles the function&#39;s responsibility. There&#39;s other valid stuff in this chapter: Martin says that function names should be descriptive, and consistent, and should be verb phrases, and should be chosen carefully. He says that functions should do exactly one thing, and do it well, which I agree with... provided that we aren&#39;t too dogmatic about how we define &#34;one thing&#34;, and we understand that in plenty of cases this can be highly impractical. He says that functions should not have side effects (and he provides a really great example), and that output arguments are to be avoided in favour of return values. He says that functions should generally either be commands, which do something, or queries, which answer something, but not both. This is all reasonable entry-level advice. But mixed into the chapter there are more questionable assertions. Martin says that Boolean flag arguments are bad practice, which I agree with, because an unadorned true or false in source code is opaque and unclear versus an explicit IS_SUITE or IS_NOT_SUITE... but Martin&#39;s reasoning is rather that a Boolean argument means that a function does more than one thing, which it shouldn&#39;t. Martin says that it should be possible to read a single source file from top to bottom as narrative, with the level of abstraction in each function descending as we read on, each function calling out to others further down. This is far from universally relevant. Many source files, I would even say most source files, cannot be neatly hierarchised in this way. And even for the ones which can, an IDE lets us trivially jump from function call to function implementation and back, the same way that we browse websites. He says code duplication &#34;may be the root of all evil in software&#34; and fiercely advocates DRY. At the time, this was quite standard advice. In more recent times, however, we generally understand that a little duplication isn&#39;t the worst thing in the world; it can be clearer, and it can be cheaper than the wrong abstraction. And then it gets weird. Martin says that functions should not be large enough to hold nested control structures (conditionals and loops); equivalently, they should not be indented to more than two levels. He says blocks should be one line long, consisting probably of a single function call. He says that an ideal function has zero arguments (but still no side effects?), and that a function with just three arguments is confusing and difficult to test. Most bizarrely, Martin asserts that an ideal function is two to four lines of code long. This piece of advice is actually placed at the start of the chapter. It&#39;s the first and most important rule: The first rule of functions is that they should be small. The second rule of functions is that they should be smaller than that. This is not an assertion that I can justify. I can’t provide any references to research that shows that very small functions are better. What I can tell you is that for nearly four decades I have written functions of all different sizes. I’ve written several nasty 3,000-line abominations. I’ve written scads of functions in the 100 to 300 line range. And I’ve written functions that were 20 to 30 lines long. What this experience has taught me, through long trial and error, is that functions should be very small. [...] When Kent showed me the code, I was struck by how small all the functions were. I was used to functions in Swing programs that took up miles of vertical space. Every function in this program was just two, or three, or four lines long. Each was transparently obvious. Each told a story. And each led you to the next in a compelling order. That’s how short your functions should be! All of this sounds like hyperbole. A case for short functions instead of long ones can certainly be made, but we assume that Martin doesn&#39;t literally mean that every function in our entire application must be four lines long or less. But the book is being absolutely serious about this. All of this advice culminates in the following source code listing at the end of chapter 3. This example code is Martin&#39;s preferred refactoring of a pair of Java methods originating in an open-source testing tool, FitNesse. package fitnesse.html; import fitnesse.responders.run.SuiteResponder; import fitnesse.wiki.*; public class SetupTeardownIncluder { private PageData pageData; private boolean isSuite; private WikiPage testPage; private StringBuffer newPageContent; private PageCrawler pageCrawler; public static String render(PageData pageData) throws Exception { return render(pageData, false); } public static String render(PageData pageData, boolean isSuite) throws Exception { return new SetupTeardownIncluder(pageData).render(isSuite); } private SetupTeardownIncluder(PageData pageData) { this.pageData = pageData; testPage = pageData.getWikiPage(); pageCrawler = testPage.getPageCrawler(); newPageContent = new StringBuffer(); } private String render(boolean isSuite) throws Exception { this.isSuite = isSuite; if (isTestPage()) includeSetupAndTeardownPages(); return pageData.getHtml(); } private boolean isTestPage() throws Exception { return pageData.hasAttribute(&#34;Test&#34;); } private void includeSetupAndTeardownPages() throws Exception { includeSetupPages(); includePageContent(); includeTeardownPages(); updatePageContent(); } private void includeSetupPages() throws Exception { if (isSuite) includeSuiteSetupPage(); includeSetupPage(); } private void includeSuiteSetupPage() throws Exception { include(SuiteResponder.SUITE_SETUP_NAME, &#34;-setup&#34;); } private void includeSetupPage() throws Exception { include(&#34;SetUp&#34;, &#34;-setup&#34;); } private void includePageContent() throws Exception { newPageContent.append(pageData.getContent()); } private void includeTeardownPages() throws Exception { includeTeardownPage(); if (isSuite) includeSuiteTeardownPage(); } private void includeTeardownPage() throws Exception { include(&#34;TearDown&#34;, &#34;-teardown&#34;); } private void includeSuiteTeardownPage() throws Exception { include(SuiteResponder.SUITE_TEARDOWN_NAME, &#34;-teardown&#34;); } private void updatePageContent() throws Exception { pageData.setContent(newPageContent.toString()); } private void include(String pageName, String arg) throws Exception { WikiPage inheritedPage = findInheritedPage(pageName); if (inheritedPage != null) { String pagePathName = getPathNameForPage(inheritedPage); buildIncludeDirective(pagePathName, arg); } } private WikiPage findInheritedPage(String pageName) throws Exception { return PageCrawlerImpl.getInheritedPage(pageName, testPage); } private String getPathNameForPage(WikiPage page) throws Exception { WikiPagePath pagePath = pageCrawler.getFullPath(page); return PathParser.render(pagePath); } private void buildIncludeDirective(String pagePathName, String arg) { newPageContent .append(&#34;\n!include &#34;) .append(arg) .append(&#34; .&#34;) .append(pagePathName) .append(&#34;\n&#34;); } } I&#39;ll say again: this is Martin&#39;s own code, written to his personal standards. This is the ideal, presented to us as a learning example. I will confess at this stage that my Java skills are dated and rusty, almost as dated and rusty as this book, which is from 2008. But surely, even in 2008, this code was illegible trash? Let&#39;s ignore the wildcard import. First, the class name, SetupTeardownIncluder, is dreadful. It is, at least, a noun phrase, as all class names should be. But it&#39;s a nouned verb phrase, the strangled kind of class name you invariably get when you&#39;re working in strictly object-oriented code, where everything has to be a class, but sometimes the thing you really need is just one simple gosh-danged function. Inside the class, we have: two public, static methods, as before, plus one new private constructor and fifteen new private methods. Of the fifteen private methods, fully thirteen of them either have side effects (they modify variables which were not passed into them as arguments, such as buildIncludeDirective, which has side effects on newPageContent) or call out to other methods which have side effects (such as include, which calls buildIncludeDirective). Only isTestPage and findInheritedPage look to be side-effect-free. They still make use of variables which aren&#39;t passed into them (pageData and testPage respectively) but they appear to do so in side-effect-free ways. At this point you might reason that maybe Martin&#39;s definition of &#34;side effect&#34; doesn&#39;t include member variables of the object whose method we just called. If we take this definition, then the five member variables, pageData, isSuite, testPage, newPageContent and pageCrawler, are implicitly passed to every private method call, and they are considered fair game; any private method is free to do anything it likes to any of these variables. But Martin&#39;s own definition contradicts this. This is from earlier in this exact chapter, with emphasis added: Side effects are lies. Your function promises to do one thing, but it also does other hidden things. Sometimes it will make unexpected changes to the variables of its own class. Sometimes it will make them to the parameters passed into the function or to system globals. In either case they are devious and damaging mistruths that often result in strange temporal couplings and order dependencies. I like this definition. I agree with this definition. It&#39;s a useful definition, because it enables us to reason about what a function does, with some degree of confidence, by referring only to its inputs and output. I agree that it&#39;s bad for a function to make unexpected changes to the variables of its own class. So why does Martin&#39;s own code, &#34;clean&#34; code, do nothing but this? Rather than have a method pass arguments to another method, Martin makes a distressing habit of having the first method set a member variable which the second method, or some other method, then reads back. This makes it incredibly hard to figure out what any of this code does, because all of these incredibly tiny methods do almost nothing and work exclusively through side effects. Let&#39;s just look at one private method. private String render(boolean isSuite) throws Exception { this.isSuite = isSuite; if (isTestPage()) includeSetupAndTeardownPages(); return pageData.getHtml(); } So... imagine that someone enters a kitchen, because they want to show you how to make a cup of coffee. As you watch carefully, they flick a switch on the wall. The switch looks like a light switch, but none of the lights in the kitchen turn on or off. Next, they open a cabinet and take down a mug, set it on the worktop, and then tap it twice with a teaspoon. They wait for thirty seconds, and finally they reach behind the refrigerator, where you can&#39;t see, and pull out a different mug, this one full of fresh coffee. ...What just happened? What was flicking the switch for? Was tapping the empty mug part of the procedure? Where did the coffee come from? That&#39;s what this code is like. Why does render have a side effect of setting the value of this.isSuite? When is this.isSuite read back, in isTestPage, in includeSetupAndTeardownPages, in both, in neither? If it does get read back, why not just pass isSuite in as a Boolean? Or perhaps the caller reads it back? Why do we return pageData.getHtml() when we never touched pageData? How did the HTML get there? Was it already there? We might make an educated guess that includeSetupAndTeardownPages has side effects on pageData, but then, what? We can&#39;t know either way until we look. And what other side effects does that have on other member variables? The uncertainty becomes so great that we suddenly have to wonder if calling isTestPage() could have side effects too. How would you unit-test this method? Well, you can&#39;t. It&#39;s not a unit. It can&#39;t be separated from the side-effects it has on other parts of the code. (And what&#39;s up with the indentation? And where are the danged braces?) Martin states, in this very chapter, that it makes sense to break a function down into smaller functions &#34;if you can extract another function from it with a name that is not merely a restatement of its implementation&#34;. But then he gives us: private boolean isTestPage() throws Exception { return pageData.hasAttribute(&#34;Test&#34;); } and: private WikiPage findInheritedPage(String pageName) throws Exception { return PageCrawlerImpl.getInheritedPage(pageName, testPage); } and half a dozen others, none of which are even called from more than one location. There is at least one questionable aspect of this code which isn&#39;t Martin&#39;s fault: the fact that pageData&#39;s content gets destroyed. Unlike the member variables (isSuite, testPage, newPageContent and pageCrawler), pageData is not actually ours to modify. It is originally passed in to the top-level public render methods by an external caller. The render method does a lot of work and ultimately returns a String of HTML. However, during this work, as a side effect, pageData is destructively modified (see updatePageContent). Surely it would be preferable to create a brand new PageData object with our desired modifications, and leave the original untouched? If the caller tries to use pageData for something else afterwards, they might be very surprised about what&#39;s happened to its content. But this is how the original code behaved prior to the refactoring, and the behaviour could be intentional. Martin has preserved the behaviour, though he has buried it very effectively. Some other mild puzzles: why do we use pageData.hasAttribute(&#34;Test&#34;) to figure out whether this is a test page, but we have to consult a separate Boolean to figure out whether or not this is a test suite page? And what exactly is the separation of concerns between PageCrawler and PageCrawlerImpl, both of which are in use here? * Is the whole book like this? Pretty much, yeah. Clean Code mixes together a disarming combination of strong, timeless advice and advice which is highly questionable or dated or both. Much of the book is no longer of much use. There are multiple chapters of what are basically filler, focusing on laborious worked examples of refactoring Java code; there is a whole chapter examining the internals of JUnit. This book is from 2008, so you can imagine how relevant that is now. There&#39;s a whole chapter on formatting, which these days reduces to a single sentence: &#34;Pick a sensible standard formatting, configure automated tools to enforce it, and then never think about this topic again.&#34; The content focuses almost exclusively on object-oriented code, and extols the virtues of SOLID, to the exclusion of other programming paradigms. Object-oriented programming was very fashionable at the time of publication. Martin himself invented three of the five principles which make up SOLID, and popularised the term. But the total absence of functional programming techniques or even simple procedural code was regrettable even then, and has only grown more obvious in the years since. SOLID is something of a non-sequitur in these domains. Even in the OO domain, opinions of SOLID have been steadily lowering in recent years. The book focuses on Java code, to the exclusion of other programming languages, even other object-oriented programming languages. Java was popular at the time, and if you&#39;re writing a book like this, it makes sense to pick a single well-known language and stick with it, and Java is still very popular and may still be a strong choice for this purpose. But the book&#39;s overall use of Java is very dated. This kind of thing is unavoidable — programming books date legendarily poorly. That&#39;s part of the reason why Clean Code was a recommended read at one time, and I now think that the pendulum is swinging back in the opposite direction. But even for the time, even for 2008-era Java, much of the provided code is bad. There&#39;s a chapter on unit testing. There&#39;s a lot of good, basic, stuff in this chapter, about how unit tests should be fast, independent and repeatable, about how unit tests enable more confident refactoring of source code, about how unit tests should be about as voluminous as the code under test, but strictly simpler to read and comprehend. But then he shows us a unit test with what he says has too much detail: @Test public void turnOnLoTempAlarmAtThreashold() throws Exception { hw.setTemp(WAY_TOO_COLD); controller.tic(); assertTrue(hw.heaterState()); assertTrue(hw.blowerState()); assertFalse(hw.coolerState()); assertFalse(hw.hiTempAlarm()); assertTrue(hw.loTempAlarm()); } and he proudly refactors it to: @Test public void turnOnLoTempAlarmAtThreshold() throws Exception { wayTooCold(); assertEquals(“HBchL”, hw.getState()); } This is done as part of an overall lesson in the virtue of inventing a new domain-specific testing language for your tests. I was left so confused by this suggestion. I would use exactly the same code to demonstrate exactly the opposite lesson. Don&#39;t do this! Which is to say nothing of the method named wayTooCold. This is an adjective phrase. It&#39;s entirely unclear what this method does. Does it set the world&#39;s state to be way too cold? Does it react to the world&#39;s state becoming way too cold? Or is it an assertion that the world&#39;s state currently must be way too cold? Methods should have verb or verb phrase names like postPayment, deletePage, or save. That&#39;s not me saying that. That&#39;s a direct quote from this book. Chapter 2, &#34;Meaningful Names&#34;: Methods should have verb or verb phrase names like postPayment, deletePage, or save. This is perfectly sound advice. And hw.setTemp(WAY_TOO_COLD); was a perfectly unambiguous line of code. What gives? And even if you guess correctly that calling wayTooCold() sets the temperature to be way too cold... there&#39;s no way that you could guess that it also calls controller.tic() internally. Earlier, we were advised to avoid code having side effects. This, also, was sound advice. And it is, again, being ignored in the actual written code example. (And since we&#39;re here, this, the original unrefactored code, is a fine demonstration of the drawbacks of unadorned Booleans. What does it mean when, say, coolerState returns true? Does it mean that the cooler&#39;s current state is good, i.e. cold enough, i.e. switched off? Or does it mean that it is powered on, and actively cooling? An enum with a few values, ON and OFF, could be less ambiguous.) * The book presents us with the TDD loop: First Law You may not write production code until you have written a failing unit test. Second Law You may not write more of a unit test than is sufficient to fail, and not compiling is failing. Third Law You may not write more production code than is sufficient to pass the currently failing test. These three laws lock you into a cycle that is perhaps thirty seconds long. The tests and the production code are written together, with the tests just a few seconds ahead of the production code. But the book doesn&#39;t acknowledge the missing zeroth step in the process: figuring out how to break down the programming task in front of you, so that you can take a minuscule thirty-second bite out of it. That, in many cases, is exceedingly time-consuming, and frequently obviously useless, and frequently impossible. * There&#39;s a whole chapter on &#34;Objects and Data Structures&#34;. In it, we&#39;re provided with this example of a data structure: public class Point { public double x; public double y; } and this example of an object (well, the interface for one): public interface Point { double getX(); double getY(); void setCartesian(double x, double y); double getR(); double getTheta(); void setPolar(double r, double theta); } Martin writes: These two examples show the difference between objects and data structures. Objects hide their data behind abstractions and expose functions that operate on that data. Data structure expose their data and have no meaningful functions. Go back and read that again. Notice the complimentary nature of the two definitions. They are virtual opposites. This difference may seem trivial, but it has far-reaching implications. And... that&#39;s it? Yes, you&#39;re understanding this correctly. Martin&#39;s definition of &#34;data structure&#34; disagrees with the definition everybody else uses. This is a very strange choice of definition, though Martin does at least define his term clearly. Drawing a clear distinction between objects as dumb data and objects as sophisticated abstractions with methods is legitimate, and useful. But it&#39;s quite glaring that there is no content in the book at all about clean coding using what most of us consider to be real data structures: arrays, linked lists, hash maps, binary trees, graphs, stacks, queues and so on. This chapter is much shorter than I expected, and contains very little information of value. * I&#39;m not going to rehash all the rest of my notes. I took a lot of them, and calling out everything I perceive to be wrong with this book would be counterproductive. I&#39;ll stop with one more egregious piece of example code. This is from chapter 8, a prime number generator: package literatePrimes; import java.util.ArrayList; public class PrimeGenerator { private static int[] primes; private static ArrayList&lt;Integer&gt; multiplesOfPrimeFactors; protected static int[] generate(int n) { primes = new int[n]; multiplesOfPrimeFactors = new ArrayList&lt;Integer&gt;(); set2AsFirstPrime(); checkOddNumbersForSubsequentPrimes(); return primes; } private static void set2AsFirstPrime() { primes[0] = 2; multiplesOfPrimeFactors.add(2); } private static void checkOddNumbersForSubsequentPrimes() { int primeIndex = 1; for (int candidate = 3; primeIndex &lt; primes.length; candidate += 2) { if (isPrime(candidate)) primes[primeIndex++] = candidate; } } private static boolean isPrime(int candidate) { if (isLeastRelevantMultipleOfNextLargerPrimeFactor(candidate)) { multiplesOfPrimeFactors.add(candidate); return false; } return isNotMultipleOfAnyPreviousPrimeFactor(candidate); } private static boolean isLeastRelevantMultipleOfNextLargerPrimeFactor(int candidate) { int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()]; int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor; return candidate == leastRelevantMultiple; } private static boolean isNotMultipleOfAnyPreviousPrimeFactor(int candidate) { for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) { if (isMultipleOfNthPrimeFactor(candidate, n)) return false; } return true; } private static boolean isMultipleOfNthPrimeFactor(int candidate, int n) { return candidate == smallestOddNthMultipleNotLessThanCandidate(candidate, n); } private static int smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) { int multiple = multiplesOfPrimeFactors.get(n); while (multiple &lt; candidate) multiple += 2 * primes[n]; multiplesOfPrimeFactors.set(n, multiple); return multiple; } } What the heck is this code? What is this algorithm? What are these method names? set2AsFirstPrime? smallestOddNthMultipleNotLessThanCandidate? Why does the code break with an out-of-bounds exception if we replace the int[] with a second ArrayList&lt;Integer&gt;? Earlier, we were advised that a method should be a command, which does something, or a query, which answers something, but not both. This was good advice, so why do nearly all of these methods ignore it? And what of thread safety? Is this meant to be clean code? Is this meant to be a legible, intelligent way to search for prime numbers? Are we supposed to write code like this? And if not, why is this example in the book? And where is the &#34;real&#34; answer? If this is the quality of code which this programmer produces — at his own leisure, under ideal circumstances, with none of the pressures of real production software development, as a teaching example — then why should you pay any attention at all to the rest of his book? Or to his other books? * I wrote this essay because I keep seeing people recommend Clean Code. I felt the need to offer an anti-recommendation. I originally read Clean Code as part of a reading group which had been organised at work. We read about a chapter a week for thirteen weeks. (The book has seventeen chapters, we skipped some for reasons already mentioned.) Now, you don&#39;t want a reading group to get to the end of each session with nothing but unanimous agreement. You want the book to draw out some kind of reaction from the readers, something additional to say in response. And I guess, to a certain extent, that means that the book has to either say something you disagree with, or not say everything you think it should say. On that basis, Clean Code was okay. We had good discussions. We were able to use the individual chapters as launching points for deeper discussions of actual modern practices. We talked about a great deal which was not covered in the book. We disagreed with a lot in the book. Would I recommend this book? No. Would I recommend it as a beginner&#39;s text, with all the caveats above? No. Would I recommend it as a historical artifact, an educational snapshot of what programming best practices used to look like, way back in 2008? No, I would not. * So the killer question is, what book(s) would I recommend instead? I don&#39;t know. Suggestions in the comments, unless I&#39;ve closed them. Update, 2020-12-19 After suggestions from comments below, I read A Philosophy of Software Design (2018) by John Ousterhout and found it to be a much more positive experience. I would be happy to recommend it over Clean Code. A Philosophy of Software Design is not a drop-in replacement for Clean Code. As the title suggests, it focuses more on the practice of software design at a higher level than it does on the writing or critiquing of actual individual lines of code. (As such, it contains relatively few code examples.) Because it&#39;s aimed at this higher level, I think it&#39;s possibly not a suitable read for absolute beginner programmers. A lot of this high-level theory is difficult to comprehend or put into practice until you have some real experience to compare it with. Having said that, I found A Philosophy of Software Design to be informative, cogent, concise and much more up-to-date. I found that I agreed with nearly all of Ousterhout&#39;s assertions and suggestions for good software design, many of which are diametrically opposed to those found in Clean Code. By virtue of providing relatively high-level advice, I feel that the book is likely to age rather better too. Of course, software development is still moving forwards as I write this. Who knows what a good programming book will look like in ten more years? </description>
      <pubDate>20 Feb 21 10:26 EST</pubDate>
      <guid>https://qntm.org/clean</guid>
    </item>
    <item>
      <title></title>
      <link>https://craigmod.com/essays/software_slump/</link>
      <description>&lt;a href=&#34;https://craigmod.com/essays/software_slump/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Something strange is happening in the world of software: It’s slowly getting worse. Not all software, but a lot of it. It’s becoming more sluggish, less responsive, and subtly less reliable than it was a few years ago. In some ways this is hyperbole. Objectively, we’ve never been able to do so much, so easily with our smartphones and laptops and tablets. We’ve never pushed more data between more places more readily. But while the insidious “worseness” I mention falls only in part on the engineering side of things, it falls harder on the more subjective, craft side of things, making it all the more worrisome. Why should we care about this? Because the majority of our waking hours take place within the confines of applications. A truth recently amplified by the COVID pandemic. And I believe software used by millions (if not billions) has a moral duty to elevate the emotional and intellectual qualities of its users. That elevation begins with craft. In the same way that physical architecture can affect a mind, so too can software. Slower, less reliable software is like Penn Station: Sure, you can catch a transfer from one train to another but the dreary lowness of the place, the lack of sunlight or sensible wayfinding will make you feel like a rat, truculent and worthless, and worse: You’ll acclimate to that feeling and accept it as a norm.1 This sense of a decline in software craft has been building for years, but it wasn’t until Apple released the iPad Magic Keyboard that I felt the pain of this hardware-software gap so acutely. So let’s start with that pairing: A superb keyboard coupled to budding but still-faulty software and work our way out to broader lands. #A Typing Ballet Make no bones about it, an iPad Pro is an enchanting object. It’s a slab too thin and light to be so powerful with so much battery life. And the iPad Magic Keyboard highlights this brilliantly. By floating the device on a bed of magnets and plush felt, and inviting you to pull the iPad off its stand as needed, the Magic Keyboard somehow enables and rewards both stand-alone and paired use of the device. Overall: An engineering marvel. The clickty-clack keyboard element of the Magic Keyboard is also great. I’m using it with an 11&#34; 2018 iPad Pro and though it can feel a bit cramped, the mechanics are satisfying, the sound quietly pleasing, and the key travel excellent all considering. The latest MacBook Pro keyboards (2020) are a degree nicer, (in fact: possibly the nicest keyboard to ever grace a laptop) but not by much. The Magic Keyboard is a bit heavy, and a bit weird to open, but aside from those niggles the quite wonderish accessory has but one glaring problem (and it’s the same problem we’ve been hemming and hawing over for years now, the same problem brought up every few months as if it were news): The software. #Fluency Sometimes, if I’m in Reader Mode in Safari and three-finger-touchpad-swipe to switch apps for a second and come back, the viewport has jumped to the top of the document. If I’m typing a tweet in Twitter’s app and hop back and forth between apps the cursor is gone, poof, lost until I tap again in the tweetbox. Not every time, but often. Which is worse than every time. It’s exasperating and speaks to low-level, foundational issues of iPadOS, an OS built for direct touch — a finger or Pencil on glass. Bear in mind — iPadOS is a branch of iOS, an operating system designed to be used with one hand while standing in an elevator. I don’t know how to fix these core iPadOS issues, but the fact that these problems persist indicates that they’re non-trivial and may never get fixed without some serious gut-level OS rewrites. Heck, back in November 2018, I wrote about these same problems in Getting the iPad to Pro: 2 Switching contexts is also cumbersome. If you’re researching in a browser and frequently jumping back and forth between, say, (the actually quite wonderful) Notes.app and Safari, you’ll sometimes find your cursor position lost. The Notes.app document you were just editing occasionally resetting to the top of itself. For a long document, this is infuriating and makes every Cmd-Tab feel dangerous. It doesn’t always happen, the behavior is unpredictable, making things worse. This interface “brittleness” makes you feel like you’re using an OS in the wrong way. More examples: Use the Share &gt; Mail option in Safari, start typing in an email address. Switch windows to check the address, Cmd-Tab back, now the address you’ve been typing has been auto-converted into a broken address. Tab down into the body of the email. Try to Shift-Tab back to edit the subject — you can’t. A bug? Shift-Tab inserts a Tab character — the cursor dumbly flops forward. The expected user experience here is if the cursor is at the start of a text-field, Shift-Tab will pull it back to the previous field, as it does in Mail on macOS. It’s not just Apple’s software: Google’s apps are egregious in their disdain for a keyboard. YouTube doesn’t even accept the universal standard of hitting the Space bar to start and stop a video. These aren’t the complaints of a laptoper-wannabe, but the concerns of literally anyone who desires “device fluency.” Hiccups in UX disrupt this fluency, make it impossible to obtain. Viewport and input reliability are table-stakes when it comes to gracefully navigating an operating system, using a device, being creative, making cool shit. Because the Magic Keyboard is well-made, and because you believe in its ability to register keystrokes accurately, the dissonance of not having commands fire off as quickly as you think them hurts all the more. I often smack Cmd-c five or six times to make sure a copy has “taken.” Copying is so hit-or-miss that visual feedback would be useful — a cursor blink, anything. And then: Cmd-v itself can take two or three tries to properly fire off. Often this is because I know where I want to paste, and pasting happens as contexts are switching. I’m Cmd-ving as the transitions are happening. I do not believe I am unique in this behavior. Essentially: The keyboard buffer on iPadOS feels non-existent. No, I can’t scaffold my terminal-dependent static-site generating publishing software on an iPad, but I’ve long since given up using this device (on which I am drafting this) for those things. The bumps I’m running into are basic operating system bumps (keyboard buffers!), polish bumps. Bumps we haven’t butted against in decades on desktop operating systems. Software should first and foremost elevate the intellectual and creative fluency of the user. As it presently stands, iPadOS makes this fluency more difficult to achieve than necessary. #That Hardware What baffles about these software moans is that Apple’s hardware is ever-more refined. While far from flawless, the entire lineup is now (finally) largely free from these “foundational” issues you see in software.3 Hardware has literal and metaphorical edges — it must be fully complete and largely bug free to ship. Software? It’s far more amorphous, like mist. Patches can be endlessly pushed. It never ends. Faulty hardware can destroy a company. Faulty software can be patched. The butterfly keyboard debacle may never be lived down. Even as I type on this improved Magic Keyboard, I can’t help but wonder: Did they really test this thing? I had three butterfly keyboards die on me, twice in the field. Not fun. Hardware failures live long in the mind. Take the iPhone’s camera as counterpoint to the butterfly keyboard. The cameras are things to admire. Stalwart, reliable. As hardware they are fabulous in their boringness. When did you last think about your iPhone camera or worry if it would work or not? A sign of great hardware and software is in forgetting about it, smoothly allowing it to integrate with your life — drawing fluency from it. The best camera is the one you have in your pocket … that reliably takes great photos. And yet: I think constantly about interacting with my photos on my iPhone because Photos (both the app and the service used by other apps to access your photos) has gotten slower over the years as the service has become more dependent on iCloud. When you swipe up in Instagram to choose a photo from your Photo Stream it routinely takes four or five seconds to show your photos. Same in Facebook Messenger. This used to be instantaneous. And the interface in, for example, Messages to select photos is uniquely baffling. (Since when do photo streams scroll sideways? And so many years on, just what the heck is the difference between “Photos” and “Recents”?) #Et tu, macOS Looking beyond iOS/ipadOS: Catalina is arguably the least stable, most disruptive-of-fluency macOS releases in recent memory. There are egregious issues of data loss in Apple Mail. Mail also simply won’t scroll — scroll! — certain HTML emails anymore. And Mail spontaneously pops forward as the frontmost application again and again for no obvious reason. The Finder — one of the oldest pieces of Mac tech, doesn’t reliably report disk usage. This worries more than most other bugs because it means some of that core code — the closest-to-the-metal bits — is being changed in ways that negatively affect stability in general-user and professional environments alike. The three primary pieces of software on macOS are probably Finder, Safari, and Mail. To have two of these show signs of instability is like ordering a salad and having half the lettuce appear as ceramic roofing tiles. It’s just weird. It shouldn’t happen, especially when these are new, critical bugs in decades-old programs. It makes you wonder what else might be broken, and what’s broken with the development cycle to allow for these bugs to ship. I asked folks on Twitter what problems they’ve had with Catalina and the list is depressingly impressive. Many of these issues dismantle user fluency. Because Catalina no longer runs 32-bit software, older, perfectly fine and functional apps have been “updated” to work with the new OS. Sometimes the developers switch to the Electron framework for development. Electron makes it easier to develop cross-platform applications, but comes at the expense of an application feeling or functioning in a way you’d expect a native application to function. Almost always, these Electron applications are slower and more cumbersome than a native version. Consistency + reliability = fluency.4 In Fast Software the Best Software I wrote about how speed and the intuition of stability are intertwined: Speed and reliability are often intuited hand-in-hand. Speed can be a good proxy for general engineering quality. If an application slows down on simple tasks, then it can mean the engineers aren’t obsessive detail sticklers. Not always, but it can mean disastrous other issues lurk. I want all my craftspeople to stickle. I don’t think Ulysses is badly made, but I am less confident in it than if it handled input and interface speed with more grace. Speed would make me trust it more. Electron — by very definition of its purpose — abdicates low-level detail obsession away from the developer and onto the framework. It feels like it fosters an anomie of craftsmanship. It’s no surprise that Electron applications (Slack, Arq 6, Dropbox, and more) feel more brittle than most native applications. That said, applications like VSCode show that Electron apps can be performant given effort and resources. Still, in the end, there’s something parabolic about Slack having been written and re-written four times now in Electron. Beyond frameworks, we’re seeing once-reliable applications suffer from feature creep and bloat. Perhaps this is endemic to the very nature of public companies and their conflation of features with user growth? For example: Dropbox has gone from a svelte, hyper-reliable file syncing service to a bloated curiosity that pegs the CPU at 200% for unclear reasons. I now keep it unloaded until I need to sync and then turn it on for just a few minutes. Which upends the core purpose of the original Dropbox: To be a seamless and OS-integrated local-and-cloud-synced file storage system. #Catalyst For arguments against Mac Catalyst, Apple’s cross-platform iOS / macOS framework, see the Twitter application. A small sample of issues (which may seem like nits but these details are important!): choppy scrolling / scrolling at a rate different than the rest of the system window resizing blanks out all content elements like the “home” button stay highlighted (as if tabbed to) for no apparent reason Most worrying: Catalyst may normalize a lack of craft and refinement. It’s important to remember that we had a solidly native-feeling Twitter client for macOS ten years ago. So this software problem was once solved, unsolved, and now re-solved in a worse way. #And Beyond Issues of badly-crafted, fluency-disrupting software extend beyond iOS or macOS onto the web itself. Gmail and Google Drive both take far longer to load than one should reasonably expect. I just ran an informal test: To go from opening a tab to composing an email in gmail took eight seconds. Twitter’s web site now loads (regardless of browser or operating system) in so many various layers and stages I never know if my internet connection is functioning properly or not. Twitter.com’s strange complexities also bring with it the ignoble award of being the only site to regularly crash Safari on my iPhone. Newspaper sites deliver hundred megabyte or greater payloads filled with ad tech. Open nytimes.com in a Chrome tab and you’ll soon deplete a fresh MacBook battery. #And Yet! There is Wonder Editing a photo on an iPad Pro with 120hz screen and latest Apple Pencil is one of the most genuinely “magical” ways to develop a photo today. Friends of mine who are professional illustrators swear by their iPads. An iPad with the folio keyboard is one of the lightest, most capable little writing devices around — you could take it up into the mountains and use it offline for hours a day for a week without needing to charge the battery, don’t have to baby it or worry about it getting lightly rained or splashed on. The latest 2020 MacBook Pros are solid refinements of the butterfly-era fiasco of machines. And when Catalina doesn’t go wonky on you, macOS strikes a superb balance between power and usability, of being able to drop into the Terminal or navigate graphically. I feel fluent and in control in a way that delights and satisfies. macOS software that adheres to craft — Things or Carbon Copy Cloner or BBEdit or Sublime Text (which, despite not being “native native” feels so solid and so responsive you’re willing to overlook its quirks) or Bear or Alfred or iA Writer or Keynote (arguably one of the best pieces of macOS software of all time) or anything by Panic, heck, even Terminal or Quicken (which, against all rational expectations is just a joy to use)5 — exists in troves, the existence of such proves to the Slacks or Twitters or Adobes of the world that it’s not impossible nor rare to produce craft-oriented software in service to user fluency, and still make a profit. In fact, there’s a business case to be made for being craft- and fluency-focused. We’ve seen entire companies with business models that could be summarized as “Bloat-Free X” emerge in recent years. Affinity is bloat-free Adobe. Install Adobe Creative Cloud on your laptop and marvel at the no fewer than a dozen processes whirling around in the background for unknown purposes. It’s no surprise Affinity Photo and Publisher and Designer have taken off. Sketch’s main feature for many years was simply: Not Adobe. And the web! When you care — when you really give a shit — the web is awe inspiring. I still can’t believe Figma is web-native (also born from the Not Adobe camp6). That an application can feel so powerful, so fast, so well-crafted and be fully web-based should be a kind of lighthouse-archetype for all other sites lost in a sea of complexity and muck and unnecessary frameworks. Recently I launched a static website/book — Ise-ji: Walk With Me — that has a potential payload of hundreds of megabytes but by using a bare-minimum of javascript, lazyloading, and optimization, the overall weight of the page is minimal, feels quick and responsive. The fact that I could host a site like this for free on Github or spin up a cheap Digital Ocean box and plop Cloudflare down in front of it is downright miraculous. The tools to make things work well and reliably are accessible (by degrees) to all. #Fluency &amp; Craft Our computing hardware is largely brilliant, refined, more reliable than ever. The core software running on it can sometimes feel regressive, moving in directions less focused on craft, consistency, and stability. Between the messiness of Catalina and the almost-but-not-quite-there-ness of iPadOS, what’s most needed now are not splashy masthead features but a reconsideration of the boring nuts and bolts, the paint on the back of the cabinets, the smoothing over of all the bumps and stutters as needed to enable device fluency — and not just a single year of cleaning up the mucky infrastructure of our compute landscape, but a reworking of the internal software culture of companies like Apple to elevate user fluency to first-class rank. It’s time to get all of this gorgeous hardware out of the software slump. #Noted: </description>
      <pubDate>25 Feb 21 17:56 EST</pubDate>
      <guid>https://craigmod.com/essays/software_slump/</guid>
    </item>
    <item>
      <title></title>
      <link>https://matklad.github.io/2020/09/20/why-not-rust.html</link>
      <description>&lt;a href=&#34;https://matklad.github.io/2020/09/20/why-not-rust.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I’ve recently read an article criticizing Rust, and, while it made a bunch of good points, I didn’t enjoy it — it was an easy to argue with piece. In general, I feel that I can’t recommend an article criticizing Rust. This is a shame — confronting drawbacks is important, and debunking low effort/miss informed attempts at critique sadly inoculates against actually good arguments. Not All Programming is Systems Programming Rust is a systems programming language. It offers precise control over data layout and runtime behavior of the code, granting you maximal performance and flexibility. Unlike other systems programming languages, it also provides memory safety — buggy programs terminate in a well-defined manner, instead of unleashing (potentially security-sensitive) undefined behavior. However, in many (most) cases, one doesn’t need ultimate performance or control over hardware resources. For these situations, modern managed languages like Kotlin or Go offer decent speed, enviable time to performance, and are memory safe by virtue of using a garbage collector for dynamic memory management. Complexity Programmer’s time is valuable, and, if you pick Rust, expect to spend some of it on learning the ropes. Rust community poured a lot of time into creating high-quality teaching materials, but the Rust language is big. Even if a Rust implementation would provide value for you, you might not have resources to invest into growing the language expertise. Rust’s price for improved control is the curse of choice: 1 2 3 4 5 6 struct Foo { bar: Bar } struct Foo&lt;&#39;a&gt; { bar: &amp;&#39;a Bar } struct Foo&lt;&#39;a&gt; { bar: &amp;&#39;a mut Bar } struct Foo { bar: Box&lt;Bar&gt; } struct Foo { bar: Rc&lt;Bar&gt; } struct Foo { bar: Arc&lt;Bar&gt; } In Kotlin, you write class Foo(val bar: Bar), and proceed with solving your business problem. In Rust, there are choices to be made, some important enough to have dedicated syntax. All this complexity is there for a reason — we don’t know how to create a simpler memory safe low-level language. But not every task requires a low-level language to solve it. Compile Times Compile times are a multiplier for everything. A program written in a slower to run but faster to compile programming language can be faster to run because the programmer will have more time to optimize! Rust intentionally picked slow compilers in the generics dilemma. This is not necessarily the end of the world (the resulting runtime performance improvements are real), but it does mean that you’ll have to fight tooth and nail for reasonable build times in larger projects. rustc implements what is probably the most advanced incremental compilation algorithm in production compilers, but this feels a bit like fighting with language compilation model. Unlike C++, Rust build is not embarrassingly parallel; the amount of parallelism is limited by length of the critical path in the dependency graph. If you have 40+ cores to compile, this shows. Rust also lacks an analog for the pimpl idiom, which means that changing a crate requires recompiling (and not just relinking) all of its reverse dependencies. Maturity Five years old, Rust is definitely a young language. Even though its future looks bright, I will bet more money on “C will be around in ten years” than on “Rust will be around in ten years” (See Lindy Effect). If you are writing software to last decades, you should seriously consider risks associated with picking new technologies. (But keep in mind that picking Java over Cobol for banking software in 90s retrospectively turned out to be the right choice). There’s only one complete implementation of Rust — the rustc compiler. The most advanced alternative implementation, mrustc, purposefully omits many static safety checks. rustc at the moment supports only a single production-ready backend — LLVM. Hence, its support for CPU architectures is narrower than that of C, which has GCC implementation as well as a number of vendor specific proprietary compilers. Finally, Rust lacks an official specification. The reference is a work in progress, and does not yet document all the fine implementation details. Alternatives There are other languages besides Rust in systems programming space, notably, C, C++, and Ada. Modern C++ provides tools and guidelines for improving safety. There’s even a proposal for a Rust-like lifetimes mechanism! Unlike Rust, using these tools does not guarantee the absence of memory safety issues. Modern C++ is safer, Rust is safe. However, if you already maintain a large body of C++ code, it makes sense to check if following best practices and using sanitizers helps with security issues. This is hard, but clearly is easier than rewriting in another language! If you use C, you can use formal methods to prove the absence of undefined behaviors, or just exhaustively test everything. Ada is memory safe if you don’t use dynamic memory (never call free). Rust is an interesting point on the cost/safety curve, but is far from the only one! Tooling Rust tooling is a bit of a hit and miss. The baseline tooling, the compiler and the build system (cargo), are often cited as best in class. But, for example, some runtime-related tools (most notably, heap profiling) are just absent — it’s hard to reflect on the runtime of the program if there’s no runtime! Additionally, while IDE support is decent, it is nowhere near the Java-level of reliability. Automated complex refactors of multi-million line programs are not possible in Rust today. Integration Whatever the Rust promise is, it’s a fact of life that today’s systems programming world speaks C, and is inhabited by C and C++. Rust intentionally doesn’t try to mimic these languages — it doesn’t use C++-style classes or C ABI. That means that integration between the worlds needs explicit bridges. These are not seamless. They are unsafe, not always completely zero-cost and need to be synchronized between the languages. While the general promise of piece-wise integration holds up and the tooling catches up, there is accidental complexity along the way. One specific gotcha is that Cargo’s opinionated world view (which is a blessing for pure Rust projects) might make it harder to integrate with a bigger build system. Performance “Using LLVM” is not a universal solution to all performance problems. While I am not aware of benchmarks comparing performance of C++ and Rust at scale, it’s not to hard to come up with a list of cases where Rust leaves some performance on the table relative to C++. The biggest one is probably the fact that Rust’s move semantics is based on values (memcpy at the machine code level). In contrast, C++ semantics uses special references you can steal data from (pointers at the machine code level). In theory, compiler should be able to see through chain of copies; in practice it often doesn’t: #57077. A related problem is the absence of placement new — Rust sometimes need to copy bytes to/from the stack, while C++ can construct the thing in place. Somewhat amusingly, Rust’s default ABI (which is not stable, to make it as efficient as possible) is sometimes worse than that of C: #26494. Finally, while in theory Rust code should be more efficient due to the significantly richer aliasing information, enabling aliasing-related optimizations triggers LLVM bugs and miscompilations: #54878. But, to reiterate, these are cherry-picked examples, sometimes the field is tilted the other way. For example, std::unique_ptr has a performance problem which Rust’s Box lacks. A potentially bigger issue is that Rust, with its definition time checked generics, is less expressive than C++. So, some C++ template tricks for high performance are not expressible in Rust using a nice syntax. Meaning of Unsafe An idea which is even more core to Rust than ownership &amp; borrowing is perhaps that of unsafe boundary. That, by delineating all dangerous operations behind unsafe blocks and functions and insisting on providing a safe higher-level interface to them, it is possible to create a system which is both sound (non-unsafe code can’t cause undefined behavior), and modular (different unsafe blocks can be checked separately). It’s pretty clear that the promise works out in practice: fuzzing Rust code unearths panics, not buffer overruns. But the theoretical outlook is not as rosy. First, there’s no definition of Rust memory model, so it is impossible to formally check if a given unsafe block is valid or not. There’s informal definition of “things rustc does or might rely on” and in in-progress runtime verifier, but the actual model is in flux. So there might be some unsafe code somewhere which works OK in practice today, might be declared invalid tomorrow, and broken by a new compiler optimization next year. Second, there’s also an observation that unsafe blocks are not, in fact, modular. Sufficiently powerful unsafe blocks can, in effect, extend the language. Two such extensions might be fine in isolation, but lead to undefined behavior if used simultaneously: Observational equivalence and unsafe code. </description>
      <pubDate>25 Feb 21 18:10 EST</pubDate>
      <guid>https://matklad.github.io/2020/09/20/why-not-rust.html</guid>
    </item>
    <item>
      <title>Why I&#39;m Done Pretending Touchscreen Infotainment Isn&#39;t a Stupid, Hazardous Fad</title>
      <link>https://www.thedrive.com/tech/39304/why-im-done-pretending-touchscreen-infotainment-isnt-a-stupid-hazardous-fad</link>
      <description>&lt;a href=&#34;https://www.thedrive.com/tech/39304/why-im-done-pretending-touchscreen-infotainment-isnt-a-stupid-hazardous-fad&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Pretty much every study of in-car touchscreens&#39; effects on drivers reaches the same conclusion: They&#39;re a serious distraction. This holds true for tech-literate younger drivers as it does for older ones who might face a steeper learning curve, and for well-rested drivers as well as those fatigued by a long day at work. That goes double for the millions of Americans like me, who live with attentional, cognitive, or similar impairments that make dealing with these overwrought gadgets while driving not just a nuisance, but an unnecessary hazard.I speak as one of the roughly eight percent of U.S. adults believed by the National Institute of Mental Health to be living with attention deficit hyperactivity disorder, or ADHD. Symptoms of this commonly misunderstood condition can include problems with emotional regulation, executive function, working memory, and of course, attention—though not in the way &#34;deficit&#34; implies. I&#39;d liken managing my focus to trying to aim a fire hose; I have plenty of water to work with, but it&#39;s coming out with so much pressure that it can be hard to keep the damn thing pointing in the right direction, never mind on target. James Gilboy 2020 Toyota Camry TRD touchscreen Getting the stream where it&#39;s needed is often possible, but it means there&#39;s but mist to spare for everything else—things outside my narrow scope can sometimes fail to register. This is perfectly manageable in normal driving, though; I can cycle through checking my mirrors, adjusting lane position, changing gears, reading signs, and so on without difficulty.Adding a touchscreen to the equation, however, introduces a second, wholly separate environment to monitor while driving. Figuratively speaking, a touchscreen is a separate space, one that competes directly with the outside world for my one-channel attention. This might not be a problem if it weren&#39;t for the poor working memory common to ADHD&#39;s inattentive and combined subtypes, which can make people like me feel like goldfish, with their (fictitious) seconds-long memories. For me, this has on multiple occasions manifested as putting food in the microwave, only to completely forget what I just did, then get hungry again, and begin boiling a pot of water for pasta—only for beeping to pleasantly surprise me with the meal I forgot I was preparing. So you can imagine that driving increasingly complex cars is similarly challenging sometimes, just with movement involved.Operating something like Ford&#39;s dated, complex Sync 3 system on the move, then, be it to alter a vehicle function or skip a podcast ad, requires constantly re-orienting myself in two separate environments, losing awareness of the previous space with each switch. This balancing act is complex on an empty road, never mind in dense traffic on Interstate 25 through Denver, my navigation app bombarding me with alternate routes to evaluate. If I had to guess what this sensory assault does to crash rates for my fellow ADHDers, which was already 42 to 47 percent higher than the average according to data gathered in the late 2000s, I&#39;d wager nothing good. Tesla, via Model3.info Tesla Model 3 infotainment system This isn&#39;t to pretend problems with touchscreens are exclusive to my mental wiring, of course. Other conditions can complicate their use, be they fleeting, like hormonal or nutritional problems, or lasting, as with dyslexia or age-related neurological decline. They&#39;re also plenty troublesome for healthy, sober people with better-understood wiring; at least one study concluded touchscreens can prolong drivers&#39; reactions times more than alcohol or marijuana, and even the U.S. Navy has taken a &#34;just because you can doesn&#39;t mean you should&#34; stance toward screens on ships&#39; bridges. (And it&#39;s worth remembering that given the generally less-than-great state of public transit in America, driving a car is often people&#39;s only way of getting around and getting to work.)I&#39;m hardly the first person in automotive media to speak out against these electronic gimmicks, which were taken to new heights by a certain technophilic, occasionally ethically challenged California-based carmaker. But prior protests have gone unacknowledged by the auto industry, which has launched into a mindless arms race of ever-bigger, soon to be dashboard-spanning screens, sending cars on a trajectory to a place nobody wants to land: More distracted drivers, and more serious crashes. Even as an early tablet adopter, having owned a first-gen iPad in high school, I&#39;m not looking forward to having a monstrous touchscreen in my peripheral vision while I try to execute a lane change. I&#39;m not excited by fiddling with sprawling, overcomplicated interfaces, or sharing the road with people who are—and might not treat distraction as a problem, having not been diagnosed with a predisposition toward it.Touchscreen infotainment systems have become central to controlling modern cars&#39; evermore complex functions, though this has come at a cost to safety and ease of use—especially for people who experience life a bit differently from what we consider typical. It may not be hard to understand why touchscreens have caught on, but it&#39;s strange to see them pass almost unquestioned as the future of human-to-vehicle interfaces. They aren&#39;t, and as someone to whom this fad presents an inordinate risk, I&#39;m done pretending they are.Got a tip? Send us a note: tips@thedrive.com </description>
      <pubDate>26 Feb 21 17:28 EST</pubDate>
      <guid>https://www.thedrive.com/tech/39304/why-im-done-pretending-touchscreen-infotainment-isnt-a-stupid-hazardous-fad</guid>
    </item>
    <item>
      <title></title>
      <link>https://betterprogramming.pub/the-3-mindsets-to-avoid-as-a-senior-software-developer-efc8fa17fc3</link>
      <description>&lt;a href=&#34;https://betterprogramming.pub/the-3-mindsets-to-avoid-as-a-senior-software-developer-efc8fa17fc3&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The 3 Mindsets to Avoid as a Senior Software DeveloperThe more we’ve learned, the more we’ve realized there’s so much more to learnPhoto by Norbert Kundrak on UnsplashAs developers, the longer we’re on the job, the more experience we gain. Eventually, we may even become the go-to person on our team or in our organization, seen as the technical guru of our field.At this level, we get a self-esteem boost, which is positive. But at times we may fall into the trap of getting an unnecessary ego. The wrong mindset creeps in, which can lead to the beginning of our downfall.I’ve been through that journey. So I now consciously remind myself not to fall into one of those mindsets again. We’ll talk about three of them.I’m a Self-Taught Programmer, and I Need No OneI once saw a tweet from a person who claimed his father (a programmer) offered to teach him how to code, and he rejected it. The author of the tweet is now a developer, describing himself as a self-taught programmer.Programming isn’t a skill one can master completely by simply taking a course or getting a degree. A lot of sweat and time spent digging and learning is needed to build one’s competency.It’s not a field that can be effectively explored and fully discovered alone. We might not have a personal mentor and we may not have taken any formal courses, but that doesn’t change the fact that we still learn by reading from somewhere — whether that be a book, tutorial, blog, manual, or even Stack Overflow. All these things were written by someone.In fact, the best way to learn programming is by working with a group of developers — through code reviews, pairings, constant discussion, and the exchange of ideas and the information learned. Attending conferences, guilds, dojos, etc. helps too.Therefore, much of what we’ve learned and where we are today isn’t a result of our self-meditation on a mountain. Instead, it’s an accumulated assimilation of the knowledge and experience others have shared with us, either directly or indirectly.We should always be grateful.I’m Better Than Younger ProgrammersSomeone responded to one of my past blogs saying, “The young ones can’t concentrate. And their productivity is vastly lower than that of us oldsters. Their code quality is way lower.” I felt disgusted by this blanket statement targeting young programmers, even if I’m not the young one.We learn, we grow, we age. With the advantage of time, we sometimes gain an upper hand in terms of both knowledge and experience in some area.However, software development is an ever-changing field. One needs to continue learning to keep abreast of a skill. Over time, some of the past knowledge we acquired and spent time on is no longer relevant. When you consider what’s still relevant, the knowledge gap between older and younger programmers becomes smaller.At times, some area of knowledge younger programmers have might surpass us, the older folks (I consider myself an older folk, having worked for more than two decades). In software development, there’s been so much new knowledge to harvest in the last 10 years, and it’s not possible for us to extract it all.Never look down on the young ones. Respect each other, and always shamelessly ask and listen to their opinion. Be open. Avoid impostor syndrome. We might be surprised to learn a lot from those younger than us.The young ones are the future. They’ll shape the world one day. Teach them as much as you can, and also learn from them as much as you can. We’re not always right.New Tech Is Just Recycled Old Tech“What more is there to learn within programming? Everything new is just an old thing with a different look — or it’s just if-else or for loops covered by fancier names and structures externally.” I thought this back then after I’ve mastered C++ and object-oriented programming.I despised Java as an inferior language to C++ and looked down on all scripts — VBScript, JavaScript, and let’s not forget HTML, which isn’t even a language to me. Python, what is it? No {} and, it’s optional to use ;? Using indentation is a must??The word evolve meant nothing to me, and after a decade, Python became the most used language.I still have the utmost respect for those with competency in C programming, having the ability to work with pointers, having proper memory management, and writing code that compiles into the smallest and most efficient executable.However, that’s no longer what mainstream software development looks like. Web programming took the world by storm in the ‘90s and early 2000s. Since the launch of the iPhone, mobile development has become a significant player in software development. And now artificial intelligence could be the next wave, which the Python language plays a significant role in enabling.Fast forward to today: Now I’m getting my feet wet in mobile development, a rapidly changing field. I just realized how wrong I was back then in despising the newer yet seemingly inferior product (i.e., Java) over what I’d mastered (i.e., C++). Java has influenced many of the programming languages we have today, and its JVM is the backbone of many new languages (e.g., Scala and Kotlin).Never despise newer tech at a glance. It’s true most of them spawn from an existing tech and may look similar (or at times even inferior), but all of these small little changes could contribute to the next leap of change.ConclusionIn the early years of my career, I worked hard and was promoted fast. My boss jokingly gave me a little advice, “Don’t get cocky.” I guess that about sums it all up.</description>
      <pubDate>02 Mar 21 12:49 EST</pubDate>
      <guid>https://betterprogramming.pub/the-3-mindsets-to-avoid-as-a-senior-software-developer-efc8fa17fc3</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.nature.com/news/neuroscience-idle-minds-1.11440</link>
      <description>&lt;a href=&#34;https://www.nature.com/news/neuroscience-idle-minds-1.11440&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Neuroscientists are trying to work out why the brain does so much when it seems to be doing nothing at all. Credit: RYAN SNOOKFor volunteers, a brain-scanning experiment can be pretty demanding. Researchers generally ask participants to do something — solve mathematics problems, search a scene for faces or think about their favoured political leaders — while their brains are being imaged.But over the past few years, some researchers have been adding a bit of down time to their study protocols. While subjects are still lying in the functional magnetic resonance imaging (fMRI) scanners, the researchers ask them to try to empty their minds. The aim is to find out what happens when the brain simply idles. And the answer is: quite a lot.Some circuits must remain active; they control automatic functions such as breathing and heart rate. But much of the rest of the brain continues to chug away as the mind naturally wanders through grocery lists, rehashes conversations and just generally daydreams. This activity has been dubbed the resting state. And neuroscientists have seen evidence that the networks it engages look a lot like those that are active during tasks.Resting-state activity is important, if the amount of energy devoted to it is any indication. Blood flow to the brain during rest is typically just 5–10% lower than during task-based experiments1. And studying the brain at rest should help to show how the active brain works. Research on resting-state networks is helping to map the brain&#39;s intrinsic connections by showing, for example, which areas of the brain prefer to talk to which other areas, and how those patterns might differ in disease.But what is all this activity for? Ask neuroscientists — even those who study the resting state — and many will sigh or shrug. “We&#39;re really at the very beginning. It&#39;s mostly hypotheses,” says Amir Shmuel, a brain-imaging specialist at McGill University in Montreal, Canada. Resting activity might be keeping the brain&#39;s connections running when they are not in use. Or it could be helping to prime the brain to respond to future stimuli, or to maintain relationships between areas that often work together to perform tasks. It may even consolidate memories or information absorbed during normal activity.“There&#39;s so much enthusiasm about the approach now, and so little basic understanding,” says Michael Greicius, a neuroscientist at Stanford University in California, who started studying resting-state networks a decade ago. Always active A set of experiments in the mid-1990s first suggested that the brain never really takes a break. Bharat Biswal, then a PhD student at the Medical College of Wisconsin in Milwaukee, was trying to find ways of identifying and removing background signals from fMRI scans, in the hope that it would improve interpretations of the signals from tasks. “The assumption was, it was all noise,” says Biswal, who is now a biomedical engineer at the New Jersey Institute of Technology in Newark. But when he looked at scans taken when people were resting in the scanner, he saw regular, low-frequency fluctuations in the brain2. Biswal&#39;s experiments suggested that neuronal activity was causing these fluctuations. Nature Podcast Your browser does not support the audio element. Let your mind wander as Kerri Smith explores resting brain activity with the help of two neuroscientists.  In the early days of resting-state research, some people were sure that they had found something profound. “When I first started looking at these networks, I was convinced we were tapping into the stream of consciousness, and this was real-time ongoing conscious processing,” says Greicius. But, he says, “I was relatively quickly disabused of that notion”. The networks of activity also appeared in altered states of consciousness such as when sleeping or under anaesthesia3,4, so they weren&#39;t necessarily linked to conscious processing.But they weren&#39;t meaningless either. Several years after Biswal&#39;s discovery, studies of the resting state in its own right began to emerge. A team led by Marcus Raichle, a neuroscientist at Washington University in St. Louis, Missouri, characterized5 activity in one such network as the brain&#39;s default mode — what they considered its baseline setting. During tasks, default-mode activity actually dropped, coming back online when the brain was no longer focusing so intensely5.The default-mode network has been joined by dozens of other flavours of resting-state network — some of which resemble the circuitry that contributes to attention, vision, hearing or movement. They seem very similar across study participants but are also dynamic, changing over time. “The fact that it&#39;s always present but modifiable tells you that it&#39;s got its importance,” says Michael Milham, director of the Center for the Developing Brain at the Child Mind Institute in New York.Still, some researchers have questioned whether these resting patterns represent anything real. After all, fMRI does not measure brain electrical activity directly: it monitors blood flow. The low-level idling activity could be an artefact. “People suspected it was lousy scanners or respiratory noise,” says Andreas Kleinschmidt, director of research at the French National Institute of Health and Medical Research&#39;s Cognitive Neuroimaging Unit in Gif-sur-Yvette. But using fMRI and electroencephalography (EEG) recordings, Kleinschmidt and his team confirmed6 that various resting-state networks are correlated with real neural activity. Credit: RYAN SNOOKShmuel and David Leopold, a neurophysiologist at the US National Institute of Mental Health in Bethesda, Maryland, did much the same7, imaging resting states in monkeys while recording the animals&#39; electrical brain activity using probes implanted deep in the visual cortex. They found correlations between resting-state networks and electrical activity in a band of frequencies around 40 hertz. Such &#39;γ activity&#39; is associated with communication between distant brain areas, and seeing it convinced Shmuel that resting-state networks represent actual brain activity. “I strongly believe that there is a neurophysiological mechanism that underlies the entire thing that we call resting-state networks,” he says. Disordered thinking It is a mechanism that may go awry in brain disorders. People with early signs of Alzheimer&#39;s disease, for example, have unusual resting-state signatures that can be detected even at very mild levels of dementia and which vary as the disease progresses8. In children with autism spectrum disorder, resting-state networks can be &#39;hyperconnected&#39;, displaying more links than for kids without the condition9. The reasons for these differences are not clear, and they may not matter to clinicians, who are interested in finding disease markers. “From a clinical perspective, you&#39;re not always going to understand why a biomarker is serving as that biomarker,” says Milham. But some neuroscientists are deeply curious as to what these fluctuations do. “It keeps me up at night,” says Timothy Ellmore at the University of Texas Health Science Center in Houston, who is studying resting brain activity in people with Parkinson&#39;s disease.Some researchers now think that resting-state networks may prime the brain to respond to stimuli. “The system is not sitting there doing nothing and waiting,” says Kleinschmidt. Cycling activity in these networks may be helping the brain to use past experiences to inform its decisions. “It&#39;s incredibly computationally demanding to calculate everything on the fly,” says Maurizio Corbetta at Washington University School of Medicine in St. Louis. He has been studying resting state using magnetoencephalography, a technique that measures magnetic fields associated with the electrical activity of neurons. “If I have ongoing patterns that are guessing what&#39;s going to happen next in my life, then I don&#39;t have to compute everything.” He likens the activity to the idling of a vehicle. “If your car is ready to go, you can leave faster than if you have to turn on the engine.”“ Resting-state activity is important, if the amount of energy devoted to it is any indication. ” But idling networks might not just save time. They may also influence perceptions — albeit unconsciously. To study how spontaneous resting activity affects perception, Kleinschmidt and his colleagues scanned10 the brains of people who were looking at a picture that can be perceived as a face or as a vase. Study participants who reported seeing a face had more spontaneous activity in the fusiform face area — a brain region that processes faces — before they were shown the picture. Kleinschmidt suspects that the brain is running several models of the world in the background, ready for one of them to turn into reality. “Ideally, you&#39;re always prepared for what happens next,” he says.Corbetta has discovered evidence in people with brain damage that resting activity can change behaviour. In unpublished work, he has found hints that lesions in frontal brain regions — caused by stroke, for example — can give rise to changes in spontaneous brain activity in distant areas. What is more, the changes to the resting activity are linked to the behavioural deficit. “This is clear evidence that resting-state impairments are affecting the way the network is recruited during a task,” he says. Zen and the art of network maintenance Raichle favours the idea that activity in the resting state helps the brain to stay organized. The connections between neurons are continually shifting as people age and learn, but humans maintain a sense of self throughout the upheaval. Spontaneous activity might play a part in maintaining that continuity. “Connections between neurons turn over in minutes, hours, days and weeks,” says Raichle. “The structure of the brain will be different tomorrow but we will still remember who we are.”Or perhaps the activity is part of the reshaping process, tweaking connections while we idle. Several teams have reported changes in resting connectivity after language and memory tasks and motor learning. Chris Miall, a behavioural brain scientist at the University of Birmingham, UK, and his colleagues have shown that spontaneous activity at rest can be perturbed by what has just happened11. The team scanned volunteers at rest, and then asked them to learn a task involving using a joystick to track a moving target. When the participants were scanned at rest again, the team could see the effects of motor learning in the resting networks. That study, and subsequent work along the same lines, suggests that “the brain is not only thinking about supper coming up, but it&#39;s also processing the recent past and converting some of that into long-term memories”, says Miall. The network changes are specific to the tasks performed.“ If your car is ready to go, you can leave faster than if you have to turn on the engine. ” Work on memory consolidation in animals backs that conclusion. It used to be assumed that memories from the day were strengthened during a night&#39;s sleep. Working with rats, however, Loren Frank and Mattias Karlsson, neuroscientists at the University of California, San Francisco, have found12 that the brain replays and consolidates new memories at any chance it gets — even when awake. “These events happen when it doesn&#39;t look like the animal is doing very much,” says Frank.He speculates that resting activity could be doing the same thing in human brains — reactivating patterns that correspond to past experiences. At the same time, activity in the networks could have a normalizing, housekeeping function too. “How do you keep the brain flexible?” Frank asks. “If you have random patterns of activity washing through your network, those can help reduce the strength of the pathways associated with what you&#39;ve just learned.” That would stop the brain from reinforcing the same pathways too often. “Perhaps down-time periods are also important for that,” he says.Shmuel says that it is still not possible to rule out the idea that this activity is just a by-product of the brain being alive. Current may flow through these circuits “simply because there is current — the brain is not dead — and there are anatomical connections that give this current a non-random structure”. But, he admits, “I hope this is not the case. Then it&#39;s extremely uninteresting.”Narrowing down the range of interesting possibilities may take time, given that the very nature of resting-state science makes it difficult to test hypotheses. When a researcher slides someone into a scanner and instructs them to think about nothing in particular, there is no task to do and no hypothesis to address. So researchers have to generate reams of data and line up hypotheses as they go along. “Resting state opens up discovery science,” says Milham enthusiastically, before admitting that, because he trained as a hypothesis-driven cognitive neuroscientist, “it&#39;s like heresy that I&#39;ve got into this”.Whatever resting activity is doing, its existence certainly proves one thing. Miall puts it bluntly: “The brain only rests when you&#39;re dead.” ReferencesRaichle, M. E. &amp;amp; Mintun, M. A. Annu. Rev. Neurosci. 29, 449–476 (2006).CAS  Article  Google Scholar  Biswal, B., Yetkin, F. Z., Haughton, V. M. &amp;amp; Hyde, J. S. Magn. Reson. Med. 34, 537–541 (1995).CAS  Article  Google Scholar  Greicius, M. D. et al. Hum. Brain Mapp. 29, 839–847 (2008).Article  Google Scholar  Boly, M. et al. Ann. NY Acad. Sci. 1129, 119–129 (2008).ADS  CAS  Article  Google Scholar  Raichle, M. E. et al. Proc. Natl Acad. Sci. USA 98, 676–682 (2001).ADS  CAS  Article  Google Scholar  Laufs, H. et al. Proc. Natl Acad. Sci. USA 100, 11053–11058 (2003).ADS  CAS  Article  Google Scholar  Shmuel, A. &amp;amp; Leopold D. A. Hum. Brain Mapp. 29, 751–761 (2008).Article  Google Scholar  Brier, M. R. et al. J. Neurosci. 32, 8890–8899 (2012).CAS  Article  Google Scholar  Di Martino, A. et al. Biol. Psychiatry 69, 847–856 (2011).Article  Google Scholar  Hesselmann, G., Kell, C. A., Eger, E. &amp;amp; Kleinschmidt, A. Proc. Natl Acad. Sci. USA 105, 10984–10989 (2008).ADS  CAS  Article  Google Scholar  Albert, N. B., Robertson, E. M. &amp;amp; Miall, R. C. Curr. Biol. 19, 1023–1027 (2009).CAS  Article  Google Scholar  Karlsson, M. P. &amp;amp; Frank, L. M. Nature Neurosci. 12, 913–918 (2009).CAS  Article  Google Scholar  Download referencesAuthor informationAffiliationsKerri Smith is podcast editor for Nature in London.Kerri SmithAuthorsKerri SmithYou can also search for this author in PubMed Google ScholarRelated linksRights and permissionsAbout this articleCite this articleSmith, K. Neuroscience: Idle minds. Nature 489, 356–358 (2012). https://doi.org/10.1038/489356aDownload citationPublished: 19 September 2012Issue Date: 20 September 2012DOI: https://doi.org/10.1038/489356a </description>
      <pubDate>07 Mar 21 18:05 EST</pubDate>
      <guid>https://www.nature.com/news/neuroscience-idle-minds-1.11440</guid>
    </item>
    <item>
      <title></title>
      <link>https://aeon.co/essays/why-is-the-deathbed-perspective-considered-so-valuable</link>
      <description>&lt;a href=&#34;https://aeon.co/essays/why-is-the-deathbed-perspective-considered-so-valuable&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;How do we find out what really matters in life? One way might be to ask those who are dying. They might occupy a perspective that allows them to see better what’s trivial and what’s truly significant. The prospect of imminent death might carry them above petty squabbles and the pursuit of money and status, and allow them a clear view of the goods that make our lives worthwhile. What really matters, it turns out, is family and relationships and authenticity. At least, that’s what people apparently report from the deathbed. There’s very little systematic research on this question, but there’s some unsystematic research. Search the internet for ‘regrets of the dying’ and chances are you’ll quickly hit upon a website or a newspaper article reporting the work of Bronnie Ware, an Australian former nurse whose blog recording bedside conversations became the basis for a bestselling book. According to her, the dying expressed five common regrets:I wish I’d had the courage to live a life true to myself, not the life others expected of me.I wish I hadn’t worked so hard.I wish I’d had the courage to express my feelings.I wish I had stayed in touch with my friends.I wish that I had let myself be happier.Broadly, people seem to wish for a more meaningful life. They wished they’d been more authentic in their activities (1; 3). They wished they’d prioritised friends and themselves, rather than work (2; 4; 5). They wished, in short, that they’d stopped and smelled the roses. Along with almost everyone else, I find it plausible that these things are very valuable, and that they’re components (though far from the only ones) of a meaningful life. But I’m not convinced that the reported regrets of the dying provide us with reasons to think them valuable. I’m sceptical, first, of the reports themselves. There are various cultural pressures that might lead people to report such regrets, whether they feel them or not, and might lead us to attribute them to the dying, whether they report them or not. Second and more importantly, I doubt that the perspective of the dying gives them a clearer view on what really matters. There are reasons to think that the view from the deathbed is worse, not better, than the view from the midst of life. Their lack of engagement in ongoing projects might leave them with an impoverished sense of their value. I’m not the first person to wonder whether deathbed regrets are epistemically privileged. The American philosopher Eric Schwitzgebel gives two reasons why we should be cautious in giving them significant weight. First, the dying might be subject to hindsight bias, in the form of a tendency to assume that their current epistemic perspective looking back on the past is identical to the perspective they should have adopted at the time. Take the advice that people should prioritise fulfilment over money. That advice is easier to find compelling from the perspective of the financially comfortable present self than it is from the perspective of the worried past self, struggling to make ends meet. Even if the person who offers the advice was herself once poor, she might easily romanticise the past, in part because memories of day-to-day life tend to be crowded out by memories of peak moments. Perhaps I recall my time of relative poverty as better than it really was, because I remember dancing to the radio in the kitchen and not struggling to pay the bills. My biased recollection could easily lead me to underestimate how much money really mattered to a decent life. Schwitzgebel’s second reason for wariness about deathbed regrets arises from the fact that the dying escape the consequences of their advice. They don’t risk accusations of hypocrisy if they fail to live up to it, because they can’t be expected to try. This lack of a stake in their advice might free them to fantasise, in a way that those of us who are called to live up to our own advice can’t. For that reason, Schwitzgebel suggests that we might do better to prefer the wisdom of 40-somethings: those who have experienced enough to have a broad perspective on life, but who still have a stake in living. While these concerns are significant, they don’t seem sufficient to eliminate any sense that the view from the deathbed is epistemically privileged. For one thing, the status of the (so-called) hindsight bias is contested: as the Australian philosopher Brian Hedden has argued, in hindsight we often have a better grasp of what our total evidence at the time actually was, and what propositions it actually supported. The objection that the dying are suspiciously free of the need to abide by their own advice seems to generalise too broadly: we often, and apparently appropriately, seek advice from people who we know won’t follow it themselves, because they won’t face the problems we face (we go to celibate priests for advice on marriage problems, for example). Together these concerns could give us reason for downgrading deathbed advice to some degree, but they’re compatible with thinking that it nevertheless has special epistemic weight. We can reinforce Schwitzgebel’s scepticism with a third worry. The regrets of the dying are platitudes, taken-for-granted pieces of folk wisdom, and that very fact is grounds for wondering about their sincerity and about their representativeness. The dying person knows that if they say they regret not making more money, they’ll be seen as shallow Ware’s The Top Five Regrets of the Dying (2012) gives us ‘anecdata’, not carefully collected evidence. We don’t know how broad a range of patients she saw in her ‘several years’ in palliative care. It probably wasn’t especially broad, since she worked for an agency that provided help in the homes of those who could afford such services. Nor do we know how systematically she has reported the regrets she heard. Perhaps certain things struck her more forcefully than others, and she was more likely to remember them. Beyond these considerations, there are reasons to worry that both what was actually said to her and what she recalls being said were shaped by expectations and cultural pressures of various sorts. The top regrets of the dying are suspiciously familiar. It turns out that the dying value exactly the things that our culture tells us all to value: the themes that fill our advertising and magazines. The variously attributed saying ‘No one ever said on their deathbed I wish I’d spent more time at the office’ and close variations thereof return close to 40,000 hits on Google, indicating how deeply such sentiments resonate with us. That friendship and family and feelings are valuable is surely part of the reason why we value them. But the everyday banality of the advice gives us reason for suspicion. The ubiquity of the advice suggests that there’s a cultural script in play here: a set of expectations that shapes what’s said and what’s heard. Perhaps our expectations lead us to fit what the dying say into the established script; perhaps Ware (for instance) turned more ambiguous or noncommittal statements into those she reports, or perhaps she recalls and reports only those that fit the script and ignores (who knows how many?) others that don’t fit. On the other hand, perhaps the script shapes what’s actually said: perhaps her reports are accurate and representative, but they were expressed because these are the kinds of things that one’s supposed to say in this kind of situation. The dying person knows full well that if they say they regret not making more money or not spending more time at the office, they’ll be seen as shallow. If cultural scripts are responsible for what’s reported or what’s remembered, it might well be that these nuggets of wisdom circulate so widely and are repeated so often not because they come from the dying (if they come from the dying at all): rather, it might be because they’re truisms that are attributed to the dying. If that’s right, the idea that we should give these regrets special weight because they’re expressed by the dying gets things backwards: we attribute these things to the dying because we give them special weight. What if further research indicates that the dying really do express such regrets, and we found out (somehow) that they genuinely express what really mattered to them at the time that they expressed them? Should we give them special weight under those conditions? I’m still sceptical. The idea that the perspective from death is epistemically privileged is one with a distinguished philosophical pedigree. Existentialists, in particular, are associated with the idea that holding the fact of our inevitable deaths in mind can help in achieving authenticity. In Being and Time (1927), Martin Heidegger argues that death individualises us, because at death we cease to be in social relations with others. This, for him, entails that awareness of my death enables me to grasp my authentic being, while evading contemplation of my death (by assimilating it to the empirical fact of the death of everyone) allows escape into inauthenticity. Other existentialists (eg, Karl Jaspers) developed this kind of thought in slightly different ways. Authenticity consists, more or less, in being true to oneself and to what is distinctive about each of us. It’s an ideal that many of us value. If Heidegger’s right, contemplation of death can give us a route to authenticity. When I’m absorbed by day-to-day concerns, I’m unable to glimpse my authentic self and what matters to me. If I manage to still the clamour without, perhaps I will better hear the voice within, and perhaps contemplation of death will enable me to hear better. The dying person, no longer distracted by these minutiae, might be in a better position to hear that authentic voice. But the appeal to authenticity, in its standard form, won’t vindicate the thought that the dying have special insight to offer to us. Authenticity is about hearing one’s own inner voice. Perhaps the dying person is in a better position to see what really matters for her. Yet there’s no reason to think that her wisdom indicates what matters most for me. Perhaps, though, the Heideggerian manoeuvre works for values other than authenticity. Perhaps there is a set of goods that are valuable for all – or almost all – of us, and the view from the deathbed helps us grasp them for the same reason it (supposedly) enables us to grasp what matters distinctively for each of us: because we’re no longer distracted by everyday matters. I want to suggest that this thought might have it backwards. The view from the deathbed might be impoverished, not enriched, because it’s outside of everyday concerns. Heidegger claims that, when we contemplate our individual death, we emerge from absorption in the everyday. The view from outside everyday life is very different from the view within it, he suggests. That’s plausible, but it doesn’t follow that the view from outside the everyday is more reliable. There are grounds for thinking it’s less reliable. You’re not going to start reading War and Peace if you know you have just 24 hours to live From within an ongoing project or enterprise, the stakes not only look different to how they appear from outside it; they are different. In an influential article on the absurd, the American philosopher Thomas Nagel notes that, when we exercise the distinctively human capacity to step back from the life in which we’re immersed and survey that life and ourselves ‘with that detached amazement which comes from watching an ant struggle up a heap of sand’, all justifications for our struggles seem to drop away. Meaning seeps away because taking this crucial step backwards entails stepping back from the kinds of concerns that constitute reasons for us. It is only from within our life projects that questions about justification can be answered at all because, in the absence of the commitments that give our struggles meaning, nothing is justified. The view from the deathbed might be the closest we can get to seeing life and its concerns from outside. From outside, we can’t grasp the significance of those concerns, not because they’re not real, but because they’re graspable only from within. Most plans and projects have a significance only for the person who’s confident she’ll live for some time to come. Saving money often makes sense only because there’ll be a later, at which I might need the money. Learning French might make sense only in view of the possibility of a visit to Paris. Planting roses might make sense only if I’ll be around to see them bloom (of course, people plant things for their children or even for strangers – trees are a storied example – but even this sort of planting often depends on having some kind of personal future, because the sapling might need care before it can survive on its own). Even starting a book, or a box set, is an enterprise that could require confidence in a personal future to make sense. You’re not going to start reading Leo Tolstoy’s 1,392-page novel War and Peace (1867) if you know you have just 24 hours to live. You won’t even start watching Game of Thrones. Once you know your death is imminent, extended plans and projects cease to have a grip on you as valuable activities; valuable for you. On the deathbed, only a narrower, more immediate, set of commitments continues to have significance. When we know that we lack a personal future, we find ourselves external to the system of justification that underwrites longer-term projects. We still understand them, but now we see them from the outside. Finding ourselves on the outside of the frames of justification that underwrite these projects, the passionate commitment of others to them seems absurd, like the ant’s struggle does to Nagel. The view from the deathbed is a perspective that has shed important sources of meaning. If that’s right, the view from the deathbed is epistemically distinctive. It’s the perspective of someone who is embedded in a simpler set of commitments: for whom simpler pleasures – those that can be realised immediately, or come to fruition relatively quickly – retain their grip, but for whom broader commitments are absurd. The view from the deathbed comes as close as is humanly possible (for those who aren’t deeply depressed) to abandoning the sets of commitments that give more extended projects meaning. It’s not because the deathbed is epistemically privileged that diachronic projects look meaningless. It’s because the perspective lacks the temporal horizon that makes sense of them. These projects lack meaning because their meaning is graspable only from within. They’re still meaningful, even if those who lack a future can’t sympathise with them. In his recent book on the midlife crisis, the American philosopher Kieran Setiya argues that these crises can arise because, as we complete our projects, they lose their meaning for us. These projects are telic: they have a goal, and it’s our commitment to this goal that makes them meaningful to us. Once we’ve achieved that goal, they come to seem absurd. Setiya counsels us to ward off the midlife crisis by finding value in the atelic: in activities that don’t have a goal beyond themselves (going for a walk for the sake of it, rather than to get somewhere, for example). Whatever the merits of his solution to the problem that he sees midlife as posing, Setiya’s distinction is a helpful one. Telic pursuits are diachronic; atelic are not, or not necessarily. From midlife, when those of us who are lucky have achieved some of our goals, these telic pursuits seem pointless and absurd. But we remain in the midst of life and have to find a way to recommit to ongoing projects (Setiya recommends finding value in the moment, in the atelic aspects of our telic activities). At each life stage, we face a different mix of telic and atelic activities. These activities are meaning-conferring for us; they constitute what’s valuable. Perhaps the perspective from the deathbed authentically reflects what matters for those who are forced to withdraw from ongoing activities, in view of their foreshortened temporal horizon, and what has value for them. But their wisdom doesn’t illuminate what has value for those lucky enough to be able to continue to engage in worthwhile telic activities. From outside the telic, only the atelic (or the pursuit of very short-term goals) retains meaning. Companionship, contemplation, beauty… they remain available to the dying and take on extra force. But their perspective is partial. Perhaps from the deathbed certain goods are grasped especially forcefully, but others slip away entirely. It’s not because these commitments lack value that they’re seen as pallid or pointless; it’s because their value can be fully grasped only from the inside. To read more on life, death and meaning, visit Psyche, a digital magazine from Aeon that illuminates the human condition through psychological knowhow, philosophical understanding and artistic insight.</description>
      <pubDate>10 Mar 21 12:37 EST</pubDate>
      <guid>https://aeon.co/essays/why-is-the-deathbed-perspective-considered-so-valuable</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.thenation.com/article/culture/robert-fulford-glenn-gould/</link>
      <description>&lt;a href=&#34;https://www.thenation.com/article/culture/robert-fulford-glenn-gould/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In the Vicinity of GeniusIn the Vicinity of GeniusHow a friendship with Glenn Gould created an unlikely cultural critic. March 12, 2021 Living in the shadow of a genius can destroy your life, but there’s much to recommend a slightly more distant proximity to incandescent talent. In his recent essay collection A Life in Paragraphs, Robert Fulford, widely regarded in Canada as the nation’s finest cultural critic (although enjoying less repute in the wider world), reflects on all the dismaying biographies that detail the miserable frailties of towering talents. He notes that the authorized life of V.S. Naipaul shows the novelist to have “monstrously mistreated both his first wife and his long-time mistress, out of a bottomless narcissism that allowed no room for the feelings of anyone but himself.” And that a spate of biographies document that Robert Frost, far from his fabricated image as a “a lovable curmudgeon” and “a national grandpa,” was “a wretched husband and father, a tyrant in personal and professional relations, jealous and vindictive.”1 With characteristic erudition, Fulford runs through the lives of troubled giants ranging from Charles Dickens to Miles Davis, from John Cheever to Stan Getz, all figures who brought grief to those around them, leading to reflections on the dark side of creativity.2 Yet Fulford’s own life offers at least one compelling counterexample. In 1941, when he was 9 years old and living in a remote corner of Toronto, Fulford befriended a new schoolmate, a boy named Glenn Gould. Fulford soon learned that Gould was his next-door neighbor. The two boys became fast friends, although even then they were unevenly matched.3 Gould’s conventional parents were fearful of using the word “prodigy,” since they didn’t want their only child to be seen as strange, but they also knew they had a world-class talent on their hands. In his memoir Best Seat in the House (1988), Fulford recalls that in high school Gould “was clearly an oddity, but he was not despised for it. His prodigious and mysterious talent made him immune (or so I recall) to the cruelty that adolescents routinely visit on the exceptional among them. When he walked home from school, waving his arms as he conducted an invisible symphony orchestra and humming the parts (‘pa-puh, duh-pa’), the other students just assumed he was acting the way geniuses were supposed to act.”4 If the young Gould was rightly assumed to be on his path to greatness, Fulford himself showed little promise. A lackadaisical student, Fulford dropped out of high school to become a newspaper copy boy, following in the footsteps of his father, another dropout turned ink-stained hack. Spending his early days covering high school hockey games, Fulford displayed a lightning typing speed—the only hint of a successful reporting and editing career. But there was no reason to expect, based on his early work, that he would become, as he has, an admired cultural critic.5 It was the friendship with Gould that made all the difference. “Music, Glenn’s music in particular, was the real beginning of my life as a thinking adult,” Fulford recalls. “Listening to him play, and listening to him talk passionately about music, rearranged my perceptions and informed me of a larger world than [high school] knew.”6 As students, Fulford and Gould would argue about music. Fulford was acquiring a taste for jazz and other forms of popular music, which Gould dismissed. Having to argue with someone as informed and quirkily opinionated as Gould forced Fulford into becoming an ad hoc critic, thus beginning a second career on top of journalism.7 In 1952, while still covering high school sports, Fulford teamed up with Gould to create New Music Associates, a quixotic effort to promote Arnold Schoenberg and other experimental musicians to provincial Torontonian ears. The venture wasn’t a success, with one ill-fated concert overshadowed by a rare Toronto hurricane. Gould, in any case, was already on the cusp of the international fame that would rob him of the time needed to be a concert promoter.8 Fulford, having acquired a taste for the arts, was set on a path of fusing journalism with cultural criticism. A true autodidact, he has found that happy middle ground between academia and the daily press, avoiding the abstruseness of scholarly writing and the glibness of deadline-driven punditry. Although a longtime columnist for the National Post, he’s also found a particularly nurturing home in Queen’s Quarterly, a literary journal that has allowed him to stretch his legs as a long-form essayist.9 A Life in Paragraphs is a rich harvest of Fulford’s Queen’s Quarterly essays and testament to the range of his intellectual curiosity. The essays cover topics from the Talmud to the tango, from the Roman Emperor Julian to Alice Munro. What holds the book together is not a set of ideas but rather a temperament: a wry, associative mind eager to apply itself to understanding human creativity. For many years, Fulford has been my model for what a good cultural critic should be: someone who thinks aloud in crisp, flexible prose aimed at a broad audience.10 Surveying the work of Walter Benjamin leads Fulford to reflect on his own failure to become a flâneur, a dandy voyeur excited by the spectacle of big-city life. “For years I have aspired to the status of at least a part-time flâneur, but something has always kept it beyond my reach,” he reflects.11 I blame the presence within me of the pale vestiges of Protestantism, a gentle but oppressively persistent fog which clouded the world around me during my Toronto childhood. We learn most when we least know we are learning; I believe I learned, without knowing it, that there was no place in our world for someone whose main joy was observation and whose main interest was noting, often with a certain disdain, the varieties of humans in his path. I walk, but usually with a purpose. I wander, but not for long; soon I look for a bench where I can read.12 Yet it’s not quite true to say that Fulford is a failed flâneur. If he’s not a flâneur of city life, A Life in Paragraphs shows him to be a flâneur of books, of museums, and of concerts. Never dogmatic, Fulford uses each cultural encounter to think through his experiences. What began decades ago as arguments with a high school friend who happened to be a genius now flourishes in essays where Fulford argues with himself.13 </description>
      <pubDate>14 Mar 21 23:53 EDT</pubDate>
      <guid>https://www.thenation.com/article/culture/robert-fulford-glenn-gould/</guid>
    </item>
    <item>
      <title>Get Better At Yanking And Putting In Vim</title>
      <link>https://peppe.rs/posts/get_better_at_yanking_and_putting_in_vim/</link>
      <description>&lt;a href=&#34;https://peppe.rs/posts/get_better_at_yanking_and_putting_in_vim/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; a couple of nifty tricks to help you copy-paste better: reselecting previously selected text (i use this to fix botched selections): gv &#34; :h gv for more &#34; you can use `o` in visual mode to go to the `Other` end of the selection &#34; use a motion to fix the selection reselecting previously yanked text: `[v`] `[ &#34; marks the beginning of the previously yanked text :h `[ `] &#34; marks the end :h `] v &#34; visual select everything in between nnoremap gb `[v`] &#34; &#34;a quick map to perform the above pasting and indenting text (in one go): ]p &#34; put (p) and adjust indent to current line ]P &#34; put the text before the cursor (P) and adjust indent to current line </description>
      <pubDate>17 Mar 21 10:05 EDT</pubDate>
      <guid>https://peppe.rs/posts/get_better_at_yanking_and_putting_in_vim/</guid>
    </item>
    <item>
      <title></title>
      <link>https://lareviewofbooks.org/article/obedience-training/</link>
      <description>&lt;a href=&#34;https://lareviewofbooks.org/article/obedience-training/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Wouldst thou approve thy constancy, approve First thy obedience, the other who can know, Not seeing thee attempted, who attest?                         — John Milton, Paradise Lost, 9.367–369 ¤ I’D LEAPT INTO puppy ownership the way many of us do: a long and tentative thought process followed by an impulsive move. One day there was a notice of a litter on Petfinder, then a trip to the shelter and a tiny girl puppy who crawled, trembling, into my lap. She had a white blaze, a spike of hair protruding from her head, and a tiny black nose that would turn pink as she grew up. I walked out holding her in my arms, only to realize, once I got home, that I was in possession of almost no supplies. My approach to pet ownership was prophetic. I had imagined my dog as a companion who would stare at me adoringly, who would romp when I wanted to romp, and snuggle when I wanted to be still. I’d learn quickly, however, that puppies have a mind of their own. Housebreaking aside (and that’s a big aside), her energy in those first few months was relentless. She chewed; she whimpered; she howled. She tangled herself in my feet when I walked; she contracted a parasite and shit in every corner of the house. She never wanted to sleep when I did, or if she did want to sleep, she certainly didn’t want to sleep alone. Something on the internet told me that she had separation anxiety and would need Prozac if she were ever to function by herself. In a moment of panic, I abandoned the internet and called a long-distance friend who specialized in training Labradors for field trials. Was there an alternative to pet antidepressants? She recommended obedience school. Specifically, she recommended a trainer approved to teach something called the “Koehler Method.” ¤ Prior to my puppy’s nervousness, the world’s first version of separation anxiety occurs in Book 9 of John Milton’s epic, Paradise Lost. Eve incites it, in the context of a lengthy conversation with Adam about how best to run their garden home. Even in paradise, they have work, though it is work of such a nature that it hardly seems such, and Adam reminds her that their task is far from “strict.” Eve, however, wants to get their garden tidy, and she remains insistent that their pruning and tending can be more efficiently accomplished if they split up. The part of this exchange I find most fascinating is the sheer length of their discussion, as well as her response to Adam at the end. He finishes by saying, well, if you think this way, then “go, for thy stay, not free, absents thee more.” And Eve replies, “with thy permission, then […] the willinger I go.” Except, I always think, he never gives her permission. I called my friend back the next day. Okay, I said, there are two registered Koehler dog trainers in Southern California. Each one, however, involves about a 45-minute commute, in good driving conditions, from where I live. Well, she responded, you asked for my advice. ¤ The Koehler method is a mode of dog training named after William Koehler, a man who began his animal training career in the military, at the Pomona Ordnance Base. In 1946, he started teaching public classes and private sessions, and, as the current Koehler Dog Training website indicates, his method has been a recognized form of obedience training ever since. While the Koehler method has various identifying characteristics, the most singular remains its focus on producing dogs who behave “reliably off leash.” In Koehler’s theory, control of your dog off leash is the most telling indication of obedience, though “control” becomes something of an oxymoron as your dog’s training matures. Koehler promises the ability to stop a free-standing dog with a single command, but also suggests that you won’t need this ability once you have it: your dog will choose, of his or her own accord, not to chase that squirrel, run out that door, cross that street. The Koehler method produces dogs who can be trusted with autonomy, who can make “better choices for [themselves].” Koehler dogs can, in other words, execute safely the workings of free will.  Koehler was also famous for being “movieland’s most experienced dog trainer.” Koehler had graduated from the military to head animal trainer for Walt Disney Studios, where he handled some of cinema’s best-known dogs. Celebrity, however — among any species — apparently comes at a cost. As indicated by the defensive tone of the introduction to his 1962 book The Koehler Method of Dog Training: Certified Techniques by Movieland’s Most Experienced Dog Trainer, Koehler’s techniques were registering as controversial with readers and owners even decades ago. Koehler advocates using choke chains and throw chains, types of negative reinforcement that many trainers nowadays forgo. “Because it proclaims the kindness of adequate discipline when needed to correct a fault that cannot be condoned, the book might disturb some folks who have nothing to offer but their own emotions,” Koehler states. He’d long been the last resort for people with dogs so problematic that it was either his training methods or euthanasia, and he defends his most extreme procedures in the context of that choice. Luckily, I’d grown up as a WASP-y New England kid, so keeping my emotions in check was no big deal. While my family didn’t practice any kind of organized religion, Puritanical ideologies were in our blood. Discomfort was a mark of character, self-denial a virtue to be praised. We walked to school through blizzards and hail, and I moved to California convinced that “self-care” was merely a West Coast rationalization for sloth. Rhetorically, characterologically, it was clear to me which sort of “some folks” the aspiring dog trainer should be. I called the listing in Ontario, California, and signed us up. ¤  It turns out that that property in Ontario was the site of Bill Koehler’s original Southern California kennel. The man I would be working with, Pat Smith, had purchased it from Koehler in the ’90s when the Koehlers had retired to Washington State. I didn’t know this when I registered, and the place, when I first sighted it, was not prepossessing — all adjacent strip malls, dirt, and heat. I’d signed up for private training sessions on Saturdays, since I couldn’t manage the commute to group classes during the week. My then-husband and I went to most of the sessions together — eight, maybe 10, weeks in total to complete the novice course. It took an effort to drive out there, and I’d had to promise Pat up front that I’d do a minimum of 20 minutes of training each day between our lessons. I knew that, like an omniscient god, he’d be able to tell if I did not. Pat and his wife Marilyn were religious, I believed. I’d seen Bible verses printed in their little sign-in shed on my first visit, Christian iconography here and there. But in all our time together, we never talked about anything but dogs. Our sessions each had a clear goal that Pat would demonstrate and then ask me to repeat: from drills on a long line, to encourage my puppy to stay close to my side; to leash work involving heeling and abrupt changes of direction; to off-leash work requiring careful attention from both of us; to prolonged sit-stays. Still, it was true: I liked the training method because it involved persistence, a very Protestant ideal. My dog liked to work, as well. She was always excited when I pulled out her leash and training collar, always bouncing as we set out. Each week we’d go home with specific homework and come back to show Pat what we had learned. In between, we practiced on our home sidewalks, at a local park, and by the baseball field as the team warmed up. We had a great training session near the football field while the marching band rehearsed, the color guard threw flags, and the tubas sounded like things possessed. We practiced in a church parking lot, too, the stained-glass windows casting color on the asphalt, both of us oblivious to their scenes. ¤ Our locations for training were influenced in part by the charge that we should seek out distractions, not avoid them. The Koehler obedience manual — The Koehler Method of Open Obedience for Ring, Home, and Field, first published in 1970 — has a memorable black-and-white photo of a German Shepherd, sitting at attention while surrounded by a cat, an agility board, a plate of food, a duck, a bag of dog treats, and a horse. “In addition to visible distractions,” the caption reads, “this area is covered with tempting scents.” The dog sits stoically, staring straight ahead at the cameraman or trainer or me. My goal, similarly, was to tempt my dog so that I’d have the opportunity to intervene. “Open wide the favorite gate,” reads a passage in Koehler’s manual. “Prop it lest a roughish wind spoil your moment. […] [Y]ou’re not heeling, you’re hoping — that he’ll spot the open gate.” A bolt, a break, a correction, and she’d be one step closer to the statuesque Shepherd who demonstrates obedience by willfully ignoring the temptations that encompass him round. Logically, I understood. In practice, as I set about finding scary trash cans, small children, and brazen squirrels, I came to feel that I wasn’t always playing fair. I was teaching my dog self-control, and every time I had to discipline her (in the etymological sense of “teach”), I was theoretically bringing her that much closer to disciplining herself. But as I would wait with her on that sidewalk, I didn’t know if I should be happier when she bolted away from the monster tuba, or happier when the previously scary skateboard zipped by and she didn’t turn her head. If she didn’t bolt, had I failed in my job as a trainer, since I hadn’t anticipated some supreme temptation yet awaiting her canine will? If she did bolt, had I failed in my job since her reaction proved that she wasn’t yet fit to govern herself? It was during these sessions that Paradise Lost started weighing heavily on my mind. I teach the epic about once a year, and students (and the poem’s original readers, too) regularly get tangled in the Miltonic presentation of predestination versus foreknowledge and, specifically, Milton’s contention that the two are not the same. “If I foreknew, / Foreknowledge had no influence on their fault, / Which had no less proved certain unforeknown,” states Milton’s God, in describing his knowledge of original sin. What God means is that free will still exists among mortals, even though he already knows everything they are going to do. God knows Adam and Eve are going to succumb to temptation, but that doesn’t mean he makes them do it. The distinction mattered to Milton, for political and personal reasons both. As a Puritan on the losing side of the English Civil War, he needed to feel himself “sufficient to have stood, though free to fall,” and if free to fall, then free to rise again. But students also get tangled in the fact that God plants the very tree that marks man’s fall and fault. Adam explains God’s reasoning to Eve thus: obedience is meaningless unless tested, so if you are obedient simply by default and not by choice, the virtue doesn’t hold. Still, it is hard not to feel that God sets them up, hard not to notice how the enlightenment that follows disobedience can hurt. Milton’s epic ends with both parents shedding tears, barred from their garden by a flaming sword. They bear the inheritance now of effort: the promise of difficulty in tilling the ground to grow crops that previously flourished, the agony of childbirth for a process required to fulfill God’s mandate that we populate the earth. Back to our sidewalk, featuring an old ice cream sandwich, a pile of poop, a hotdog bun with flies. Maybe I do have more of those “emotions” that Koehler cautions the trainer against. I seem to feel a doggy pull of desire, a postlapsarian guilt, a sadness that doesn’t come from my pup. She loves the sandwich yet forgives me my correction, moving seamlessly from correction to task. She doesn’t know what in the world could be more interesting than what’s on that sidewalk, but the fact that I’m telling her to look at me suggests that something could. She doesn’t know I’ve engineered her temptation. She trusts me, every time. ¤ Milton himself wasn’t the most trustworthy of men. He wrote Paradise Lost in 1667, as a “reformed Puritan” who had newly declared allegiance to the king. Ostensibly a retelling of the war in Heaven and the Biblical creation and fall of man, his epic easily reads as a political allegory too, with Royalists invited to read Milton as a now-devoted subject of the crown, Oliver Cromwell as the rebellious Satan, and Charles II as the omnipotent God who punishes disobedience and restores glory and order to us all. And yet, the epic also has the potential to reflect on the “lost” paradise of Cromwell’s Republic and the alternate forms of government that Milton and his kind had hoped to maintain. Even Milton’s first readers often found Satan the most charismatic of his characters and saw his God as tyrannical, hard to like. I love teaching the epic to undergraduates because Milton is so good at playing to both sides. And Milton himself may not have known, fully, where his sympathies resided. By the time he wrote Paradise Lost, his work of “long choosing, beginning late,” he was in his 50s and completely blind. He composed the poem orally and dictated it to his daughters to be transcribed, and throughout the poem, his blindness provides a constant source of anxiety and fear. His agony doesn’t simply concern the condition of his blindness, as Milton hopes to claim the disability as a sign of grace, of proof that, as his life is harder than that of others, so he must be more important to this world than most. Suffering, however, can also simply be punishment for past crimes. As a royalist, Milton should believe in the monarch as God’s deputy on earth. As a Puritan, Milton supported the execution of a king. Might not his blindness be the result? Every time I read the epic, Milton seems trapped between these readings, desperate to prove his election, tortured by his guilt. And yet, in either reading, suffering resonates as a requirement of attention. To be noticed by God, in any capacity, is to feel pain. No wonder those of us who grow up in his tradition feel at times the need to hide. My dad, my gentle and most beloved teacher, once told me that his job as a parent was to discipline me: to see and show me my mistakes. ¤  After a full course of novice obedience lessons, we drove to Pat’s property for our final exam. My dog had mastered, to a point, heeling off leash, sit-stays for a long duration, the drop-on-recall, and various other techniques. The point of the exam was to walk her through these skills one final time in front of Pat; it would mark the end of our training-treks to Ontario, though not the end of the rituals we had rehearsed. “Once a dog has a solid course of novice training,” Pat had told me, “it sets. The dog can get rusty, but the skills are ingrained.” He’s been right: over a decade later, she’ll periodically ignore me, but when I go back to our basic routines, she proves that all her training habits remain intact. Her ignorance these days, then, is willful. She races out the door after that squirrel by choice. I don’t remember a lot about her exam. I do remember that the morning began atypically gloomy and that, as we drove east, the clouds piled up. By the time we got to Pat’s house, it was windy, wet, and cold, and my little dog became bedraggled as she marched through her tasks. Our last test was an off-leash sit-stay from a distance, in which I stood about 30 yards in front of her and was supposed to ask her to come. I placed her on her stay, turned heel, and walked off. When I turned back, she was sitting there, small and solitary, her coat matted with water, her little face peering ahead. Would it have been so hard to stay together? Would life have been so different if one of us had not strayed? I waited a few moments before I issued the recall, the two of us getting ever damper in the rain. Her anticipation was palpable, the moment of connection, rich. I felt that “invisible string” I conjure up for my children; I felt her eyes fixed on me from across the field. The sky darkened, and still I stayed. ¤ Emily Hodgson Anderson is a professor of English at University of Southern California. She is the author, most recently, of Shakespeare and the Legacy of Loss (2018). </description>
      <pubDate>23 Mar 21 13:11 EDT</pubDate>
      <guid>https://lareviewofbooks.org/article/obedience-training/</guid>
    </item>
    <item>
      <title>Accountancy is the Priesthood of Modern Life</title>
      <link>https://blind-spots.org/2020/11/09/accountancy-is-the-priesthood-of-modern-life/</link>
      <description>&lt;a href=&#34;https://blind-spots.org/2020/11/09/accountancy-is-the-priesthood-of-modern-life/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; BlindSpots returns on its mission to analyse ideas that get out of hand. Accountancy-thinking is ideologically over-dominant in our decision making at the levels of both business and government. Like anything that becomes ideologically over-dominant, it becomes above question in people’s minds and begins to behave like a religion. The Brains of the Operation Most of us have seen pictures of society pyramids in history class. They show a social hierarchy in a given time or place. The one depicting medieval Europe was probably the most common. I like this one as it shows how the aristocracy and the church shared the top spot on the pyramid between them. In what Thomas Piketty calls ternary societies (split in three between peasant commoners, the learned clerical class, and the aristocratic warrior class), the learned clerical class’s function is to tell stories that bring meaning and try to make sense of the world, both for the commoner farmers and merchants, and for the aristocratic knights and lords.  The church functioned as this learned class in medieval Europe, being the wardens of schools and universities for nearly two millennia. India’s Brahmins functioned in a comparable way. Aside from functioning as Hindu priests, any work involving creativity, thinking, or administration of any kind was reserved for people born into this class. Crucially, they didn’t take vows of celibacy as European priests did, meaning they could reproduce out in the open and so, did not have to recruit from the commoner and noble classes, as the European church did. This learned class stands side by side with the wielders of power at the top of the pyramid, acting as a moral compass and as the learned councillors of the ones making the decisions. Think of druids, of King Arthur’s wizard Merlin, or of the Maesters depicted in Game of Thrones. A Thought Experiment Imagine if our society had an extremely influential profession, one that was seen as so obviously correct about everything that to not make our decisions according to what they had learned when they were being educated would make you seem irresponsible or insane. Imagine all of society was deferential to the teachings of this profession.  A society where medical teaching informed all policy and decision making would actually have a fairly strong argument for it. Imagine such a society. All good business decisions were ones that brought about the best medical outcomes. All public policy would be designed to maximise medical health. We would probably be a lot healthier, but would be any happier? I would miss chips, whiskey, and partying.  I’m asking you to imagine not so much a society where doctors themselves are bossing us all around, but a society where the teachings of the medical profession dictated to everyone else and were obediently followed without question in every area of life, well beyond medical settings.   In fact, as the governmental responses to covid has shown, we don’t even let the medical and health expertise dictate policy to us! Of all the professions that exist, the best candidates for that spot at the top of the pyramid seems to me to be the medical and scientific professions, but even at that, it’s still not a great idea. So why do we let accountancy dictate decision making to so much of society? The Syllabus, Not the People It’s my suggestion that the thinking and teaching underpinning accountancy are at the top spot on our society’s pyramid and that this doesn’t always work well for us. I’d like to emphasise that my point is that accountancy as a discipline has influence beyond the limits of where its influence belongs. In no way do I intend to suggest that it is without value as a profession, or to insult anyone that practices it, but to analyse places where its teachings are applied in areas beyond the borders of where it’s appropriate, which in turn leads to suboptimal decision-making. My aim is to write about the limitations of over-applying accountancy thinking in business, then to show how this thinking dominates at the governmental level where its dominance is having even more dire consequences for our societies and our world.  Lastly, we’ll cover how these two areas give us clues to how a lot of the problems our world faces seem to stem from our apparent confusion about money at its most basic level.  We seem to have drifted into believing that money is more real than the things that money can buy.  In Commerce – Emperor and Pope The American Sarbanes-Oxley act of 2002 was a reaction to corporate and financial abuses like those seen at Enron and WorldCom. One of the ways in which it sought to safeguard against scandals of that stature was by demanding that both chief executive and chief financial officers sign off on financial statements. In practice, this has gradually elevated the importance of CFOs to a level approximately as powerful as the CEO. If the CEO is charged with deciding the strategic direction of a firm, the CFO’s voice and perspective has come to the forefront of how the route there should be navigated. More importantly: due to the finance department’s newfound influence at the tip of the pyramid, the other business functions have come to be compelled to think and act in submission to the teachings of accountancy and finance. Functions that don’t recite the requisite prayers back run the risk of being shunned as irresponsible or profligate heretics.  Side by side, the CEO and CFO have come to rule together as Emperor and Pope. The Emperor, tasked with strategic direction, and the Pope — treated as if they’re the only one that can count and calculate correctly —  given almost total and unquestioned trust over how to execute on the plan that gets the firm to its strategic goal. In Commerce – Cost over Benefit Drawing on some observations from my own career, the theory I was thought during my education (which was split between an undergraduate in marketing and a postgraduate in finance), and conversations with the accountant friends in my life: I want to lay out some of the flaws in accountancy-thought I believe can put more, not less, pressure on a company’s budgets and profits. What’s more, it is difficult, though not impossible, to find much discussion of these flaws.  When there is little or no discussion of the flaws in certain teachings or modes of thinking, it tends to amount to tacit acceptance of those faults as being ‘right’.    At the centre of these flaws is accountancy’s tendency to default to the mentality of parsimony (known as scabiness, in Ireland). Of course, there are times to be frugal, but having default approaches of any kind can cause lazy decision making that leans on a preconceived course of action that hasn’t bothered to factor in the specific details of the scenario being dealt with in the given moment. One place I observed this inclination towards frugality was in college lectures that covered pricing and price elasticity. Pricing is a discipline that both finance and marketing claim as their own but I noticed that finance and accountancy students had a very different attitude to it compared to us marketing students who were mostly career salespeople, account managers, and digital marketers; natives to the realm of generating income for our organisations. Total income from a given product or service is obviously a very simple equation:  (units sold X price per unit) The accountancy students tended to instinctively lean towards maximum mark-up on each individual unit but didn’t seem to have quite as good a feel for the idea that in some cases, the units sold figure can be massively affected by price (price elastic). Even after this being explained to them it seemed that there was just something in them that kept slipping back into the thinking that the ‘units sold’ variable was fixed. This is where parsimony can actually shoot itself in the foot. In cases where the price is elastic, you can actually make more total income by charging less and making less profit per unit.  At one energy utility employer; the sales and marketing teams had regular debates with the pricers (who fell under the finance team at this particular firm). What really confused the finance side was our point that when you are able to sell enough of something all at once: “Volume can be its own margin”. Our point, of course, was that some tenders and deals are so big that the volumes sold under them are equal in time and effort to dozens of small or medium deals. It just doesn’t appear to be in accountancy’s DNA to have a feel for this. Accountants themselves will tell you that it’s not really in their remit to understand the causes or probabilities that lead to business outcomes, but to record and account for them after the fact. This can lead to an inclination in the profession to either visualise commerce flowing backwards, with the final numbers booked at the end coming first, and all of the marketing and sales processes happening afterwards, or for it to fall into the very common post hoc ergo propter hoc fallacy where B is thought to be caused by A just because it came after it. For these reasons, I believe pricing should be firmly in the hands of people who’s heritage and background is in generating revenue as long as it is commonsensical and isn’t producing losses, which hopefully didn’t need to be said. Another way parsimonious accountancy-thought can manifest is its ability to distort cost/benefit reckoning. My experience has indicated to me that monetary cost dominates accountancy thinking to the neglect of benefit, which is mostly given mere lip service. At one employer, my team worked with an engineer who was approaching full qualification, working while completing their bachelor of science dissertation. Engineers in their category and specialisation are as abundant as shit from a rocking horse in Europe yet they were being paid about one-quarter of the market rate due to their not having graduated yet. Their management team knew that we ran the risk of losing them once they had their parchment, which would bring us from having four of their kind down to three in Europe when we really needed six. Our organisation’s accountants, in their infinite wisdom, had hardwired rules about maximum increases in pay for existing employees which blocked this engineer’s manager from being able to give them a raise in line with their market rate. We lost them to a competitor for 5x what we were paying them. Astoundingly, if they didn’t already work for us, we would have been able to easily match or exceed the offer from our competitor. This point was made during this period of wrangling over the engineer’s salary but was pushed aside since its logic clashed with the gospel according to accountancy. In terms of cost/benefit, the savings here did not cover the loss of a talented engineer that had been developed at our firm, only to be snapped up by a competitor when they graduated. It would have been cheaper to increase their pay by a factor of six and hold on to them. In fairness to the accountancy discipline, measuring benefit is a much more complex task than measuring monetary cost, which tends to be very clear cut. Perhaps this fact indicates a need for cost/benefit analysis to always function as a strictly bottom-up process as again, accountancy is intended to measure, capture, and record business results after they’ve happened, and not to be an imposition from above on how business is done. Especially since, if you owned geese that laid golden eggs, accountancy’s instinct would be to find savings on their feeding costs. At another employer, I managed teams of salespeople who generated subscription revenue for the organisation. My role was simple enough when you zoomed out: manage a budget that pays staff and logistics to bring in enough subscription income to cover the budget spent 3-5 times over. We hoped to bring in €3-5 for every €1 we spent.  As a fleet of vehicles we used was approaching the end of a lease-hire agreement, we had the option of buying them out for a lump sum worth about 5 month’s worth of lease – let’s say €3k each or, getting a new fleet for another 3-year contract at €600/each per month. As someone whose responsibility it was to own a profit and loss account, I was happy to get maybe four or five years of use from the fleet at a vastly reduced cost as all we would have to do was fuel and maintain them. The accountants, however, were more worried about the abstract and frankly imaginary concept of depreciation that would come from us technically now owning these assets and them losing 10% of their value per year on paper. This theoretical drawback was seen as more important than the real-life cost-effectiveness of being able to win revenue for the organisation with this logistical cost stripped out. Compared with €600 per month, 10% of the value being lost off each car each year would only have amounted to €300 anyway! This means we paid 24x more just to be able to say we didn’t own the vehicles. An academic concept from the accountancy scriptures won the day against real-life cash flow. But accountancy always knows best when it comes to anything to do with money, right? Like the priests and bishops of old, it has become quite standard for the accountants within the finance function to stand at the apex of influence within our organisations and tell the rest of us what and how to think about business. It is routine that the finance function is more influential than and dictates to marketing and other income-generating teams but it is extremely rare to see the inverse. It is even becoming increasingly common for CFOs to hold dominion over cybersecurity and other IT functions when it would be inconceivable to imagine a finance department answering into a computer science-educated Chief Technology Officer. Please let me know of any such examples if you have them!  Other examples I would be very curious to hear about are organisations that have a system of recourse or appeal if their accountants have decreed or dictated something to another business function that doesn’t make commercial sense.  Is their moral and intellectual authority derived from the suggestion that they are the only ones that can count? Is accountancy hailed as the only discipline within business that is numerically literate and so the only one that can be trusted with figures? One welcome development that could soon counter the weight of accountancy in firms’ decision making is the emergence of the Chief Revenue Officer and amalgamation of marketing, sales, and account management into a single revenue function. This could have the twin benefits of making the revenue-generating functions become more rigorous, analytical, and quantitative (something that can often be a weak point for them) in addition to making the finance department lean away from their go-to cost-cutting instincts and take a more balanced approach to thinking about revenues and costs together. Let’s now leave business and commerce behind and see how accountancy thinking is causing some even more bizarre disconnects from reality at the more macro level of governing states. In Government: The Accountocracy The nineteenth-century Scottish essayist, Thomas Carlyle called economics ‘the dismal science’ in 1849.  He was referring to Thomas Malthus’s dreary but widely accepted theory that human populations would increase exponentially but that food production would stay forever where it was at the time, or at least be unable to grow in proportion. “If they’d rather die, they had better do it, and decrease the surplus population” -Ebeneezer Scrooge, A Christmas Carol, 1843   This outlook is very reminiscent of the pricing debate (units X price) mentioned above. The pessimistic suggestion is that one variable is static while the other one moves freely. People assumed that productivity wouldn’t be able to keep pace and feed us all. I’d suggest that, despite the theory being debunked by technological advancement, dozens of millennia of struggle to feed ourselves is a long way from working its way out of our genes after just a few decades of plenty.  “Abundance is harder for us to handle than scarcity” –Nassim Nicholas Taleb, Antifragile, 2012 Beyond that, most of us live in a household that has to bring in as much or more money than it gives out. The organisations that most of us work for are the same in that respect. In fact, it is difficult for us to imagine an entity that doesn’t have this basic law of gravity keeping it anchored to the ground. Because of this, we’ve developed a way of imagining our nation-states as scaled-up shops, businesses, or houses that have to make money and budget to pay the bills the same way any bar, hair salon, or family home would have to. We borrow terms like ‘income’’ and ‘debt’, from the world of household or business finance and shoehorn government finances painfully into these ill-fitting clogs. Why are they ill-fitting? Modern Monetary Theory has the answer. And it answers it, over and over again, with one simple counter-question: What household or business creates its own money?  For any country that issues it’s own (reasonably valuable) currency, there is a completely separate set of rules and limits to how spending works. Under no circumstances is this to be misconstrued as a lack of rules and limits – just that there are different ones. Let’s get back to why ‘revenue’ and ‘debt’ are not actually what they are labelled as for nation-states that issue their own fiat currencies (currencies that are not pegged to precious minerals or other currencies). Revenue: Another term for income. At the government level, the subliminal suggestion is that the government’s ‘earnings’ must come from extracting taxes from wages and other economic activity before it has money to spend on anything. Conservatives, in particular, love to paint taxes and the government spending that it funds as ‘your money’ being taken from you. Margaret Thatcher’s rebuke that “eventually you run out of other people’s money to spend” has set the tone of economic thinking for 40 years now, and this delusion is far from dead today. Let’s trace back an imaginary British Pound, from the moment it was confiscated from some poor hard done-by bar worker in the form of income tax, to where it originally came from in the first place. Where was that pound before it was taxed? It may have been in some punter’s pocket before he walked into the given Wetherspoons and spent it on a half-pint of disgraceful Carling. Before that? It was with whoever gave it to the punter with the poor taste in beer. But how did that pound make its way into circulation in the first place? If Thatcher’s government saw it as ‘other people’s money’, then who gave it to the British government to issue originally? It’s so extremely counter to our generally accepted narrative about money to think that it was created and spent into the economy before it was taxed, but how could it physically have worked any other way unless there is some other entity giving British pounds to the British? For this reason, ‘revenue’ is not income or earnings for a currency issuer at all. It is a way of extracting money back out of the economy again after it had issued it in there in the first place. The government creates and holds cash before it circulates it into the economy, it doesn’t need to ‘earn’ it by taxing, because it can make as much of it as it wants. Debt: Whenever a currency-issuing country gives figures for its national debt, it is quoting the total value of all its bonds that people or other governments hold. In plain English: a bond is a coupon worth the amount of money deposited in it, that pays off chunks of interest until the original deposit matures at a pre-agreed time, and is paid back. For the bondholder, it functions like a savings account that pays out some interest every year until the bond matures and those savings have to be withdrawn. That’s it.  Government debt is not debt as we imagine it at our smaller household/business scale; it’s people saving money in ‘the bank of the USA/EU/Japan/Switzerland.’ A currency-issuing country changes the money on a bondholder’s ‘normal’ bank account every time they ‘lodge’ an interest payment with them each year and, when the bond expires, tops up the holder’s normal bank account with the value of the original bond amount and changes the value of their bond account to 0. All of this is carried out on a keyboard without ever even bothering to print the cash. We are not served well by borrowing the terms ‘revenue/income’ and ‘debt’ from private finance and imposing them on a country’s finances because they are literally not what they say on the tin in that setting.  Back to how there are still limits, just different ones. The number one point is that deficits or debts aren’t the limits, inflation is. Governments can create as much cash as they want but if they spend too much, inflation can occur. How much cash is too much? What is the limit on the cash level before it becomes inflationary?  The simplest answer is: when there is too much cash circulating in an economy relative to the amount of buyable stuff, it produces inflation.  When all the fancy, academic sounding terms are said and done, inflation is just another term for shortage of stuff for the amount of money that there is, or, put the other way around, too much cash chasing too few resources. Of course, one pressure valve here is that if the government is spending on things that are actually creating more productive resources (like power stations or business start-up grants), then there is plenty more actual stuff to balance out the amount of cash that’s been issued into the economy. Conservative economists tell stories that suggest that the amount of buyable stuff is static, though. Here again, we see yet another manifestation of the parsimonious accountancy habit of thinking that one of the moving parts in an equation is fixed. This anxiety, borne of an inability to acknowledge that some spending – particularly investment spending – actually creates more resources, is probably the main reason governments have tended to do too little to lubricate economies after crises and so, delay our emergence from them. Tragically, it took fifteen years of stubborn deflation (too little cash circulating for the amount of buyable stuff) before Western regimes stopped worrying that we were on the verge of inflation. How much dangerously low blood pressure would a doctor have to see before they felt they could stop thinking everyone had hypertension? When the economy is overheating with inflation, the government can siphon cash back out again as tax to restore a healthier balance in the cash-to-stuff ratio. This distinction between money and the stuff money can buy is the most important element in all of this. It is key to rising above the hypnosis we are all under in our accountocracies. Money is something that us little people and organisations have to worry about, but as you scale up to the level of nation-states, real resources and their management is what the focus is really on since they have control over the issuing of money. One of the things that has most obscured this reality is the fact that we have very selective rules about when we need to ‘find the money’ that the government spends and when we don’t. Sadly, when we discuss education or healthcare we usually talk about ‘where that money is going to come from’. Ireland’s education budget in 2021 will be €8.9bn, or 2.3% of GDP. In 2008, when the Irish state decided to underwrite and guarantee the banking system to protect it from collapse, it committed to a €64bn bailout, or 17% of GDP – or almost eight years worth of our education spend, and with little or no discussion of “where that money would come from”. Stephanie Kelton has detailed how she saw this selective double-standard at work as an economist at the US senate budget committee. There, she saw how any bill that proposed spending on healthcare, education, or infrastructure always needed an attached taxation or borrowing proposal to ‘cover’ that spending. Defence spending along with corporate or financial sector bailouts, however, did not. The latter were seen as strictly necessary and so it was completely acceptable for the Federal Reserve to simply lodge dollars electronically into bank accounts for these kinds of expenditures. She even followed the passage and approval of a bill to transfer hundreds of billions of dollars into Boeing’s bank account in exchange for a fleet of fighter aircraft. The transaction was carried out on a computer keyboard in the Federal Reserve and the bill had no taxation or borrowing provisions attached. Asking where we’ll get the money for weapons or corporate bailouts is seen as unpatriotic. Asking where we’ll get the money for schools and hospitals is seen as responsible. What is most interesting about MMT is that the ‘radical’ side of this debate are simply describing how things already work, while the orthodox or conservative side are clinging to a theory that described a practice that ended long ago. Deficits have not mattered to currencies since they abolished the gold standard in the 1970s but the orthodox side seems unwilling to update the theory to match what the practice has been since then. We just seem to be unable to stop imaging our nation-states as shops! Money is The Map, Resources the Territory I’ve written before about the blinding effect abstractions can have on us when we drift into thinking the symbols are more real than the things they were created to represent. Money is a representation of value. A scoring system against which we can grade the worth of real life things. Without the resources to buy with it, it is completely worthless. Accountancy-thought, due to the ease with which costs can be counted compared to benefit, or money can be counted compared to the value of real-life stuff, has led us to develop a habit of over-focus on cash to the neglect of the things that cash can buy, which is presumably what we were all in it for in the first place. This reversal in our understanding of reality, where we now think real-life stuff orbits around money, can be seen everywhere. Economists wondering if the world economy will ever recover from the covid pandession when all the physical makings of a booming economy and resources are sitting there is a baffling situation if you realise stuff is more real than money. Believe it or not, we are actually debating whether or not we should: Lubricate the economy by injecting cash into it to get all of our warehouses, factories, and businesses up and running and producing again once the pandession is over or:Leave all of those factories, warehouses, and businesses idle for lack of cash when they could be used to produce real life goods, services, employment, and prosperity again once we are able to get back to normal. You know an idea has us more than we have it when we start sacrificing practical reality for the sake of a notion. Is this real life? Or are we hypnotised by yet another religion, one dressed up as a social science with a thin veneer of mathematics camouflaging it as a science? If we’re going to continue to give accountancy the run of the place, there are a number of fundamental flaws in its thinking that need to be resolved in a hurry. </description>
      <pubDate>24 Mar 21 14:04 EDT</pubDate>
      <guid>https://blind-spots.org/2020/11/09/accountancy-is-the-priesthood-of-modern-life/</guid>
    </item>
    <item>
      <title>No one gives a shit what programming language you use</title>
      <link>https://georgestocker.com/2021/03/28/no-one-gives-a-shit-what-programming-language-you-use/</link>
      <description>&lt;a href=&#34;https://georgestocker.com/2021/03/28/no-one-gives-a-shit-what-programming-language-you-use/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I’ve had it up to here (gestures at nose level) with the bullshit around programming language elitism. Back when I thought it mattered I was just as bad as the bullshit I’m seeing today, but when I see this elitism in 2021, from people who long ago should have learned better, it just sets off my I-can’t-fucking-take-this alarm. This tweet made its way into my timeline via screenshot; and because it’s Sunday I’m going to respond to it, because I have nothing better to do at the moment. The littlest is taking a nap, the older two are playing on the Switch, and my wife and I are snuggling on the couch. Normally I’d sigh and save my words, but not today. Not today. I’ve written about the plague of UBM on our industry previously, but unfortunately that plague isn’t going anywhere. It has deep roots that we all must work to yank out, lest it destroy the good parts of our industry. One such example is the elitism expressed in the above tweet, and in the continual billing of software programming as ‘craftsmanship’, and equating good software developers with ‘craftsman’, as if software exists for its own sake, and purity is the goal. It’s performative craftsmanship, and deeply unlike what you see from actual crafts-people in their field. Take Jiro from Jiro dreams of Sushi, an example of a master of the ‘craft’ of making Sushi. Jiro doesn’t tout that he’s the best at it; but everyone else does. That’s a major difference from UBM’s (and others) self-identifying as a craftsman, is that Jiro doesn’t do that. He humbly learns and tries to get better, and more importantly tries to serve others. He doesn’t parade around with a self-given label that says “Sushi Craftsman” even though he ostensibly is. If you’re good at what you do and you bring value to others, they’ll label you with the appropriate moniker. This performative craftsmanship is a load of horseshit that we’ve given too much credence to. I hear it from ‘agile’ podcasts, and from UBM and his ilk, and even otherwise level-headed programmers express some sort of allegiance to taking pride in the code as a necessary bar to being a programmer. Anytime you hear a podcast talk about ‘code quality’ in isolation, or about ‘being a professional’, you’re hearing this same sort of elitist attitude towards software development. The only people who have to proclaim they’re professionals are people who are ostensibly not professionals. It’s another example of the idea that you can’t label yourself by your words, others label you by your actions. In another example of this performative culture of software craftsmanship, and I’ll use wood-working as an example because I grew up in that culture, woodworkers take pride in the finished product, and seeing that finish product being used. They are not all hot on the tools they use, and nor do they value the art of planing for its own sake. No, it’s always in furtherance of the ultimate goal: to produce something another human will enjoy using. Or put another way, no one gives a shit what programming language you use. No one gives a shit what your framework or technology stack is. I can guarantee without even checking that every single fortune 50 company uses Excel in ways that would potentially cost hundreds of millions of dollars in losses if there were an error. Do you think the person that pays for the VBA programmers cares that they’re using VBA? No. Do you think the programmer is just jonesing to replace VBA with clojure? Not if they understand what side their bread is buttered on, they don’t. I was going to make a bullshit analogy to a programming language being like a hammer, but that’s too simplistic. The purpose of the software defines the stack you end up using, and there are far more considerations than “this is a nice language to use”. In fact, I’d wager that the niceness of the language is probably the least important part of choosing a language. Just ask everyone who bet the farm on coffeescript. But the “you should use language X because it’s great” mantra doesn’t stop there. It’s a symptom of a larger problem: Thinking of programming as a worthwhile pursuit on its own, and focusing on the act of programming while ignoring how that software will be used. With the katas and the code quizzes, we tend to lose sight of the fact that programming for its own sake is mental masturbation, useful only in the most idle of circumstances. Writing tools and frameworks only help if they have a purpose outside of our own gratification. A programming language exists to put software into people’s hands and to make their lives better. It does not exist for the idle purpose of being good at that language, and it certainly doesn’t exist as gatekeeping, as a way of separating programmers into social classes. The elitist fucktards that create these social classes do so because it makes them feel important. They have created a bar and put themselves above that bar, while the lowly working programmer should follow their example if they want to be a ‘software craftsman’ or a ‘software professional’. It’s self-serving poppycock (another word for bullshit, I’m certain). It’s also why if you brand yourself as a ‘clojure programmer’, there’s literally only two sets of people who will ever care: Other programmers you want to impress, or a recruiter who is looking for a ‘clojure programmer’. Go say you’re a clojure programmer to your spouse or family, and be prepared to realize how much bullshit language segmentation is to people outside of our industry. I used to suck at programming. I mean, I still do, but I used to too. So I get that it feels good to ‘be good’ at programming. But being good at programming doesn’t matter if you’re not producing valuable software for people to use with those skills. If the most gifted singer in the world doesn’t sing to an audience, does it matter how gifted they are? So you want to pick a framework or a language? Great. The last person you should listen to is UBM, and the second to last person you should listen to is me. Pick the language and stack based on your team’s needs and comfort, based on your business’s need and risk tolerance, and based on how easy it will be to produce software for your target users in that language or framework. That’s your criteria. Not what I think, not what UBM thinks, and certainly not the language or framework de jour on Hacker News. And if you find yourself proclaiming everyone should be using your favorite programming language or framework, save it. We don’t give a shit. </description>
      <pubDate>29 Mar 21 10:01 EDT</pubDate>
      <guid>https://georgestocker.com/2021/03/28/no-one-gives-a-shit-what-programming-language-you-use/</guid>
    </item>
    <item>
      <title>How People Learn to Become Resilient</title>
      <link>https://www.newyorker.com/science/maria-konnikova/the-secret-formula-for-resilience</link>
      <description>&lt;a href=&#34;https://www.newyorker.com/science/maria-konnikova/the-secret-formula-for-resilience&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Norman Garmezy, a developmental psychologist and clinician at the University of Minnesota, met thousands of children in his four decades of research. But one boy in particular stuck with him. He was nine years old, with an alcoholic mother and an absent father. Each day, he would arrive at school with the exact same sandwich: two slices of bread with nothing in between. At home, there was no other food available, and no one to make any. Even so, Garmezy would later recall, the boy wanted to make sure that “no one would feel pity for him and no one would know the ineptitude of his mother.” Each day, without fail, he would walk in with a smile on his face and a “bread sandwich” tucked into his bag.The boy with the bread sandwich was part of a special group of children. He belonged to a cohort of kids—the first of many—whom Garmezy would go on to identify as succeeding, even excelling, despite incredibly difficult circumstances. These were the children who exhibited a trait Garmezy would later identify as “resilience.” (He is widely credited with being the first to study the concept in an experimental setting.) Over many years, Garmezy would visit schools across the country, focussing on those in economically depressed areas, and follow a standard protocol. He would set up meetings with the principal, along with a school social worker or nurse, and pose the same question: Were there any children whose backgrounds had initially raised red flags—kids who seemed likely to become problem kids—who had instead become, surprisingly, a source of pride? “What I was saying was, ‘Can you identify stressed children who are making it here in your school?’ ” Garmezy said, in a 1999 interview. “There would be a long pause after my inquiry before the answer came. If I had said, ‘Do you have kids in this school who seem to be troubled?,’ there wouldn’t have been a moment’s delay. But to be asked about children who were adaptive and good citizens in the school and making it even though they had come out of very disturbed backgrounds—that was a new sort of inquiry. That’s the way we began.”Resilience presents a challenge for psychologists. Whether you can be said to have it or not largely depends not on any particular psychological test but on the way your life unfolds. If you are lucky enough to never experience any sort of adversity, we won’t know how resilient you are. It’s only when you’re faced with obstacles, stress, and other environmental threats that resilience, or the lack of it, emerges: Do you succumb or do you surmount?Environmental threats can come in various guises. Some are the result of low socioeconomic status and challenging home conditions. (Those are the threats studied in Garmezy’s work.) Often, such threats—parents with psychological or other problems; exposure to violence or poor treatment; being a child of problematic divorce—are chronic. Other threats are acute: experiencing or witnessing a traumatic violent encounter, for example, or being in an accident. What matters is the intensity and the duration of the stressor. In the case of acute stressors, the intensity is usually high. The stress resulting from chronic adversity, Garmezy wrote, might be lower—but it “exerts repeated and cumulative impact on resources and adaptation and persists for many months and typically considerably longer.”Prior to Garmezy’s work on resilience, most research on trauma and negative life events had a reverse focus. Instead of looking at areas of strength, it looked at areas of vulnerability, investigating the experiences that make people susceptible to poor life outcomes (or that lead kids to be “troubled,” as Garmezy put it). Garmezy’s work opened the door to the study of protective factors: the elements of an individual’s background or personality that could enable success despite the challenges they faced. Garmezy retired from research before reaching any definitive conclusions—his career was cut short by early-onset Alzheimer’s—but his students and followers were able to identify elements that fell into two groups: individual, psychological factors and external, environmental factors, or disposition on the one hand and luck on the other.In 1989 a developmental psychologist named Emmy Werner published the results of a thirty-two-year longitudinal project. She had followed a group of six hundred and ninety-eight children, in Kauai, Hawaii, from before birth through their third decade of life. Along the way, she’d monitored them for any exposure to stress: maternal stress in utero, poverty, problems in the family, and so on. Two-thirds of the children came from backgrounds that were, essentially, stable, successful, and happy; the other third qualified as “at risk.” Like Garmezy, she soon discovered that not all of the at-risk children reacted to stress in the same way. Two-thirds of them “developed serious learning or behavior problems by the age of ten, or had delinquency records, mental health problems, or teen-age pregnancies by the age of eighteen.” But the remaining third developed into “competent, confident, and caring young adults.” They had attained academic, domestic, and social success—and they were always ready to capitalize on new opportunities that arose.What was it that set the resilient children apart? Because the individuals in her sample had been followed and tested consistently for three decades, Werner had a trove of data at her disposal. She found that several elements predicted resilience. Some elements had to do with luck: a resilient child might have a strong bond with a supportive caregiver, parent, teacher, or other mentor-like figure. But another, quite large set of elements was psychological, and had to do with how the children responded to the environment. From a young age, resilient children tended to “meet the world on their own terms.” They were autonomous and independent, would seek out new experiences, and had a “positive social orientation.” “Though not especially gifted, these children used whatever skills they had effectively,” Werner wrote. Perhaps most importantly, the resilient children had what psychologists call an “internal locus of control”: they believed that they, and not their circumstances, affected their achievements. The resilient children saw themselves as the orchestrators of their own fates. In fact, on a scale that measured locus of control, they scored more than two standard deviations away from the standardization group.Werner also discovered that resilience could change over time. Some resilient children were especially unlucky: they experienced multiple strong stressors at vulnerable points and their resilience evaporated. Resilience, she explained, is like a constant calculation: Which side of the equation weighs more, the resilience or the stressors? The stressors can become so intense that resilience is overwhelmed. Most people, in short, have a breaking point. On the flip side, some people who weren’t resilient when they were little somehow learned the skills of resilience. They were able to overcome adversity later in life and went on to flourish as much as those who’d been resilient the whole way through. This, of course, raises the question of how resilience might be learned.</description>
      <pubDate>05 Apr 21 18:05 EDT</pubDate>
      <guid>https://www.newyorker.com/science/maria-konnikova/the-secret-formula-for-resilience</guid>
    </item>
    <item>
      <title>I Am a Heroin User. I Do Not Have a Drug Problem</title>
      <link>https://nautil.us/issue/96/rewired/i-am-a-heroin-user-i-do-not-have-a-drug-problem</link>
      <description>&lt;a href=&#34;https://nautil.us/issue/96/rewired/i-am-a-heroin-user-i-do-not-have-a-drug-problem&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Carl Hart is a neuroscientist and Ziff Professor of Psychology at Columbia University—he was the first tenured African-American professor of sciences at Columbia. His research focuses on the “behavioral and neuropharmacological effects of psychoactive drugs in humans.” Hart’s new book, Drug Use For Grown-Ups, is a bold and engaging effort to counter what he sees as generations of misinformation and moral grandstanding about drug use. Today’s “sensationalistic media coverage of the opioid crisis continues a long, awful tradition of exploiting ignorance and fear to vilify certain members of our society,” Hart writes. The media is not the only problem. Scientists, he states, “have frequently overinterpreted and distorted” drugs’ effects on the brain.Hart reports that more than 70 percent of drug users—whether they use alcohol, cocaine, prescription medications, or heroin—do not meet the health criteria for drug addiction. In Drug Use for Grown-Ups, Hart strives to “present a more realistic image of the typical drug user: a responsible professional who happens to use drugs in his pursuit of happiness.” With genial candor, Hart presents himself as a model drug user. “I am now entering my fifth year as a regular heroin user,” he writes. “I do not have a drug-use problem. Never have. Each day, I meet my parental, personal, and professional responsibilities. I pay my taxes, serve as a volunteer in my community on a regular basis, and contribute to the global community as an informed and engaged citizen. I am better for my drug use.”Nautilus caught up with Hart to discuss his drug use and his sharp points about science and society. He was as casually bold in conversation as he is in Drug Use for Grown-Ups.HABIT OF A HIGHLY EFFECTIVE PERSON: “My heroin use is as rational as my alcohol use,” Carl Hart writes. “Like vacation, sex, and the arts, heroin is one of the tools that I use to maintain my work-life balance.”Courtesy of Carl HartYou say “most drug-use scenarios cause little or no harm and that some responsible drug-use scenarios are actually beneficial for human health and functioning.” How so?Let’s just talk about alcohol first. When you’re at a wedding reception, alcohol serves as a social lubricant. People are more gregarious. They talk, they interact. The same is true with cocaine at parties, heroin among friends, or opium among friends, NDMA among lovers. It enhances empathy, openness, and forgiveness, all of these pro-social attributes.Drug research, you write, is full of bad science. If you had to name one example, what would it be?The notion that drug addiction is a brain disease. That encapsulates all that’s wrong with today’s science in this area. There is absolutely no data in humans to show that drug addiction is a brain disease. Yet the narrative, the dogma, the dominant perspective is that it does. Even though nobody will dispute that, there’s absolutely no data in humans to support that statement.Yet opioids do change the brain biologically, do they not?Yes, opioids bind to a class of receptors called endogenous opioids, which you find in endorphins, for example. Opioids bind to these receptors—just like natural chemicals do—which results in a response. In some cases, because of decreased sensitivities and certain types of pain, they may enhance a sense of euphoria. So it’s really just facilitating what’s already in the body naturally, a system that helps in our survival. Think of fructose or glucose. We add sugar to our tea, our coffee, whatever we have, we add more and more because we like it, it tastes good, and it enhances pleasure. It can give you energy. It can make life more interesting. Humans do not live on logic alone. And so sometimes we do these things, and that’s OK. People become addicted because they once had a middle-class-paying job that made them someone in their community. How have scientists “overinterpreted and distorted” the effects of drugs on the brain?Take brain imaging. People often show one image of someone’s brain. Let’s say this person is addicted to methamphetamine, according to DSM criteria, versus the brain of someone who’s not addicted. If you see some difference, some researchers have a propensity to make more out of the differences than are there. There’s a wide range of brain structural sizes, such that when we think about one person’s size of their nucleus accumbens, it may be smaller or larger than somebody else’s nucleus accumbens. But both of the nucleus accumbens, despite their sizes, are within the normal range of human variability. It’s like height. One guy might be 5’10”, another guy might be 6’2”. But we don’t say the guy who’s 5’10” is height deficient. We just say that he’s in a normal range, and he’s not as tall as the other guy. We wouldn’t say one is deficient versus the other. In neuroscience, one of the things that has happened, particularly when it comes to drugs, people have over-interpreted the differences to mean pathology, when, in fact, both of the brain structures are within the normal range of human variability. The overinterpretation is to interpret it as being pathological.You say the opioid crisis has been sensationalized, and write, “People are not dying because of opioids; they are dying because of ignorance.” What do you mean?Some people don’t know not to mix specific sedatives with opioids. For example, they don’t know not to mix large amounts of alcohol or large amounts of antihistamines. Specific combinations can lead to respiratory depression, which can lead to death. Another point of ignorance involves people who buy street drugs and don’t necessarily know if the drugs contain contaminants. That’s the kind of ignorance I’m talking about.So it’s the mix of drugs that is the problem, not opioids like heroin themselves? Yes, the majority of opioid deaths occur as a result of combining opioids with multiple sedatives. But there are certain opioids that do concern us if taken alone and the person isn’t aware that they have this particular opioid. Those are fentanyl and the fentanyl analogs. These drugs are a lot more potent than something like heroin, meaning they require less of it to produce the effect. Most of the public aren’t seeking fentanyl or its analogs, but people are tainting things like heroin and oxycodone pills with fentanyl or an analog.One way to deal with this tainting, this contamination, is to have free drug-checking facilities, where people can submit samples of their drug and get a chemical readout of what is contained in the substance. That way they’ll know whether to take the substance or how much of it to take. The public also needs to know that most people who use these drugs are not addicts. If you understand that, then you know that for the people who do become addicted, we have to look beyond the drug and look at the person’s environment, their life. Do they have co-occurring psychiatric illnesses? Do they have pain that is not treated? All of these kinds of issues become important.At what point does biological change in the brain lead to physical addiction?Physical addiction occurs as a result of opioids—or any other drug, alcohol too—being in the body for consecutive weeks or periods, in particularly high doses. And then the body tries to compensate. For example, with opioids, one of the things that happens is that your gut, your gastrointestinal system, slows down the receptors. Your body is trying to compensate by speeding up the gastrointestinal tract. So when the drug abruptly leaves after several weeks of constant administration of the opioid, now the body is unprepared for the drug not being there and it overcompensates. It really ramps up the motility of the gastrointestinal tract, which causes diarrhea, among other things. It can give you energy. It can make life more interesting. Humans do not live on logic alone. Why do some people get addicted and not others?The amount of drugs they take, the period at which they take it. Some people can take opioids for extended periods of time. As long as they keep the doses fairly low and they don’t take multiple doses a day, they probably won’t experience physical dependence. It’s just like with alcohol. Most people drink alcohol on a regular basis, but they don’t become physically dependent. Whereas others drink every day in large amounts, and they will become physically dependent.Why can’t people overcome addiction?One of the major reasons people can’t overcome it is because we’re not very good at treating addiction in this country. Just think about why people become addicted. A large number become addicted because of co-occurring psychiatric illnesses, because of pain issues, because they once had a middle-class-paying job that made them someone in their home, someone in their community. Those jobs are gone. Then there’s no healthcare or there’s poor education. If your treatment is not addressing these issues, people are not going to overcome it. But if we have treatments that are holistic, and they’re looking at the individual, and not so much the drug, then we’re good. But if we’re just talking about the drug, then we’re already behind the eight ball, then we will lose that battle.Your definition of addiction follows the DSM-5, which refers to a “substance use disorder” and values functioning over regular ingestion of a substance. How do you define “functioning”?Functioning is determined by whether a user is happy in meeting their obligations, whatever they may be, whether they’re work-related, whether they’re family-related, or other social sorts of things. The person is not stressed out about their substance use. In fact, they’re cool with it. That’s functioning. The person’s happiness is more important. That supersedes any other thing.You write that, contrary to the cultural myth, regular use of recreational drugs doesn’t damage the brain. What’s the frequency associated with recreational?Yeah, I’m sorry. I couldn’t think of a better term. I don’t really like that term.Try another one.I don’t know a term. I simply mean people who take drugs, like alcohol users, somebody who may have a glass of wine or two every night for dinner, whereas somebody else may only drink on the weekend. It’s a wide range. And the same can be true with cocaine or heroin. That’s what I mean. People are functioning and don’t have these psychosocial disruptions. They’re meeting their obligations. They’re happy with their life. The notion that drug addiction is a brain disease encapsulates all that’s wrong with today’s science in this area. You write, “Despite the current false narrative, the addiction rate among people prescribed opioids for pain in the United States, for example, ranges from less than 1 percent to 8 percent.” Why do we have that false narrative?It serves many purposes. It allows for the “war on drugs.” It allows for people to be moralistic; for treatment providers to have a raison d’etre. And what would the media have to write about?In the context of the opioid crisis, might your arguments supporting opioid use be used by drug makers to defend themselves in court?I don’t think they can because they’re being sued for minimizing the addiction potential of oxycodone preparations. It’s clear how they were supposed to inform the public, and it’s clear what they did. They were just not as forthright as they should have been. They were forthright with morphine, so why not be forthright with oxycodone?Why do you use heroin?That’s like saying, “Why do you use alcohol?” For the same reasons: a social lubricant, alter my consciousness. It’s a lot less toxic on my liver than alcohol and it’s really good at producing euphoria.Where do you get the heroin?It’s no one’s business. I would only say it’s always tested for quality.You write that heroin has made you a better person. How?It helps me to think about the impact of my behavior on other people, and then make the appropriate adjustments where I may have caused people harm, or anguish, or anxiety, stress. I try to rectify that. It’s a great solace in that way, it helps me to be patient with people—to be all the things that we hope our children will be. That’s what I’m trying to do. And it helps me to do that.In your book, you describe the death of your dog, Kenya. You note that in those moments after the veterinarian arrived to put the dog down, you got caught up in what drugs were going to be used to put him down, and that enabled you to escape the very strong emotions you were feeling. I wondered if that has a broader significance in your experience with drugs?It’s a hard thing to say. I’m a typical American male. I mean typical in the sense that we’ve been all lied to about hiding our emotions. They’re forcing us to not emote and forcing us not to express pain, and hurt, and all of those kinds of things. And so thinking about drugs, in my learning about drugs, has been a way to avoid dealing with the emotions that are there and that are appropriate for humans to express. And that’s what that scene with Kenya was. I was trying to illustrate that I’ve been screwed up like many American males by not facing my emotions, by not sharing my emotions, by pretending that I’m hard and emotionless. And that’s not healthy for a human.You write that only “healthy, responsible adults” should use recreational drugs. Who should decide who is the healthy, responsible adult? Excellent point. Certainly not me. I was really trying to make the point that I was writing the book for grown-ups. And I was trying to say that just because you’re 18 or 21, whatever, it doesn’t mean you’re grown-up. Part of being grown up is you have this freedom, but with freedom comes responsibility. And that’s the point I was trying to illustrate. I can’t decide. We think about somebody who’s driving an automobile. There are a lot of irresponsible people who drive automobiles, but we do our best to make sure they are a certain age, that we have speed limits, that people wear seatbelts, that they pass some competency exam. We have all of these things in place to try and help us, to serve as proxies. But we really don’t know. And I certainly don’t know who’s the grown-up. And I certainly don’t think that we should have some person sitting there deciding who’s a grown-up, because it gives us an opportunity to exclude other people. Lord knows I don’t want that.Your birthright to use drugs is part of the pursuit of happiness, you write. But others might say, “It’s my birthright not to take vaccines, not to wear a COVID-19 mask, to storm the Capitol.” Has “my” pursuit of happiness become dangerously exclusive?That’s great. That’s good. We have those birthrights as long as we don’t prevent others from pursuing their rights. When you’re infringing on other people’s rights, then you no longer have those rights. But as long as you’re not disrupting other people’s ability to pursue their rights, then, cool. If you’re not wearing a mask, when we have this highly communicable disease, then you’re potentially impacting the rights of other people. I think of my birthright as the basis of my liberty to control my body, and put what I want in my body as long as I’m not interfering with other people’s ability to do the same. I am allowed to pursue happiness as I see fit, as long as I’m not disrupting other people’s ability to do the same. And that’s part of the responsibility that’s required of anybody who is exercising these rights.Mark MacNamara is a journalist and lives outside Asheville, North Carolina. His articles for Nautilus include “How Psilocybin Can Save the Environment” and “The Artist of the Unbreakable Code.” Mark MacNamara Posted on February 17, 2021 Get the Nautilus newsletter The newest and most popular articles delivered right to your inbox! </description>
      <pubDate>18 Feb 21 09:57 EST</pubDate>
      <guid>https://nautil.us/issue/96/rewired/i-am-a-heroin-user-i-do-not-have-a-drug-problem</guid>
    </item>
    <item>
      <title></title>
      <link>https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html</link>
      <description>&lt;a href=&#34;https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 404 File not found The site configured at this address does not contain the requested file. If this is your site, make sure that the filename case matches the URL. For root URLs (like http://example.com/) you must provide an index.html file. Read the full documentation for more information about using GitHub Pages. GitHub Status — @githubstatus </description>
      <pubDate>01 Oct 21 12:14 EDT</pubDate>
      <guid>https://lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html</guid>
    </item>
    <item>
      <title>Energy, and How to Get It</title>
      <link>https://www.newyorker.com/magazine/2021/11/08/energy-and-how-to-get-it</link>
      <description>&lt;a href=&#34;https://www.newyorker.com/magazine/2021/11/08/energy-and-how-to-get-it&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Picard’s purview was perhaps more descriptive than prescriptive. “Energetic constraints, energetic flow, and the forces that drive energetic flow—these questions aren’t taken into account as much as they should be,” he said. “The way of the future is understanding personalized energy flows. The last ten years of personalized medicine has been taken over by genomics. The premise is that if you can sequence it you’ll know whether you’ll get sick or stay healthy. That’s where all the money goes. It’s a lucrative hypothesis, but it’s doomed to yield incomplete answers. The genome is static. Health is so dynamic.”“People are somewhat gorgeous collections of chemical fires, aren’t they?” Harold Brodkey wrote, in the story “Angel.” “We are towers of kinds of fires, down to the tiniest constituencies of ourselves, whatever those are.” Some years ago, without thinking, I introduced two friends of mine, B. and M., to each other, in a loose crew of people meeting up in a bar before a concert. B. and M. were both married. “I love your energy!” B. told M. Everyone laughed: such cheese. The next day, he called me and asked for her number. Such trouble. M. began referring to him, when discussing him with others, as “Energy”; she liked his, too. Their marriages didn’t survive the radiative flux, and B. and M. now live together, in a gravitational field of their own, otherwise known as Essex County, New Jersey. (When I told M. recently that I was writing about energy, the kind you feel, she said, “Talk about how annoying it is that everyone says they are tired. Tired is universal. We are exhausted until we die.”)B. and M.’s energy is of a different, albeit related, category—the kind you project, or perceive in others. This one has something to do with vigor as well, but also charisma, aura, and temperament. It has a spiritual dimension, to those who perceive or credit such phenomena, and a social one. In some circumstances, good energy may just be a matter of radiance, of good skin, teeth, hair, posture, which are in many respects themselves functions of robust health. Or it may comprise kindness, attentiveness, optimism, humor—the ability to make other people feel good about themselves. There may be intangibles at play. Pheromones, assurance, electromagnetics, pixies.To the extent that there is an overlap between the kind of energy you feel and the kind you project—a three-part Venn diagram of bio, mojo, and woo-woo—the concept has an array of ancient antecedents. In the Upanishads, prana, Sanskrit for “breath,” is the vital breath that animates body and soul, and all of existence, much like chi. Posidonius, the Stoic, proposed the existence of a life force that emanates from the sun. (Picard, the mitocentric, also cites the sun: it initiates a life cycle—photosynthesis, glucose, oxygen, ATP—that happens to have mitochondria as its linchpin.)Many of the variations on such ideas are pseudoscientific, the purview of quacks and crazies, or of spiritual adepts who may have been mistaken for them. Esotericism encompasses a variety of impossible-to-substantiate phenomena that persist best, in our quasi-scientific era, as metaphors or abstractions. In the eighteenth century, Franz Mesmer introduced his concept of mesmerism, or animal magnetism, involving a universal vital fluid that passes in and out through our pores. Baron Carl von Reichenbach, some decades later, described an electromagnetic substance he named the Odic force, after the Norse god Odin, which sensitive souls could perceive emanating from others’ foreheads. Early in the twentieth century, the French philosopher Henry Bergson identified an “élan vital,” which impels consciousness and evolution. Schopenhauer had his “will to live,” and, of course, for Freud, the source of the oomph within was the libido. Freud got some of his ideas from the work of the American neurologist George Miller Beard, who, in the years after the Civil War, had identified a condition called neurasthenia, arising out of the exhaustion of the nervous system. Headaches, fatigue, and impotence were the symptoms of what Beard called “American nervousness.” The cause, he proposed, was the stress of modern civilization, the most salient manifestations being “steam-power, the periodical press, the telegraph, the sciences, and the mental activity of women.”And then there was orgone, discovered, or imagined, by Wilhelm Reich, the Austrian psychoanalyst and fallen Freudian. Reich—who fled Germany in 1933 and pursued his experiments in Norway and New York before settling in rural Maine, where he could keep an eye out for U.F.O.s—sought to find physiological proof of the libido. In the lab, he hooked his subjects up to an oscillograph (one of them was a young Willy Brandt, the future West German Chancellor) and, with a microscope, discerned pulsating particles he called “bions,” which he claimed were the source of a mysterious life force called orgone. Orgone, he said, was blue, and was responsible for the color of the sky. Later, he invented a device called the orgone accumulator, an insulated shed the size of an outhouse, lined with metal panels. Among other things, it was said to enhance orgasms; the subject, preferably naked, would sit inside and accumulate orgone. It accumulated adherents, anyway—including Norman Mailer, Saul Bellow, J. D. Salinger, and Sean Connery—despite there being no legitimate evidence of orgone’s existence or benefits. Reich’s machine inspired the Orgasmatron, in Woody Allen’s “Sleeper,” and Dr. Durand Durand’s Excessive Machine, in “Barbarella.” The federal government, suspicious of Reich’s free-love evangelism and his associations with Communists, hounded him for years, and eventually jailed him for shipping orgone accumulators across state lines. He died of a heart attack in 1957, at Lewisburg Federal Penitentiary.A year ago, my wife gave me, as a gift, an Oura ring, my first so-called wearable. A hint, perhaps. I slipped it on next to the wedding ring, and it began feeding data about my exercise and sleep to an app on my phone. Yes, people have been using technology to track their steps and heart rates for a long time now—Fitbit, Apple Watch—but I’d considered such devices dorky, and vaguely sinister. Self-improvement can grate; data tracking can infringe. But maybe I needed a shove, and I was curious to see some numbers behind the brownout afternoons.“The apes accepted you, but my sister’s a whole other story.”Cartoon by Benjamin SchwartzThe Oura motivated me to get out and move—steps, miles, calories. I took long, aimless walks that I imagined would add weeks to my life, like injury time in a soccer match. (It would take a lot of injury time to make up for the hot dogs, if, as a recent study suggested, each one shortens the life span by thirty-six minutes.) Harder work, not surprisingly, yielded higher scores. Jog, or bike, or run stairs, then excitedly check the app. The lure of better numbers, more carrot than stick, was energizing in itself, even if the ring’s criteria seemed kind of arbitrary, maybe overgenerous. The instrument is blunt, but it will cut.The ring also conditioned me to begin each morning with a Christmas-stocking jolt of anticipation. Oh boy, new data. “How’d you sleep?” my wife would ask, as one does.“Don’t know yet.”Most days, the numbers weren’t good: Santa leaves a lump of coal. My sleep patterns were lousy and seemed to augur an early demise. It turned out that what might feel like restorative slumber—after a keen night out, for example, or a bout of hard work—was instead my body struggling to process the poison I’d put into it. The time in bed was more work than rest.The Oura emphasized the concept of “readiness”—a measure of recuperation. The relevant data point was heart-rate variability, or H.R.V. Your heart rate, like most of the body’s involuntary functions, is controlled by the autonomic nervous system, which has two components: the sympathetic nervous system and the parasympathetic one. The former fires the fight-or-flight impulse; it activates when you experience stress, or excitement, or overindulgence. The latter is the restorative impulse: “rest and digest,” “feed and breed.” The sympathetic system stimulates adrenaline, which dilates your pupils, raises your pulse, opens your airways, and interferes with signals to your bladder. (This is why fear can cause people to piss themselves.) The parasympathetic does the opposite—it settles you down. Ideally, these two systems achieve balance. You rev up, you calm down. You push, you heal. H.R.V. supposedly measures this state of concord. Counterintuitively, higher variability is said to reflect greater balance, and better health. Low H.R.V. correlates to a range of diseases and to earlier mortality. My H.R.V., especially after I’d had a few, was very low.“You can only manage what you measure,” Will Ahmed, the founder of Whoop, another tracking device, told me last month. By now, I had on three wearables: the Oura ring, a Whoop band on my left wrist, and a Levels glucose monitor behind my left triceps. I’d heard about Whoop from a doctor and journalist named Bob Arnot, a standup-paddleboard masters world champion and competitive ski-mountaineering racer, and the author of the recent book “Flip the Youth Switch.” Dr. Bob, who is seventy-three, is a high-energy guy—maybe a freak.Clearly, despite our best efforts, energy is not evenly distributed, whether because of genetics or fate, nature or nurture. People blessed with it may ascribe it to their own virtue, perseverance, or self-discipline, and will sometimes wield the descriptor “low-energy” as a slight, as though Eeyores are contagious. The idea that you can train, will, or even medicate yourself into a permanent state of pep, charisma, and accomplishment lends an atmosphere of piety to the energy-assessment dance. It’s all a matter of attitude, they say, as though attitude were not itself determined by energy. Think positive! It takes energy to change habits and alter circumstances. One can adjust certain knobs, but it can feel like a chore to deduce which knobs do what.“I fundamentally believe this is something you have control over,” Arnot said, when I called him. He credited his apparently prodigious mental energy to what he called “associative thinking.” Lately, he’d been composing a trumpet concerto and studying Python, calculus, machine learning, Arabic, and Swahili. “I don’t sleep much. I’ve always been a hopeless overachiever. Whatever I do is the opposite of what I call ruminating.” Whoop, he said, had helped him maximize his workouts and his downtime. His H.R.V. readings got better each month (H.R.V. typically worsens as you get older), and he reckoned that his biological age was much lower than his chronological one.</description>
      <pubDate>04 Nov 21 08:42 EDT</pubDate>
      <guid>https://www.newyorker.com/magazine/2021/11/08/energy-and-how-to-get-it</guid>
    </item>
    <item>
      <title></title>
      <link>https://kk.org/thetechnium/class-1-class-2-problems/</link>
      <description>&lt;a href=&#34;https://kk.org/thetechnium/class-1-class-2-problems/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; There are two classes of problems caused by new technology. Class 1 problems are due to it not working perfectly. Class 2 problems are due to it working perfectly. One example: many of the current problems with facial recognition are due to the fact that it is far from perfect. It can have difficulty recognizing dark skin tones; it can be fooled by simple disguises; it can be biased in its gendering. All these are Class 1 problems because this is still a technology in its infancy. Much of the resistance to widely implementing facial recognition stems from its imperfections. But what if it worked perfectly? What if the system was infallible in recognizing a person from just their face? A new set of problems emerge: Class 2 problems. If face recognition worked perfectly, there would be no escaping it, no way to duck out in public. You could be perfectly tracked in public, not only by the public, but by advertisers and governments. “Being in public” would come to have a different meaning than it does now. Perfect facial recognition would probably necessitate some new varieties of public commons, with different levels of disclosure. Furthermore, if someone could hack the system, it’s very trustworthiness would be detrimental. A faked ID could go far. We don’t question perfect tech; when was the last time you questioned the results of a calculator? Another example: Self driving cars. Self-driving cars don’t self-drive very well. They are getting better, but for the next several decades their problems will be Class 1 problems of imperfect function. We will demand near perfect results from robot-drivers (a higher standard than we demand from human drivers), so all the hard problems of detecting edge cases, acts of god, and the weird behavior of human drivers will prevail. Eventually, the tech will be perfected, and then we will encounter its Class 2 problems. In the Class 2 regime, driving a car yourself may be outlawed as too dangerous. The imperfections of human drivers may be incompatible with perfect robot drivers. When the system fails (say from a solar storm) its perfection may not permit it to degrade gracefully to accommodate less-than perfect drivers. A well-functioning robot car infrastructure might lead to more intersections with pedestrians; we might become more comfortable walking alongside silent automobiles that never crashed — until they did. Class 1 problems arise early and they are easy to imagine. Usually market forces will solve them. You could say, most Class 1 problems are solved along the way as they rush to become Class 2 problems. Class 2 problems are much harder to solve because they require more than just the invisible hand of the market to overcome them. Take cell phones. The first versions of consumer cell phones were too big, they only worked in some places, they had frustratingly short battery life, and their rings and talking on them were disruptive. Most importantly only the rich could afford them, in a new inequality. At the time many saw these problems as inherent in the technology. Yet years of intense market forces fixed most of those problems, making smartphones that silently vibrated, and had quiet text, and became so cheap and ubiquitous every adult on the planet has one. Unlike computers, they rarely crash, are easy to operate, and are extremely reliable. They just work. The cell phone quickly jumped into Class 2 problems. Whereas once the problem was “not everyone has this technology that doesn’t work very well” now the problem is “everyone has this technology that works very well.” We now contend with a technology that is present everywhere, all the time. Billions of people around the globe are connected 24/7, which allows all kinds of information, ideas, as well as rumors and disinformation to ricochet and touch everyone in an intimate way. The technology can suggest, recommend and “guide” us through the billion-eyed cacophony of everyone talking at once. Mob fears and beliefs can take over. Whispers are amplified and distorted as they cascade through friends of friends. The difference between Class 1 and Class 2 problems is that Class 2 problems cannot be solved by the market alone. Entrepreneurial spirit and the profit-mode are perfectly capable of solving most Class 1 problems. But Class 2 tech has already been perfected, and is ubiquitous — it works and everyone has it. What can the market do in this case? Making it better and selling more aren’t options anymore; those are saturated. What can the market do if facial recognition works perfectly and is everywhere? If robot drivers are the default? If everyone is connected to everyone all the time? These kind of system challenges require a suite of extra-market levers, such emerging cultural norms, smart regulation, broad education, and reframing of the problem. These are soft, slower moving forces that are currently not given the attention they deserve. To deal with ubiquitous accurate facial recognition when it comes (and it will come) requires a societal consensus on what it means to have a face that is both personal and public, to re-evaluate what public or private even means, to ensure symmetry between watchers and the watched, and to encourage expansive ideas around the very notions of identity of any type. A lot of this work is beyond the realm of dollars, and will take place in schools, courts, forums, communities, tweets, congresses, books, and late at night. When technologies reach the state that they work extremely well and become ubiquitous, their problem domain shifts from the realm of quick cycles powered by money, to the slower cycles of cultural imagination. To solve the problem of perfect facial recognition demands an expanded imagination, society wide, with new and different ideas about our face and identity. The latest fashionable tech is crypto. While the math behind blockchain is utterly reliable the implementations so far have many Class 1 problems. Crypto is hard to use, easy to trip up, biased to early adopters, an energy hog, and of marginal utility except to make money. But all these problems will be overcome by entrepreneurs. Someday blockchain will be ubiquitous and boring. It will be perfected and its wide-spread adoption will enable many thousands of new types of organizations and relationships that we can’t even imagine today. Blockchain tech could unleash collaborations of several million members working on one project in real time, or orgs that are far more leaderless than today. When crypto succeeds that way, it will graduate to Class 2 problems. At that point, entrepreneurs alone won’t solve those. These new problems will require a social imagination to revision what orgs do and what they are for, to re-imagine what transparency in a group means, to re-evaluate the role of money, or even the meaning of money. These in turn will launch new social expectations and norms of behavior, and in turn as a consensus forms, new legislation to codify the norms. Class 1 problems are caused by technology that is not perfect, and are solved by the marketplace. Class 2 problems are caused by technology that is perfect, and must be solved by extra-market forces such as cultural norms, regulation, and social imagination. </description>
      <pubDate>08 Nov 21 12:36 EST</pubDate>
      <guid>https://kk.org/thetechnium/class-1-class-2-problems/</guid>
    </item>
    <item>
      <title>How to build a second brain as a software developer</title>
      <link>https://aseemthakar.com/how-to-build-a-second-brain-as-a-software-developer/</link>
      <description>&lt;a href=&#34;https://aseemthakar.com/how-to-build-a-second-brain-as-a-software-developer/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Say you’re hungry. Where’s the first place you look for food? The farm where it’s grown, the forest where you can forage for it, the supermarket or your own pantry and fridge first?Unless you’re living in pre-industrial times, your decision looks something like this:That last step is probably redundant in recent history but the main idea still stands. You don’t go straight to the ultimate source of food when you’re hungry. You look for the closest local link in your food supply chain (which is usually your own pantry/fridge) before moving up the chain if no food exists there.The thing is, while we apply this process to food, we don’t apply it to our information/content diets. As software developers/engineers, this has far-reaching consequences to our productivity and mental health. We’re knowledge workers, so our consumption of information is as essential as food for us to survive and thrive.That’s where the concept of a “second brain” comes in.What is a second brain?The phrase “second brain” was coined by productivity expert Tiago Forte. In his words:Building a Second Brain is an integrated set of behaviors for turning incoming information into completed creative projects. Instead of endlessly optimizing yourself, trying to become a productivity machine that never deviates from the plan, it has you optimize an external system that is more reliable than you will ever be. This frees you to imagine, to wonder, to wander toward whatever makes you come alive here and now in the moment.Essentially, a second brain is a personal knowledge management system that serves as an extension of your mind so you don’t have to think as hard or remember as much. You offload thinking and remembering to your private second brain.Like Sherlock Holmes’ “mind palace”, it’s a place to store all of your lingering thoughts and curate the information you consume on a daily basis from books, the Internet and other sources so that you don’t get overwhelmed with unnecessary info and take action with the knowledge that matters.Most, if not all, software developers have a shot-gun approach to finding the information we need to solve issues or progress/learn new things. If we have a problem we need help with, we’ll usually follow a pseudo-supply chain that goes something like this:As you can probably tell, there’s no “local link” in this supply chain that we’ve developed. Google is the closest link, meaning we use the massive, unsorted network that is the Internet and Google’s interpretation of our problem as our “second brain”.By scouring the massive Q&amp;A databases of StackOverflow, the millions of videos and articles on YouTube and personal blogs or complex developer documentation, we forget one thing.We’re going directly to the farm/forest every time we need food.A “second brain” is the solution to this problem.By building our own “second brain”, we can create our own local link in the information supply chain that holds relevant information optimised for our needs instead of relying on Google and scouring the Internet every time.Let’s look at how you can build one.How to build a second brain as a software developerStep 1 – Choose an applicationYou need something to serve as the system that houses your second brain.There are 15 qualities the ideal second brain system has as specified by Tiago Forte (the original creator of the second brain concept), split into deal breakers, must-haves and nice-to-haves:Deal breakers1. Quick capture and editing2. Scales to thousands of notes without performance lag3. Basic formatting options4. Strong search feature5. Ability to handle images and attachments6. Private space, with public sharingMust-haves7. At least 3 levels of hierarchy8. Many ways to capture information9. Native and web versions10. Capturing and syncing across multiple devices11. Exportable as plain textNice-to-haves12. Side-by-side viewing13. Bullets or lists14. Automatic date stamps15. TagsWhile it’s possible to use manual methods like a notebook or sticky notes, it’s difficult to scale these to thousands of notes, handle images and code snippets, allow portability for quick capture/editing and most importantly allow for searchability.Hence why a digital system is essential (especially in the context of software development).At time of writing, there are heaps of digital note-taking apps in the market. But, for the purposes of this article, I’m just going to list the ones that are most widely used as “second brains”/personal knowledge management systems:NotionEvernoteRoam ResearchOneNoteHonourable mentions: Workflowy, Obsidian, SupernotesAll of the listed apps meet the requirements needed for a second brain/personal knowledge management system. Each app works in different ways and has its benefits and drawbacks so pick the one that works best for you. Your second brain is for your eyes and use, not someone else’s.Related readingThe Essential Requirements for Choosing a Notes App as Your Second Brain – Forte LabsI’ve argued strongly for the category of notes apps as the ideal home of your personal knowledge library, which I call a “second brain.” I believe there are ten core capabilities that digital apps uniquely provide to the note-taking process: Searchability: type in a few characters and see everything that matches, regardless of where it’s … Read morefortelabs.coHow to choose the right note-taking appThere’s a note-taking app for everyone. The challenge: it’s hard to decide which one’s the best for your use case and your note-taking style.nesslabs.comStep 2 – Create a general-purpose structure/wireframeNext, we need to structure our chosen application to allow us to capture, sort and retrieve relevant information efficiently. There’s two organisational systems that stand out here:ZettlekastenThe PARA Method (recommended)The Zettlekasten approach advocates for a disciplined approach to note-taking. Notes are captured “atomically” in the shortest and most modular manner possible. Once captured and split into components, you then search for other relevant notes you’ve taken in the past to link them to. Finally, you update your overall network of notes to make your recently captured note accessible. Having a network of notes is invaluable but it requires a lot of time and effort.The PARA Method reduces the fatigue of information capture. There’s no linking needed. It involves simply splitting all information into four categories based on purpose and timely relevance:(Tasks) – info stored in Projects or in memoryProjects – goals to be achieved with deadlines – info needed nowAreas – standards/broad categories with no end date – info needed a bit laterResources – topics/themes of ongoing interest – info needed somedayArchives – info that you won’t use for a while and doesn’t fit into the other three categories – info not really needed/held just-in-caseThese categories are all nested. Tasks are nested under Projects which fall under Areas which fall under Resources, with Archives serving as the catch-all category.Projects serve as your short-term memory and hold information you’ll likely need now/soon while the deeper you go (i.e. Areas → Resources → Archives), the later the information is needed.It’s also a good idea to have an entrypoint (or Inbox) for all of your captured info before it’s sorted into one of the above categories.Related readingRemembering what you Read: Zettelkasten vs P.A.R.A.I spent over a year researching better ways to take notes and remember what I read. Here’s what I found.www.zainrizvi.ioStep 3 – Create data structures for captured informationThe above general-purpose wireframes describe where to position and sort information, but not how to describe information to meet our needs. We need to provide the right metadata in our captured information so that it can be easily organized and retrieved later.This single article can never hope to encapsulate every single piece of information needed by every software developer. Still, here’s some examples of information most developers need and how they can effectively store it.Q&amp;As – building your own StackOverflowQ&amp;As are the most raw and direct form of information. Our search for information starts based on a question we’re looking to have answered or a problem we want solved. Q&amp;As can typically be found on sites like StackOverflow or developer forums and online communities.Question – the question you had (not necessarily the question the source author had)Answer – the answer that helped you (if possible, rewrite the original answer to be more relevant to your original question)Tags – keywords to make this easily search-able in futureLinks/related reading – links to other useful information (keep your notes as lean as possible)(Date) – should be automatically captured by your note-taking appCode snippets and boilerplatesCode snippets and boilerplates are extremely actionable and designed to be copy-pasted in specific contexts. When considering the data structure for these, note that code from one language and one context cannot necessarily be used in projects with a different language/context.Also, unlike Q&amp;As, code snippets can’t be easily searched for based on their text contents. That’s why it’s important to put all snippets in a general structure (like a Notion database or a table with filters) that allows you to search based on language, type and context. For example, you should be able to search for all your front-end JavaScript snippets with a styling context.Language – e.g. JavaScript, Python, GoType – e.g. front-end, back-end, deployment/DevOpsContext – create your own multi-select categories to further describe the purpose of the snippet e.g. is this to do with styling, making API calls, creating utility functions etc.(Description) – a text-based description of the snippetSnippet – the actual snippet/boilerplate (wrapped in code tags)(Dependencies) – link to any other code or packages that are relevant or that you need to use this snippetDocumentation templatesThese are basically text snippets. Templates for every bit of documentation your project needs like READMEs, CONTRIBUTING files, code review processes or test plans should be stored here.While these are text-based, searching for a specific document (like a README) is still not easy as it sounds. That’s why, like code snippets, it’s important to keep documentation templates in a general structure that’s able to be filtered.Type – e.g. front-end, back-end, deployment/DevOpsLocation – where does this template go? e.g. repository, project management, clientTitle – e.g. README, CONTRIBUTING, QA planTemplate – the actual template(Description) – a text-based description of the snippetResourcesResources are the least actionable, but most educative forms of information. They are the videos, courses, tutorials (and more) that you look for when you want to learn more about a broad area rather than fix a specific issue.For instance, if you wanted to learn about Python, you’d save the best Python learning resources (i.e. tutorials, videos, articles and courses) you’ve come across in your second brain. This way, if you ever want to learn about Python in the future, you don’t have to repeat your search.Since resources are your version of longer-term memory, you have to put some effort into making them easily retrievable in future.Name – a search-optimized name of the resourceAuthor – the original author of the resourceCategory – e.g. video, article, book, tweet, course, websiteURL – where can you access this resource (note – you don’t want the entire resource in your database, you just want a link to minimise bloat)Status – have you completed reviewing this resource?Tags – keywords not in the name to make this easily search-able in future(Rating) – how effective this resource is at solving your problem or teaching you somethingStep 4 – Determine your information workflowsThe priority now is figuring out:How you’ll approach finding the information you need to solve problemsHow information will be sorted once it arrives into your second brainFinding infoThis is the part where you add your second brain to your information supply chain. Instead of going directly to Google when you have a particular issue, you now go to your second brain first.Since we’ve neatly organised our content based on its purpose, relevance and timeliness, here’s our workflow for finding the information we need to solve problems we come across:Sorting infoIdeally, you’ll have an inbox section where all incoming captured info is preliminarily stored. From here, it needs to be sorted into the appropriate category (based on the PARA method).Depending on how relevant and timely it is, it needs to go into the appropriate project, area or resource umbrella.For instance, if you’re currently working on a MERN stack project, then a code snippet on “How to set up MongoDB with Express” is very relevant and should go in the appropriate Project section. On the other hand, a tutorial on “How to create a REST API with Django” is less relevant and might go in the Resources section for future use.You’ll ideally want to sort daily to review your incoming information and prevent information overload.Summary – flowchartThis is by no means a comprehensive guide. I fully acknowledge that I may have missed out on a lot of information you need to get a well-oiled second brain system working. For a more complete, general-purpose guide on how to build a second brain, check out Tiago Forte’s course. He’s far more of an authority on personal knowledge management/second brain systems than I am.My main goal with this article was to show all developers that the shot-gun approach of Googling every single problem they come across is inefficient and unproductive. I hope I accomplished that and showed you a better way of doing things.Further readingBuilding a Second Brain: An Overview – Forte LabsThis is a summary of Building a Second Brain, my online course on capturing, organizing, and sharing your knowledge using digital notes. How many brilliant ideas have you had and forgotten? How many insights have you failed to take action on? How much useful advice have you slowly forgotten as the years have passed? We … Read morefortelabs.coBuilding a Second Brain: The Illustrated NotesMaggie is a designer, illustrator, and anthropologist. This is her digital garden for growing visual explanations about design, culture, and programmingmaggieappleton.com</description>
      <pubDate>17 Nov 21 11:14 EST</pubDate>
      <guid>https://aseemthakar.com/how-to-build-a-second-brain-as-a-software-developer/</guid>
    </item>
    <item>
      <title></title>
      <link>https://amaca.substack.com/p/how-i-got-wealthy-without-working</link>
      <description>&lt;a href=&#34;https://amaca.substack.com/p/how-i-got-wealthy-without-working&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;If you are a Software developer, it’s easier than ever to become a millionaire.What most haven’t figured out yet, is how to become wealthy without working too hard.TLDR How-ToDon’t do a startup.Specialise in something that every company likes a lot. Avoid niche tech, go for huge popular things (eg. AWS, JS, Python, React).Take a $ 500-1000 / day, full-remote job (If you don’t know how to find them, there’s a chapter about it).Don’t live in big cities (go for smaller, more liveable towns).Leave any job that requires too much work or too many meetings. Meetings are the mind-killer. You’ll quickly find another gig because you specialized.Take 2 months for holidays in-between contracts.Do this for 7 years while investing most of your salary in a diversified portfolio. Don’t go too crazy with Crypto.Congratulations. You’re now a millionaire like any other senior programmer that invested, but you’ve worked less, and for fewer years.Keep reading for the details, I’ll show my investment portfolio later in the article. Consider subscribing to this mailing list, it makes me happy to see the numbers go up and keeps me going:My storyPreamble: I&#39;m extremely lucky. If we live in a Simulation and life is just a Video Game, mine was started on difficulty level “Very Easy”: I was born in a developed country in the 80s (Italy), with access to fresh water, very good carbohydrates and a decent education system. So what you&#39;re going to read is 99% luck and 1% planets alignment. Please, correct success stories for selection bias.On the 14th December 2010, I joined a protest in Rome, admittedly not understanding very well what was happening. That ended up being one of the most violent protests in recent Italian history, with thousands of peaceful protesters in the midst of riots, cherry bombs, cars on fire, and police vans hurled at crowds of young people.Such was the political atmosphere after the 2008 financial crisis. At distance, that was just the consequence of 20 years of Italian economic decline and the following reduction in opportunity for the younger generation.I didn&#39;t understand what was happening, but I wanted to escape all of that and, like many southern Europeans I migrated to the north in search of opportunity.I worked in the UK for a few years as an employee and later as a contractor. Then I burned out and figured I couldn&#39;t handle the city anymore, so I quit my job and did some travelling.At some point, I realised I was running with everyone else: Go to work every day, 3 weeks holiday, mortgage. Even Friday-At-Home didn’t feel like a treat anymore. So, many years before the Covid Pandemic, I started doing remote-only. When the Pandemic came I felt the world caught up with me.In the beginning, I was riddled with doubts: can I have a Software career only working remotely? Will I find clients? Will people that go to the Office have more opportunities than me? Am I just lazy? In hindsight, I should have just relaxed more.define(wealthy)Most people measure wealth as the sum of all the bank accounts and assets. Hence, maximising wealth becomes just a matter of maximising the total amount of currency you have. This is the &#34;Greedy capitalist formula&#34;: max(current value of assets).We know life is very multidimensional, but to some extent, we all end up using that formula to estimate what&#39;s better. We know it&#39;s wrong but we still use it for many choices because it&#39;s simple and universal. It&#39;s also easy to justify to our peers: I did that because of this gain (or avoided loss). Humans are not very rational, and we need to phrase what we do to others to make sense of our own decisions. Without wording our thinking to someone else, our brains don&#39;t really function. This is also why almost all forms of psychology work: because talking to others works. So, what should we optimise instead?My personal mantra is: “try to get 80% of the reward with 20% of the work”. I’m not interested in giving 100% to get 100%: that is an overcrowded game. Making money is procrastinatingMoney is the ultimate optionality: it&#39;s the right to buy something later. The cost of that option is the inflation rate.So when we don&#39;t know what to do, we focus on making more money, assuming we are going to figure out what to do with it later.This can be a wise approach, as having savings increases our freedom, but it gets overdone: it becomes a form of procrastination.When making life decisions, going in the direction of more money can be wise. However, we must keep in mind that when we choose money, we don&#39;t choose much. We just decide to decide later.So instead of &#34;make more money&#34;, I look for more free time, more agency, more future freedom. That often means looking for more remunerative opportunities and quitting a Contract to jump on a better paid one. But it also means quitting a job because that company does too many meetings and wants me on the screen for too many hours. I want to be on the screen 2 hours a day and make 80% of the salary I could make. This philosophy is exemplified in these three points I’ll expand on:Do not competeEscape Big Cities and Go RemoteCompound1. Do not competeThe single most important choice you can make in your financial life is to escape the rat race. Use the market, do not compete in it.If there are many jobless painters in the market, do not make painting your job. If you like to paint, please paint! but do not make it your job.Do not fight the market. If there is a high demand for UX Designers and you have some design skills, spend some resources to get into UX design.As Peter Thiel says: in the last 20 years the only engineering fields worth pursuing were Petroleum Engineering and Computer Science. I don&#39;t mean you should get into those fields, just that you should figure out how not to compete with everybody else. Find a niche. You can get better than anybody at something, but not without consuming your life working. Some people are really into what they do, to the point they end up working all the time. That is fine with me, but it&#39;s not what most people want. Work always requires some level of sacrifice: do not self improve *ad infinitum*. There is a whole category of things you learn about only after you learn to read English: *Self-improvement* is one of those things. Some parts of the Anglosphere Global Culture are more evident if you were raised outside of it. Self-improvement starts from the idea that you can improve by applying discipline. You could be better, healthier, wiser, wealthier if you just applied yourself. This is cool.But please, do not spend your life self-improving. At some point, we&#39;re going to die, self-improved or not. Don&#39;t look at unproductive time as wasted time. A lot of us are going to die of unpredictable diseases, some of us young. Really, don&#39;t spend your life getting fitter, healthier, more productive. We are all going to die, and Earth will explode in the Sun in a few billion years: please, enjoy some now.I&#39;m advocating a sort of Moderate Hedonism. Work and self-actualisation can be rewarding and meaningful, but looking at the sea from a hammock and drinking too much beer with friends three times a week is also a completely respectable way of waiting for the gravitational swallowing of the planet.2. Escape big cities and Go RemoteI could just say Go Remote. But more importantly: do not live in big cities. Big cities are the geographical locations of the rat race, it&#39;s where you go to compete.&#34;Yes, but big cities have more opportunities and more connections&#34;. True. Probably they are more bearable at a young age, when grappling with the existential urge to mate. I started doing remote-only contracting in 2014, and I had doubts about its sustainability. Then the pandemic came, and now I only find remote job offers in my inbox. If you think your company is not gonna accept it, change company. Also, they will be forced to change in the next ten years. I think Remote-Only work, like Electric cars, is happening way faster than people think.Remote-First and Fully-Remote are here to stay, and probably the biggest change in the history of tertiary work 1.3. CompoundCompounding is only Third in this list, but it could be first.We live in the age of Nation-States. Nation-States are political constructs, like Italy, Japan or the United States. This idea of a contiguous big land area with homogeneous culture and administration is pretty recent in historic terms. Nation-States really like to have their own currency, or &#34;money&#34;.Nation-State&#39;s money is designed to favour the goals of the Nation-State. It can be printed and used for the goals of the State.The monetary system is really a Spreadsheet that keeps track of the hours worked by everybody. Now, the important detail here, is that the Nation-State (really the Central Bank) can arbitrarily steal hours from everybody to enact its political goals.If you invest €100 in some asset that returns 5% per year, after the first year you will have €105. The following year you won&#39;t have €110, but €110.25. That tiny difference is the real way to get rich. People don&#39;t take that tiny difference seriously enough. In 10 years that tiny difference will make all the difference.In general, if you have P amount of money, and invest in something that returns r every year, after n years you will have:P(1+r)^nThat&#39;s an exponential function. People that get wealthy take a sit on the exponential function ride, while regular people lose to inflation. You don&#39;t need to do much, nor to be clever. Just take a sit on the exponential ride.Note: If you take the previous formula, and make it continuously compounding, you get what finance books call the Time Value of Money: e^tr . Money is literally defined as exponential so, don&#39;t sit on your linear ass.I’ll include my investment portfolio later in the article.How to find remote jobsI’m mostly contacted by Companies at this point, and I have a very explicit Remote-Only contractor profile. But there are many ways of going about this. There are now plenty of professional networks that focus on Remote-Only contracting. The most important in the US is Toptal. They have a solid interview process, but it’s not the only one anymore. There are now also Remote-First recruiting companies, like Turing, or Hired, and plenty of boutique remote-only agencies. Signing up with these agencies can help you lend your first contracts, they will give you helpful feedback on how to improve your skillset. Also, most traditional recruiting companies are just pivoting to Full-Remote as they are seeing the tide move.Probably the most important trick is this: build a public online profile. A website, or a blog where you show at least a couple of side projects (websites, apps, GitHub projects) and previous work experiences. Portfolio matters: in this business companies really don’t care much about your Degrees, as much as your Portfolio. It’s not easy to be considered Senior, but the demand for skilled developers has been rampant since the 90s. So, if you can show something you’ve built and deployed on your own, you’re very well positioned. Decent spoken English matters as well, but I’ve met many remote professionals that built a solid remote career without being Shakespeare.Once you have a visible public profile, if you find it hard to get into networks like Toptal, you can instead go for Permanent roles at remote-first companies. There is now plenty, and you can find them via websites like remoteok.io or weworkremotely.com.Submitting your CV / Portfolio directly to the “Careers” page is surprisingly effective. Tweak your portfolio, show what you have done, submit a lot of CVs to many companies over months, try again and again.My Investment PortfolioMy Investment Portfolio has changed a lot over the years. On average I’ve been more aggressive in the past, but I’ve always used trite and well known ETFs that keep track of US / European stocks and bonds. Nothing fancy! Just regular passive investing. I didn’t get to $1 million with Crypto (unfortunately).This is what my portfolio looks like now when accounting for various Trading / Bank accounts:Risk should depend on your age and needs. I try to keep Crypto around 10%, so I tend to rebalance as it grows. I’m pretty conservative now, as I’m not comfortable with the recent overexcited markets, and I don’t really feel like I need my money to grow.“Developed stocks” is composed of ETFs that track the S&amp;P, FTSE and European stocks. I basically have a Boomer portfolio, but I’m in my 30s.Note: I didn’t include in the portfolio a house I own, which would be my only Real Estate investment.If you are not familiar with passive investing and ETFs, please do some reading: 2About AmacaAs you’ve probably figured out, I’m a Senior Software professional from Europe, and Amaca is my newsletter. I decided to stay pseudo-anonymous to be able to write without filters, without compromising my professional identity.Over the last 10 years, I managed to save up and invest a good sum, but I still take contracts every now and then, without the pressure.I write about Programming, Investing, Contracting, CryptoEconomics and anything that I fancy. Consider subscribing to this mailing list and following me on Twitter @AmacaCapital. Seeing the numbers go up makes me happy and keeps me going:Share</description>
      <pubDate>18 Nov 21 12:26 EST</pubDate>
      <guid>https://amaca.substack.com/p/how-i-got-wealthy-without-working</guid>
    </item>
    <item>
      <title></title>
      <link>https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7</link>
      <description>&lt;a href=&#34;https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Backpressure explained — the resisted flow of data through softwareBackpressure (or back pressure) is something nearly every software engineer will have to deal with at some point, and for some it’s a frequent problem. But the term itself isn’t nearly as understood and recognized as such.In this post I’m going to elaborate on what exactly backpressure is, when it’s common, and the strategies we can use to mitigate it.I recently gave a talk about this at ReactiveConf:The DefinitionIn the software world “backpressure” is an analogy borrowed from fluid dynamics, like in automotive exhaust and house plumbing.The Wikipedia definition:Resistance or force opposing the desired flow of fluid through pipes.In the context of software, the definition could be tweaked to refer to the flow of data within software:Resistance or force opposing the desired flow of data through software.The purpose of software is to take input data and turn it into some desired output data. That output data might be JSON from an API, it might be HTML for a webpage, or the pixels displayed on your monitor.Backpressure is when the progress of turning that input to output is resisted in some way. In most cases that resistance is computational speed — trouble computing the output as fast as the input comes in — so that’s by far the easiest way to look at it. But other forms of backpressure can happen too: for example, if your software has to wait for the user to take some action.By the way, you might eventually hear someone use the word “backpressure” to actually mean something has the ability to control or handle backpressure.For example, if someone says: “I made this new library with built-in backpressure…” they probably mean it somehow helps you manage backpressure, not that it actually intrinsically is the cause of it. Backpressure is not “desirable” except when it’s inescapable and you want to protect something else from receiving it.Even with these definitions in hand, what “backpressure” really means can still be a little hazy at this point. I find many people have their “ah ha” moment after hearing some examples.Examples of BackpressureI Love Lucy: Chocolate FactoryLet’s start with an analogy. In the 50s television show “I Love Lucy” there’s an episode in which Lucy is working at a candy packaging plant. Her job is to take candy from a conveyor belt and wrap each of them in paper. Easy enough, except she quickly finds the conveyor’s speed is faster than she can handle. Hilarity ensues.Thanks to my old co-worker Randall Koutnik for suggesting this example years ago!This is a perfect example of backpressure. She actually tries two different ways of dealing with it: setting some aside to get to later (buffering), and eventually she starts eating and hiding them in her hat (dropping). However, in the case of a chocolate factory, neither of these backpressure strategies are viable. Instead, she needed them to slow down the conveyor; in other words, she needs to be able to control the producer’s speed. We’ll talk more about strategies in a bit!Reading and Writing FilesNow we’ll talk about software-related backpressure. The most common case is when dealing with the file system.Writing to a file is slower than reading a file. Imagine a hard drive that provides an effective read speed of 150 MB/s and a write speed of 100 MB/s. If you were to read a file into memory as fast as you can, while at the same time wrote it back to disk — again as fast as you can — you’d have to buffer 50 MB every second. That’s a growing deficit! You won’t be able to start to catch up and pay down that debt until the input file has been completely finished being read.Now imagine doing this with a 6 GB file. By the time you’ve read the file completely, you’d have a 2 GB buffer you still need to finish writing.6 GB / 150 MB = 40 seconds150 MB - 100 MB = 50 MB deficit50 MB x 40 = 2 GB !!!That’s a lot of wasted memory. On some systems this might even exceed the amount of available memory. Or imagine this is a web server performing this operation each for multiple requests. Hopefully it’s clear this approach is not viable in a lot of cases.Fear not, the solution is simple: only read as fast as you can write. Nearly all I/O libraries will provide abstractions to do this for you automatically, often with the concept of “streams” and “pipes”. Node.js is a great example.Server CommunicationThe next example is communication between servers. Today it’s very common to employ a microservice architecture where responsibilities are separated into multiple servers.Backpressure can commonly occur in this scenario when one server is sending requests to another faster than it can process them.If server A sends 100 rps (requests per second) to server B, but server B can only process 75 rps, you’ve got a 25 rps deficit. Server B might be falling behind because it needs to do processing or it might also need to communicate with other servers downstream.Either way, server B needs to deal with backpressure somehow. Buffering that 25 rps deficit is one option, but if that increase remains constant it’ll soon run out of memory and fall over. Dropping requests is another option, but it’s a pretty common requirement that dropping requests is not acceptable.The ideal option is for server B to control the rate in which server A sends requests, but again that’s not always viable — if server A is requesting on behalf of a user, you can’t always tell or control the user to slow down, but sometimes you can! Still, it’s often better to have the requesting server buffer, so you can better distribute the memory load down-stream, where the stress is, and not impact other requestors.For example, if three different types of services (A, B, C) all make requests to the same downstream service (Z), and one of the four (A) is experiencing high load, service Z could effectively tell service A to “slow down” (control the producer) causing service A to buffer the requests. If that continued, eventually service A would run out of memory, however, the other two services (B, C) would remain up, as would the downstream service Z because it wasn’t allowing one misbehaving service to deny equal access for the others. An outage in this case might be inevitable, but we limited the scope and prevented a cascading Denial of Service. This would be a case of controlling the producer, who then buffers because they can’t control their own producer (the user). In this case someone has to buffer, but who is the important part.When I worked at Netflix, this sort of backpressure was common. I talk about it in one of my talks. Ultimately, which strategy we used to control backpressure depended on the use case. Sometimes we were able to control the producer using things like RSocket and gRPC.If handling services-at-scale interests you, you’ll want to learn more about Chaos Engineering and how you can intentionally induce these failures in production to test resiliency and failovers/fallbacks.Rendering UIThe final example of backpressure is rendering a UI app. Backpressure happens when you can’t render as fast as you need to. It could be as simple as trying to render a giant list, debouncing rapid keyboard events, or more complex situations like trying to display the output of a WebSocket that emits 20,000 events per second.Because this sort of backpressure can be very complex, often involving death by a thousand cuts, let’s focus on the WebSocket example since it’s easier to see how it can go wrong.If a WebSocket is emitting 20k, 100k, or even 200k+ messages per second, there’s no way you’re going to be able to render each of those messages as they come in. Zero chance. So we need some sort of backpressure strategy. If we can’t control the rate from the server we’ll need to either buffer or drop on the client-side.In the case of buffering, you could perhaps accumulate incoming messages in an array and periodically flush it on each requestAnimationFrame, rendering that list into the DOM. Alternatively, you could buffer them for some duration, like every second. If you’re appending them into an existing table, you’ll probably also want to use some form of table virtualization because rendering 100k rows is also going to be a huge bottleneck — and a form of backpressure!Depending on the actual number of events coming in, one of these approaches might work. Otherwise, your only other option is to “drop”: sample only a percentage of the incoming messages and filter out the others.Backpressure StrategiesAside from scaling up your available compute resources, how you handle backpressure can pretty much be summed up with three possible options:Control the producer (slow down/speed up is decided by consumer)Buffer (accumulate incoming data spikes temporarily)Drop (sample a percentage of the incoming data)Technically there’s a fourth option — ignore the backpressure — which, to be honest, is not a bad idea if the backpressure isn’t causing critical issues. Introducing more complexity comes at a cost too.Controlling the producer is by far the best option. When it’s actually possible, it has no real tradeoffs to speak of, other than the overhead of the control mechanism itself. You don’t need to use excess memory buffering things on your end, and you don’t need to be lossy and drop data.Unfortunately, of course controlling the producer isn’t always possible. The most clear case is when dealing with user input. Try as we might, it’s not easy to control our users!Buffering is what most people will reach for next. However, always keep in mind that buffering is dangerous if unbounded — meaning you don’t have a size or time limit to the buffer. Unbounded buffers are a common source of memory crashes in servers.When used, always ask yourself: is it possible that the rate the buffer grows will ever exceed the rate it is drained for any appreciable amount of time? Refer back to the Server Communication example above for an example of this.In fact, some people advocate that buffers should never be left unbounded. I’m not that strict, but I would say that it is often better to start dropping than to fall over completely (run out of memory).Dropping is the last strategy, and as mentioned it’s often combined with buffering too. Sampling based on time is the most common, e.g. 10% of data per second.Choosing a StrategyWhen deciding which approach to take, the User Experience (UX) can often help guide us. Is it a good UX updating a table 100k per second, even if we could? Probably not. Would the user rather see a sample updated per second? Or perhaps rearchitect things so that the stream is sent to a database that can be read by the user as-needed, more slowly? Only your unique circumstances can tell, but just remember the UX can guide you!It’s very common for developers to spend a lot of time tuning performance, only to end up with a bad UX, when the good UX wouldn’t have had the performance issue to begin with.Code for BackpressureI wanted to keep most of this post agnostic of the language/platform you code for, since backpressure is a problem we all have to deal with. But, there are some patterns you’ll see across ecosystems and I imagine many of my readers will be Web folk.The term “stream” is unfortunately ambiguous. Elaborating on them all will have to be saved for another post, but in summary:PullWith pull-based streams the consumer controls the producer. Usually it’s a 1:1 request → response style, but there are patterns for request(n) as well, like Flowables in RxJava. Some other pull-based streams are Node.js Streams, Web Streams, and Async Iterators — IxJS is a great async iterator library for you JS folks, there are also ports in other languages).Depending on who you talk to, many of these are considered hybrid “push-pull” if the response is pushed to you asynchronously. Some consider normal synchronous iterators traditional “pull” streams, some make no distinction between asynchronous/synchronous call them both just “pull”.PushWith push-based streams, the producer is in control and pushes data to the consumer when it’s available. Often push-streams are used when dealing with user input; they model the producer accurately since you can’t control the user. There are lots of libraries, but by far the most popular are implementations of the Reactive Extensions aka Rx. There’s RxJS, RxJava, and probably an RxYourFavoriteLanguage.SummaryWhen I first heard the term “backpressure”, I’ll admit I was intimidated. It felt like jargon being used to make the person sound smart — and unfortunately, sometimes it is. But in truth it is a real thing, and knowing about it and how to combat it can empower you to tackle even bigger problems and scale. From a small thing like dealing with rapid mouse moves to managing thousands of servers, backpressure is everywhere.Did this post adequately explain backpressure? Do you have feedback? Let me know in the comments or on Twitter: @_jayphelpsThanks to Estèphe in the comments for the reminder about scaling as a possible solution to backpressure!</description>
      <pubDate>01 Dec 21 11:20 EST</pubDate>
      <guid>https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7</guid>
    </item>
    <item>
      <title></title>
      <link>https://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html</link>
      <description>&lt;a href=&#34;https://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Storm in the stratosphere: how the cloud will be reshuffled 2021-11-30 Here&#39;s a theory I have about cloud vendors (AWS, Azure, GCP): Cloud vendors1 will increasingly focus on the lowest layers in the stack: basically leasing capacity in their data centers through an API. Other pure-software providers will build all the stuff on top of it. Databases, running code, you name it. We currently have cloud vendors that offer end-to-end solutions from the developer experience down to the hardware: What if cloud vendors focus on the lowest layer, and other (pure software) vendors on the layer above? Feel free to bring this up in five years to make me embarrassed about how wrong I turned out to be. But let me walk you through my thinking—I think some of it is quite well illustrated through the story of Redshift. Redshift and what happened Redshift is a data warehouse (aka OLAP database) offered by AWS. Before Redshift, it was the dark ages. The main player was Teradata, which had an on-prem offering. Startups said no to SQL and used Hadoop—SQL was kind of lame back then, for reasons that in hindsight appear absurd. I&#39;m very happy we&#39;re out of this era. Anyway, one vendor was a company called ParAccel. AWS licensed their technology, rebranded it Redshift, and launched in 2012. Redshift at the time was the first data warehouse running in the cloud. It was a brilliant move by AWS, because it immediately lowered the bar for a small company to start doing analytics. You didn&#39;t have to set up any infrastructure yourself, or write custom mapreduce and reload the jobtracker all day. You could spin up a Redshift cluster in AWS, feed it humongous amounts of data, and it would … sort of just work. Enter Snowflake Snowflake2 is a $100B+ publicly traded company. Basically their entire offering is a data warehouse that looks fairly similar to Redshift.3 If you looked at Redshift in 2012, there was a lot of things that favored it. AWS had large economies of scale, had control of the underlying substrate (EC2), and could make larger investments in building the software. Maybe because of the value of lock-in, they could even subsidize the development of Redshift and make up the money through other products. Anyway, this is what it looks like nine years later4: What happened? But my more general question is: what are the forces that favor a company like Snowflake?5 And what does that mean for other cloud products? What if….? There&#39;s some sort of folk wisdom that the lowest layer of cloud services is a pure commodity service. So in order to make money you need to do at least one of: Make money higher up in the stack. Use services higher up in the stack to lock customers in. Then make money lower in the stack. I think there&#39;s some truth to these, at least looking historically, but there are some interesting trends blowing in the other way: The “software layer on top” is getting incredibly competitive. There&#39;s so many startups going after it, fueled by cheap VC money and willing to burn billions of dollars on building software. Cloud vendors might be pretty happy making money just in the lowest layer. Margins aren&#39;t so bad and vendor lock-in is still pretty high. Startups are coming for the cloud There&#39;s never been this many companies going after services that traditionally belonged to the cloud vendors: What&#39;s going on? Probably a confluence of a lot of things. If I was tired, I would just shrug and say something like “startup circle of life, whatever”. And I think this is roughly one factor, but I see at least 3 different ones: Incentives at big companies often makes it hard to ship new crazy ideas. At the same time, VCs are pouring in money into the segment. If you&#39;re an ambitious person, do you go work at AWS? Or do you join an early stage startup, or create your own? It&#39;s expected that innovation shifts away from big companies to startups. Software vendors can build for all the cloud vendors at the same time. I think this was a real benefit for Snowflake, since a lot of their early customers were banks who care about multi-cloud, but more generally it also expands the market size vs the reach of any cloud vendor. A lot of the successful cloud products started out as internal services. This has been an amazing source of products, that have been battle-tested at Amazon, Google, and Microsoft scale, and it makes sense that those tools are a great match for their big enterprise customers. But the flipside of the extreme focus on scale, reliability, and configurability, is that the developer experience has become an attack vector, in particular when you look at mid-market and smaller customers who may care more about improving developer productivity. Slightly larger companies like Uber, Netflix, and Airbnb have a history of teams leaving to commercialize internal tools (often through the intermediate step of open sourcing it). Somewhat subjectively and anecdotally, these tools tend to have a much higher focus on developer experience. Maybe owning the lowest layer isn&#39;t so bad? Let&#39;s say a customer is spending $1M/year on Redshift. That nets AWS about6 $500-700k in gross profits, after paying for EC2 operational cost and depreciation. If that customer switches their $1M/year budget to Snowflake, then about $400k7 goes back to AWS, making AWS about $200k in gross profits. That seems kind of bad for AWS? I don&#39;t know, we ignored a bunch of stuff here. Snowflake&#39;s projected8 2022 research and development costs are 20% of revenue, and their sales and marketing costs are 48%! For a million bucks revenue, that&#39;s $700k. Translated back to AWS, maybe AWS would have spent $300-400k for the same thing? Seems reasonable. Now the math suddenly adds up to me. AWS basically ends up with the same bottom line impact, but effectively “outsources” to Snowflake all the cost of building software and selling it. That seems like a good deal for them! What about lock-in? The other argument for why AWS should build their own software services is that it increases lock-in. So maybe Redshift in itself isn&#39;t a cash cow, but it decreases the churn on EC2. I&#39;m not so sure? I spent six years as a CTO and moving from one cloud to another isn&#39;t something I even remotely considered. My company, like most, spent far more money on engineer salaries than the cloud itself. Putting precious eng time on a cloud migration isn&#39;t worth it unless cloud spend starts to become a significant fraction of your gross margins. Which is true for some companies! But those are in a minority. A significant factor in all of this is that existing infra provides significant “gravity”. It&#39;s not like you can just pick whichever cloud vendor has the cheapest database and run your DB there. You want to run things in the same cloud provider9 and in the same data center10. Looking at sign-up flows for cloud products gives us a hint: The screenshots above show the onboarding for Snowflake, Confluent, and MongoDB (Atlas). They all ask: What&#39;s your cloud vendor? What region? Note that the only options for the first questions are AWS, GCP, and Azure. The other side of the equation of a potential cloud migration is—how much money you can save? And I think the truth it&#39;s never going to be substantial, since no one wants to start a price war. Being in a fairly stable oligopoly seems pretty nice and cozy, and if I was cloud vendor, I wouldn&#39;t try rock the boat. The cloud in 2030 We&#39;re roughly 10 years into the shift to the cloud, and even though it feels like it&#39;s transformed how we build software, I think we&#39;re still just getting started. I really wouldn&#39;t be surprised to wake up in a world where most developers don&#39;t interact with cloud vendors. Big transformations tend to happen in two stages. The first step happens when some new technology arrives and people adopt to it in the simplest way that lets them retain their conceptual models from the existing world. The real transformations happens later, when you rethink the consumption model because the new world opens up new ways to create value. The way we consumed music didn&#39;t change materially when Apple started selling songs online. The real transformation happened when providers like Spotify realized the whole notion of ownership didn&#39;t matter anymore. If you think about it from that angle, the last 10-15 years look a bit like a dumb “lift and shift”. Crudely speaking, we took computers and we put them in the cloud. Is that really the right abstraction for where we are? I don&#39;t think so. I think new higher level tools will let us focus on building application code and not worry about the underlying infrastructure. Startups are coming for your code The forces I&#39;ve talked about have been most clear when you look at Snowflake vs Redshift, but you can see it in other places too. Transactional databases is another very exciting area. But where I think we&#39;ll see the most change is how software vendors will increasingly run customer code. This isn&#39;t exactly a new idea—Heroku launched in 2007, and AWS Lambda in 2014. Kubernetes has been this interesting trend in the last few years on what I think is still essentially an inevitable march towards a fully “serverless” world, whatever that means to you. I wonder if a weird corollary of this is… maybe it&#39;s actually really good for the planet? The computers sitting in the cloud are ridiculously underutilized—my guesstimate of average CPU utilization is that it&#39;s maybe 10%? If I had a Ph. D. in bin packing, I&#39;d go looking for a job at some serverless infrastructure provider right now. One way to tell the story in 2030 looking backwards is that the cloud vendors needed software running on top of it, so they had to provide that themselves first, in order to drive cloud adoption. Luckily they already had a bunch of internal stuff they could ship! But eventually the market matured, and they could focus on the place in the stack where they had the strongest advantage. Predictions The cloud market will grow to $1T/year in revenue. Ok, that&#39;s almost entirely noncontroversial. Most engineers will never interact directly with cloud vendors, but through services on top of those. The database market (OLAP, OLTP, you name it) will be dominated by vendors running on top of cloud vendors, where the underlying layer is completely abstract. We will have some amazing new runtimes, finally figuring out the developer experience problems that are currently holding serverless solutions back. We will see a lot of partnerships between startups and cloud vendors, where a cloud vendor may concede an area and try to be a preferred partner with a startup instead. Kubernetes will be some weird thing people loved for five years, just like Hadoop was 2009-2013, but the world will move on. Resource utilization in the cloud will be much better, and engineers will spend an order of magnitude less time thinking about resource allocation and provisioning. IBM has finally given up on “hybrid multi-cloud”. YAML will be something old jaded developers bring up after a few drinks. You know it&#39;s time to wrap up at the party at that point. This generated a bunch of comments on Hacker News, most of which strongly disagree! Looking forward to see what the world looks like in 10 years. Thanks to Josh Wills, Akshat Bubna, and Sarah Catanzaro for feedback on an earlier version of this post! I&#39;m ignoring the CDN world a bit here. It&#39;s very clear right now that Cloudflare is doing an amazing job owning the stack all the way from developer experience to networking equipment. But frankly I don&#39;t see any long-term foundational difference operating 300 small data centers vs 30 large ones. Cloudflare has done exceptionally well staying ahead of the innovation game, but I suspect the same economic forces that apply to AWS et al eventually apply to them too. ↩︎ A fun thing is I randomly had lunch with the Snowflake founders in 2012 and they offered me a job the next day. The company was like… 10 people in total? ↩︎ There was one major architectural difference of Snowflake vs Redshift. A very underappreciated tech shift is how much faster networks got around that time. Up until that point, the wisdom was “move compute to data”, but Snowflake bet early on a full decoupling. AWS launched Athena in 2016 which was based on Presto, not Redshift, and launched Redshift Spectrum in 2017 which lets you query data in S3 through Redshift. As a bizarre coincidence, Redshift just launched a serverless product today, which is something they probably should have done a long time ago. ↩︎ This is from DB rankings. ↩︎ A pretty long history about Redshift vs Snowflake, which also points out that AWS unintentionally ended up incentivizing their sales teams to recommend Snowflake to customers. ↩︎ The margins on EC2 is reportedly about 50% and about 60% for AWS as a whole. Just comparing instance pricing for Redshift and for EC2 for equivalent instance types also gives a good idea of the Redshift markup above EC2. ↩︎ Snowflake is a public company and you can compute their margins from their latest quarterly report. ↩︎ From Snowflake&#39;s investor presentation ↩︎ Security is another topic that drives this, although security to some extent works against startups: the bar to adopt additional AWS products is often lower than completely new vendors. ↩︎ Cloudflare is actively going after the very high AWS egress fees and it&#39;s going to be interesting to see how that plays out. ↩︎ Tagged with: cloud, programming, infrastructure, software, popular </description>
      <pubDate>02 Dec 21 08:51 EST</pubDate>
      <guid>https://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html</guid>
    </item>
    <item>
      <title>Hayao Miyazaki Prepares to Cast One Last Spell</title>
      <link>https://www.nytimes.com/2021/11/23/t-magazine/hayao-miyazaki-studio-ghibli.html</link>
      <description>&lt;a href=&#34;https://www.nytimes.com/2021/11/23/t-magazine/hayao-miyazaki-studio-ghibli.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;ImageCredit...Takahiro KaneyamaNo artist has explored the contradictions of humanity as sympathetically and critically as the Japanese animation legend. Now, at 80, he’s coming out of retirement with another movie.Nov. 23, 2021Listen to This ArticleTo hear more audio stories from publications like The New York Times, download Audm for iPhone or Android.THE SCREEN IS black, and then comes the first frame: Hayao Miyazaki, the greatest animated filmmaker since the advent of the form in the early 20th century and one of the greatest filmmakers of any genre, is seated in front of a cast-iron stove with a pipe running up toward the ceiling, flanked by windows propped half open. Sun burns through the branches of the trees outside. Three little apples perch on a red brick ledge behind the stove. He wears an off-white apron whose narrow strap hooks around the neck and attaches with a single button on the left side — the same style of apron he has worn for years as a work and public uniform, a reminder that he is at once artist and artisan, ever on guard against daubs of paint — over a crisp white collared shirt, his white mustache and beard neat and trim, and his white hair blurring into a near halo as he gazes calmly at me through owlish black glasses, across the 6,700 miles from Tokyo to New York.I have one hour to ask questions. It is a rare gift, as Miyazaki has long preferred not to speak to the press except when absolutely necessary (which is to say, when he’s prodded into promoting a film), and has not granted an interview to an English-language outlet since 2014. Our conversation has been brokered by the newly opened Academy Museum of Motion Pictures in Los Angeles, which mounted the first North American retrospective of his work in September, with Studio Ghibli’s cautious assent; Jessica Niebel, an exhibitions curator, cites him as an exemplar of an auteur who “has managed to stay true to himself” while making movies that are “approachable to people everywhere.” I know I am lucky to have this time, and yet it feels wrong to meet Miyazaki this way, at a distance (due to Covid-19 travel restrictions) and through a computer, a machine he has so famously shunned.ImageCredit...Hayao Miyazaki © 1997 Studio Ghibli – NDFor, in an age of ever-advancing technology, his animated films are radical in their repudiation of it. From “My Neighbor Totoro” (1988), with its vision of gentle friendship between two children and an enormous growling forest creature whom only they can see, to the ecological epic “Princess Mononoke” (1997), whose title character, a human raised by wolves, first appears sucking blood out of a wound in her wolf mother’s side (the hero, an exiled prince, takes one look at her blood-smeared face and falls in love), to the phantasmagorical fable “Spirited Away” (2001), in which a timid girl must learn pluck and save her foolish parents (who’ve been transformed into pigs) by working at a bathhouse that caters to a raucous array of gods, Miyazaki renders the wildest reaches of imagination and the maddest swirls of motion — the stormy waves that turn into eel-like pursuers in “Ponyo” (2008), the houses rippling and bucking with the force of an earthquake in “The Wind Rises” (2013) — almost entirely by hand. And unlike Walt Disney, the only figure of comparable stature in animation, Miyazaki, who is now 80, has never retreated to the role of a corporate impresario, dictating from on high: At Studio Ghibli, the animation company he founded with the filmmaker Isao Takahata and the producer Toshio Suzuki in 1985, he’s always worked in the trenches, as part of a team of around a hundred employees devoted just to production, including key animators and background, cleanup and in-between artists, whose desks he used to make the rounds of daily for decades. (His own desk is hardly bigger than theirs.) He still draws the majority of the frames in each film, numbering in the tens of thousands, himself. Only occasionally has he resorted to computer-generated imagery, and in some films not at all.“I believe that the tool of an animator is the pencil,” he tells me. (We speak through an interpreter, Yuriko Banno.) Japanese pencils are particularly good, he notes: The graphite is delicate and responsive — in the 2013 documentary “The Kingdom of Dreams and Madness,” directed by Mami Sunada, he mocks himself for having to rely on a soft 5B or even softer 6B as he gets older — and encased in sugi (Japanese cedar), although, he muses, “I don’t see that many quality wood trees left in Japan anymore.” He adds, “That’s a true story,” then laughs, leaning in to the screen, and I think of the ancient, moss-cloaked trees in “Princess Mononoke,” cut down to fuel Lady Eboshi’s ironworks, and of their counterparts in the Shiratani Unsuikyo Ravine on the island of Yakushima in the south, which Miyazaki visited while location scouting for the film. The oldest cedar there, 83 feet tall and nearly 54 feet in circumference, is believed to be more than 2,600 years old, making it one of the oldest trees on earth. (The forest of the film does not exactly correspond to the ravine, Miyazaki has said: “Rather, it is a depiction of the forest that has existed within the hearts of Japanese from ancient times.”)ImageCredit...Hayao Miyazaki © 1997 Studio Ghibli – NDMiyazaki lives with his wife, Akemi, a former fellow animator — they met as colleagues at Toei Animation nearly 60 years ago on the movie “Gulliver’s Travels Beyond the Moon,” and married in 1965; she stopped working to raise their two sons, at his request, and, he has said in the past, “hasn’t forgiven” him — in Tokorozawa, northwest of Tokyo, where the Totoro Fund (supported in part by donations from the Miyazakis) has purchased more than 10 wooded hectares, dense with oak and camphor trees, for conservation. But today he is speaking to me from the Tokyo suburb of Koganei, from a small building a short walk away from the headquarters of Studio Ghibli that he uses as a private atelier. He sometimes affectionately calls it Buta-ya, Japanese for “pig house.” (He is fond of pigs, and often sketches himself as one.) Out front he parks his cloud-gray Citroën 2CV, with a tiny nine-horsepower engine and a rollback roof that leaks when it rains (the model was discontinued in 1990); a wine-colored version of it appears in the careening cliffside chase scene in his directorial debut, “Lupin III: The Castle of Cagliostro” (1979). Every December, he puts cuddly stuffed goats, mementos of his work on the “Heidi: A Girl of the Alps” TV series in the ’70s, in the kitchen window to greet passing children. When the Academy Museum requested a goat to display in its exhibition, he demurred: The children would miss them.Buta-ya was meant to be a retirement office, where Miyazaki could pursue personal projects. He built it in 1998, after announcing that he would make no more feature films, then returned to Studio Ghibli the next year with the story idea that would become “Spirited Away,” the highest-grossing movie in Japanese history until last fall’s “Demon Slayer: Mugen Train” (an extension of a popular manga franchise and part of a different strain of Japanese anime, focused on action and vengeance, with a video-game-like feel). “Spirited Away” won the 2002 Academy Award for best animated feature, the only film from outside the West to ever do so. In 2013, he said again that he was done with film, and that time, having directed 11 features in 34 years, he was taken seriously: Studio Ghibli shut down its production department.Yet here he is now, making a new film. “Because I wanted to,” he says, and grins, like a grizzled thief come back for one last heist.GORGEOUS, PROFOUND, BORDERLESS in possibility — yes, yes, but above all, Miyazaki’s films are thrilling. He is a master of suspense, whether sending a fugitive girl skittering down a rickety pipe that pops off the wall as she runs (“Spirited Away”), or swooping after a novice witch reeling on a broomstick because she’s forgotten how to fly and must quickly relearn so she can rescue her friend, a boy who’s dangling from a dirigible and about to crash into a clock tower (the 1989 “Kiki’s Delivery Service”). His visual style is at once commanding and intimate, a mix of fluid, loose lines and an accumulation of detail — in contrast to more mainstream anime’s labor-saving preference for caricature and clipped movement — that enables him to invoke the immediacy of life without being beholden to its precise contours. He deploys a palette of saturated colors, bright but never gaudy, standing out against cool grays and dun tones, and pays attention to quicksilver adjustments of light and shade, especially the shadows within shadows that give featheriness and depth to the night. He is equally expressive in close-up and panorama, and virtuosic in his open skies, creating clouds that are almost characters unto themselves, whether high-heaped loomers, broad swaths of rubble or voluptuous whorls like the heavy heads of flowers, stained by sunset or the deepening blues of day. (The Academy Museum’s retrospective includes a green-carpeted knoll where visitors may rest and gaze up at a video of passing clouds.)ImageCredit...Hayao MiyazakiAnd how easily Miyazaki slips from one register to the next, from hushed to clamorous, often in the same scene, as in the exquisitely timed comedy of towering Totoro, with his giant claws, standing beside two little girls at a bus stop in the dark. It’s raining; one girl offers him an umbrella, an instrument he has never encountered before. A toad stares at him from across the road, as if equally perplexed. We squint up at the trees to see a few particularly fat raindrops falling from a branch. They plonk down on the umbrella, loud, and Totoro startles. More drops come, a scattering of drumbeats, and his eyes widen. He heaves his body up in the air and lands with a boom, and all the drops caught in the trees come crashing down, his own personal storm. And then — because of course there’s more — the bus arrives, only it’s a scampering cat with headlight eyes and a door that opens in its side to whisk Totoro away.But Miyazaki is a realist, too. Toward the end of his 2004 film, “Howl’s Moving Castle,” which is mostly devoted to magic — a girl is transformed by a witch into an elderly woman, a wizard shape-shifts into a dark man-bird, a castle uproots itself and clanks around on clawed feet — a great-bellied airship looms into view and starts dropping bombs on a cobblestone town. Black clouds and flames surge over houses; the sky hangs red. No war takes place in the source material, a 1986 novel by the British writer Diana Wynne Jones. This is Miyazaki’s memory.He was born in 1941, the same year that Japanese planes attacked Pearl Harbor, and he was 4 years old when American planes attacked the city of Utsunomiya, where his family had been evacuated from Tokyo. He recounts in “The Kingdom of Dreams and Madness” how he saw a glow at the window and hid under a bridge, his legs in a ditch. With the incendiaries still falling, his father carried him up the riverbank and to a small truck so they could escape. As Miyazaki and his father settled into the vehicle’s bed, a woman with a child asked if they could come, too, but they were left behind. “We left them behind,” Miyazaki says. A month later, the United States dropped atomic bombs on Hiroshima and Nagasaki, and Japan surrendered. More humiliations followed: the emperor’s renunciation of divinity, the dismantlement of the country’s armed forces and a formal abjuring of war, enshrined in the Constitution.ImageCredit...Hayao Miyazaki © 1986 Studio GhibliImageCredit...Hayao Miyazaki © 1988 Studio GhibliAlthough Miyazaki was too young to comprehend the magnitude of what was taking place, that time remains a cornerstone of his work, as it was and has been for many Japanese artists who came of age during the war or in its aftermath. The late antiwar painter Tatsuo Ikeda, who was born in 1928 and conscripted as a teenager to become a kamikaze pilot — the country’s defeat saved him — started out making portraits for American soldiers from snapshots of their girlfriends or wives, and went on to create eerie black-and-white tableaus that bristle with malformed animals and punishing machines. Haruki Murakami, born in 1949 in Kyoto, the former seat of the imperial court, writes novels of deadpan humor that surreally interrogate the legacy and persistence of Japanese nationalism.And perhaps the most harrowing Japanese war film ever made is Studio Ghibli’s 1988 “Grave of the Fireflies,” adapted by Takahata from a 1967 short story by Akiyuki Nosaka about two children left homeless in the wake of an air raid. It bears the freight of Takahata’s own memories of fleeing a firebombing as a 9-year-old — he was born in 1935 — as his feet were burned by melting asphalt, and wandering without food for two days. “No one gave him anything, not even potato vines,” Miyazaki recalls in “The Kingdom of Dreams and Madness.” (Astonishingly, in its first release, “Grave of the Fireflies” was paired with “My Neighbor Totoro” as a double bill: anguish and solace.)Arguably, the rise of Japanese animation itself, in both its monster/superhero and more lyrical veins, was a direct response to the shock of defeat and anxiety over atomic fallout and the threat of genetic mutations. The monster Godzilla first appeared in a live-action 1954 film as a dinosaur, roused from the bottom of the ocean by an American hydrogen bomb test, who spews radiation over Tokyo in a visceral re-enactment of an air raid. (Miyazaki tells me that he remembers watching the movie and being reminded of American warplanes “dropping bombs from high above, out of reach.”) If Godzilla was fear and rage incarnate, Astro Boy — known in Japanese as the Mighty Atom, and introduced by the animation pioneer Osamu Tezuka in a 1951 manga, followed by an animated TV series starting in 1963 — sublimated anxiety into heroism: A boy robot whose body is powered by nuclear energy gets abandoned by his maker (giving him kinship with the war’s many orphans), but learns to use his abilities to fight for peace.Miyazaki’s movies, with their warplanes and intrusions of Western décor and dress, keep circling back to the traumatic moment when Japan, which until the mid-19th century had kept itself closed off to the outside world, was forced to embrace the West and Western values. The devastated population complied in confused haste, as if to erase the shame of recent history and their own complicity in a war waged by a nationalist government out of a belief in Japan’s cultural superiority. (Some saw this as a capitulation to the West and a fatal loss of dignity; in 1970, the writer Yukio Mishima died by ritual suicide in protest, after shouting, “Long live the emperor!”) Niebel, of the Academy Museum, suggests that Japanese audiences are drawn to Miyazaki’s work because it’s essentially nostalgic. There’s a yearning, faintly mournful, for an older Japan, one free of both imperialistic hubris and Western materialism.ImageCredit...Photo by Florent TanetBut part of his films’ greatness is that they can also be loved by viewers who never sense the dark current below. In “Porco Rosso” (1992), the hero may be an embittered war veteran, but he’s also, literally and delightfully, a pig flying a plane, and is spectacularly good at it.MIYAZAKI’S FATHER WAS not a bystander in the war. He ran a munitions factory that produced wings for the military’s fearsomely acrobatic Mitsubishi A6M Zero fighter planes, which in the last months of the war were converted for kamikaze missions. In a 1995 newspaper essay in The Asahi Shimbun, Miyazaki describes his father as something of a grifter, bribing officials to accept defective parts. After Japan’s surrender, when there were no more planes to furnish, his father used leftover duralumin, an aluminum alloy that had helped keep the Zero lightweight and dangerous, to make flimsy spoons, which he pawned off on impoverished customers desperate for household goods. Later, he briefly turned the factory into a dance hall, before bringing the family — Miyazaki is the second of four sons — back to Tokyo.Although Miyazaki never set foot in his father’s factory, which was off limits as a military site, he was entranced by airplanes and the liberation of flight from an early age. (Ghibli is both the hot, dusty wind that sweeps through the Libyan Desert and the name of an airplane, the Caproni Ca.309 Ghibli, a World War II Italian reconnaissance bomber.) This obsession has manifested in almost every film, in humans who turn into flying creatures or simply walk on air; in fanciful machines like the flaptors in “Castle in the Sky” (1986), propelled by four translucent wings; and in reproductions of real-world aircraft, as in “Porco Rosso,” in which the hero’s wrecked seaplane, inspired by the 1920s-era Italian racer Macchi M.33, is rebuilt by an all-female crew to ready it for a climactic dogfight, and in “The Wind Rises,” which tells the (not entirely) true story of the designer of the Zero, Jiro Horikoshi, who in the film as in life opposed the war and whom Miyazaki portrays as reluctant to see the beautiful machines he’s created deployed as emissaries of death — a stand-in for Miyazaki’s father, or the man he might have been.As Miyazaki grew older, he found fault with his father both for profiting off the war and for never expressing any shame or guilt. (He shares this troubled inheritance with the writers W.G. Sebald, born in 1944 in the Bavarian Alps, who had to grapple with his father’s past as a soldier in Hitler’s Wehrmacht, and the Nobel Prize winner Patrick Modiano, born in the suburbs of Paris in 1945 not long after V-E Day, whose own father kept company with collaborators and profiteers.) And yet, Miyazaki wrote in 1995, “I am like him” — a man of contradictions: a filmmaker who condemns the proliferation of images even as he contributes to it; an artist who has devoted his career to children but was rarely home to take care of his own; an environmentalist who can’t bear to give up his cigarettes or wheezing car; a professed Luddite who revels in the mechanics of modern vehicles but tries “not to draw them in a fashion that further feeds an infatuation with power,” as he has written; a pacifist who loves warplanes; a brooder with a dark view of how civilization has squandered the gifts of the planet, who nevertheless makes films that affirm the urgency of human life.ImageCredit...Courtesy of the artist. Photo by Joshua ScottThis embrace of contradictions may be why Miyazaki’s movies, although beloved in the West (if not as wildly successful as in Japan, where his last five films combined took in close to 100 billion yen in their first release, or around $873 million), in some ways thwart the Western mind. Absent are the dominating themes of monotheism — a fall from an original state of grace, followed by redemption — and a clear dichotomy of good and evil. “I’m not a god who decides on what is good and bad,” Miyazaki tells me. “We as humans make mistakes.” In his world, there are few outright villains or even truly bad characters, only characters who do bad things. Lady Eboshi wreaks havoc on the forest in “Princess Mononoke” but also gives sanctuary to brothel workers and those afflicted with leprosy. No-Face, the gliding black shroud who eats people in “Spirited Away,” turns out to be simply lonely and, when soothed, spits out his victims. Even the mutant stampeding army of trilobite-like behemoths from the toxic jungle in “Nausicaä of the Valley of the Wind” (1984), who kill the heroine by flinging her into the air and trampling her underfoot, end up restoring her to life with the touch of their golden antennae.So Disney was never an influence. (Miyazaki has gone so far as to say, in a 1988 lecture, that he hated Disney’s movies and their easy sentimentality: “To me, they show nothing but contempt for the audience.”) Instead, Miyazaki looked to works like the French animator Paul Grimault’s “The King and the Mockingbird” (released in different forms in 1952 and 1980), in which a chimney sweep and a shepherdess flee from a vain and despised tyrant king through a cavernous 296-story castle while a coterie of animals mounts a revolution, and the Armenian animator Lev Atamanov’s “The Snow Queen” (1957), whose heroine self-effacingly sacrifices her shoes to a river to beg for help in finding her lost friend, and whose gleefully amoral, knife-wielding Robber Girl — who captures the heroine and steals her bonnet and muff, then is horrified and furious to find herself moved to tears by her victim’s tale of woe — is a forerunner to the wolf girl of “Princess Mononoke.”Curiously, considering the limitations on women’s professional progress in Japan (which makes the country an outlier among developed nations), Miyazaki’s heroines outnumber his heroes. Within the world of anime, these characters are called shojo, girls of an in-between age, no longer quite children and not yet women; but where shojo were typically passive figures subject to romance narratives, Miyazaki’s girls display formidable know-how and independence. They take on jobs, organize households, fight battles and rescue boys from near death — all matter-of-factly, without ever trumpeting notions of girl power. Although some are princesses, they resist the trappings of fairy tales: Princess Mononoke doesn’t live in a palace. Chihiro, in “Spirited Away,” is awkward and lacks the big eyes that traditionally signify beauty and vulnerability in anime, while Sophie, the mousy milliner in “Howl’s Moving Castle,” spends most of the movie in the guise of a stooped old woman. Even when the spell is broken and her youth returns, her hair remains gray. It’s a reminder that something has been forever lost; that, even with the most powerful magic, there can be no reset, no starting over.ImageCredit...Takahiro KaneyamaAmerican animated films of today, by contrast, still tend to culminate in a happily ever after, or at least a vanquishing of foes. (“We have a desire for closure,” Niebel says.) Miyazaki offers something more nebulous and even unsettling. The resurrection in “Nausicaä of the Valley of the Wind” is a stark exception, for elsewhere in his oeuvre, death is not defeated, only at best delayed. Prince Ashitaka in “Princess Mononoke,” whose body has been progressively consumed by the dark stain of a curse, is never completely cured; a shadow remains on his arm, and he is separated from the girl he loves by a sense of duty — he to the humans of Iron Town, she to the wolves of the forest — although they promise to visit each other. Cruelty, too, is not so much punished as neutralized, as when the youthful-appearing Witch of the Waste in “Howl’s Moving Castle” is reinstated to her true age and revealed to be a doddering old lady, whom Sophie spoon-feeds without complaint, despite still suffering from the witch’s curse. Recovery may be possible, but not full restitution.In a 1991 directorial memo for “Porco Rosso,” a farce that includes a preening American pilot eyeing a career as a Hollywood star and a snarling gang of sky pirates who prove helpless when confronted with a gaggle of schoolgirls, Miyazaki cautions, “We must treat every character respectfully. We must love their foolishness. ... One common mistake — the belief that to draw a cartoon is to draw someone sillier than oneself — must be avoided at all costs.” At the heart of the film is a hard-bitten bounty hunter who takes on the guise of a pig out of a sense of guilt at having survived World War I while his fellow pilots died. (Miyazaki describes the film to me as “a boy’s dream.”) The woman he loves but doesn’t believe he deserves laments this “curse,” but only he can free himself from it, by no longer condemning that part of himself.“In the town that I live in, I have precious friends, but I also have people I detest,” Miyazaki tells me. “That is what human society is all about.” Even his friends are flawed, and not just them. He says, “It’s a mirror of who I am.”IT IS TEMPTING to read Miyazaki’s protestations as simple humility, and to cast him, against his will, as a sort of secular saint. In many ways he fits the part: the benevolent neighborhood uncle who brings joy to children through his work, picks up trash from the river on his days off and, over the past two and a half decades, has made quiet pilgrimages to a sanitarium near his home for patients with leprosy who, for much of the 20th century, faced segregation by law in such facilities. One patient became a friend, and Miyazaki held his hand when he was dying.ImageCredit...Courtesy of the artist. Photo by Joshua ScottBut Takahata, Miyazaki’s mentor at Toei Animation in the ’60s and ’70s and, eventually, his greatest rival, dismisses this hagiography in the afterword to “Starting Point” (1996), a collection of Miyazaki’s early interviews, lectures and essays, writing, “Hayao Miyazaki is a man who struggles. ... He weeps, is playful, loves people, expects too much of their talents, howls at his broken dreams, becomes enraged.” The brilliant and notoriously perfectionist Takahata, who once took eight years to finish a film, died in 2018, but he still casts a shadow; Miyazaki spent 15 years working with Takahata before becoming a director himself, and even though his movies at Studio Ghibli consistently outperformed Takahata’s at the box office, he still craved his mentor’s approval. (Suzuki, in a 2014 memoir, insists that Takahata is the only viewer whom Miyazaki has ever wanted to please.)To Takahata, Miyazaki’s contradictions made sense: Miyazaki is both an auteur, able to control and perfect every detail in his films, and an idealist endlessly disillusioned by the real world that eludes his grasp, and thus he rants, “yells destructively nihilistic things and blurts out statements that make him sound as though he aspires to become a dictator.” Miyazaki himself has always acknowledged his capacity for anger. To help his staff of animators understand how to draw the rampaging boar god turned demon in “Princess Mononoke,” whose flesh writhes with leechlike forms, he explained that he himself sometimes experienced a rage so strong it could not be contained inside his body.Takahata recounts how in his early days at Toei Animation, Miyazaki would sometimes scare colleagues “by suddenly screaming, ‘Let this damned studio burn down!’” This wasn’t an entirely metaphorical statement, Takahata points out, given Tokyo’s history of earthquakes and fire, and Japan’s precarious position at a place where four tectonic plates creep and shift. If Miyazaki was speaking then with the impishness of the provocateur, later in his career his insistence on facing certain realities took a serious turn. J. Raúl Guzmán, an assistant curator at the Academy Museum, learned while helping to put together the retrospective that some Japanese viewers were shocked by Miyazaki’s depictions of a violent ocean storm in “Ponyo” and the Great Kanto Earthquake of 1923 in “The Wind Rises,” which his father lived through as a boy. The scenes were painful reminders of the country’s vulnerability — so painful that after the earthquake and tsunami of 2011, which triggered a meltdown at the Fukushima Daiichi nuclear power plant, the broadcast network Nippon TV banned “Ponyo” from the airwaves for months.In the wake of the meltdown, Studio Ghibli hung a banner from the roof with a statement rejecting nuclear power. But the country was divided on how to respond to the disaster. By exposing Japan’s weaknesses, Fukushima also heightened sentiments of neo-nationalism. There were calls to revise Japan’s postwar Constitution, which states that “the Japanese people forever renounce war as a sovereign right of the nation and the threat or use of force as means of settling international disputes,” and to allow the country to once again establish offensive military forces. Miyazaki has strongly and publicly voiced his opposition to remilitarization, earning a ferocious backlash from right-wing commentators online. But they’re shouting into the void: Miyazaki doesn’t even own a computer. He isn’t there.ImageCredit...Takahiro KaneyamaAFTER THE WAR, Japan was shattered, occupied by the enemy, its cities in rubble. Food shortages left many hungry; American G.I.s handed out candy to children on the streets but, Miyazaki has written, he “was too ashamed” by Japan’s defeat to approach the soldiers. He was a shy, sickly boy — at one point, he nearly died — who took sanctuary in drawing, the one skill with which he could earn the attention and admiration of his peers. His mother was ill, too, suffering for years from spinal tuberculosis, and spent long stretches hospitalized like the mother in “My Neighbor Totoro” and the young wife in “The Wind Rises.” But the money his father had stockpiled from government wartime contracts helped keep the family in comfort, and in 1959 Miyazaki wound up at the prestigious Gakushuin University in Tokyo, which was originally established in the 19th century as a school for the nobility and whose students have included Emperor Naruhito and the singer and artist Yoko Ono.It was a time of upheaval for Japan, with traditional agriculture giving way to heavy industrial development and the economy growing at breakneck speed. Studying Japanese industrial theory, Miyazaki began thinking of himself as a Marxist. He was drawn to the Anpo demonstrations of 1960 against Japan’s security treaty with the United States and authoritarian measures by the Japanese government, although he remained on the sidelines. He had started drawing manga in high school and, after graduating from university, took a job at Toei, where he quickly became the secretary general of the animators’ union, negotiating for better working conditions. Although he would eventually move away from Marxism — “no matter what class people are born into, idiots are still idiots and good people are still good,” he said in 1994 — he still thinks “there are many things we can learn from it,” he tells me; it’s just that no one philosophy in the world “would enable all of us to live happily.”Miyazaki does not like to frame his work in explicitly ideological or moral terms. The mission of his films, he says, is to “comfort you — to fill in the gap that might be in your heart or your everyday life.” But his movies are haunted by his grief over the damage humans have done to the natural world. This may in part be a vestige of Shintoism, the indigenous faith of Japan, which holds that kami — at once specific supernatural beings and the divine essence within them — reside in all things. (Miyazaki follows no specific creed, but he has said that “sweeping the garden clean is already a religious act.”) As a teenager in the late ’50s, Miyazaki walked the streets of a Tokyo under constant construction, choked with dust. In 1964, when he was organizing workers at Toei, Japan hosted the Olympics and introduced the first bullet trains, which ate up the 320 miles between Tokyo and Osaka in four hours. By 1968, Japan was rich, second only to the United States in gross national product, and one of the most polluted countries on earth. (Thanks to the passage of strict environmental regulations, it is now one of the least.)ImageCredit...Courtesy of the artist, assisted by Yukimi Furusono. Photo by Joshua ScottIn “Spirited Away,” an oozing, fetid spirit comes to the bathhouse to be cleansed, and the intrepid heroine seizes what she thinks is a thorn in his side but turns out to be a bicycle. This unleashes a torrent of trash from his sludgy form: a refrigerator, a toilet, a traffic light. He is in fact an ancient river spirit, poisoned by pollution. Haku, the young apprentice, is a river spirit, too, but has forgotten his origins since his river was filled in and paved over to make way for apartments. Near the end, the film presents a fantasy of a world reclaimed by nature, as water fills a dry riverbed and spreads out into a vast sea — as if a visual riposte to the Italian painter Giorgio de Chirico’s desolate urban piazzas — untroubled save for a train that skims across its surface.There is a curious mix of fatalism and hope in Miyazaki’s work. The forest spirit in “Princess Mononoke” is murdered, despite the hero and heroine’s hardest efforts; yet the forest lives on. “For me, the deep forest is connected in some way to the darkness deep in my heart,” Miyazaki said in a 1988 interview. “I feel that if it is erased, then the darkness inside my heart would also disappear, and my existence would grow shallow.” At the same time, Miyazaki resists romanticizing nature as purely benign, again rejecting a binary of good and evil. The boars, wolves and apes in “Princess Mononoke” can’t agree on how to protect the forest, and when the boar god is struck by a bullet, he succumbs to hatred and attempts to ravage Prince Ashitaka’s village. Even then, Ashitaka’s first instinct is not to kill him but to plead with him to leave. “When you meet something that is very strange that you haven’t met before, instead of being scared of it, try to connect with it,” Miyazaki tells me.Where in the ’50s the still-raw memory of wartime destruction gave rise to monsters like Godzilla, specters of failed imperialism, Miyazaki’s work is notable in its insistence that we can learn to live alongside unfamiliar, even terrifying figures. Miyazaki once said that he wanted to make a version of “Beauty and the Beast,” only his interest was the beast. A trace of the fairy tale appears in “Howl’s Moving Castle,” in the desperate scene when the heroine follows bloody bird footprints down a dark hallway to find the wounded wizard in a feathered heap, unable to change back to his fully human self, trying not to die in his glittering lair embedded with the toys of the boy still buried inside him. Ashitaka, in “Princess Mononoke,” must wrestle a beast of his own: When he lets his arrows fly at the boar god turned demon, he gets too close and is infected by the creature’s rage. He, too, will begin to hate, the growing mark on his arm informs him. The only way to save himself is to master the true monster: within.ImageCredit...Hayao Miyazaki © 1989 Eiko Kadono – Studio Ghibli – NImageCredit...Hayao Miyazaki © 2013 Studio Ghibli – NDHDMTKSTUDIO GHIBLI MIGHT never have existed had Suzuki, now 73, not found a way to get past Miyazaki’s anger. The two men met in 1979, when, as the editor of an animation magazine, Suzuki showed up at Miyazaki’s workplace to procure an interview. (I speak with Suzuki in a separate online session, in which he is as loquacious as Miyazaki is evasive.) As Suzuki recalls, the filmmaker, in the throes of preproduction for his first feature, wanted nothing to do with him and accused him of “ripping off children” by making them buy his magazine. Rather than give up, Suzuki grabbed the desk next to Miyazaki’s and started working on the magazine there. The men sat hunched without speaking all day and into the night, until finally Miyazaki stood up to go home at 4 a.m. He told Suzuki he’d be back at 9 a.m., and so Suzuki returned then, too. Another day passed in silence. Only on the third day did Miyazaki start to talk.Thus was born a friendship that would turn into an intimate creative collaboration: For his next film, “Nausicaä of the Valley of the Wind,” Miyazaki consulted with Suzuki on matters from the intricacy of the drawing style to the final scene, which Suzuki persuaded him to change (in the first version, the heroine simply dies, which Suzuki thought deprived the audience of catharsis). After that film’s release, Suzuki realized they would have to start their own studio because no one else would foot the bill for such labor-intensive productions. Although he has held different positions at Studio Ghibli over the decades (among them president and, currently, producer), his true role is as Miyazaki’s confidante and consigliere. They used to talk almost daily and now meet once a week — during my conversation with Miyazaki, he notes that Suzuki is sitting beside him, off-screen, urging him to finish his new film, which has thus far taken four years — and when they disagree on an idea, Suzuki, at least by his own account, tends to win.Suzuki tells me that when Miyazaki came to him just over a year after retiring to say he wanted to make another film, “I was like, ‘Give me a break.’” He tried to talk him out of it, suggesting that Miyazaki’s best work was behind him. When his last film, “The Wind Rises,” came out in 2013, it did well at the box office but fell short of his previous four features, perhaps because it dealt directly with Japan’s culpability in the war, still an uncomfortable subject. But ultimately Suzuki caved in because, he says, “The whole purpose of Studio Ghibli is to make Miyazaki films.” What will happen, then, when Miyazaki does retire for good? His older son, Goro, 54, has made a few films for the studio, including the entirely computer-animated “Earwig and the Witch,” released in the United States last winter to mostly critical reviews that took less issue with the film itself than with the break in Ghibli tradition. (Miyazaki’s younger son, Keisuke, 51, is a printmaker.)On the CoversImageCredit...Hayao Miyazaki © 1988 Studio GhibliImageCredit...Hayao Miyazaki © 2001 Studio GhibliBut Suzuki also points out, when discussing the differences between Japanese and American animation, that in the West, we always need to know how things end. At Ghibli, the last scene is often a mystery. Because each movie requires so much drawing, production must begin before Miyazaki is even halfway through his storyboards. When he was making “Spirited Away,” No-Face was at first just a spooky passer-by; only later did Miyazaki decide to promote him to a major character. Later, the director of animation begged him not to draw any new characters, so he came up with the idea of Yubaba, the coldhearted bathhouse operator, having a kindhearted identical twin, which turned out to be both a crucial plot point and a sounding of a favorite theme: that in all of us there is a duality and the potential for both good and bad.Neither Miyazaki nor Suzuki will share much about the forthcoming film, beyond the fact that it is based on a 1937 novel by Genzaburo Yoshino. The story concerns a 15-year-old boy in Tokyo, small for his age and fond of mischief, whose father has recently died. In the English translation by Bruno Navasky, published in October, the boy gazes out at the city and is overwhelmed: “The watching self, the self being watched, and furthermore the self becoming conscious of all this, the self observing itself by itself, from afar, all those various selves overlapped in his heart, and suddenly he began to feel dizzy.” The actual content of the film could be anything — Suzuki has described it as “fantasy on a grand scale” — since Miyazaki doesn’t so much borrow stories as liberate them from their origins. (In the pseudobiographical “The Wind Rises,” he gives the real-life Jiro Horikoshi a fictional wife dying of tuberculosis.) All Suzuki will share is that he recognizes himself in one of the characters, who is not human.It is time. Miyazaki rubs the top of his head and lights a cigarette, one of his signature king-size, charcoal-filtered Seven Stars. I am allowed one last question. “The title of your next film is ‘How Do You Live?,’” I say. “Will you give us the answer?”The smile comes only after he speaks: “I am making this movie because I do not have the answer.”Audio produced by Jack D’Isidoro.</description>
      <pubDate>01 Dec 21 11:20 EST</pubDate>
      <guid>https://www.nytimes.com/2021/11/23/t-magazine/hayao-miyazaki-studio-ghibli.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://dimitarsimeonov.com/2021/12/12/opt-out-of-cynicism</link>
      <description>&lt;a href=&#34;https://dimitarsimeonov.com/2021/12/12/opt-out-of-cynicism&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 12 December 2021 A cynic is a man who knows the price of everything, and the value of nothing. Oscar Wilde Welcome to part 8 of the series on “How to be a game changer”. You can find the previous posts here 1, 2, 3, 4, 5, 6, 7. Growing up in post-socialist-turned-cowboy-capitalist Bulgaria, I grew up around a lot of cynical behavior and absorbed it deep into me. It was the water I was swimming in, and I knew no better. There was always this feeling that attempts at improvements are futile. If anyone tried to improve the system in any way, they will face a great opposition, and any value they bring forward will be immediately vultured away. This made it obvious for me to see how any changes will be abused and rendred futile. I also became good at rationalizing the existing status quo. There’s this example which stuck with me, that if someone created a coin operated parking meter, another one will quickly figure out how to steal the coins out of it. Thus, the attempt to bring order will fail, and the rationalization is that we are a motivated but backstabbing people which get in our own interest. And today, I still don’t know much better, and sometimes view the world through cynical eyes. A cynicism frame and mindset that turns a lot of my life into a zero-sum game. When I recently realized that I’ve been carrying this cynical mindset for a long time, my legs felt soft, and my body started shaking. I was reckoning all the mistakes I’ve made with this mindset. But what do I mean by being cynical? When you’re conversing with someone, it is often easy to detect the frame through which they view the world and the interaction with you. Cynics think that the world is never changing on a fundamental level, but it only changes cosmetically, and superficially. Cynics may look at some new trend and call it “lipstick on a pig”. According to cynics, the world operates in a certain way, the elites are always the same, and will always be the same. The cynics think we are always playing the same game that has always been played. For example, a cynic may think that media’s only purpose is to sell your attention to advertisers - like I did here. And often they might be right. We never try to be cynical. At least not consciously. Instead, we try to make models of the world. And we often simplify those models by ignoring the fact that the world today is different than the world ten years ago, and alien to the world a hundred years ago. We take a mental shortcut, but we end up in a dead end. Optimists, and even sometimes pessimists, see the world changing, for better or worse, and with it, the games we play change as well. They’re also both right to some degree. Cynics have fixed mindset, whereas optimists have growth mindset. The opposite of an optimist isn’t a pessimist, as they both realize that the world is changing and time moves forward. A pessimist also has a growth mindset, except they have a negative sign in front of it. The opposite of an optimist is the cynic. We are more likely to detect cynicism in others than in ourselves. For example if you approach a friend suggesting you both bake a pie together, they start talking about dividing the pie - they have the cynical fixed mindset. Or if a friend complains a lot, that’s also a cynical mindset - they don’t see a way to change the world, or their environment, and all they have left is complaining. When we detect in others, we’ll be more likely to detect in ourselves as well, and not only filter cynics out of our lives, but sometimes re-align them to a growth mindset, and attract other partners with growth mindset. Growth mindset says “yes and…” instead of “yes, but…”, or “no, but…”. And we may even re-align ourselves, to start recognizing all the ways the world may change - for better or worse. One of the strongest medicines against cynical mindset which I’ve tried is to do a “no-complain challenge”. I find its effects to be enormous and durable for years. In this challenge, you move a bracelet from one wrist to the other when you catch yourself complaining. The goal is to make it 21 days without having move the wrist. I did it years ago, and by the end of it, it felt as if I am in a different universe. With the years, some of its effect has rubbed off, and I am due a refresher. By realizing that the world is always changing, and by stopping to complain, we opt out of seeing the world through cynical glasses, and become part of the change. When we realize that our life can change, we can change it. Thanks to Sylvain Kieffer for spotting grammar typos in this post. </description>
      <pubDate>13 Dec 21 22:57 EST</pubDate>
      <guid>https://dimitarsimeonov.com/2021/12/12/opt-out-of-cynicism</guid>
    </item>
    <item>
      <title>What Ever Happened to Flickr?</title>
      <link>https://www.techspot.com/article/2384-flickr/</link>
      <description>&lt;a href=&#34;https://www.techspot.com/article/2384-flickr/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In 2007, Flickr was the most popular dedicated photo-sharing site on the web, and growing exponentially in terms of new images uploaded. There was no Instagram or Unsplash around, and essentially that&#39;s what Flickr could have become. A decade later, in 2018, Flickr was sold to the relatively unknown company SmugMug. What could Yahoo!, the site&#39;s former owner, have done so poorly in the years in between? How could Instagram have taken the lead so quickly after its launch in 2010? Is Flickr headed toward a virtual grave, or is it still a compelling service for some people? A Promising Start In 2004, the most popular sites on the web were Yahoo!, MSN, AOL and other sites that offered news stories and indexes of recommended websites. User participation was usually limited to comments on news stories and online forums. Flickr was considered a pioneer of the Web 2.0 era, alongside the likes of MySpace, Facebook, Blogger and YouTube, whose content was generated mostly by their users. Flickr was launched in 2004, just like Facebook, by Ludicorp, founded by the married couple, Stewart Butterfield and Caterina Fake. The image hosting service became an instant hit for its effective use of features that are considered obvious today, such as tags, favorites, comments, groups, sets (i.e. albums), the ability to list another user as a friend (or &#34;family&#34; for selective sharing), and the ability to embed photos in a &#34;weblog.&#34; Flickr had two account types: free accounts, limited to 20MB of uploads per month, and Pro accounts, with up to 2GB of monthly uploads for $25 per year. Yahoo! purchased Ludicorp in 2005, for a sum estimated to be around $25 million. Compared to the $1 billion that Facebook paid for Instagram in 2012 (to the amazement of many), it now looks ridiculous. At first, it looked like Yahoo!&#39;s resources would help Flickr become one of the largest sites on the web: in 2006, the upload limit was raised to 100MB per month for free accounts, and lifted altogether for Pro accounts. In 2007, Flickr was ranked as the 19th-largest site on the web by Alexa. Years of Neglect In January 2007, Yahoo! announced that all Flickr users would have to associate their accounts with Yahoo! accounts, which required them to provide more personal information to keep using Flickr. While annoying the community isn&#39;t a recommended tactic, Flickr&#39;s real problem started later that same year. In September 2007, the iPhone was announced, and companies such as Facebook immediately started working on mobile apps for their sites, which would become available to the public in 2008. Whether it was the result or the cause of Yahoo!&#39;s indifference, Fake and Butterfield left the company in 2008. Yahoo! only launched an official Flickr app in late 2009, giving Facebook and potentially many others plenty of time to become the go-to choice for sharing photos among mobile users. When the app finally launched, it lacked most of the features that made desktop users choose Flickr over Facebook in the first place: it could only show images in resolutions up to 600 pixels wide, it didn&#39;t include the &#34;interesting&#34; section, it couldn&#39;t edit images, and it removed the EXIF data from photos when uploading. Besides relying on Yahoo!&#39;s website for logging in, the app couldn&#39;t create a new account, send push notifications, upload several images at once, download images to the iPhone, delete images, or edit their properties. Devastating punishment for Yahoo!&#39;s neglect came in 2010 with the launch of Instagram. At first, Instagram didn&#39;t even have hashtags or a desktop version. Except for filters, all it did was make the sharing of images from iPhones easy. With Instagram around, the improvements to Flickr&#39;s app over time didn&#39;t look exciting. The fact that Flickr&#39;s app had an Android version before Instagram didn&#39;t matter much either. By 2012, Instagram had added an Android version, Facebook&#39;s financial backing, and 50 million monthly active users. A Late Comeback In late 2012, Yahoo! finally launched Flickr 2.0 – the iPhone app that Flickr users had wanted for years. The &#34;interesting/nearby&#34; section displayed images side by side, keeping their distinct aspect ratios, similarly to the &#34;justified view&#34; that Flickr&#39;s site had offered for almost a year. The &#34;contacts&#34; section let you scroll horizontally for more images from the same author, or vertically for images from other contacts. When you pinched to zoom in on an image, the app would load a higher-resolution version of it. The app&#39;s built-in camera had editing options, including filters. The new app arrived alongside an Android version, and a new plan of 1TB of storage for both Pro and free users in 2013. While the price of an ad-free Pro account was doubled to $50 per year, the improvements helped make Flickr more popular than ever before. It only had one problem: everyone&#39;s friends were already on Instagram. In 2014, Flickr launched an official iPad app. In 2015, once Google Photos became independent of the infamous Google+ social network, Flickr quickly fell out of favor, despite a quick response with its Uploadr app. Noah&#39;s Ark of Photos In 2017, Verizon purchased Yahoo!, and reorganized it under the name Oath (now Verizon Media). Less than a year later, Flickr was sold to SmugMug. The new owner, with its more limited resources, announced that free accounts would become limited to 1,000 images, regardless of file size, and ended the policy of keeping the Pro account fee at $25 per year for legacy Pro users. In 2019, SmugMug started deleting Flickr images of free users, except for the newest 1,000 and Creative Commons images. User Frank Michel estimated that the site had lost 63% of its images as a result. In 2020, SmugMug increased the fee for a Pro account to $60 per year, saying that the site was still losing money. Despite all of those concerning changes, Flickr isn&#39;t quite as unpopular as you may think: it&#39;s constantly ranked by Alexa among the top 500 sites globally, and among the top 300 in the U.S. It would appear that an old community of professional photographers is keeping the site alive. Unless SmugMug can sell Flickr to a bigger company or come up with a new and revolutionary feature, however, the site&#39;s remaining years may be few... The Aftermath Today, the most popular image sharing service is Google Photos, known for its ability to recognize people and places in photos and create albums of photos containing them. For years, it provided unlimited free storage of images up to 16MP, and videos up to 1080p. This, combined with Google&#39;s resources and integration with Android phones, drove user adoption to the masses, however as of 2021 it now only provides 15 GB of storage for free. Instagram remains the most popular social network based around images. Professional photographers tend to prefer Unsplash, now owned by Getty Images. DeviantArt is basically Unsplash for graphical artists. Those who want to embed images on sites that don&#39;t store them (like Reddit was until 2016) use services like Imgur, which doesn&#39;t even require a user account. The leading source for GIF-style images is Giphy, purchased by Facebook for $400 million in 2020. TechSpot&#39;s &#34;What Ever Happened to...&#34; Series The story of software apps and companies that at one point hit mainstream and were widely used, but are now gone. We cover the most prominent areas of their history, innovations, successes and controversies. ICQ WinAMP Netscape GameSpy AIM MSN Messenger Flickr Hotmail GeoCities Masthead credit: Evgeny Ptr. </description>
      <pubDate>29 Dec 21 14:16 EST</pubDate>
      <guid>https://www.techspot.com/article/2384-flickr/</guid>
    </item>
    <item>
      <title>Hang your code out to DRY</title>
      <link>https://johan.hal.se/wrote/2022/01/09/hang-your-code-out-to-dry/</link>
      <description>&lt;a href=&#34;https://johan.hal.se/wrote/2022/01/09/hang-your-code-out-to-dry/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I recently read an article by Jason Swett where he argues against Sandi Metz’ old chestnut “duplication is better than the wrong abstraction” and it made me feel like I have to articulate why I’m Team Sandi here. I’m sure I’ve used the saying a million times in discussions with colleagues and, as such, I feel personally attacked by Jason’s article and need to defend my honor. You can go read the article first – he spends some time thinking about what “abstraction” even means, but to me it’s always been pretty clear since it’s specifically pitted against duplication. You have a piece of code doing A, and you have a piece of code doing B, they’re very similar, so you combine them into a single concept. On re-reading Sandi’s original article it says kind of what I remember it saying, but it also… kinda doesn’t? There’s a lot more talk about programmers honoring the abstractions of elders who came before them, and less talk about the important part: how duplication can sometimes seem like a fantastic candidate for extraction and abstraction, but is in fact a nasty mirage that will strangle you in the dark and leave you slowly coagulating in a pool of your own technical debt. The old man and the &lt;b&gt; I think some of my conviction here comes from having spent roughly half my career (something like 2001-2011, get off my lawn youngsters!) firmly in the frontend camp. If you weren’t around for the thunderous mayhem of the second browser war, this might be an interesting new fact to you: in Internet Explorer 6 and earlier, you were limited to a single class per element. We developed some pretty rad workarounds down in the trenches, while &lt;font&gt; tags whizzed ominously past our heads, exploding in the distance: I remember stacking semantically empty divs inside each other to get more class names, and at some point I think I made a script that would create an IE-only include by turning classes like .foo and .bar into .foo-bar in a colorful burst of combinatorics. (Don’t judge me, I was young and needed the money.) Anyway, my point here is that composition in HTML was absolutely out of the window for almost a decade – if you had a .box-with-header class, well, that was it. That was the one you had. This was vindictive Old Testament stuff: no atoms to save you, ASP couldn’t output a style tag to save its life, you were on your own and the browser was a hostile and fickle companion that would absolutely end you if you didn’t know how to use the underscore hack or trigger hasLayout in the right places. So what did that mean for you when considering components? Well, you had to think both twice and thrice before committing to an abstraction that covered two or more cases. Oh, you found something that looks just like .box-with-header? Well, if you cram it into the same abstraction you better hope it never ever diverges from that platonic ideal because if it does, hoo boy are you fucked. Your designer will come to you and say something like “this box needs a more pronounced border and the text has to flow around it, plus if…” and by that time you’re already kind of checked out of the conversation because there’s a loud ringing in your ears, and your mind keeps replaying that beach landing scene from Saving Private Ryan. Sure, we had Style Guides and rudimentary component libraries, but we also had fucking deadlines. People sometimes went down into the pit with a torch to refactor a handful of well-used classes, never to be seen again. This inflexibility would mean the CSS quickly became append-only, never through any conscious process or decision, but because trying to reconcile changes to an abstraction that covered more than one use case was so enormously painful. We’d all nod and smile and pay lip service to the idea of re-use but in the day-to-day it was just a far better bet to duplicate code even if something seemed very similar. We’d only consider abstracting something if we were absolutely certain it wouldn’t come back to bite us in the ass later. Ghosts of the past Well, we’re in more modern times now, CSS was always a little weird about this stuff, and browsers have gotten a lot friendlier. But the funny part is that I’ve seen the same story play out in React codebases too: someone writes a nice and clean &lt;Header&gt; component. Then of course it has to look different when you’re logged in, and then yeah, it needs to change a bit when you’re Admin, and oh there’s another layout where we need to hide the main options when adding credit cards, and then someone realizes it shouldn’t be fixed on the registration page and then Dark Mode and suddenly you’re looking at spiders and cockroaches scuttling across a cursed 600-line component that’s absolutely lousy with arguments and control structures. At this point everyone dejectedly agrees that it would probably be best to split it into several components. The next day, the lead developer quits. A story as old as time itself. These days I mostly do Rails, and abstractions seem to come naturally to Ruby. It’s part framework-related, since you’re encouraged to think about your domain in terms of objects and what they’re responsible for, and part language-related because Ruby is first and foremost a great OO language. It feels a lot easier to reach for an abstraction when you see something that looks like something else, and so, in Ruby land, we do. But unlike Jason in his article, I would urge caution: you should let your code walk a few miles in its own moccasins before trying to mash it up with something else. It’s particularly important if you just wrote it! The fresher the code, the less you know about it. You haven’t lived with it yet, its borders and neighbors are fuzzy, its requirements might change. You have to hang it out for a while before it can be properly DRY. Living with duplication is tedious but straightforward – you make a change here, you might need to make a change there, too. Living with an ill-fitting abstraction is finicky, error-prone, and hard to understand – if you make a change here, what does that mean for your other, similar-but-not-quite use case? Over the years, I’ve come to see if statements as kind of a code smell. If I find myself two levels into an if statement, I’ll lean back in my beautiful, well-weighted Herman Miller chair, leather creaking expensively, and I’ll furrow my brows and grunt like a caveman. I just don’t like ‘em. You’ll find a lot of if statements in bad abstractions. Don’t go there just yet: better to channel your inner Kabat-Zinn, live with the duplication until you can see the path forward clearly, and then place an exquisitely cut and molded abstraction on your concepts that you know will fit. </description>
      <pubDate>12 Jan 22 09:45 EST</pubDate>
      <guid>https://johan.hal.se/wrote/2022/01/09/hang-your-code-out-to-dry/</guid>
    </item>
    <item>
      <title>How to Mislead with Facts</title>
      <link>https://consilienceproject.org/how-to-mislead-with-facts/</link>
      <description>&lt;a href=&#34;https://consilienceproject.org/how-to-mislead-with-facts/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Verified facts can be used to support erroneous conclusions. Here is how we can put an end to that.Fact-checking has become popularized as the definitive process for certifying truth in the media. This has occurred in response to the proliferation of a wide variety of internet subcultures, often based largely upon misinformation. Propaganda and bad faith communication are all too common, making the checking of facts an important part of sensemaking. While fact-checking is necessary, it is often not enough to provide the whole picture. Under current conditions of escalating culture and information war, facts themselves have become weapons.[1] Neither propaganda nor bad faith communication require the speaking of falsehoods.[2] It is often more effective to mislead and misinform through a strategic use of verified facts. The ability to critique and correct for the misuse of facts in public culture is an essential component of the democratic way of life. Unfortunately, today it is standard practice for both institutions and individuals from all sectors of society to offer strategically cherry-picked and decontextualized facts,[3] set within a predetermined emotional or ethical frame.[4] This way of using facts is an effective tool to bring some people towards previously unappealing conclusions. It also provides rhetorical ammunition to those already predisposed to drawing these conclusions. While honestly passing the scrutiny of the fact-checkers, such an approach is nevertheless far from entirely truthful. Verified facts are collected as ammunition for culture war, rather than for the sake of gaining a comprehensive understanding. In today’s so-called “post-truth” digital media landscapes, the practice of weaponizing facts has become widespread, microtargeted, and optimized for psychological impact. The normalization of mishandling facts threatens to undermine people’s sense of living in a shared reality. For some, it goes so far as to undermine the idea that reality can be known at all. Democratic forms of government are now being undermined by the mishandling and misrepresentation of “facts.” Stopping our descent into a “fact-blind culture” requires a new approach to the way we pay attention to and talk about “the facts.” For those seeking to improve the state of the epistemic commons, and address 21st-century challenges to sensemaking, there is no way forward that does not involve fundamental upgrades to how “facts” are handled in public discourse.[5] There is a growing body of literature on fact-checking as a media practice.[6][7][8][9] The fields of epistemology and the philosophy of science now have sub-branches seeking to address the crisis concerning “facts” in public culture. A thriving international movement of fact-checking is leading to the establishment of many new organizations aimed at certifying the truth. The details of these efforts can be found elsewhere.[10] Despite often earnest effort, the recent growth of fact-checking is not making the situation obviously better. Some argue that more fact-checking is in fact making things worse. How can that be? The answer is that fact-checking—the verification of specific claims—does nothing to address the three primary ways in which facts can be used to mislead (see the box below). Because fact-checking offers official verification, it permits easier use of facts to mislead and misinform. This sounds counterintuitive. But the more accepted a fact is, the greater its effect when it is made part of a misleading campaign. Image created by Adaptive CulturesA misleading campaign of facts runs according to some combination of the three primary ways outlined in the box above. Information campaigns that are factually truthful but nevertheless misleading are the stuff of classic propaganda, as we have documented in our recent series on the problem of modern propaganda.[11] For decades there has been cumulative innovation in the industry of public relations.[12] Techniques for misleading with facts have been continually and scientifically advanced. Facts become weapons for use in politically charged discourses in which winning is more important than accurately representing larger and more complex truths. Today, the strategic misuse of facts is becoming a common practice employed by everyday citizens on social media. Many people post to their social media feeds only those facts they endorse, which support their existing beliefs and ideologies. Verified facts are collected as ammunition for culture war, rather than for the sake of gaining a comprehensive understanding. Microtargeting then caters to these preferences, ensuring that there is a steady supply of cherry-picked facts on offer. The resultant filter bubbles and algorithmic radicalization have been discussed in our related paper on 21st-century information warfare.[13] The algorithmic radicalization prevalent on social media does not require “fake news.” Because facts can be used to mislead, extreme polarization and ideological identity capture can occur when individuals engage with information that is factual. This is possible when facts are taken out of context, cherry-picked, and emotionally loaded. Facts become weapons for use in politically charged discourses in which winning is more important than accurately representing larger and more complex truths. This debases the usefulness of “facts” and fact-stating discourses, which is to debase a necessary component of adequate public sensemaking.[14] But what would happen if we decided to slow down and think about the facts together? What if we really wanted to understand what was going on in a way that accounted for all the facts and their various frames and interpretations? The rivalrous “checking” of facts must give way to a more collaborative mutual understanding of facts. With a focus on education, this approach requires that individuals seek earnestly to evaluate the complexity of factual claims. Working together, individuals engage in a collaborative process to understand the implied significance and meaning of the facts in question, including all the associated complexity and nuance. There are four essential ways of understanding facts (see Box 2). Understanding facts requires a process that transcends but includes the familiar process of fact-checking, adding considerations of context, representativeness, and framing. Interpreting a fact involves values and judgments not determined by the fact itself.Image created by Adaptive CulturesBeyond simply verifying a fact, it must be placed in context and positioned relative to all other closely relevant facts. This includes gathering facts about the methods used to generate the fact in question. Verifying one fact requires verifying many others, while working towards presenting as comprehensive a network of related truths as possible. The emotional impact of any given fact is always complex. Interpreting a fact involves values and judgments not determined by the fact itself. Verification of factuality is only the beginning of a larger process of meaning-making, which involves considerations that cannot be reduced to specific debates about the “facts.” Our task is to create new processes for determining what counts as a shared, socially meaningful, mutually understood “truth.” Misleading with facts can only be done when attention is not paid to all four ways involved in the comprehensive understanding of facts. Fact-checking as currently practiced typically only focuses on one of the four. Educational efforts aimed at improving public discourse must consider more than how to detect deceptions and lies. There is a great deal more to understanding a fact than knowing if it is true. And there is a great deal more to understanding complex realities than agreeing on a set of facts. The point of this article is not that fact-checking is bad, but that it is necessary yet partial. As it stands, it is inadequate as a response to information war—but this does not mean it should be abandoned. The future of our civil society and public sphere depends upon drastically upgrading current approaches to dealing with “facts.” Our task is to create new processes for determining what counts as a shared, socially meaningful, mutually understood “truth.” Obviously, this requires more than making sure that every fact is checked. It is possible to expand our approaches to dealing with facts in public discourse in ways that include more complexity, nuance, and perspective-taking. A start would be to have fact-checking sites and discussions informed by the models offered above, instead of being constrained to only “checking.” Until such steps are taken to improve public culture, it will remain as easy to mislead with facts as it is to manipulate through deception—perhaps even easier. The stakes are high when it comes to the future of “facts.” As has been made clear: the mishandling of facts eventually breaks public sensemaking, and the breaking of public sensemaking eventually breaks society. The clock is ticking. As more and more “facts” pile up, our culture nevertheless gets farther and farther away from reality. Information warfare is now systematically and rapidly undermining the possibility of coherent public fact-stating discourses. This leads to a situation where political and public relations campaigns begin to operate more explicitly and self-consciously outside the truth. Of course, “facts” remain important—especially if they are officially verified—because they can be used as ammunition. But the larger, more complex truth is lost, accepted as a casualty of culture war. This kind of cynical, post-truth culture is antithetical to democratic ways of life. But the solution is not to create centralized “Truth Committees.” These would serve as the official legitimators of censorship, becoming the ultimate authorities on shared social reality. Open societies are defined, in part, by the free flow of reliable facts through public culture. They are different thereby from societies that route information through narrow channels and give over individual judgment to the dictates of authorities. Responsibility for the integrity of public fact-stating discourses should be distributed throughout civil society. The movement around the advancing field of fact-checking should not seek to consolidate power, but to distribute it. There is no technical “fix” or simple solution for improving the overall tenor and complexity of public communication about facts. There are, however, possibilities for digital technologies to enable educational initiatives of profound depth at massive scale. The same technologies that are now being used to mislead us with facts can be used to help us piece all the facts together and place them in the right context. The now crucial nexus of digital technologies, education, and politics can be reconfigured to allow for widespread learning and mutual understanding. Even though these facts are clear, there is, as always, the question of what we choose to do with them. </description>
      <pubDate>02 Feb 22 09:37 EST</pubDate>
      <guid>https://consilienceproject.org/how-to-mislead-with-facts/</guid>
    </item>
    <item>
      <title>Against an Increasingly User-Hostile Web</title>
      <link>https://neustadt.fr/essays/against-a-user-hostile-web/</link>
      <description>&lt;a href=&#34;https://neustadt.fr/essays/against-a-user-hostile-web/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; We&#39;re quietly replacing an open web that connects and empowers with one that restricts and commoditizes people. We need to stop it. - Parimal Satyal, 2 november 2017. See also: my Paris Web 2020 talk based on this article and the Hacker News discussion threads from 2017 and from 2020. I quit Facebook seven months ago. Despite its undeniable value, I think Facebook is at odds with the open web that I love and defend. This essay is my attempt to explain not only why I quit Facebook but why I believe we&#39;re slowly replacing a web that empowers with one that restricts and commoditizes people. And why we should, at the very least, stop and think about the consequences of that shift. The Web: Backstory (If you want, you can skip the backstory and jump directly to the table of contents). I love the web. I don&#39;t mean that in the way that someone might say that they love pizza. For many of us in the early 2000s, the web was magical. You connected a phone line to your computer, let it make a funny noise and suddenly you had access to a seemingly-unending repository of thoughts and ideas from people around the world. It might not seem like much now, but what that noise represented was the stuff of science fiction at the time: near-instantaneous communication at a planetary scale. It was a big deal. I was an average student at school. Despite well-meaning and often wonderful teachers, I didn&#39;t thrive much in a school system that valued test performance and fact-retention over genuine curiosity. Had it not been for the web, I might have convinced myself that I was a poor learner; instead, I realized that learning is one of my great passions in life. What remains of my fan site for German powermetal band Gamma Ray from 2001, archived thanks to the wonderful folks over at Archive.org I was 11 when I set up my first website. Growing up in Nepal, this was magical. Almost everything I love today—design, aviation, cosmology, metal music, computation, foreign languages, philosophy—I discovered through the many pages that found their way to my web browser. All I needed were curiosity, a phone line and that strange little electrical song. And good old Netscape Navigator. Netscape Navigator 4.04, source: A Visual Browser History, from Netscape 4 to Mozilla Firefox The web enabled that. It&#39;s one of humanity&#39;s greatest inventions. And now, we the architects of the modern web—web designers, UX designers, developers, creative directors, social media managers, data scientists, product managers, start-up people, strategists—are destroying it. We&#39;re very good at talking about immersive experiences, personalized content, growth hacking, responsive strategy, user centered design, social media activation, retargeting, CMS and user experience. But behind all this jargon lurks the uncomfortable idea that we might be accomplices in the destruction of a platform that was meant to empower and bring people together; the possibility that we are instead building a machine that surveils, subverts, manipulates, overwhelms and exploits people. It all comes down a simple but very dangerous shift: the major websites of today&#39;s web are not built for the visitor, but as means of using her. Our visitor has become a data point, a customer profile, a potential lead -- a proverbial fly in the spider&#39;s web. In the guise of user-centered design, we&#39;re building an increasingly user-hostile web. If you work in the design/communication industry, consider this essay introspective soul-searching by one of your own. If you&#39;re a regular web user, consider this an appeal to demand a better web, one that respects you instead of abusing and exploiting you. Note: The entire essay is rather long so feel free to skip to individual parts: The Web was Born Open: a very brief history of the web The Modern Web (of Deception): the disturbing state of the web today Track the Trackers, an Experiment: with whom websites are sharing your information Gated Communities: recentralization and closed platforms The Way Forward: open tools, technologies and services for a better web The Web was Born Open It all began in the early 90s. The Internet—the physical network that allowed computers around the world to communicate—was already in place but it remained inaccessible to most people. You had to know how to use a local client to connect to a remote FTP, Usenet, Gopher or an email server. This was before the days of ubiquitous graphical user interfaces so you had to type funny commands into a terminal, one of those black screens with green text that that hackers supposedly use to do Bad Things. Usenet Archives from 1981 on gopher server Quux.org, accessed 31 October 2017 via lynx Meanwhile, Tim Berners-Lee was working as an independent contractor at CERN in Geneva. Frustrated with how difficult it was to find, organize and update technical documentation, he proposed a solution that involved &#34;global computer networked information system&#34; that &#34;presented users with a web of interlinked documents&#34;, called Mesh. Pretty soon it became apparent that WWW—World Wide Web, as it came to be known—could do more than just link technical documents. The world&#39;s first website, accessed 31 October 2017 via lynx On April 30 1993, CERN made a bold decision. It decided to release WWW into the public domain. It renounced all intellectual property rights and essentially invited anyone at all, anywhere in the world, to play with it. Later, the director of CERN who approved the decision said that he was inspired by Richard Stallman&#39;s vision of free, open software. Had CERN decided otherwise and patented the technology to then license it for money, the web would arguably not have taken off the way it did. It might have died out like the Minitel did in France. The web as we know it was born of a vision to create an open system that brought people and ideas together, with documents that &#34;may reside on any computer supported by that web&#34;. Advances in the hyper-text transfer protocol (HTTP), network infrastructure, web browsers and standards, consumer Internet access, accessible hosting and blogging platforms led to a massive democratization and adoption of the web. Soon, anyone could put a document on the web and any document could link to any other. It created a completely open platform where a writer in Nepal could freely share her ideas with a dancer in Denmark. A climate science student in Nairobi could access data from the McMurdo weather station in Antarctica. You could start reading about logical fallacies and end up on a website about optical illusions. Read about the history of time-keeping and end up learning about Einstein&#39;s special theory of relativity. All interests were catered to. Information could truly be free: transverse borders, cultures and politics. That is the web at its best. My own journey from designing that first website as an 11-year old &#34;webmaster&#34; in Nepal to writing this article as a UX Consultant in France has its origin in that 1993 decision by CERN. The Modern Web (of Deception) The modern web is different. It&#39;s naturally different from a technological standpoint: we have faster connections, better browser standards, tighter security and new media formats. But it is also different in the values it espouses. Today, we are so far from that initial vision of linking documents to share knowledge that it&#39;s hard to simply browse the web for information without constantly being asked to buy something, like something, follow someone, share the page on Facebook or sign up to some newsletter. All the while being tracked and profiled. Almost every website you go to today reports your activities to third parties that you most likely neither know nor trust. They record where you come from, what pages you visit, how long you stay on each, where you click and where you go next. In fact, since so many websites report to the same third parties, these companies can essentially have your web history on file as you go from link-to-link, website to website. Like an omnipotent eye embedded on Sir Berners-Lee&#39;s global system of interlinked documents, noting down everything you do and reporting to private entities who then sell this information for profit. These companies build profiles, anonymous at first, with your interests and navigational behavior. These profiles can then get increasingly personal: they might include your email addresses, home address, income, educational history, political affiliation, information on your family. Over time, they can cross-reference all this information with your location data to figure out where you work, which restaurants you go to, where your gym is. Recently, we even learned that Google was able to associate your offline purchases with your online ad viewing history (albeit anonymously, it would appear). Once they have that, they can look into your behavior and psychology: what kind of ads do you tend to click on? What kind of messages resonate most with you? What are the best strategies to influence your opinion? Screenshot of Mr. Alexander Nix presenting the work of Cambridge Analytica, video The Power of Big Data and Psychographics on Youtube The Leave campaign responsible for Brexit in the United Kingdom and Donald Trump&#39;s 2016 presidential campaign both bought the services of a certain Cambridge Analytica, a company that boasts a gigantic database containing personal details amounting to &#34;close to four or five thousand data points on every adult in the United States&#34; (their own words). The goal? Craft hyper-personalized messages to change voting behavior based on your individual personalities, and by extension, your attitudes, opinions and fears. So if you are identified as a dad of three young kids in rural Texas, the message is nuanced to suggest that only a certain candidate will be able to protect your family against real or imagined threats. If you are identified as a patriot who&#39;s previously posted comments about gun rights and the second amendment, it might be about crime rates and how the opposition is trying to take your constitutional rights away from you. You become a manipulable data point at the mercy of big corporations who sell their ability to manipulate you based on the data you volunteer. This is the equivalent of someone following you in real life as you go about your everyday business, like a private eye who notes down with whom you meet, what you talk about, what you spend time looking at in stores. A private eye who takes notes and then sells it to the highest bidder. But you got to enter the store for free, so you should be so glad. The stores might also justify it. &#34;Sure it&#39;s a bit invasive, but we&#39;ll be able to give you better recommendations if we know what you like&#34;. But how do they get all this personal information -- where you live, who your friends are, what your religion and ethnicity are, where you were last night, what you bought on Monday? Most of it you volunteer yourself on social platforms like Facebook, Twitter and Instagram. The little share buttons you see on websites aren&#39;t just there to make it easy for you to post a link to Facebook; they also allow Facebook to be present and gather information about you from pretty much any website. But how can you know that any of this is true? Track the Trackers: An Experiment Perhaps you think I&#39;m being a tad too dramatic. In your defense, all of this does sound like some dystopian fantasy. But I&#39;m not that great a fiction writer quite yet. Let me illustrate my point with a little experiment. We&#39;ll pick a major website that you might visit regularly and identify third parties it shares your information with. We&#39;ll need a few things: a test website Webbkoll, a web privacy check tool by Dataskydd.net, a Swedish association for data protection and privacy (of which I&#39;m a proud member) and A web inspector Let&#39;s take an article that was published around the time I first started working on this article (which is last year; I&#39;m a slow writer): Astronomie : la sonde Juno s’est mise en orbite autour de Jupiter (Astronomy: space probe Juno put in orbit around Jupiter). Le Monde article Astronomie : la sonde Juno s’est mise en orbite autour de Jupiter If you run this URL through Dataskydd&#39;s Webbkoll and a web inspector tool (I used Chromium&#39;s web inspector), you learn a few interesting things: the page is 3.1 MB in size, makes about 460 HTTP requests of which 430 are third-party requests (outside of its parent domain) and takes 20 seconds to fully load on a fast 3G connection (from Paris, France). It also stores 100 cookies (these are little pieces of text stored on your computer by websites other than lemonde.fr; cookies are normally used to save session information but can also be used to identify and track you) and contacts 118 third-parties. And if all this weren&#39;t enough, your connection to LeMonde and the majority of third-party connections are over unsecure HTTP protocol (instead of the more secure HTTPS, which should be a basic requirement). That&#39;s a lot of big numbers for an article of 1500 words, three images and one video. Now let&#39;s look at some of the third parties that the page connects to when you load it: Weborama: advertising platform for analytics, digital marketing and behavioral targeting Visual Revenue: predictive analytics platform AppNexus: multimedia content monetization service Outbrain: &#34;online advertiser specializing in presenting sponsored website links&#34; (Wikipedia) Facebook: a social network and micro-targeted advertising platform Cedexis: a multi-CDN application delivery platform Note: In an earlier version of the article, I had mistakenly identified Cedexis as an &#34;ad-delivery platform&#34;, which it is not. My apologies to Cedexis for the error. Some of these are simply tools to manage content delivery but many are advertising or content monetization platforms. Companies like Weborama make money by selling information about you. When people say, &#34;you&#39;re the product,&#34; it isn&#39;t just some analogy, it accurately reflects the business propositions of many such companies. What&#39;s surprising is that the bulk of the information transferred between LeMonde and you doesn&#39;t even concern the actual article. If you were to isolate the actual content—the words, images and video—and put it in an HTML file, it would weigh considerably less than 3.1 MB and would make a lot fewer requests. If fact, I did just that and made three versions : Version A: With the original text (including comments, images and video) Version B: With the original text (including comments, images) but no video Version C: With just the original text (including comments), no images or video Some numbers: Original (LeMonde.fr) Version A Version B Version C Page Size 3,1 MB 1 MB (32%) 183 KB (5,8%) 17 KB (0,54%) Load Time 20,9 s 4,6 s (19,4%) 2,8 s (9,6%) 662 ms (3,2%) Requests (total) 459 108 (23,5%) 5 (1%) 1 (0,2%) Requests (third-party) 436 64 (14,7%) 4 (0,9%) 0 Third Parties Contacted 118 17 (14,4%) 2 (11,8%) 0 Cookies (total) 100 16 (16%) 0 0 Cookies (third-party) 73 16 (21,9%) 0 0 Text (% of Page Size) 0,5 % 1,7 % 9,5 % 100 % Text + Images (% of Page Size) 5,8 % 17,9 % 100 % Text + Images + Video (% of Page Size) 32,3 % 100 % Note: Data on the number of requests (first- and third-party) and cookies (first- and third-party) comes from Dataskydd Webbkoll. The rest of the data comes from Chromium&#39;s built-in web inspector. All connections were made from Paris, France with cacheing disabled and the bandwidth throttled to simulate a &#34;fast 3G&#34; connection. You can run these numbers yourself; they should vary only nominally depending on where you are. If you find errors, please let me know. Those are some very interesting figures. Some observations: The actual article (text and three images, version B) makes up less than 6% of the total size of the page on LeMonde.fr. This means that 94% of the data transferred between you and LeMonde.fr has nothing to do with the article. What about the video, you ask? Before you even play it, that one video adds over a 100 requests (60 of which are to 15 additional third parties) and 16 third-party cookies. It also adds over 800 KB of data. Again, this is before you even decide to play the video. The video might be related to the content, but it’s doing a lot more than that. Even compared to the version with the video (Version A), the LeMonde article makes about 450 additional third party requests, of which 370 are to about 100 additional third parties, storing 100 additional cookies (55 of which are third party cookies). It also adds over 2 MB to the page. All that is data that has nothing do with and is completely unnecessary to load the article you&#39;re reading. The text + image version (Version B) is able to load the entire text and the 3 images with only 5 requests and no cookies whatsoever. Adding a video should reasonably add one or two more requests and maybe one cookie, not 450 requests and 100 cookies, the majority of which are on behalf of companies you neither know nor trust, including those who track and sell your data for profit. The Le Monde page will continue to periodically transfer data and make additional requests even after it has completely loaded and as you scroll and interact with the page. If you monitor network traffic, a lot of this data is going to third-party tracking scripts. For example, a request is made to Xiti.com (a web analytics company) every few seconds. If you don&#39;t use a content blocker, you will notice that in just a matter of minutes, over 30 MB of data will be transfered between your browser and the 100+ third parties. The number of requests will go into the thousands. This will continue to rise as long as you leave your browser open. Essentially, this means that about 94% of the data being transferred and 99% of the requests being made have nothing to do with the article itself. Le Monde might principally be a newspaper in its printed version, but the online version is an invasive, insecure advertising platform with good content (in that order). If you&#39;re curious, try using Webbkoll on other websites you visit to see how privacy-friendly and respectful these websites are. We&#39;ll get into how to protect yourself from these third-party trackers later on in the article. All this might not be illegal (although there&#39;s some doubt, especially now that in the context of up the upcoming European General Regulation on Data Protection), but it is rather disrespectful towards the user. Not only are these websites breaking my trust—when I visit your website, I entered into contact with you, not 80 other websites—but they are loading content from websites neither know nor trust. Some of which have been know to spread malware. Using an ad/content-blocker isn&#39;t cheating the system; it&#39;s taking very basic precautions that websites like Le Monde can&#39;t be bothered to take to protect you. For me, it&#39;s a basic necessity in the modern web. If you&#39;re reading this and are wondering what to do to protect yourself, skip ahead to the The Way Forward section. If you run a website and you put official share buttons on your website, use intrusive analytics platforms, serve ads through a third-party ad network or use pervasive cookies to share and sell data on your users, you&#39;re contributing to a user-hostile web. You&#39;re using free and open-source tools created by thousands of collaborators around the world, over an open web and in the spirit of sharing, to subvert users. Gated Communities One of the most impressive things about the Internet (and consequently also the web) is that it is decentralized. No central authority gets to decide which page is more important than others and you don&#39;t have to play by anyone else&#39;s terms to publish and read what you want. There isn&#39;t anything like a main server that stores the code that runs the Internet; it&#39;s just a protocol on a physical backbone (of undersea cables). You could buy a Raspberry Pi Zero today for less than 10€, connect it to the Internet, set up a chat server on it, give it a public address and the world would be able to connect to it and talk to one other. Sure, it might not perform too well and no one might actually use it, but it is technically possible. But most of the time we spend on the web today is no longer on the open Internet - it&#39;s on private services like Facebook, Twitter and LinkedIn. While Facebook provides a valuable service, it is also a for-profit, company. Their source of revenue is advertising. It is the epitome of centralized. Francisco Goya&#39;s The Naked Maja (1800) Try posting a picture of the Francisco de Goya&#39;s &#34;The Naked Maja&#34; or your naked breasts (if you&#39;re a woman) on Facebook; it&#39;ll almost certainly be removed. It&#39;s against their terms of use. To use their platform, you have to agree to whatever conditions they set, however absurd. If you replace the open web with Facebook, you&#39;re giving up your right to publish and share on your terms. The data that you post there does not belong to you; you&#39;re putting it in a closed system. If one day Facebook decides to shut down—unlikely as that might seem today—your data goes with it. Sure, you might be able to download parts of it, but then what? Tumblr Blog Our Incredible Journey, &#34;cataloging the thrilling opportunities start-ups are offered when their incredible journey continues by being bought by an exciting company. However, as a user of the start-up’s service, your own incredible journey must end, because all of your photos and writing and checkins and messages and relationships must now be deleted&#34;. This works because they know you&#39;ll agree to it. You&#39;ll say you don&#39;t have a choice, because your friends are all there—the infamous &#34;network effect&#34;. This is Facebook&#39;s currency, its source of strength but also a crucial dependency. And this is what we often fail to realize: without its users—without you— Facebook would be nothing. But without Facebook, you would only be inconvenienced. Facebook needs you more than you need it. And they do their best to keep you on their website as long as possible. Your attention is worth a lot to a lot of companies who are convinced that traditional advertising is dead and that micro-targeted campaigns work better. (And they mostly do, from their point of view). This drives them to come up with absurd techniques to create addiction: wish your friend happy birthday, wish your colleague a happy work anniversary (who does that?), here&#39;s a video we made about you, three friends are going to an event near you, continue watching the video you started even as you scroll, be the first to comment, react to this photo, tell everyone what you&#39;re up to. The longer you stay, the more information you give, the more valuable your profile—and the platform—is to advertisers. I&#39;m not saying that what Facebook is doing is entirely unethical. It has to make money to make up for the resources it employs to keep the website running and it does so by advertising. Every time you choose to use a free service like Instagram, LinkedIn, Gmail or Snapchat, you are paying for the convenience with your eyes, your data and your attention. There&#39;s nothing inherently wrong as long you as you understand and consent to this exchange of value. But do you? Does your daughter? Your dad? What I&#39;m against is the centralization of services; Facebook and Google are virtually everywhere today. Through share buttons, free services, mobile applications, login gateways and analytics, they are able to be present on virtually every website you visit. This gives them immense power and control. They get to unilaterally make decisions that affect our collective behavior, our expectations and our well-being. You&#39;re either with them or out. Well, I chose out. You see, the web wasn&#39;t meant to be a gated community. It&#39;s actually pretty simple. A web server, a public address and an HTML file are all that you need to share your thoughts (or indeed, art, sound or software) with anyone in the world. No authority from which to seek approval, no editorial board, no publisher. No content policy, no dependence on a third party startup that might fold in three years to begin a new adventure. A website on Doom level design on Geocities from 1999, accessed October 31, 2017 via Archive.org That&#39;s what the web makes possible. It&#39;s friendship over hyperlink, knowledge over the network, romance over HTTP. In fact, the browser you&#39;re reading this on (Chrome, Firefox, lynx, whatever), the web server that&#39;s hosting this website (Nginx), the operating system that this server runs on (Ubuntu), the programming tools used to make it all work (python, gcc, node.js...) -- all of these things were created collectively by contributors all around the world, brought together by HTTP. And given away for free in the spirit of sharing. The web is open by design and built to empower people. This is the web we&#39;re breaking and replacing with one that subverts, manipulates and creates new needs and addiction. The Way Forward If you want to protect yourself (as a user) from predatory web marketing companies and defend the open web, there a few things you can do today at an individual level. If you&#39;re a web professional (a designer, UX consultant, strategist, programmer...), there are a number of considerations for better respecting your users and protecting their privacy (and your integrity). Here&#39;s a basic list: For end users (you, dear reader) If you use Chrome as your main browser, consider switching to the open-source version called Chromium. Better yet, switch to Mozilla Firefox, developed by the not-for-profit Mozilla Foundation that has a solid record of defending your privacy. Consider minimalist browsers like Min (and choose to block all ads, trackers and scripts) to browse news websites. Install a content/ad blocker for your browser: I recommend uBlock Origin (available for Firefox, Chrome and Safari on most platforms). You can also complement this with the Electronic Frontier Foundation&#39;s Privacy Badger tool that protects you from invasive ads and third-party tracking. Install HTTPS Everywhere for your browser; this forces your information through secure, encrypted channels (HTTPS vs HTTP one) if possible. It can also be configured to only allow connections to HTTPS websites. Think about how much information/details you provide to social media platforms like Facebook, LinkedIn, Twitter and Instagram. They already have quite a lot (including the ability to recognize you by name on photographs), but what other information are you volunteering? Where you are, whom you&#39;re with, information about your friends? Consider quitting social networks, especially Facebook (but download your data first!). What would you miss the most? Are there alternatives? Consider alternatives to free services provided by the likes of Google and Facebook. Today, if both of these companies shut down (or implement policies I don&#39;t like), I would mostly be fine because my contact with them is limited. I use DuckDuckGo and Startpage for search (free); FastMail for email and calendar (less than 40€ a year) ; HERE WeGo for maps (free); Signal, email and IRC for messaging (free, along with iMessage, Whatsapp and Twitter); Digital Ocean for web hosting (about 5€ per month). Pay for services and content that you like, if you are able. If you like reading The Guardian, for example, consider subscribing. If your favourite YouTube channel is on Patreon, consider pledging a small amount per video. If you like services like Pinboard.in that charge in return for a useful service, buy it. There&#39;s mutual respect when both the user and the service provider know what basic service they are buying/selling. At the very least, consider that the platforms you use need you more than you need them. You have power over them (unfortunately, in numbers) and they know it. If enough people care about privacy and respect for their data and time, platforms will have to adapt to stay relevant. For web professionals (you, fellow industry colleague) Consider not putting share buttons everywhere. They&#39;re visual noise and make third party connections every time the page is loaded (adding to load time). If you have to, create your own instead of using ones provided by Facebook and co. (so that a click is needed before a request is made to their servers) Support HTTPS. It&#39;s super easy (and free!) with Let&#39;s Encrypt so you don&#39;t have an excuse to not respect your users&#39; privacy Think about accessibility also in terms of page size, load times and tech requirements: will your website work without Javascript? What percentage of the total weight of your page is actual information? How many third party requests are you making? How long would it take to load on a 56.6k dial-up or on EDGE? How does it render for speech readers? Can it be read via a text-based browser? (It&#39;s a fun experiment; try visiting your website with a text-based browser like lynx or Links). Refuse client requests to implement hyper-invasive technologies like canvas fingerprinting. Consider replacing Google Analytics with a more privacy-respecting analytics software like Piwik. Even better if you can host it yourself! Minimize third-party dependencies like Google Fonts (you can self-host them instead). Avoid ad networks (like the plague!) if possible. Serve your own ads by selling ad space the old school way if you&#39;re able. If not, explore privacy-respecting methods of serving ads, including developments powered by the blockchain (like the Basic Attention Token). Respect Do Not Track. Carefully consider the benefits of hyper personalisation and retargeting. The benefits are debatable but the long term consequences might be disastrous. Ask yourself: would you be okay with a company collecting as much data (as you seek to collect) on your teenage daughter, your nephew in college, your husband or your grand-mother? Consider business models where you actually respect your clients and your website visitors instead of using them. If you can&#39;t be honest about your business model with your client, maybe you need to ask questions. Thoughts and feedback It all comes down to one simple question: what do we want the web to be? Do we want the web to be open, accessible, empowering and collaborative? Free, in the spirit of CERN’s decision in 1993 or the open source tools it&#39;s built on? Or do we want it to be just another means of endless consumption, where people become eyeballs, targets and profiles? Where companies use your data to control your behaviour and which enables a surveillance society—what do we want? For me, the choice is clear. And it&#39;s something worth fighting for. I hope this article has been interesting. If you have thoughts—you agree, disagree, have reservations, other ideas or a suggestion—I&#39;d love to hear them! This article is on GitHub; if you&#39;d like you can send a pull request with edit suggestions (like Anders and many others did, thank you!). You can also get in touch via email (userhostileweb—at—neustadt.fr) or, if you&#39;re on Hacker News or Reddit, share your thoughts there. — ← back home Against an Increasingly User-Hostile Web, written by Parimal Satyal on 7 November 2017 and published on Neustadt.fr. This text is in the public domain with a CC0 1.0 Universal license; you are free to do whatever you want with it (obviously doesn&#39;t apply to the photos or examples I&#39;ve included). A link back is nice but not required. </description>
      <pubDate>15 Feb 22 12:56 EST</pubDate>
      <guid>https://neustadt.fr/essays/against-a-user-hostile-web/</guid>
    </item>
    <item>
      <title></title>
      <link>https://spacetime.dev/plausibly-deniable-encryption</link>
      <description>&lt;a href=&#34;https://spacetime.dev/plausibly-deniable-encryption&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; plausibly deniable encryption February 20th, 2020 It is safe to assume that in any useful cryptosystem Ck C_k there exists at least one person with access to the key k k . An adversary with sufficient leverage can bypass the computational cost of a conventional attack by exerting their influence on this person. The technique is sometimes referred to as rubber-hose cryptanalysis and it gives the adversary some serious creative freedom. The security properties of the cryptosystem now rely not on the assumed difficulty of mathematical trapdoor functions but on some person’s tolerance to physical or psychological violence. A thief knows that pointing a gun will unlock a safe much faster than using a drill. An adversarial government will similarly seek information using torture and imprisonment rather than computational power. Many countries have key-disclosure legislation. In the United Kingdom, RIPA was first used against animal-rights activists to unlock data found on machines seized during a raid on their homes. The penalty for refusing to hand over key material is up to two years in prison. Say Alice has a cryptosystem Ck C_k whose security properties rely on the secrecy of the key k k . To defend against attacks of this form Alice needs some way to keep k k a secret. She could, Claim that k k is not known. This includes if it has been lost or forgotten. Claim the ciphertext c c is random noise and so is not decryptable. Provide an alternate key j j under which decryption produces a fake plaintext. Suppose Mallory is the adversary who wants k k and suppose Alice makes a claim X X in order to avoid revealing k k . Defining success can be tricky as Mallory can ultimately decide not to believe any claim that Alice makes. However we will simply say Mallory wins if she can show ¬X \neg X and therefore assert that Alice has access to k k and is able to provide it. So for Alice to win, X X must be unfalsifiable and hence a plausible defence. As a side note, if Alice knows and can demonstrate ¬X \neg X whereas Mallory cannot, then clearly she is missing some necessary information. Kerckhoffs’s principle says that the security of a cryptosystem Ck C_k should rely solely on the secrecy of the key k k , so in general we want proving ¬X \neg X to require knowing k k . We will ignore weaknesses related to operational security or implementation. For example if Mallory hears Alice admit to Bob that she is lying or if she finds a fragment of plaintext in memory then Alice has lost. However these situations are difficult to cryptographically protect against and so we assume security in this regard. Pleading ignorance (1) of k k is an easy strategy for Alice as it leaves little room for dispute and it can be deployed as a tactic almost anywhere. Mallory must show that k k is known and this is difficult to do without actually producing it. Perhaps the key was on a USB device that has been lost, or was written down on a piece of paper that burned down along with Alice’s house. Mere forgetfulness however implies that the data does exist and the only barrier to retrieving it is in accessing Alice’s memories. This may not be satisfactory. Asserting the non-existence (2) of the ciphertext is equivalent to claiming that k k does not exist and so cannot be disclosed. Plausibility comes from the fact that ciphertext is indistinguishable from random noise. This means that given some potential ciphertext c c an adversary cannot say if c c is uniformly sampled or if c=Ek(m) c = E_k(m) is a valid message m m encrypted under some key k k . To prove that c c is not random noise Mallory must produce k k and compute m m , which is assumed to be infeasible. TrueCrypt and VeraCrypt allow the creation of hidden volumes and hidden operating systems. The idea is that an ordinary encrypted volume will have unused regions of the disk filled with random data, and so a hidden volume can be placed there without revealing its existence. On-disk layout of an encrypted VeraCrypt volume. Suppose we have a boot drive with a standard volume protected by the key k1 k_1 and a hidden volume protected by the key k2 k_2 . The existence of the unencrypted boot-loader reveals the fact that the standard volume exists and so Mallory can confidently demand its key. Alice may safely provide Mallory with k1 k_1 thereby revealing the innocuous contents of the standard volume. However when Alice enters k2 k_2 , the boot-loader fails to unlock the standard region so instead it tries to decrypt at the offset where the hidden volume’s header would reside. If the hidden volume exists and if the provided key is correct, this operation is successful and the boot-loader proceeds to boot the hidden operating system. This is an example of providing a decoy decryption (3) but you may notice that Alice also had to claim that the remaining “unused” space on the drive is random noise (2) and not valid ciphertext. The necessity of a secondary claim is not a special case but a general property of systems that try to provide deniability in this way. Providing a plausible reason for the existence of leftover data can be tricky. VeraCrypt relies on the fact that drives are often wiped with random data before being used as encrypted volumes. In other situations we may have to be sneakier. This strategy does have some practical limitations. If the volume hosts an operating system, the innocuous OS has to be used as frequently as the hidden one to make it seem legitimate. For example if Alice provides the key and Mallory sees that the last login was two years ago, but she knows that Alice logged in the day before, then Mallory can be pretty sure something is off. Also consider what happens if Mallory sees a snapshot of the drive before and after some data is modified in the hidden volume. She then knows that there is data there and that it is not simply the remnants of an earlier wipe. The Dissident Protocol Imagine a huge library where every book is full of gibberish. There is a librarian who will help you store and retrieve your data within the library. You give her a bunch of data and a master key. She uses the master key to derive an encryption key and a random location oracle. The data is then split into book-sized pieces, each of which is encrypted with the derived key. Finally each encrypted book is stored at a location provided by the oracle. More formally, assume “library” means key-value store. Consider a key-derivation function Φ:K→K×K \Phi : K \to K \times K and a keyed cryptographic hash function H:K×N→K H : K \times \mathbb{N} \to K , where K K is the key space. We also define an encryption function E:K×M→C E : K \times M \to C and the corresponding decryption function D:K×C→M D : K \times C \to M , where M M and C C are the message space and ciphertext space, respectively. Alice provides a key k k which Faythe uses to derive the sub-keys a,b=Φ(k) a, b = \Phi(k) . Alice then provides some data p p which is split into chunks p1,p2,…,pn p_1, p_2, \ldots, p_n , where every pi p_i is padded to the same length. Finally, Faythe stores the entries {Ha(i):Eb(pi)} \{ H_a(i) : E_b(p_i) \} in the key-value store. For decryption, again Alice provides the key k k and Faythe computes the sub-keys a,b=Φ(k) a, b = \Phi(k) . She then iterates over i∈N i \in \mathbb{N} , retrieving the values ci c_i corresponding to the keys Ha(i) H_a(i) and computing Db(ci)=Db(Eb(pi))=pi D_b(c_i) = D_b(E_b(p_i)) = p_i , stopping at i=n+1 i = n + 1 where the key-value pair does not exist. The plaintext is then p=p1∥p2∥…∥pn p = p_1 \mathbin\Vert p_2 \mathbin\Vert \ldots \mathbin\Vert p_n , after unpadding each pi p_i . Some extra consideration has to go into integrity and authentication to prevent attacks where the data Alice stores is not the data she gets back out. We leave this out here for simplicity’s sake. Suppose the library contains n n books in total. Mallory cannot say anything about Alice’s data apart from that its total size is less than or equal to the amount of data that can be stored within n n books. If, under duress, Alice is forced to reveal a decoy key that pieces together data from m m books, she needs some way to explain the remaining n−m n - m books that were not used. She could claim that, The key for those books has been lost or forgotten. They are composed of random noise and so cannot be decrypted. They belong to other people and so the key is not known to her. This will look mostly familiar. Alice is trying to avoid revealing her actual data by providing a decoy key that unlocks some innocuous data. She then has to make a secondary claim in order to explain the remaining data that was not decrypted under the provided key. Claiming ignorance (A) has the same trivial plausibility argument and practical limitation as before (1). Asserting that the leftover books are composed of random bytes (B) requires an explanation for how they came to be there. She could say simply that she added them but this is a can of worms that we want to keep closed. If some software implementation decides how many decoy books to add, it would necessarily leak information to Mallory about the expected frequency of decoys. This value can be compared with Alice’s claim of n−m n - m decoys to come up with an indicator of whether Alice is lying. We have the same problem if the frequency is decided randomly as the value would have to lie within some range. We can get around this by asking Alice herself to decide the frequency, but this is messy and humans are bad at being unpredictable. In any case, this strategy boils down to Alice claiming “I added decoy entries explicitly in order to explain leftover data”, and this would rightly make an adversary extremely suspicious. A better way to utilise B is for Faythe to replace books that are to be deleted with random data instead of removing them outright. Then Alice can claim that the remaining books have been deleted and therefore the data no longer exists and cannot be decrypted. This way potentially any number of leftover books can be easily explained, but it does mean that the size of our library will only increase over time. Claim C is new and has some appealing properties but it can’t be used on a personal storage medium—like Alice’s laptop hard drive—as there is unlikely to be a plausible reason for other people’s data to be there. Imagine instead that the “library” is hosted on a service shared by multiple people. Then it is easy for Alice to claim that the remaining entries are not hers. Mallory would need leverage over every other person using the service in order to disprove Alice’s claim. Such a service has to be carefully designed however. For example if it stored how much space Alice is using then this value can be compared with Alice’s claim and Mallory wins. There are some drawbacks of this scheme. There is an overhead in storing data in discrete, padded chunks. Modifying data in a non-trivial way may be expensive. Overwriting entries instead of removing them uses up storage space that is “wasted” in the sense that it does not hold any useful data. In designing this protocol what I have found is that we have to be extremely careful to avoid losing our deniability. Any implementation has to be verified to ensure that it does not fall short in this regard. However we now have something that lets you have an arbitrary number of independent “folders” stored amongst numerous indistinguishable packets, with an adversary being unable to infer any information other than the maximum size of the stored data. This is a powerful property but it should be considered as part of the whole picture including your threat model and usability requirements. There is an experimental client implementing the spirit of this protocol here. As of the time of writing, it is not ready for serious use. However there are some exciting ideas I have for making this into a production ready and usable client in the (hopefully) near future. {home : : subscribe with rss/atom} 2020-12-16 : : rosen: censorship-resistant proxy tunnel 2019-07-20 : : memory retention attacks 2019-07-18 : : encrypting secrets in memory 2019-06-27 : : mutable strings in go 2019-05-02 : : to slice or not to slice 2017-08-03 : : memory security in go 2017-07-30 : : quantum key-exchange </description>
      <pubDate>18 Feb 22 15:17 EST</pubDate>
      <guid>https://spacetime.dev/plausibly-deniable-encryption</guid>
    </item>
    <item>
      <title></title>
      <link>https://vaghetti.dev/posts/times-are-great/</link>
      <description>&lt;a href=&#34;https://vaghetti.dev/posts/times-are-great/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; It is a good time to be in tech right now: soaring salaries, daily LinkedIn recruiter spam, people bootcamping their way into career switches to tech, remote work allowing you to work for a major tech company making triple digit salaries from the beach. It really feels like every single company is trying to recruit programmers nowadays, and doing whatever they can to get them. I’m certainly happy with the current state of affairs, but I can’t help but wonder: How does it end? Things are obviously good for us programmers and we would certainly like for them to stay this way. Business owners, on the other hand, are not quite happy. You see, no sane CEO is going to issue a press release saying “yeah, business is booming, record profits something something, great opportunities for double digit growth in something something Asia, by the way I’d be really happy if I could pay less money to my employees”. That’s just not the kind of thing you say, but everyone kinda knows it is true. The CEO’s job is to make as much profit as possible. Higher salaries mean less profit. It’s just how the game works. Salaries are a matter of supply and demand. Lots of companies trying to hire not-that-many programmers means salaries go up, if it was the other way around, they would go down. So how do the good times end? Either demand for tech workers goes down or supply goes up. Supply goes up The markets are doing their thing. Everyone sees tech workers making money and just generally having a good time and they think: “Hey that sounds like a good career choice”. Every year we get more Computer science graduates. Code bootcamps are also booming. But these are long term changes that are barely making up for the also increasing demand, so I don’t see any meaningful change coming from this front at last in the short to medium term. Demand goes down This is where I think the most interesting (and faster) changes could happen. Jeff Besos famously said “Your margin is my opportunity”. Tech worker wages are certainly a massive opportunity right now! There is a lot of money to be made by making products that allow businesses to get things done with less programmers, maybe no programmers at all. I know it sounds kind of evil putting it like that, but bear with me. Developer tooling improvements Developers are expensive. If a new IDE shows up and they can deliver stuff 5% faster, that means you can have 5% less programmers! Of course the IDE has a price, but it is almost certainly cheaper than 5% of a developer. I haven’t seen that much improvement in developer tooling over the last couple of years, but AI is making some really meaningful progress in this field with things like github copilot. For now it only delivers some smarter auto complete suggestions, not that far in the future we could see AIs finding bugs in our code, or even generating tests. No code / Low code People have been trying this for decades and it never really worked out. I can’t think of a single product that was able to get to any meaningful degree of success running on top of some low code platform. But hey, deep learning was in a similar spot in the 90s and now we have real products that use it and it mostly works. The incentive has never been greater. The more expensive developers get, the more money there is to be made by making them less necessary. Decreased investment I think this is the most likely scenario. Much of the demand for programmers right now stems more from expectations of future profits than from profits being currently made. Many companies hiring now are unprofitable companies surviving on investor money with the expectation of future profit. If the economic conditions change and investors decide they don’t like tech companies anymore, many of them will inevitably close. Less companies hiring also means less demand. Conclusion This post might have sounded gloomish, at least for programmers, but honestly I don’t think there’s that much to worry about. Even if we enter some new dotcom bubble and a good chunk of our startup ecosystem dies off, demand for tech is spreading across all industries, businesses everywhere are digitizing their operations and these systems will need to be maintained somehow. Also, salaries are notoriously sticky, so instead of a salary reduction we could see a workforce reduction instead. Maybe it is better to start replying to the linkedin recruiter spam just in case? </description>
      <pubDate>21 Feb 22 08:31 EST</pubDate>
      <guid>https://vaghetti.dev/posts/times-are-great/</guid>
    </item>
    <item>
      <title></title>
      <link>https://macwright.org/2020/05/10/spa-fatigue.html</link>
      <description>&lt;a href=&#34;https://macwright.org/2020/05/10/spa-fatigue.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The emerging norm for web development is to build a React single-page application, with server rendering. The two key elements of this architecture are something like:The main UI is built &amp; updated in JavaScript using React or something similar.The backend is an API that that application makes requests against.This idea has really swept the internet. It started with a few major popular websites and has crept into corners like marketing sites and blogs.I’m increasingly skeptical of it.There is a sweet spot of React: in moderately interactive interfaces. Complex forms that require immediate feedback, UIs that need to move around and react instantly. That’s where it excels. I helped build the editors in Mapbox Studio and Observable and for the most part, React was a great choice.But there’s a lot on either side of that sweet spot.The high performance parts aren’t React. Mapbox GL, for example, is vanilla JavaScript and probably should be forever. The level of abstraction that React works on is too high, and the cost of using React - in payload, parse time, and so on - is too much for any company to include it as part of an SDK. Same with the Observable runtime, the juicy center of that product: it’s very performance-intensive and would barely benefit from a port.The less interactive parts don’t benefit much from React. Listing pages, static pages, blogs - these things are increasingly built in React, but the benefits they accrue are extremely narrow. A lot of the optimizations we’re deploying to speed up these things, things like bundle splitting, server-side rendering, and prerendering, are triangulating what we had before the rise of React.And they’re kind of messy optimizations. Here are some examples.Bundle splitting.As your React application grows, the application bundle grows. Unlike with a traditional multi-page app, that growth affects every visitor: you download the whole app the first time that you visit it. At some point, this becomes a real problem. Someone who lands on the About page is also downloading 20 other pages in the same application bundle. Bundle splitting ‘solves’ this problem by creating many JavaScript bundles that can lazily load each other. So you load the About page and what your browser downloads is an ‘index’ bundle, and then that ‘index’ bundle loads the ‘about page’ bundle.This sort of solves the problem, but it’s not great. Most bundle splitting techniques require you to load that ‘index bundle’, and then only once that JavaScript is loaded and executed does your browser know which ‘page bundle’ it needs. So you need two round-trips to start rendering.And then there’s the question of updating code-split bundles. User sessions are surprisingly long: someone might have your website open in a tab for weeks at a time. I’ve seen it happen. So if they open the ‘about page’, keep the tab open for a week, and then request the ‘home page’, then the home page that they request is dictated by the index bundle that they downloaded last week. This is a deeply weird and under-discussed situation. There are essentially two solutions to it:You keep all generated JavaScript around, forever, and people will see the version of the site that was live at the time of their first page request.You create a system that alerts users when you’ve deployed a new version of the site, and prompt them to reload.The first solution has a drawback that might not be immediately obvious. In those intervening weeks between loading the site and clicking a link, you might’ve deployed a new API version. So the user will be using an old version of your JavaScript frontend with a new version of your API backend, and they’ll trigger errors that none of your testing knows about, because you’ll usually be testing current versions of each.And the second solution, while it works (and is what we implemented for Mapbox Studio), is a bizarre way for a web application to behave. Prompting users to ‘update’ is something from the bad old days of desktop software, not from the shiny new days of the web.Sure: traditional non-SPA websites are not immune to this pitfall. Someone might load your website, have a form open for many weeks, and then submit it after their session expired or the API changed. But that’s a much more limited exposure to failure than in the SPA case.Server-Side RenderingOkay, so the theory here is that SPAs are initially a blank page, which is then filled out by React &amp; JavaScript. That’s bad for performance: HTML pages don’t need to be blank initially. So, Server-Side Rendering runs your JavaScript frontend code on the backend, creating a filled-out HTML page. The user loads the page, which now has pre-rendered content, and then the JavaScript loads and makes the page interactive.A great optimization, but again, caveats.The first is that the page you initially render is dead: you’ve created the Time To Interactive metric. It’s your startup’s homepage, and it has a “Sign up” button, but until the JavaScript loads, that button doesn’t do anything. So you need to compensate. Either you omit some interactive elements on load, or you try really hard to make sure that the JavaScript loads faster than users will click, or you make some elements not require JavaScript to work - like making them normal links or forms. Or some combination of those.And then there’s the authentication story. If you do SSR on any pages that are custom to the user, then you need to forward any cookies or authentication-relevant information to your API backend and make sure that you never cache the server-rendered result. Your formerly-lightweight application server is now doing quite a bit of labor, running React &amp; making API requests in order to do this pre-rendering.APIsThe dream of APIs is that you have generic, flexible endpoints upon which you can build any web application. That idea breaks down pretty fast.Most interactive web applications start to triangulate on “one query per page.” API calls being generic or reusable never seems to persist as a value in infrastructure. This is because a large portion of web applications are, at their core, query &amp; transformation interfaces on top of databases. The hardest performance problems they tend to have are query problems and transfer problems.For example: a generically-designed REST API that tries not to mix ‘concerns’ will produce a frontend application that has to make lots of requests to display a page. And then a new-age GraphQL application will suffer under the N+1 query problem at the database level until an optimization arrives. And a traditional “make a query and put it on a page” application will just, well, try to write some good queries.None of these solutions are silver bullets: I’ve worked with overly-strict REST APIs, optimization-hungry GraphQL APIs, and hand-crafted SQL APIs. But no option really lets a web app be careless about its data-fetching layer. Web applications can’t sit on top of independently-designed APIs: to have a chance at performance, the application and its datasource need to be designed as one.Data fetchingSpeaking of data fetching. It’s really important and really bizarre in React land. Years ago, I expected that some good patterns would emerge. Frankly, they didn’t.There are decent patterns in the form of GraphQL, but for a React component that loads data with fetch from an API, the solutions have only gotten weirder. There’s great documentation for everything else, but old-fashioned data loading is relegated to one example of how to mock out ‘fetch’ for testing, and lots of Medium posts of varying quality.Don’t read this as anti-React. I still think React is pretty great, and for a particular set of use cases it’s the best tool you can find. And I explicitly want to say that – from what I’ve seen – most other Single-Page-Application tools share most of these problems. They’re issues with the pattern, not the specific frameworks used to implement it. React alternatives have some great ideas, and they might be better, but they are ultimately really similar.But I’m at the point where I look at where the field is and what the alternative patterns are – taking a second look at unloved, unpopular, uncool things like Django, Rails, Laravel – and think what the heck is happening. We’re layering optimizations upon optimizations in order to get the SPA-like pattern to fit every use case, and I’m not sure that it is, well, worth it.And it should be easy to do a good job.Frameworks should lure people into the pit of success, where following the normal rules and using normal techniques is the winning approach.I don’t think that React, in this context, really is that pit of success. A naïvely implemented React SPA isn’t stable, or efficient, and it doesn’t naturally scale to significant complexity.You can add optimizations on top of it that fix those problems, or you can use a framework like Next.js that will include those optimizations by default. That’ll help you get pretty far. But then you’ll be lured by all of the easy one-click ways to add bloat and complexity. You’ll be responsible for keeping some of these complex, finicky optimizations working properly.And for what? Again - there is a swath of use cases which would be hard without React and which aren’t complicated enough to push beyond React’s limits. But there are also a lot of problems for which I can’t see any concrete benefit to using React. Those are things like blogs, shopping-cart-websites, mostly-CRUD-and-forms-websites. For these things, all of the fancy optimizations are trying to get you closer to the performance you would’ve gotten if you just hadn’t used so much technology.I can, for example, guarantee that this blog is faster than any Gatsby blog (and much love to the Gatsby team) because there is nothing that a React static site can do that will make it faster than a non-React static site.But the cultural tides are strong. Building a company on Django in 2020 seems like the equivalent of driving a PT Cruiser and blasting Faith Hill’s “Breathe” on a CD while your friends are listening to The Weeknd in their Teslas. Swimming against this current isn’t easy, and not in a trendy contrarian way.I don’t think that everyone’s using the SPA pattern for no reason. For large corporations, it allows teams to work independently: the “frontend engineers” can “consume” “APIs” from teams that probably work in a different language and can only communicate through the hierarchy. For heavily interactive applications, it has real benefits in modularity, performance, and structure. And it’s beneficial for companies to shift computing requirements from their servers to their customers browsers: a real win for reducing their spend on infrastructure.But I think there are a lot of problems that are better solved some other way. There’s no category winner like React as an alternative. Ironically, backends are churning through technology even faster than frontends, which have been loyal to one programming language for decades. There are some age-old technologies like Rails, Django, and Laravel, and there are a few halfhearted attempts to do templating and “serve web pages” from Go, Node, and other new languages. If you go this way, you’re beset by the cognitive dissonance of following in the footsteps of enormous projects - Wikipedia rendering web pages in PHP, Craigslist rendering webpages in Perl - but being far outside the norms of modern web development. If Wikipedia were started today, it’d be React. Maybe?What if everyone’s wrong? We’ve been wrong before.Follow-ups &amp; commmentary&#34;In defense of the modern web&#34;, Rich HarrisFriday Night Deploys (Podcast) #22: A Brief Discussion On The State Of The Modern WebFrontend First (Podcast): Read &amp; DiscussA Ready-To-Try Concept in Response to “Second-guessing the modern web”</description>
      <pubDate>11 May 20 16:06 EDT</pubDate>
      <guid>https://macwright.org/2020/05/10/spa-fatigue.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://cs.stanford.edu/~pliang/</link>
      <description>&lt;a href=&#34;https://cs.stanford.edu/~pliang/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; My goal is to develop trustworthy systems that can communicate effectively with people and improve over time through interaction. I broadly identify with the machine learning (ICML, NeurIPS) and natural language processing (ACL, NAACL, EMNLP) communities. Computers can do a lot, but tapping into their full power requires the rather non-trivial ability to program. I&#39;m interested in building systems that learn to translate natural language descriptions (e.g., in English or Chinese) into programs (e.g., in Python or C++). Such systems would unlock the full power of computing to a much wider audience. A while back, I wrote a friendly introduction to natural language interfaces (XRDS magazine 2014) and a slightly more technical survey article on executable semantic parsing (CACM 2016). One idea we&#39;ve explored is to &#34;naturalize&#34; a programming language gradually into a natural language (ACL 2017). One can also use natural language to describe classifiers directly rather than requiring labeled data (ACL 2018). The tension between the fuzziness of machine learning and the crispness of logic also fascinates me. On this note, we showed that neural networks can solve SAT problems with surprising accuracy despite not being told explicitly what a SAT problem is (ICLR 2019). Despite the successes of machine learning, otherwise high-performing models are still difficult to debug and fail catastrophically in the presence of changing data distributions and adversaries. For example, on the SQuAD reading comprehension dataset we created (EMNLP 2016), we showed that state-of-the-art systems, despite reaching human-level benchmark performance, are easily fooled by distracting sentences in a way that no human would be (EMNLP 2017). Given society&#39;s increasing reliance on machine learning, it is critical to build tools to make machine learning more reliable in the wild. We&#39;ve worked on using influence functions to understand black-box models (ICML 2017), semidefinite programming to provide certificates a neural network is safe from a class of adversaries (NeurIPS 2018), and distributionally robust optimization to ensure the fairness of machine learning models over time (ICML 2018). Finally, I am a strong proponent of efficient and reproducible research. We have been developing CodaLab Worksheets, a platform that allows researchers to run and manage their experiments by maintaining the full provenance of an experiment from raw data to final results. Most of our recent papers have been published on CodaLab as executable papers. We are actively looking for contributors, so please contact me if you&#39;re interested! Here is some code for older projects. </description>
      <pubDate>29 Nov 20 11:11 EST</pubDate>
      <guid>https://cs.stanford.edu/~pliang/</guid>
    </item>
    <item>
      <title></title>
      <link>https://thume.ca/2016/07/16/advanced-hackery-with-the-hammerspoon-window-manager/</link>
      <description>&lt;a href=&#34;https://thume.ca/2016/07/16/advanced-hackery-with-the-hammerspoon-window-manager/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Advanced Hackery With The Hammerspoon Window Manager Along with Dash, Sketch and Papers, one of the main reasons I haven’t yet switched to Linux is Hammerspoon. Hammerspoon gives me most of the power that a fancy Linux tiling window manager and configurable desktop would give me, without having to switch operating systems. It’s fully configurable with Lua, has tons of built in modules and it is simple to write your own modules. I think of it more as a general-purpose tool for modifying OSX’s user interface than just a window manager. This post explores some of the ways I’ve used Hammerspoon to greatly enhance my general OSX-using experience. Window Hints The first Hammerspoon module I wrote was a port of Slate’s window hints, which if you’ve ever used Vimium or Vimperator, are like link hints for windows. They allow you to switch to any window with only two keystrokes: One shortcut to bring up icons and letters for every window, and then simply hitting the key corresponding to the window you want. The module was written mostly in a single evening as a native Lua module (originally for Mjolnir, the precursor to Hammerspoon). It didn’t take much time, and is very enjoyable to use, and because the module was added to the core Hammerspoon distribution, lots of other people can also benefit from it. Window Tabs The second Hammerspoon module I wrote was one that allows you to add tabs to any OSX Application. The tabs sit in the top right of the title bar and allow you to easily switch between windows of an app with keyboard shortcuts (e.g ctrl+tab number) and later by clicking. This was originally motivated by my switching to Spacemacs and it not having a good solution for working on many different projects like Vim tabs. This module allowed me to wrangle Emacs windows to more easily switch between different projects. I later repurposed it to switch between Sublime Windows for the same reason when I switched back to Sublime Text. This module was very different to write since it was pure Lua. It uses Hammerspoon’s various powerful built-in modules including the drawing module, the app watcher module, and the window listener module. Mouth Noises Most recently I contributed a module for recognizing mouth noises. It is based off some low-latency high-accuracy mouth noise recognizers I wrote during my research term at the UWaterloo HCI lab. Personally I use this module to scroll pages hands-free while lying down on the couch with my laptop. Previously I had to contort my hand into a cramped position on my chest to scroll with the trackpad while lying on my back. It’s one of my zanier uses of Hammerspoon but it is nice to use nonetheless. Just goes to show the variety of user interface scripting tasks Hammerspoon can do. Custom Window Management Hotkeys I love being able to customize my window management shortcuts perfectly for the kind of things I normally do. I have a custom modifier key on my keyboard that is dedicated to window management I call hyper. Pressing hyper in combination with the left home row jumps directly between my most frequently used apps (Chrome, Sublime, iTerm2, Mail, Path Finder) and a pair of keys that mark a certain window and focus it, for all the other apps I use occasionally like PDF readers when writing LaTeX. Pressing hyper with the right home row moves a window between full screen, halves of the monitor, and between screens. Various other hyper shortcuts do things like toggling mouth noise recognition. I also have a hotkey I can hit when I plug in my external monitor that arranges all my apps between monitors in the way I like them instantly. Miscellaneous Hackery I’ve used Hammerspoon for some one-off tasks, especially when I want to bind things to global keyboard shortcuts. An example of this is a weekend project I did to make a mouse controlled by head movements detected by an accelerometer on a microphone headset. I used Hammerspoon to send serial commands to the microcontroller when I pressed a shortcut to toggle the mousing on and off. Conclusion I hope this has given you some ideas about how you can use Hammerspoon to make your computing experience more pleasant. Check out my Hammerspoon config to see how I configure everything and tie it all together. For more inspiration check out the amazing things asmagill does in his config. He has experimental modules for all sorts of things like drawing calendars, custom app menus, fonts and speech control. </description>
      <pubDate>07 Feb 21 15:16 EST</pubDate>
      <guid>https://thume.ca/2016/07/16/advanced-hackery-with-the-hammerspoon-window-manager/</guid>
    </item>
    <item>
      <title>When did we give up on persuasion?</title>
      <link>https://unherd.com/2021/02/the-dying-art-of-persuasion/</link>
      <description>&lt;a href=&#34;https://unherd.com/2021/02/the-dying-art-of-persuasion/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; When was the last time you read an article, an opinion piece, that you felt was trying to persuade you of something? To argue a position that you don’t hold, and make you believe it? I suspect such experiences are rare. It is easier to write things for people who already agree with you: to make them cheer or feel clever, or to remind them how dreadful the other lot are. It’s also more fun.  I’m not talking about reading a column that disagrees with you. I’m sure you read them regularly, or at least the headlines: pieces get hate-shared all the time among people who disagree with them. But they are not written to persuade, and readers are not persuaded. The intention, I think, is to provoke a reaction, to elicit cheers and boos. Not, primarily, to change minds. On Saturday, a long-awaited New York Times article was published, about the blog Slate Star Codex (SSC). To get you up to speed: SSC is a blog by Scott Alexander, a pseudonymous Californian psychiatrist, part of a community of Bay Area nerds and weirdos, widely known as the rationalists. They care about human biases, artificial intelligence and doing good with charity. (I’ve written about them in my book.) In June, the NYT’s tech reporter Cade Metz contacted Alexander, and said he was going to write a piece about SSC, and particularly about how remarkably good the rationalist community was at predicting the course of the Covid pandemic. I spoke to Metz, and reassured Alexander and the Rationalists to the best of my ability that I thought it would be in good faith, rather than a hit job. But then Metz and the NYT said they would reveal Alexander’s real name in the piece. Alexander thought this would endanger his relationships with his patients, and took down his blog. He has since quit as a psychiatrist, re-ordered his life, and set up a new website. Now, half a year later, the NYT piece is out. I don’t want to get into whether or not it is a hit-job; others have done that. I will say that it comes perilously close to outright misrepresentation. For example, Metz says that “in one post, [Alexander] aligned himself with Charles Murray, who proposed a link between race and IQ in The Bell Curve.” But the line in which he “aligns himself” with Murray is on whether there is a genetic component to poverty (which surely there must be), not race: race is not mentioned in the post at all. It is, in essence, guilt-by-association. What interests me, though, is that SSC, and the rationalists, are seen as gateways to hard-right thinking: to “race realism”, to men’s rights activism. And I think that persuasion is a key part of the story. Because, on the face of it, the idea that the rationalists are secret fascists is strange. A 2019 survey of SSC’s readers found that self-described “conservatives” were outnumbered 8:1 by self-described “liberals” and “social democrats”; there were rather more “libertarians”, but still far fewer, and weirder subcultures like “alt-right” and “neoreactionary” existed only in slightly larger numbers than “Marxists”. They are far more anti-Trump than the American population.  But the NYT piece is far from the first article to suggest that, nonetheless, the rationalist community is an “an on-ramp to radical views” that allows “extremist views to trickle into the tech world”. Partly, that’s because the rationalist community is explicitly a place for reasoned, polite debate, and almost any views are welcome as long as they are expressed respectfully and can be backed up with evidence or reasoning. Inevitably, that means precisely those views which cannot find expression elsewhere tend to gravitate to it. But also, I think, it’s because SSC tries to persuade people. Read something of his, on some controversial subject. Billionaire philanthropy, for example, is not always popular: long articles have been written about why it is actually a bad thing, because it whitewashes billionaire reputations, allows them to control society, and is unaccountable to democratic institutions. All of which is reasonable. Scott Alexander, though, thinks that on balance, billionaire philanthropy does more good than harm, and that the movement against it will hurt the world.  It’s easy to imagine a newspaper article that attacks “billionaire-bashers”, that lists all the great things that Bill Gates or Jeff Bezos have done with their philanthropy, and makes fun of the idiots who think that stopping them doing that will improve things. Alexander, on the other hand, talks directly to people who disagree with him, who think billionaire philanthropy should be curbed: “I’m against this. I understand concern about the growing power of the very rich. But I worry the movement against billionaire charity is on track to damage charity a whole lot more than it damages billionaires.” It seems a small thing, a single phrase, “I understand concern” — but it is not. It demonstrates that the piece is intended to change minds. It says to those anxious about billionaire philanthropy that their worries about inequality and democratic unaccountability are real — I’m on your side! — but look, there might be these other things that you’ve not thought about. Whether or not you end up agreeing with Alexander on the particular case, he’s trying to win you over. Another example. “Free speech” has become a left-right battleground issue, and the instances we read about are always of right-wing speech being limited by left-wing activists. So, inevitably, left-wing people think it’s a partisan attack on them, or a smokescreen for people who just want to say unpleasant things (which, let’s be clear, it often is). But Alexander takes a different tack. In one post, for example, he calls attention to a woman fired for “having a Kerry-Edwards bumper sticker on her car” by her George W Bush-supporting boss. The point is, or at least the effect on me was, to drag the issue away from partisan sniping. It wasn’t firing shots in the culture war, it was talking to liberals and left-wingers, trying to persuade. I should, nervously, admit that I was persuaded on one topic that is much more highly charged: the gender imbalance in various professions, notably tech. Alexander argues that straightforward discrimination can’t be the only factor behind the male dominance of some fields: he points out, for instance, that sexist attitudes kept almost all women out of almost all professions until relatively recently. Law, medicine, academia, journalism, you name it. Now, though, he says, lots of professions are female-dominated: “men make up … only 25% of new psychologists, about 25% of new paediatricians, about 26% of forensic scientists, about 28% of medical managers, and 42% of new biologists.” Women make up half of new medical students, half of new law students, the large majority of new journalism students and psychology students. Most of these jobs are comparable in pay and status to computer programming. “Yet for some reason, engineering remains only about 20% female.”  He argues convincingly that there is no detectable difference in ability in maths, or computer science, or engineering between the two sexes. But, he says, women are on average more likely to be interested in careers where you deal with people, rather than with systems or things.  And this distinction explains why, for instance, women make up the large majority of gynaecologists, paediatricians, psychiatrists and family doctors (American GPs), while men make up the large majority of radiologists, anaesthetists and surgeons. Either we have to posit that radiologists are much more sexist than psychiatrists, or we have to say there’s some other, major, factor going on. Alexander suggests that it’s about interests: that there are large and systematic differences in what men and women are interested in, and that translates into systematic differences in their choice of profession. And it does seem to me that anaesthetists and surgeons can treat patients as “systems” or “things” to a much greater degree than GPs or paediatricians. Of course this is just a statistical difference, and individual men and women vary widely — but, he says, it is probably part of the story at a population level.  That piece, and others by him on the topic, persuaded me that sexist discrimination alone is not enough to explain the gender difference in tech or many other fields. (Do read the post, rather than arguing with my short synopsis of it, if you disagree.) This is why, I think, he and the rationalists are seen as a gateway to the hard right. If you are on Team A in the big internet fight, and you want to beat Team B, then someone who comes along and talks, in Team A language, to Team A people, to make them believe things that are associated with Team B — then that person is worse than the most fire-breathing Team B zealot. He’s not a foreigner, he’s a traitor. He’s not a combatant, he’s a spy. He’s a fifth columnist.  Of course, Alexander would say that he’s not trying to win people over to Team B. He’s a member of Team A; he just wants to understand things! But, of course, this is exactly what a traitorous spying fifth-columnist would say. He comes here talking the language of inclusion and diversity and liberalism, but he actually tries to convince people that sexist discrimination in tech is less of a problem than you think. And the worst thing is – it works. People do change their minds. I did; I am less sure about a lot of things than I was before I read SSC, and I think that’s what caused it. (I’ve changed my mind the other way, too, towards more stereotypically liberal positions: he has convinced me that trigger warnings are good.) That is scary. Particularly if you’re a Team A partisan, and you see other Team A partisans losing their will to fight, as they become less certain that Team A actually has all the right answers. Or if your identity is heavily tied up with your political beliefs, and changing them would feel like changing who you are. Maybe radicalisation is a real problem; maybe these controversial debates are fine in some tiny gated community of nerdy weirdos, but then when they come out into the wider world, they take it too far and end up in some strange corner of the internet. Rationalism is indeed a gateway to dangerous beliefs, says Scott Aaronson: “insofar as once you teach people that they can think for themselves about issues of consequence, some of them might think bad things. It’s just that many of us judge the benefit worth the risk!”  But I don’t think that’s the real problem. I don’t think that’s really why rationalist writers are seen as dangerous. I think it’s because if you think all of this is a big fight — if debate is war, and arguments are soldiers — then someone coming along and killing your soldiers behind your lines is simply the enemy, even if they’re wearing your uniform. And at the end of the day, traitors and spies get the harshest punishments of all.  </description>
      <pubDate>17 Feb 21 13:31 EST</pubDate>
      <guid>https://unherd.com/2021/02/the-dying-art-of-persuasion/</guid>
    </item>
    <item>
      <title>Stop Trying to Make Hard Work Easy</title>
      <link>https://superorganizers.substack.com/p/stop-trying-to-make-hard-work-easy</link>
      <description>&lt;a href=&#34;https://superorganizers.substack.com/p/stop-trying-to-make-hard-work-easy&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Nir Eyal, author of Indistractable, explains how to cope even when work is hard. Nir Eyal thinks we’re spending too much time trying to make work easy. He’s a behavioral design expert who taught at Stanford and has written two best-selling books.  But he thinks most of the productivity panaceas, like forming habits or trying to get into flow, that we all turn to in order to get our work done aren’t always as useful as we might hope: Learn moreThis post is for paying subscribers Subscribe →Or, login. </description>
      <pubDate>14 May 20 00:06 EDT</pubDate>
      <guid>https://superorganizers.substack.com/p/stop-trying-to-make-hard-work-easy</guid>
    </item>
    <item>
      <title></title>
      <link>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</link>
      <description>&lt;a href=&#34;https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; You might know that I’ve posted funny emails I’ve received on my blog several times in the past. The kind of emails people send me when they experience problems with some device they own (like a car) and they contact me because my email address happens to be visible somewhere. People sometimes say I should get a different email address or use another one in the curl license file, but I’ve truly never had a problem with these emails, as they mostly remind me about the tough challenges the modern technical life bring to people and it gives me insights about what things that run curl. But not all of these emails are “funny”. Category: not funny Today I received the following email From: Al Nocai &lt;[redacted]@icloud.com&gt; Date: Fri, 19 Feb 2021 03:02:24 -0600 Subject: I will slaughter you That subject. As an open source maintainer since over twenty years, I know flame wars and personal attacks and I have a fairly thick skin and I don’t let words get to me easily. It took me a minute to absorb and realize it was actually meant as a direct physical threat. It found its ways through and got to me. This level of aggressiveness is not what I’m prepared for. Attached in this email, there were seven images and no text at all. The images all look like screenshots from a phone and the first one is clearly showing source code I wrote and my copyright line: The other images showed other source code and related build/software info of other components, but I couldn’t spot how they were associated with me in any way. No explanation, just that subject and the seven images and I was left to draw my own conclusions. I presume the name in the email is made up and the email account is probably a throw-away one. The time zone used in the Date: string might imply US central standard time but could of course easily be phony as well. How I responded Normally I don’t respond to these confused emails because the distance between me and the person writing them is usually almost interplanetary. This time though, it was so far beyond what’s acceptable to me and in any decent society I couldn’t just let it slide. After I took a little pause and walked around my house for a few minutes to cool off, I wrote a really angry reply and sent it off. This was a totally and completely utterly unacceptable email and it hurt me deep in my soul. You should be ashamed and seriously reconsider your manners.I have no idea what your screenshots are supposed to show, but clearly something somewhere is using code I wrote. Code I have written runs in virtually every Internet connected device on the planet and in most cases the users download and use it without even telling me, for free.Clearly you don’t deserve my code. I don’t expect that it will be read or make any difference. Update below, added after my initial post. Al Nocai’s response Contrary to my expectations above, he responded. It’s not even worth commenting but for transparency I’ll include it here. I do not care. Your bullshit software was an attack vector that cost me a multimillion dollar defense project. Your bullshit software has been used to root me and multiple others. I lost over $15k in prototyping alone from bullshit rooting to the charge arbitrators. I have now since October been sandboxed because of your bullshit software so dipshit google kids could grift me trying to get out of the sandbox because they are too piss poor to know shat they are doing. You know what I did to deserve that? I tried to develop a trade route in tech and establish project based learning methodologies to make sure kids aren’t left behind. You know who is all over those god damn files? You are. Its sickening. I got breached in Oct 2020 through federal server hijacking, and I owe a great amount of that to you. Ive had to sit and watch as i reported: fireeye Oct/2020Solarwinds Oct/2020Zyxel Modem Breach Oct/2020Multiple Sigover attack vectors utilizing favicon XML injectionJS Stochastic templating utilizing comparison expressions to write to data registersGet strong armed by $50billion companies because i exposed bullshit malware And i was rooted and had my important correspondence all rerouted as some sick fuck dismantled my life with the code you have your name plastered all over. I cant even leave the country because of the situation; qas you have so effectively built a code base to shit all over people, I dont give a shit how you feel about this. You built a formula 1 race car and tossed the keys to kids with ego problems. Now i have to deal with Win10 0-days because this garbage. I lost my family, my country my friends, my home and 6 years of work trying to build a better place for posterity. And it has beginnings in that code. That code is used to root and exploit people. That code is used to blackmail people. So no, I don’t feel bad one bit. You knew exactly the utility of what you were building. And you thought it was all a big joke. Im not laughing. I am so far past that point now. /- Al Al continues Nine hours after I first published this blog post , Al replied again with two additional emails. His third and forth emails to me. Email 3: https://davidkrider.com/i-will-slaughter-you-daniel-haxx-se/Step up. You arent scaring me. What led me here? The 5th violent attempt on my life. Apple terms of service? gtfo, thanks for the platform. Amusingly he has found a blog post about my blog post. Email 4: There is the project: MOUT Ops Risk Analysis through Wide Band Em Spectrum analysis through different fourier transforms.You and whoever the fuck david dick rider is, you are a part of this.Federal server breaches-Accomplice to attempted murder-Fraud-just a few. I have talked to now: FBI FBI Regional, VA, VA OIG, FCC, SEC, NSA, DOH, GSA, DOI, CIA, CFPB, HUD, MS, Convercent, as of today 22 separate local law enforcement agencies calling my ass up and wasting my time. You and dick ridin’ dave are respinsible. I dont give a shit, call the cops. I cuss them out wheb they call and they all go silent. I’ve kept his peculiar formatting and typos. In email 4 there was also a PDF file attached named BustyBabes 4.pdf. It is apparently a 13 page document about the “NERVEBUS NERVOUS SYSTEM” described in the first paragraph as “NerveBus Nervous System aims to be a general utility platform that provides comprehensive and complex analysis to provide the end user with cohesive, coherent and “real-time” information about the environment it monitors.”. There’s no mention of curl or my name in the document. Since I don’t know the status of this document I will not share it publicly, but here’s a screenshot of the front page: Related This topic on hacker news and reddit. I have reported the threat to the Swedish police (where I live). This person would later apologize. </description>
      <pubDate>19 Feb 21 09:08 EST</pubDate>
      <guid>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</guid>
    </item>
    <item>
      <title></title>
      <link>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</link>
      <description>&lt;a href=&#34;https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; You might know that I’ve posted funny emails I’ve received on my blog several times in the past. The kind of emails people send me when they experience problems with some device they own (like a car) and they contact me because my email address happens to be visible somewhere. People sometimes say I should get a different email address or use another one in the curl license file, but I’ve truly never had a problem with these emails, as they mostly remind me about the tough challenges the modern technical life bring to people and it gives me insights about what things that run curl. But not all of these emails are “funny”. Category: not funny Today I received the following email From: Al Nocai &lt;[redacted]@icloud.com&gt; Date: Fri, 19 Feb 2021 03:02:24 -0600 Subject: I will slaughter you That subject. As an open source maintainer since over twenty years, I know flame wars and personal attacks and I have a fairly thick skin and I don’t let words get to me easily. It took me a minute to absorb and realize it was actually meant as a direct physical threat. It found its ways through and got to me. This level of aggressiveness is not what I’m prepared for. Attached in this email, there were seven images and no text at all. The images all look like screenshots from a phone and the first one is clearly showing source code I wrote and my copyright line: The other images showed other source code and related build/software info of other components, but I couldn’t spot how they were associated with me in any way. No explanation, just that subject and the seven images and I was left to draw my own conclusions. I presume the name in the email is made up and the email account is probably a throw-away one. The time zone used in the Date: string might imply US central standard time but could of course easily be phony as well. How I responded Normally I don’t respond to these confused emails because the distance between me and the person writing them is usually almost interplanetary. This time though, it was so far beyond what’s acceptable to me and in any decent society I couldn’t just let it slide. After I took a little pause and walked around my house for a few minutes to cool off, I wrote a really angry reply and sent it off. This was a totally and completely utterly unacceptable email and it hurt me deep in my soul. You should be ashamed and seriously reconsider your manners.I have no idea what your screenshots are supposed to show, but clearly something somewhere is using code I wrote. Code I have written runs in virtually every Internet connected device on the planet and in most cases the users download and use it without even telling me, for free.Clearly you don’t deserve my code. I don’t expect that it will be read or make any difference. Update below, added after my initial post. Al Nocai’s response Contrary to my expectations above, he responded. It’s not even worth commenting but for transparency I’ll include it here. I do not care. Your bullshit software was an attack vector that cost me a multimillion dollar defense project. Your bullshit software has been used to root me and multiple others. I lost over $15k in prototyping alone from bullshit rooting to the charge arbitrators. I have now since October been sandboxed because of your bullshit software so dipshit google kids could grift me trying to get out of the sandbox because they are too piss poor to know shat they are doing. You know what I did to deserve that? I tried to develop a trade route in tech and establish project based learning methodologies to make sure kids aren’t left behind. You know who is all over those god damn files? You are. Its sickening. I got breached in Oct 2020 through federal server hijacking, and I owe a great amount of that to you. Ive had to sit and watch as i reported: fireeye Oct/2020Solarwinds Oct/2020Zyxel Modem Breach Oct/2020Multiple Sigover attack vectors utilizing favicon XML injectionJS Stochastic templating utilizing comparison expressions to write to data registersGet strong armed by $50billion companies because i exposed bullshit malware And i was rooted and had my important correspondence all rerouted as some sick fuck dismantled my life with the code you have your name plastered all over. I cant even leave the country because of the situation; qas you have so effectively built a code base to shit all over people, I dont give a shit how you feel about this. You built a formula 1 race car and tossed the keys to kids with ego problems. Now i have to deal with Win10 0-days because this garbage. I lost my family, my country my friends, my home and 6 years of work trying to build a better place for posterity. And it has beginnings in that code. That code is used to root and exploit people. That code is used to blackmail people. So no, I don’t feel bad one bit. You knew exactly the utility of what you were building. And you thought it was all a big joke. Im not laughing. I am so far past that point now. /- Al Al continues Nine hours after I first published this blog post , Al replied again with two additional emails. His third and forth emails to me. Email 3: https://davidkrider.com/i-will-slaughter-you-daniel-haxx-se/Step up. You arent scaring me. What led me here? The 5th violent attempt on my life. Apple terms of service? gtfo, thanks for the platform. Amusingly he has found a blog post about my blog post. Email 4: There is the project: MOUT Ops Risk Analysis through Wide Band Em Spectrum analysis through different fourier transforms.You and whoever the fuck david dick rider is, you are a part of this.Federal server breaches-Accomplice to attempted murder-Fraud-just a few. I have talked to now: FBI FBI Regional, VA, VA OIG, FCC, SEC, NSA, DOH, GSA, DOI, CIA, CFPB, HUD, MS, Convercent, as of today 22 separate local law enforcement agencies calling my ass up and wasting my time. You and dick ridin’ dave are respinsible. I dont give a shit, call the cops. I cuss them out wheb they call and they all go silent. I’ve kept his peculiar formatting and typos. In email 4 there was also a PDF file attached named BustyBabes 4.pdf. It is apparently a 13 page document about the “NERVEBUS NERVOUS SYSTEM” described in the first paragraph as “NerveBus Nervous System aims to be a general utility platform that provides comprehensive and complex analysis to provide the end user with cohesive, coherent and “real-time” information about the environment it monitors.”. There’s no mention of curl or my name in the document. Since I don’t know the status of this document I will not share it publicly, but here’s a screenshot of the front page: Related This topic on hacker news and reddit. I have reported the threat to the Swedish police (where I live). This person would later apologize. </description>
      <pubDate>19 Feb 21 09:09 EST</pubDate>
      <guid>https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/</guid>
    </item>
    <item>
      <title></title>
      <link>https://suade.org/dev/12-requests-per-second-with-python/</link>
      <description>&lt;a href=&#34;https://suade.org/dev/12-requests-per-second-with-python/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 12 requests per second A realistic look at Python web frameworks If you take a look around the blogosphere at various benchmarks for Python web frameworks, you might start to feel pretty bad about your own setup. Or, alternatively, super-hyped about the possibilities.Consider, for instance, the incredible work of the guys at magic stack, getting 100,000 requests per second from uvloop in a single thread. This is on par with compiled language like Go&#39;s performance.But that benchmark doesn&#39;t really cover a fully fleshed out web framework, right? We need a lot more functionality and structure from our frameworks than reading and writing bytes. What about fully fleshed-out web-frameworks in python?One such framework is Sanic, which again has been shown to have similar performance: 100,000 requests per-second. Or there&#39;s Vibora. Not only does this claim to be a drop-in replacement for Flask, but it also has its own templating engine. And it handles 350,000 requests per second!Even more mind-blowing is Japronto which claims an insane 1.2 million requests per-second in a single thread 🤯 trouncing the performance of other languages and frameworks:Recently we&#39;ve been doing a lot of work improving the performance of our Python APIs. Currently we&#39;re running Flask, and we initially had a single question: how can we serve more requests from a single worker thread? But looking at these benchmarks had us asking more:Can we meaningfully compare them to our setup?How realistic are they for a full production application?Would we be better using one of these frameworks over Flask?In other words, how much should we trust these benchmarks? And to what extent should they influence our choice of technology?In order to answer these questions, in this post, I benchmark a realistic Flask application along with it&#39;s Sanic equivalent. I&#39;m going to guess that most readers come from a background with one of the more &#34;traditional&#34; Python frameworks (Flask or Django), and it&#39;s certainly more relevant to devs here at Suade Labs. For this reason, I run the Flask app in a number of different ways, to see what the best bang for our buck is: how performant can we make our application with (almost) zero changes to the code? Along the way we&#39;ll pick up some tips for the original question: how can we serve more requests from a single worker thread?Sidenote: if you&#39;re new to Python&#39;s web frameworks, or its asynchronous libraries, take a look at [1] from the addenda at the bottom of this post for a quick explainer. This post mostly assumes you know these things.The baselineFirst let&#39;s run some simple &#34;Hello, World!&#34; benchmarks on our system to get a meaningful baseline for comparison. For reference, the Flask benchmarks on techempower give 25,000 requests per second.Here&#39;s our Flask app:app = Flask(__name__) @app.route(&#34;/&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;]) def hello(): if request.method == &#34;GET&#34;: return &#34;Hello, World!&#34; data = request.get_json(force=True) try: return &#34;Hello, {id}&#34;.format(**data) except KeyError: return &#34;Missing required parameter &#39;id&#39;&#34;, 400I ran it under a variety of conditions. First &#34;raw&#34; via python app.py, and then under Gunicorn with a single sync worker via gunicorn -k sync app:app and finally Gunicorn with a single gevent worker via gunicorn -k gevent app:app. In theory Gunicorn should handle concurrency and dropped connections much better than the raw python, and using the gevent worker should allow us to do asynchronous IO without changing our code [2a]. We also ran these benchmarks under PyPy, which in theory should speed up any CPU-bound code without making any changes (if you haven&#39;t heard of PyPy see [2b] in the addenda below for a quick explanation and some terminology).And what about Sanic? Well, here&#39;s the &#34;rewrite&#34; of our app:app = Sanic(__name__) @app.route(&#34;/&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;]) async def hello(request): if request.method == &#34;GET&#34;: return text(&#34;Hello, World!&#34;) data = request.json try: return text(&#34;Hello, {id}&#34;.format(**data)) except KeyError: raise InvalidUsage(&#34;Missing required parameter &#39;id&#39;&#34;)And here are the results:Some technical details: I used Python 3.7 with the regular CPython interpreter and Python 3.6 with PyPy 7.3.3. At the time of writing, running 3.6 is the latest PyPy interpreter, and their Python 2.7 interpreter is faster in some edge cases, but as Python 2 is officially dead, I don&#39;t believe it productive to benchmark. My system details are available in the addenda [3]. I used wrk to actually execute the benchmarks.I&#39;ll break the results down in two parts. First: Sanic dominates, with 23,000 requests a second, although running our Flask app under Guncorn + gevent and PyPy does a pretty good job at keeping up. Second: what&#39;s going on with the performance range for our Flask app?Under CPython, we see that using Gunicorn quadruples the number of Flask requests per second from 1,000 to 4,000 and using a gevent worker adds a mild (sub 10%) speed boost to this. The PyPy results are more impressive. In the raw test, it is churning through 3,000 requests a second; it received the same 4x speed boost from Gunicorn, getting us to 12,000 requests a second; finally with the addition of gevent, it cranks up to 17,000 requests a second, 17x more than the raw CPython version without changing a single line of code.I was quite struck by the fact that gevent had such little effect on the CPython process - probably this is because the CPU is maxed out at this point. On the other hand, it seems that PyPy&#39;s better speed means it is still spending time waiting on system calls / IO, even under Gunicorn. Adding gevent to the mix means that it switches between concurrent connections, processing them as fast as the CPU will let it.To get a real sense of this, I ran the benchmark whilst monitoring CPU usage. Here&#39;s a short test against the raw app under PyPy:You can see that the program hops between CPU cores and rarely utilises 100% of a given core. On the other hand, here&#39;s part of a much longer test against the Gunicorn gevent worker under PyPy:Now it&#39;s evident that there is no switching between CPU cores (the process has become &#34;sticky&#34;) and the individual core is being utilised to a far higher degree.Key takeaways: Sanic wins. PyPy is fast. Run your &#34;traditional&#34; app under Gunicorn.Realistic benchmarksThe benchmark above, while fun, is pretty meaningless for real-world applications. Let&#39;s add some more functionality to our app!First, we&#39;ll allow users to actually store data in a database, which we&#39;ll retrieve via an ORM (in our case SQLAlchemy, the de-facto stand-alone ORM in python). Second, we&#39;ll add input-validation to make sure our users get meaningful error messages, and that we&#39;re not accepting junk that crashes our app. Finally we&#39;ll add a response marshaller to automate the process of converting our database object to JSON.We&#39;ll write a simple book store app, for a publishing house. We have a number of authors each writing zero or more books in several genres. For simplicity, each book has only a single author, but can have multiple genres - for example we could have a book which is in both the &#34;Existential Fiction&#34; and &#34;Beatnik Poetry&#34; categories. We&#39;re going to add 1 million authors to our database and roughly 10 million books. [4]Our SQLAlchemy models look a little like this: class Author(db.Model): id = db.Column(UUIDType, primary_key=True) name = db.Column(db.String, nullable=False) ... # snip! class Book(db.Model): author_id = db.Column( UUIDType, db.ForeignKey(&#34;author.id&#34;), nullable=False, index=True ) author = db.relationship(&#34;Author&#34;, backref=&#34;books&#34;) ... # snip! To marshal these, we use Marshmallow, which is a popular Python marshalling library. Here&#39;s an example of the Marshmallow model for the Author overview: class Author(Schema): id = fields.Str(dump_only=True) name = fields.Str(required=True) country_code = EnumField(CountryCodes, required=True) email = fields.Str(required=True) phone = fields.Str(required=True) contact_address = fields.Str(required=True) contract_started = fields.DateTime(format=&#34;iso&#34;) contract_finished = fields.DateTime(format=&#34;iso&#34;) contract_value = fields.Integer() In our endpoints these are used for validating input and returning results like so: @bp.route(&#34;/author&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;]) def author(): &#34;&#34;&#34;View all authors, or create a new one.&#34;&#34;&#34; if request.method == &#34;GET&#34;: args = validate_get(marshallers.LimitOffsetSchema()) limit = args[&#34;limit&#34;] offset = args[&#34;offset&#34;] authors = Author.query.limit(limit).offset(offset).all() return jsonify(marshallers.authors.dump(authors)) if request.method == &#34;POST&#34;: author = Author(**validate_post(marshallers.author)) db.session.add(author) db.session.commit() return jsonify({&#34;id&#34;: author.id}) The full source code can be viewed in the GitHub repo. Here, the thing to note is that marshallers.foo is an instance of a Marshmallow schema, which can be used both to validate a Foo input, for instance in a POST request, as well as to marshal Foo instances ready for returning as JSON. In order to actually perform asynchronous database requests, some fancy footwork is required with patching libraries, which depends on which postgres connector you use. SQLAlchemy does not support this out of the box, and in fact its primary developer has a great post arguing that an async ORM is not always a great idea. Juicy technical details in addenda [5], but beware that just using a Gunicorn gevent worker will not necessarily get you what you want.PyPy tends to suffer a performance hit when using C-extensions and libraries instead of pure python, conversely CPython should get a performance boost from the C-based libs. To take account of this I tested two different underlying database connectors: both psycopg2 and a pure-python counterpart pg8000, and two different classes of async gunicorn worker: gevent and a pure-python counterpart eventlet. What about the Sanic rewrite of our app? Well, as mentioned SQLAlchemy is not really async, and it definitely doesn&#39;t support python&#39;s await syntax. So if we want non-blocking database requests we have three choices:rewrite our models and queries with a different ORM (Tortoise looks interesting)choose a library like databases which allows us to keep the models / SQLAlchemy core for queries, but loose a lot of the featuresskip all of this and just plug raw SQL into the asyncpg driverWe&#39;ll get the best code from 1, but it will also involve the most thought and re-writing. It pulls in many other considerations: for instance, schema migrations, testing, how to deal with missing features (SQLAlchemy just does a lot of advanced stuff that other ORMs don&#39;t do). The fastest application will probably come from 3, but also the most technical debt, pain and opacity.In the end I opted for 2 and almost immediately wished I&#39;d done 1. In part this was due to some incompatibilities between the various libraries. But it also made joins very tedious and hacky to marshal correctly. After this brief diversion, I switched to Tortoise ORM which was really pleasant in comparison!With the new ORM, our code is as follows:@bp.route(&#34;/author&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;]) async def author(request): &#34;&#34;&#34;View all authors, or create a new one.&#34;&#34;&#34; if request.method == &#34;GET&#34;: args = validate_get(request, marshallers.LimitOffsetSchema()) limit = args[&#34;limit&#34;] offset = args[&#34;offset&#34;] authors = await Author.all().prefetch_related( &#34;country_code&#34; ).limit(limit).offset(offset) return json(marshallers.authors.dump(authors)) if request.method == &#34;POST&#34;: author = Author(**validate_post(marshallers.author)) await author.save() return json({&#34;id&#34;: author.id})Notice in the above that I had to &#34;prefetch&#34; (i.e. join) the country code table. This had to do with difficulty expressing that I wanted a foreign key constraint, but not a relationship/join in Tortoise ORM. There is undoubtably some voodoo I can do to fix this, but it&#39;s not super-obvious. The country code table just consists of the 300 or so ISO 3166 country codes, so is probably in memory and any overhead will be marginal.Key takeaways: Switching frameworks requires you to evaluate and choose an entire ecosystem of libraries, along with their peculiarities. Sanic and Tortoise are really nice and have great ergonomics for working with asyncio. Working without an ORM is tedious.The resultsLet&#39;s start with the /author/&lt;author_id&gt; endpoint. Here we select a single author, by primary key, from the database - collect a summary of each of their books and package the whole lot up to return to the user.Since I wanted at least some business logic in our app, I added what I consider to be an interesting field to the Author model and AuthorDetail marshaller:@property def genres(self): result = set() for book in self.books: result.update(book.genres) return sorted(result) This essentially says that, to return the author&#39;s genres, we have to pull out all of their books&#39; genres, and then merge into a deduplicated and sorted list.As expected, the pure python libraries performed a little better than their C-based counterparts under PyPy and a little worse under CPython. Because nothing outside of a micro-benchmark is entirely neat, this was not always the case, and in fact the difference was completely marginal, so I didn&#39;t include all of the results. See addenda [6] for full results.No matter what libraries or setup we use here, we&#39;re performing less requests than the worst &#34;Hello, World!&#34; example in the intro. What&#39;s more, it seems like the asynchronous PyPy worker does worse than the synchronous one with high concurrency - which sort of flips the original benchmark on its head! Which pretty conclusively answers the other questions we had: &#34;Hello, World!&#34; benchmarks are not realistic and bear little relation to our actual application.Another conclusion we can draw is clear: if the database is fast, use PyPy to make the Python app fast too. Whatever interpreter you choose, the difference between asynchronous and synchronous workers is not really too big: certainly we could pick the best performing in each case, but it may have been noise [7]. Sanic performs a little less than twice as well as CPython + Flask, which is impressive, but probably not worth the effort of rewriting the app if we can get this for free under PyPy.The /author overview endpoint gives pretty much the same results. But let&#39;s see what happens if we put a little more load on the database. To simulate a complex query we&#39;re going to hit /author?limit=20&amp;offset=50000, which should give the database something other to do than looking up by primary key. There&#39;s also some python work to be done validating parameters and marshalling 20 authors. Here&#39;s the result:This time it&#39;s clear that, along with PyPy, using asynchronous gunicorn workers, or an async framework like Sanic goes a long way to speeding up our app. This is the mantra of async: if you make long / irregular requests in your application, use asyncio, so that you can perform other work while waiting for a reply. At a certain point, our database hits maximum capacity and the number of requests per second stops increasing. We can take this to the extreme, by increasing the offset to 500,000:Both our sync workers are now hitting a blazing 12 requests per second 😅 Using async workers seems to help a lot, but oddly Sanic struggles here. I think the Sanic result was more to do with the extra join in my Tortoise ORM code I mentioned earlier. I expect it put a tiny bit of extra load on the database. It&#39;s a valuable lesson in switching frameworks: to maintain performance you also have to choose, evaluate and tune several libraries, not just the one.For reference, during the async benchmarks, the database was hitting 1050% CPU usage, while the API was cruising along at 50%. If we want to serve more users, one thing is clear: we&#39;re going to need to upgrade our database! Let&#39;s hope we don&#39;t have any other applications using this database, because they&#39;re probably going to be in trouble!Key takeaways: PyPy wins. Sanic is fast, but not that fast. You should probably run your &#34;traditional&#34; app with an async worker.ConclusionsIn reality most of the &#34;super-fast&#34; benchmarks mean very little except for a few niche use-cases. If you look at the code in detail, you&#39;ll see that they&#39;re either simple &#34;Hello, World!&#34; or echo servers and all of them spend most of their time calling hand-crafted C code with Python bindings.That means that these tools are great if you want to build a proxy, or serve static content, possibly even for streaming. But as soon as you introduce any actual Python work into the code you&#39;ll see those numbers plunge. If you rely upon the speed of these frameworks, then it will be hard to maintain that level of performance without e.g. cythonising all of your code. If you plan on writing almost no Python, then choosing these frameworks is the best option. But presumably, you&#39;re writing an application in Python because you need more than a simple &#34;Hello, World!&#34; and you&#39;d actually like to write quite a bit of Python, thank you very much!If your service is receiving 100,000 requests a second, it&#39;s likely that the specific Python framework you use is not going to be the bottleneck. Especially if your API is stateless and you can scale it via Kubernetes or similar. At that point, a good database, with decent schema design and good architecture are going to matter far more. Having said that, if you do want more processing power, use PyPy.Having the ability to run with some asynchronous capability offers clear advantages if database or service requests are likely to be anything other than instantaneous. Even if requests are usually instantaneous, picking an asynchronous runner is a low-cost way to bullet proof your app against intermittent delays. Whilst async-first frameworks like Sanic give you this out of the box, you can just as easily use a different Gunicorn worker with your Flask or Django app.What we&#39;ve seen in the benchmarks is that schema design, database choice and architecture will be the bottlenecks. Going with one of the new fully async frameworks purely for speed will probably not be as effective as just using PyPy and an async Gunicorn worker. I also found it gave me a kind of decision paralysis, asking many more questions like: if we can keep our latency low, is it more or less performant to use a synchronous Foo client written in C, or an async one written in pure Python?That doesn&#39;t mean that these frameworks aren&#39;t great pieces of engineering, or that they&#39;re not fun to write code in - they are! Actually I ended up loving the usability of Tortoise ORM when compared to kludging something together with SQLAlchemy core and databases, and I loved the explicitness of writing await Foo.all() over an implicit query queue and connection pool.For me, all of this emphasises the fact that unless you have some super-niche use-case in mind, it&#39;s actually a better idea to choose your framework based upon ergonomics and features, rather than speed. One framework I haven&#39;t mentioned that seems to have next-level ergonomics for industrial applications (request parsing, marshalling, automatic API documentation) is FastAPI.Right now I&#39;m satisfied that our combination of Flask, Gunicorn and gevent running under PyPy is pretty much the fastest we can go in all scenarios. We&#39;ll be actively exporing FastAPI in the near future, not for its benchmarks, but for its features.Like working on interesting problems and digging deep in to tech? We&#39;re hiring: https://suade.org/lead/Addenda(1)  Most &#34;traditional&#34; Python web frameworks fall under a standard called WSGI, where requests are handled in sequence: request comes in, is processed, reply is sent, next request comes in, etc. Most of the &#34;new-school&#34; Python frameworks use Python&#39;s asyncio library and a different standard called ASGI, which means that while waiting for IO (e.g. for bytes to arrive over the web) the application can switch to working on a different request. In theory this allows more requests to be served because our app does not sit around waiting - but the app still only works on one request at any one time, so this is not parralelism. See this great blog post for an explanation.(2a) Gunicorn is a runner for WSGI applications, which handles a lot of the tricky networking part of running a web application well. It has a number of &#34;worker types&#34;, which change how it handles requests. Whilst the WSGI framework only supports synchronous connections, the gevent and eventlet workers use the concept of green threads to switch context during any IO - thus giving implicit asyncio.(2b) In everyday parlance, we use Python to describe both the language, and the interpreter we use to run that languge. But there are other interpreters that can run your python code. The standard Python interpreter is called CPython by those in the know (because it&#39;s written in C). PyPy is a different interpreter which implements python in a subset of python! It includes a just-in-time compiler, which can actually make your code run faster.(3) I ran this benchmark on my Dell XPS 15, with a 12-core 2.20GHz Core i7 i7-8750H and 32GB of RAM.(4) For reference, estimates of the number of books on Amazon are somewhere in the 1-20 million range. It&#39;s not that we expect to have an Amazon sized site, although if we&#39;re struggling with performance, maybe that&#39;s a good use case! But this is a fairly simple way to generate a high workload within the database.(5) Actually the problem lies with the psycopg driver, which uses sockets coded in pure C, rather than Python&#39;s socket library. This means that gevent cannot patch the socket and switch context during IO. The solution is to use psycogreen to patch psycopg. Without this, a gevent worker can end up performing more like a sync worker. If using either eventlet or pg8000 this is not required. Under PyPy we end up with several layers of patching that need to be applied in sequence: first patch with gevent, then psycopg2cffi, then psycogreen.(6) See the spreadsheet here.(7) Actually this curve continues at a pretty flat level out to 1000 concurrent connections. To get a better understanding of this kind of load, separate application and benchmarking servers with something like NGINX should probably be used. This article was written on January 26, 2021 </description>
      <pubDate>19 Feb 21 09:10 EST</pubDate>
      <guid>https://suade.org/dev/12-requests-per-second-with-python/</guid>
    </item>
    <item>
      <title></title>
      <link>https://prog21.dadgum.com/123.html</link>
      <description>&lt;a href=&#34;https://prog21.dadgum.com/123.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Recovering From a Computer Science EducationI was originally going to call this &#34;Undoing the Damage of a Computer Science Education,&#34; but that was too link-baity and too extreme. There&#39;s real value in a computer science degree. For starters, you can easily get a good paying job. More importantly, you&#39;ve gained the ability to make amazing and useful things. But there&#39;s a downside, too, in that you can get so immersed in the technical and theoretical that you forget how wonderful it is to make amazing and useful things. At least that&#39;s what happened to me, and it took a long time to recover.This is a short list of things that helped me and might help you too.Stay out of technical forums unless it&#39;s directly relevant to something you&#39;re working on. It&#39;s far too easy to get wrapped up in discussions of the validity of functional programming or whether or not Scheme can be used to make commercial applications or how awful PHP is. The deeper you get into this, the more you lose touch.Keep working on real projects related to your area of interest. If you like designing games, write games. If you like photography, write a photo organizer or camera app. Don&#39;t approach things wrong-way-around, thinking that &#34;a photo organizer in Haskell&#34; is more important than &#34;a photo organizer which solves a particular problem with photo organizers.&#34;If you find yourself repeatedly putting down a technology, then take some time to actually learn and use it. All the jokes and snide remarks aside, Perl is tremendously useful. Ditto for PHP and Java and C++. Who wins, the person who has been slamming Java online for ten years or the author of Minecraft who just used the language and made tens of millions of dollars?Don&#39;t become an advocate. This is the flipside of the previous item. If Linux or Android or Scala are helpful with what you&#39;re building, then great! That you&#39;re relying on it is a demonstration of its usefulness. No need to insist that everyone else use it, too.Have a hobby where you focus the end results and not the &#34;how.&#34; Woodworkers can become tool collectors. Photographers can become spec comparison addicts. Forget all of that and concern yourself with what you&#39;re making.Do something artistic. Write songs or short stories, sketch, learn to do pixel art. Most of these also have the benefit of much shorter turnaround times than any kind of software project.Be widely read. There are endless books about architecture, books by naturalists, both classic and popular modern novels, and most of them have absolutely nothing to do with computers or programming or science fiction.permalink January 15, 2012previouslyFollow-up to &#34;A Programming Idiom You&#39;ve Never Heard Of&#34;A Programming Idiom You&#39;ve Never Heard Of2011 RetrospectiveUser Experience Intrusions in iOS 5Photography as a Non-Technical Hobby</description>
      <pubDate>03 Jun 20 01:08 EDT</pubDate>
      <guid>https://prog21.dadgum.com/123.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://latecheckout.substack.com/p/reddit-organized-lightning</link>
      <description>&lt;a href=&#34;https://latecheckout.substack.com/p/reddit-organized-lightning&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Thank you for supporting Late Checkout. This is a collaboration between Late Checkout and The Generalist. I hope you like it. “You ever stop to realize how fragile all this is?” George Carlin wears a black long-sleeved t-shirt, hunched over a microphone. In the minimalism of his outfit and the starkness of his bald pate, he almost resembles a tech executive delivering a keynote. But he’s too rumpled for that, too delightfully sour, as if a satyr stumbled through Steve Jobs’ wardrobe. “It wouldn’t take much to throw us right back into barbaric times. All you’d have to do would be eliminate electricity.” Carlin continues, embarking on one of his trademark quite-frightening-very-funny tirades, the flashes of incandescent, articulate fury that made him a genius. No electricity means no lights, he explains. No batteries. No fuel. No water. No computers. It means jail doors springing open, crime flooding the streets. It’s a bleak, bitter joke and serves as a coherent expression of Carlin’s particular weltanschauung: life is a sleepwalk along a knife-edge. It also shades his view of electricity and what it represents. When harnessed, it’s a mollifying, enlightening force; the thin membrane between civilization and barbarism. If Carlin is all rapid-fire verbosity in his 2005 set, he’s an expert of restraint in a complementary line: “Electricity is just organized lightning.” There are a hundred ways someone might describe a company like Reddit, but this is what we think of most. In its particular energy and vibrancy, its wild potential and ugly foibles, its kindness and cruelty, the social media platform occupies the liminal space of Carlin’s electricity. Just as it is close to some of our unseemliest instincts, so too is it a vehicle for creativity and connection. In short, Reddit is organized lightning. As the conductor of cultural currents, it may have more upside than any other social media company. In this piece, we’ll explore: Reddit’s topsy-turvy historyWhy it’s undervaluedWhere the company is headedThe Galaxy Brain moves to take it to the next levelr/HistoryFew companies have the narrative arc of Reddit. It’s a classic hero&#39;s tale with a call to adventure, challenges and setbacks, and a triumphant return.The beginningThe same year that Carlin delivered his iconic set, University of Virginia roommates Steve Huffman and Alexis Ohanian took a drive. They headed for Boston, where computer scientist, writer, and entrepreneur Paul Graham was scheduled to give a talk. Huffman, a computer science student himself, was the motivating force, but he would have Ohanian to thank for brokering a meeting with Graham. As the story goes, at the end of the talk, Ohanian asked if Graham would come out to a drink with them. Over a few beers, Graham encouraged the pair to apply to his new incubator in Cambridge: Y Combinator. Like so many others, Ohanian and Huffman didn’t get in with their first idea. The pair pitched “My Mobile Menu,” a way to order food via text. While Graham didn’t like that MMM, he invited them to come back to Boston for a brainstorm. By the end of an hour, they’d hit on a new mission: build the “front page of the internet.” Reddit was born. It didn’t take long for the founding duo to ship v1. Written in Graham’s beloved Lisp, the site served as a fairly straightforward link aggregator that allowed for upvoting and downvoting. The first 100 accounts on the site were created by either Ohanian or Huffman to give the appearance of activity. A snapshot from the site from that period (thank you WayBackMachine) illustrates just how much of the culture was baked in early on, featuring conspiracy theories, wonkish dev chat, strong opinions, and “spez,” Huffman’s username.‍Growing pains and the acquisitionSoon enough, Reddit added a third member. Aaron Schwartz, a coding prodigy also in YC’s cohort, merged his project, Infogami, with the company and hopped aboard. As Schwartz later told it, his reprogramming of the site from Lisp to Python coincided with a meaningful uptick in traffic: When I first started at Reddit, growth was slow. The site was put online very early — within weeks of starting work on it — but for the first three months it hardly got above three thousand visitors a day, which is about baseline for a useful RSS feed. Then, in a couple weeks of marathon coding sessions, we moved the site from Lisp to Python and I wrote an article about it for my blog. It got a lot of attention — Hell hath no fury like a Lisp fan scorned — and even today I still run into people at parties who, when I mention that I worked at Reddit, say &#34;Oh, the site that switched from Lisp.&#34;Around that time traffic really started taking off. In the next three months, our traffic doubled twice.Reddit was off to the races, though it wouldn’t follow the parabolic growth trajectory of other social media companies. Instead, the site grew rapidly but relatively steadily. Still, even after the company reached millions of visitors a month, generating revenue remained a challenge. Again, from Schwartz: We still had no idea how to make money. We sold t-shirts on the site, but every time we made a little bit of money on those we spent it on ordering more t-shirts. We signed up with a major Web ad representative to sell ads on our site, but they never seemed to be able to sell any ads for us and we rarely made more than, literally, a couple of dollars a month. Another idea we had was licensing the &#34;Reddit technology&#34; to let other people build sites that worked like Reddit. But we couldn&#39;t find anyone who wanted to license it from us.That context explains why Reddit was tempted to sell just a year after founding, giving up ownership to Condé Nast for a reported $10 - $20 million. (The deal closed on Halloween). Over the following years, that initial trio drifted away from the company: Huffman to found Hipmunk, Ohanian to a Kiva Fellowship, and Schwartz to pursue web activism. (As you may know, Schwartz would go on to commit suicide in 2013.) Reddit continued to grow under corporate ownership, though it&#39;s hard not to feel a shadow hanging over that trajectory. What might have been had the company capitalized instead of selling? How might Huffman, Ohanian, and Schwartz have stewarded Reddit and developed the product with the right support? The comebackPerhaps recognizing it was not the best guardian of a social media business, Condé Nast spun Reddit out in 2011, with parent company Advance Networks serving as a passive shareholder. Yishan Wong joined as CEO the following year, leaving in 2014 after the company refused to move its HQ closer to his home in Daly City. He was replaced by Ellen Pao on an interim basis with Ohanian returning as executive chairman. When Pao was ousted in 2015, driven out of her role by a user base incensed over the firing of a popular Reddit employee, Huffman stepped back into the CEO seat. The band was back together. It’s hard to overstate just how central Reddit was to pop-culture over the decade that saw Ohanian and Huffman found the business, sell it, and return once more. In both good and bad moments, the valorous and vile, Reddit was there. It served as the de facto town square for events like “The Fappening,” the Sony Pictures hack, the Boston Bombing, GamerGate, and Pizzagate. Since retaking the reins, Huffman has shown a desire to grow Reddit’s product and stamp out the site’s most toxic elements. The mobile experience was improved. The site was redesigned. Video was introduced. And, thousands of hate-filled subreddits were expunged. Concerns that a more hands-on approach to moderation would curb the anarchic good-times vibe of the platform have been emphatically repudiated this year, with Reddit the ground-zero for the epic GameStop short squeeze. Despite being one of the internet’s most popular websites, sixteen years after it was founded, Reddit feels like it&#39;s just getting started.r/ValueReddit is one of the most misunderstood and undervalued companies in the world. A little over a week ago, Huffman announced the company had raised $250 million in new capital at a valuation of $6 billion. The round was led by Dubai-based firm Vy Capital, with follow-ons from a16z, Sequoia, and Tencent. To which we have to say: $6 billion? That’s it? ‍In a world in which Clubhouse is worth $1 billion after a year of (impressive) traction, and Dispo is pegged at $100 million for an app still in beta, Reddit — cultural petri-dish, bastion of expression, distributed hedge-fund, meme-maker, group-therapist — is only worth $6 billion?A thought experimentHumor us. You’re given the chance to look at two social media companies. Your job is to pick the one that you think has the highest valuation. Here are a couple details. Company 1MAUs: 353 millionMAU growth YoY: 7%Company 2 MAUs: 430 millionMAU growth YoY: 30% Now, you’re right to be a bit grumpy with us. You don’t have much to go on, right? No revenue, no costs, no churn. How are you supposed to figure out which is most valuable without that information? But if any space is defined by user growth, above anything else, it’s social media. So which do you choose?Company 1 is Twitter, worth $58 billion. Company 2, with more MAUs and 4x the growth, is Reddit. From a growth an engagement perspective, Reddit compares favorably to many of social media&#39;s public behemoths with close to the same number of MAUs as Pinterest.‍Yet, the implied value of a Reddit user is criminally low: ‍Public and private market investors effectively value a Facebook DAU at 3.5x the price of a Reddit one, and a Facebook MAU at 19x of the Reddit equivalent.Explanations and misconceptionsWhat is the rationale behind this aggressive discounting? We think it stems from outdated historical data and an anachronous view of the product. Sure, historically, Reddit has done a poor job of bringing in revenue on a per-user basis. In 2019, Reddit was said to have an average revenue per user (ARPU) of $0.30, much less than Facebook at $7.37, Twitter at $9.48, Pinterest at $2.80, and Snap at $2.09. But the revenue figure this calculation was based on was over 8 months old at the time, and the succeeding two years have seen strong strides in product and advertising. For example, advertising revenue was up a reported 90% year-over-year in Q4 of 2020. ARPU is likely considerably higher today. Another common argument is that Reddit users are less valuable because of the pseudonymity of the platform. Previously, you could sign up for an account without registering an email. That meant the company had less personal information about you, making targeted advertising more difficult and less valuable. But this is no longer the case. You now need an email to sign up, and as for relevant information...well, Reddit is quite literally a mapping of your interests. Joining r/Bitcoin is a pretty good sign you might be interested in a BlockFi advertisement while hopping into r/entrepreneurship makes you a good target for Stripe. Ultimately, the valuation displacement may come from misunderstanding and the stubbornness of outdated perceptions. Reddit is not a new name. And, in its design — despite improvements — it feels antiquated, still more similar to Craigslist than TikTok. But there are signs management is, finally, thinking big.r/RoadmapThat more expansive thinking wasn’t necessarily expressed by Huffman after the recent round. Instead, the CEO noted that “our strategy isn’t materially changing.” At the same time, he outlined the company’s desire to expand geographically and push its video efforts. In addition to those stated interests, management also seems invested in crypto. Before making a few recommendations of our own, we’ll dig into these three initiatives.Video Reddit was slow to bring video onto the platform, only adding native capabilities in 2017 (video ads were available for several months prior). Since then, it’s become a more prominent feature with the homepage often showcasing popular clips. The company followed that up with the introduction of live streaming in 2019, through a program called the Reddit Public Access Network. RPAN, as it is now known, allows select broadcasters to stream for 2-3 hours each, with time extensions earned through user rewards. Certain subreddits (r/talentShow, for example) offer live streaming outside of those windows. RPAN is still relatively new but it represents an interesting experiment for a few reasons. Firstly, because it provides a default for users. For so long, Reddit has been the platform of information overload: newcomers to the homepage are overwhelmed by the sheer density of information. RPAN changes that calculation. Instead of having to pick your way toward an interesting post, you can drop into a stream that’s been chosen for you. By restricting, Reddit offers a less cognitively-complicated way to browse. Secondly, RPAN reinforces spending behavior. In addition to selling ads, Reddit monetizes by selling “coins” to its members. These can then be used to tip or “gild” other users for their contributions to the platform. Data from Subreddit Stats illustrates that gilding per subscriber is highest in communities with RPAN availability.‍Intuitively, this makes sense. Live performances have a certain intimacy and allure that encourages spending. To the extent that Reddit wants to create a culture of user generosity and build an endemic economy (think Roblox for snarky adults) RPAN may serve as an educating force. Beyond RPAN, those interested in the company may wish to keep a lookout for more interactive video features to emerge. In December, Reddit announced it was purchasing Dubsmash, a short-form video platform similar in tone and functionality to TikTok. As part of the deal, Dubsmash will remain independent but Reddit will begin to integrate the company’s video filtering. Perhaps just as critical as this new functionality is the audience Dubsmash brings. While Reddit skews male, Dubsmash is 70% female. It’s also a markedly diverse platform, with a reported 25% of black teenagers in the US on the app.Our takeaway from all of this? Video has historically been an amphetamine for social media platforms — speeding up growth and supercharging engagement. That Reddit is so far behind here is worrying in some respect, but the recent moves and acquisitions show a desire to take it seriously. The upside of that decision could be significant.Geographical expansionFor all of Reddit’s domestic popularity, the Gospel of Snoo has a rather limited following outside its native land. Of its roughly 430 million users, 222 million come from the US, over 50%. The drop-off after that is severe, with Australia providing the next largest tranche at 17.5 million, and India in third with a bit over 13.5 million.StatistaIf anything, these figures reinforce the importance of a viable video strategy. Reddit has two disadvantages in terms of internationalization when compared to other social networks: Text-firstWhy does TikTok export so well? Because video is universal (or at least relatively so). A funny clip in one country is often funny in another. Moreover, music — at the heart of a platform like TikTok — has global appeal. The result is that content in an English-speaking country can go viral just as easily in a French-speaking one. Reddit doesn’t have this luxury. Since most of its content is written in English, its appeal is fundamentally limited in non-English speaking countries. Group-basedBut, we hear you say, what about Twitter? Isn’t that fundamentally text-first too? You’re right. But the social graph on Twitter, as with many other networks, is organized on the individual level. Yes, you can follow “Topics” if you want to, but mostly you subscribe to hearing the thoughts of a particular person. That’s great for you as a user: the minimum viable number of users required to create a new source for your feed is 1. By following a single person, you start seeing their content. That granularity and low user to source ratio makes it easy to construct a feed of interesting information in the language of your choosing. Reddit is a little different. You don’t follow an individual (you can, but it’s not the default), you choose to follow a community, and a community is only valuable once a certain number of users are involved. As a result, the minimum viable number of users required to create a new source for your feed is higher. This is a challenge for internationalization. If I’m an Urdu speaker interested in history, the r/history subreddit is of limited use to me. Sure, I can translate if I want to, but that&#39;s a cumbersome, time-consuming experience out of line with the frictionlessness of consumer internet products. Instead, I have to segment by both interest and language, hoping there&#39;s an equivalent community available. And even if it is, for it to be sufficiently attractive, that community needs a user count (n) &gt; 1. Realistically, it may need n &gt; 50.‍With those challenges in mind, it’s intriguing to think about where Reddit might want to grow its presence. Looking at the company’s Career page offers a few hints.‍These are intriguing choices. Both France and Germany are affluent and have large English-speaking populations, making it easier to get off the ground and grow or maintain ARPU. Moreover, building a dedicated presence in these countries allows Reddit to accumulate native content in German and French. Neither are large markets today: Germany has a little under 4 million Reddit users while France has a meager 1 million. There&#39;s an opportunity to realize meaningful revenue from both markets. Thinking about what comes next provokes more excitement. France and Germany may serve as testing grounds for pushes into Spain and Portugal. While the first two languages have roughly 150 million native speakers worldwide, the latter two boast 701 million. By establishing an active base in Spain, Reddit accumulates communities that can be accessed and enjoyed by users in Mexico, Argentina, Colombia, and so on. The same applies to Portugal and Brazil. For now, Reddit’s expansion looks fairly cautious. But in due time and with the right selection, it may greatly expand its reach. CryptoWhile Facebook fumbles through the wreckage left by the artist formerly known as Libra (now Diem), Reddit has rolled out some promising integrations of on-platform cryptocurrency. Built on top of Ethereum, Reddit has minted endemic currencies for two subreddits: r/CryptoCurrency and r/FortNiteBR. For participating and creating content on r/CryptoCurrency, users are rewarded with “Moons,” which can be subsequently used to purchase special badges, or additional functionality (like responding to posts with a GIF). The size of your holdings also affect your influence on polls held by the subreddit: the richer you are, the more your vote matters. A similar system exists on r/FortNiteBR with fans of Epic’s game receiving payment in “Bricks.”As it stands, this effort is not particularly revolutionary, with both currencies functioning similarly to loyalty points. But it lays the groundwork for a more extensive push into creating a native economy. And, in the meantime, it further normalizes the tipping culture that Reddit uses to monetize. While Huffman might downplay Reddit&#39;s dynamism, it’s apparent from these projects that plenty is going on beneath the surface.r/GalaxyBrainFormer CEO Yishan Wong said, “In the context of social media, Reddit is more about the media than the personalities.” As part of the Huffman revolution, the company should rethink that positioning. Moreover, it should make no-brainer improvements to the core product, make it easy to transact, lean into its most ludic elements, and ultimately, unbundle itself. Here are our nine suggestions, organized from easiest to trickiest, simplest to most profound. No brainersReddit has never been the most adventurous product organization. When Huffman returned in 2016 the company had one iOS developer. For years, Redditors used third-party clients to access the site on their phones. And while a comprehensive redesign was rolled out in 2019, it leaves plenty to be desired. There are three no-brainer improvements Reddit should make. Redesign browsingWhen you visit the homepage or a subreddit, you’re faced with a list-view of different posts. To see one of them, you click it. To see comments, you have to click again. To go to the next post, you have to page back, scroll, and click again. In theory, this isn’t so different from Twitter. But because of the relative information density of a Reddit post, it quickly becomes cumbersome. To get to the part you care about, you have to click twice. To see something new you have to click another two times. Compare this to something like Imgur. The platform was started as a way for Redditors to host photos, another example of Reddit allowing others to fulfill core functionality. Today, when you visit Imgur, a card-based feed organized by virality appears. So far, this isn’t so dissimilar to Reddit. Once you click a card though, things change: you’re thrust into consumption mode. By paging left or right, you automatically see the next post, along with all associated comments. This a powerful change, making it easier for the user to quickly find the content they care about instead of clicking in and out of different posts. It also makes serving ads more compelling — instead of a sponsorship taking up a fraction of the page that a user can quickly scroll past, ads on Imgur look exactly like a post and you default to seeing them in your browsing flow. Why hasn’t Reddit done this already? It may speak to a lack of confidence around the broad appeal of “hot” posts or insecurity around the company’s recommendations. This brings us to our next point…Improve recommendationsEven after a user subscribes to a particular subreddit, recommendations are unengaging. Just as importantly Reddit doesn’t do a good job of suggesting newsubreddits to check out and join. This is hard to quantify, so let’s look at one of our profiles. Now look, I’m something of a lurker on Reddit, enjoying the comments of others but rarely participating myself. To that extent, perhaps the company doesn’t know that much about me, even though I belong to a couple of dozen communities. When I visit the homepage today, here’s what I see:‍What do I have to choose from? Two r/wallstreetbets posts (very rarely visit), one r/CareerSuccess post with 0 comments and 1 upvote (can’t remember ever visiting), a r/VenturedCapital blog with 0 comments and 1 upvote, a r/science post with an insanely long headline that I will certainly not read while browsing, an even longer headline from r/uselvid, a live stream with no information, and something from r/ThatsInsane that looks vaguely entertaining. These are the best eight posts Reddit could find to grab my attention?Now, let’s look at the suggestions the homepage makes. It shares the day’s “Top Growing Communities” and “Trending Communities.” As a user, it’s unclear what the difference is between the two, and it’s also obvious they haven’t been chosen for me.‍Does it make sense that I’m being encouraged to check out the German Netflix community? How about r/CasualIreland, r/Munich, or r/StadtEssen? (Looks like they may have found that German country manager after all…) The dreadfulness of this page is infuriating partially because it would take so little to make it better. Why not show me communities similar to those I’ve signed up for? (And message that in copy). Or show me subreddits that people I’ve interacted with belong to?Make search functionalThe final no-brainer improvement Reddit should make is improving its search function. Again, it’s somewhat mind-boggling that the company has reached this point without a better solution. Reddit is one of the largest compendiums of fascinating information on earth. It has over a decade of nuanced, interesting, informative conversation to be discovered, each embedded within a distinct community. But finding information within a community is nigh-on-impossible because Reddit’s search cannot be limited to a certain subreddit. That means if I want to know what people on r/politics have said about Gavin Newsom, my only choices are to browse and hope I come across something, or search across the entirety of the Reddit platform. That means wading through posts from r/atheism, r/conservative, and others that I don’t care about. Beyond allowing search to be filtered by subreddit, the company should weigh results by context. Right now, results can only be filtered by engagement and timeframe. There appears to be no priority for posts from users I’ve interacted with. Meaningful upgradesOk, we’ve got the easy stuff out of the way. Now, it’s time for Reddit to tackle three more meaningful projects: elevating the profile, empowering creators, and making it easy to buy (and sell) on the platform. Elevating the profileIn 2012, Barack Obama came on Reddit for an AMA. Interest in the event was so popular it crashed the site. But has Obama been back since then? Has he (or more reasonably, his team) posted once in the intervening nine years? When you search for him as a user, there’s no sign of him. Compare that to Twitter: he has 129.5 million followers and last tweeted 21 hours ago. As it stands, high-profile people have little incentive to maintain a presence on the platform. No social capital seems to be granted for a large following, and the ability to influence is limited. Now, this is part of what makes Reddit unique. Faceless accounts can share fascinating tidbits with impunity. But to encourage usage from compelling personalities (and encourage the social jockeying that makes social media work), Reddit needs to make profiles a bigger deal. Let’s looking at Huffman’s profile, u/spez:While there are some nice additions here (we’ll talk about avatars in a minute), it doesn’t feel like a particularly compelling profile. There’s no follower count (Karma is too fluid and ineffable a currency to gauge influence), bios are limited, there are no pictures, no location, no obvious place to link outside projects.The result is a portrait of what seems like a fairly non-descript user — not the CEO of fascinating company. As it stands, users are unable to adequately benefit from the work they do on the platform. The solution may be in leaning into the creator economy movement. Creator-firstThere are signs that Reddit is thinking through the profile issue and taking the creator movement seriously. This past week, Peter Yang, author of a newsletter called “Creator Economy” announced he had joined Reddit to lead their efforts in this field. Again, Reddit’s “Careers” page further validates this interest. The company is hiring a Social Media Partnerships Manager in the space, along with a Product Manager. While neither description gives a great deal of insight into what will be built, they do note the desire to facilitate true creator engagement and monetization. That may involve revamped profiles as well as better moderation and community management tools. Rather than hosting a community on Slack or Circle, this new team should work to have them build on Reddit. This could prove incredibly powerful. Not only would users be able to join (and pay) within an experience they’re familiar with (a real reduction of friction), Reddit could drive traffic to chosen creators. For paid communities, that would improve top-line revenue in a way that tools like Slack couldn’t. Create an economyReddit is already a place people look for jobs, homes, and second-hand items. Today, those transactions occur off-platform, but there’s no reason why Reddit shouldn’t bring that in-house. In particular, Reddit should make it easy for subreddits to pay-gate their community, sell merchandise (physical and digital), and post jobs. In effect, moderators work for free right now, but if supported correctly, many might be able to make a living from it, allowing them to devote their time to improve their corner of the platform. (There are a few subreddits which allow for some degree of pay-gating).The contours of this strategy are beginning to manifest with Reddit’s encouragement of tipping and its cryptocurrency experiment. In time, we’d love to see the company run an economy equivalent to Roblox: users would buy Redditbux with cash and spend it on Reddit Marketplace, buying access to private subreddits, purchasing swag, and acquiring new flair or NFTs. This final category is worth lingering on. Non Fungible Tokens (NFTs) are having a moment, with NBA Topshots constantly selling out, Mark Cuban tokenizing and shilling his tweets, and a remastered GIF of Nyan Cat selling for nearly $600K. Reddit is perfectly positioned to capitalize on this momentum. After all, Reddit is the place memes are made. It would make sense for it to become the place to sell them, too. Finally, adopting an intrinsic currency would create a virtuous free cash flow cycle (as discussed in our Roblox piece) and further entwine Redditors with the platform with users loathe to forfeit Redditbux or the items purchased with it. Galaxy Brain movesIf Reddit achieves everything we’ve listed so far, they might see their value appreciate 5-10x. But the company feels uniquely ready to take on some truly radical projects thanks to Huffman’s hand at the tiller and the capital they’ve brought in. Our final three ideas are on the more wild side: embrace the metaverse, own the conversation from end-to-end, and unbundle its winners. The SnooniverseOhanian is said to have doodled up the design for Snoo, Reddit’s mascot, in his senior year of UVA. Over the years, Snoo has taken on different forms for major holidays, and become an essential part of Reddit’s brand. But it was only in December of last year that users were given the chance to deeply customize their own Snoo.The “Avatar Builder” allows Redditors to customize their Avatar with freely available styles, or upgrade to Premium to unlock other forms of expression. It’s a savvy way to create more engaging profiles (though there’s still work to be done) and encourage members to sign up for a paying membership. It lays the groundwork for Reddit to move more directly into gaming-style environments.Over the years, Reddit has used April Fools’ Day as an opportunity to test out new functionality. These almost always take the form of a game. The Button, 2015. A one-minute countdown timer that reset every time a new user pressed it. It finally ran down to zero in June of 2015 after a user failed to press it within the 60-seconds.Place, 2016. At r/place, Redditors could choose to color in a single pixel of a large canvas. Before coloring another pixel, users had to wait 5 to 20 minutes. Eventually, groups began to collaborate to create pixelated flags, pictures, and even a rendering of the Mona Lisa.Circle of Trust, 2018. Users could create private, password-gated communities that showed up in a subreddit with a special animation. To grow the group, users  shared the password. But if a new member wanted, they could “betray” the community, meaning no new members could be added. Sequence, 2019. The closest Reddit has ever gotten to a “Stories” feature, Sequence allowed groups to vote on sentences or GIFs to add to a &#34;sequence.&#34; Bit by bit, a story formed. Imposter, 2020. Users answer the same set of questions. An AI, the “imposter,” reads these answers and comes up with an answer of its own. Then each player is tasked with determining which of five possible answers is computer-generated. The mechanics in several of these are not so dissimilar to hit games like Among Us. Could Reddit host a game like that? Or give tools to users to create new games of their own? Already, the company seems interested in monetizing like free-to-play mobile games do, incentivizing the purchase of coins to buy badges and cosmetic upgrades. How much could Reddit juice this aspect through simple subreddit-specific games? As a starting point, it would be interesting to see the company experiment with spatial audio and video, giving communities the ability to open “rooms” which avatars could use to roam around and socialize. In the long-run, Reddit could build out the richness of these spaces to become synthetic social environments, similar to those on Roblox. The result might look something like a metaverse. Own the conversational pipeline What is Reddit exceptional at? Facilitating conversation. But its current structure is only set up for one particular type of asynchronous dialogue. That’s allowed places like Discord to flourish on the back of the platform’s community-building work. Take the case of someone like u/DeepFuckingValue (DFV), the Redditor at the heart of the $GME short squeeze. To spread his gospel, what did his user journey look like? He bought his position in $GME on ETrade. Then he discussed it on Reddit. He followed that up with videos on YouTube. And perhaps, like so many others from the r/WallStreetBets forum, he jumped over to Discord to discuss his research. ‍Except for DFV’s first step, the purchase, Reddit should own every aspect of this pipeline. The fact that it doesn’t speaks to some of the failings we’ve already enumerated: if Reddit had rich profiles, true creator support, and better video, DFV wouldn’t need to go to YouTube. If Reddit supported voice rooms, the migration to Discord would be superfluous. As it stands, Reddit is letting a lot of the value it creates leak into other platforms. While it seems to understand the importance of video, the company should pay attention to the emergence of platforms like Clubhouse, too. Audio should become a core part of the Reddit platform. Let’s hope we see something voice-related for 2021’s April Fools’ experiment. (Btw, I think this will be the case. If someone wants to take the other side of a $50 bet, respond to this email. The first person to respond, we have a deal.) Unbundle itselfThere’s no better place to end. To unlock the next step-change in valuation and protect against niche competitors, Reddit should unbundle itself. What does this mean practically? Again, Imgur provides inspiration. In 2019, the platform announced that it had created “Melee” an offshoot app designed specifically for gamers. While much of the core functionality is the same, it made it easier to trim your game highlights and import content from Xbox, Playstation, Switch, Nvidia, and other platforms. Imgur recognized they had a strong interest-specific community and decided it was worth building a home and toolset just for them. Reddit can do the same thing with its most popular, active, and lucrative subreddits. This doesn’t mean the company has to create new apps for each community, but it should create dedicated teams to build for its winning communities like r/science, r/gaming, r/investing, r/photography, and more. The cryptocurrency work Reddit has done with r/CryptoCurrency and r/FortNiteBR suggests management is thinking about this — we hope there’s much more to come.“It&#39;s like electricity - we don&#39;t really know what it is, but it&#39;s a force that can light a room.” Ray Charles was talking about the “soul” when he uttered those words, but he could have been talking about Reddit. A decade and a half after its inception, it still feels like the world doesn’t really know, doesn’t understand, what Reddit is. And yet, it is still here, at the center of culture, influencing our markets and politics, lighting up the room. To structure Reddit too rigidly, to overly organize its lightning, would kill some of the magic. But in its quest to realize its full power, Huffman and company may wish for a little less illumination and a little more definition. Thank you to Alex Kuang, Yashar Nejati, Peter Yang, Greg Neufeld for their thoughts on this subject. If you enjoyed this post, you’ll like more of The Generalist deep dives. Subscribe hereShould we do more posts together? DM or @ me on Twitter if we shouldTo keep up to date with future Late Checkout posts, consider subscribing. Don’t miss out</description>
      <pubDate>23 Feb 21 13:10 EST</pubDate>
      <guid>https://latecheckout.substack.com/p/reddit-organized-lightning</guid>
    </item>
    <item>
      <title>Your Language Sucks, It Doesn’t Matter</title>
      <link>https://matklad.github.io//2020/09/13/your-language-sucks.html</link>
      <description>&lt;a href=&#34;https://matklad.github.io//2020/09/13/your-language-sucks.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Sep 13, 2020 This post describes my own pet theory of programming languages popularity. My understanding is that no one knows why some languages are popular and others aren’t, so there’s no harm done if I add my own thoughts to the overall confusion. Obviously, this is all wild speculation and a just-so story without any kind of data backed research. The central thesis is that the actual programming language (syntax, semantics, paradigm) doesn’t really matter. What matters is characteristics of the runtime — roughly, what does memory of the running process look like? To start, an observation. A lot of software is written in vimscript and emacs lisp (magit being one example I can’t live without). And these languages are objectively bad. This happens even with less esoteric technologies, notable examples being PHP and JavaScript. While JavaScript is great in some aspects (it’s the first mainstream language with lambdas!), it surely isn’t hard to imagine a trivially better version of it (for example, without two different nulls). This is a general rule — as soon as you have a language which is Turing-complete, and has some capabilities for building abstractions, people will just get the things done with it. Surely, some languages are more productive, some are less productive, but, overall, FP vs OOP vs static types vs dynamic types doesn’t seem super relevant. It’s always possible to overcome the language by spending some more time writing a program. In contrast, overcoming language runtime is not really possible. If you want to extend vim, you kinda have to use vimscript. If you want your code to run in the browser, JavaScript is still the best bet. Need to embed your code anywhere? GC is probably not an option for you. This two observations lead to the following hypothesis: Languages generally become popular when they bring innovative runtime, or when they have runtime exclusivity. The quality of the language itself is secondary. Let’s see some examples which can be “explained” by this theory. C C has a pretty spartan runtime, which is notable for two reasons. First, it was the first fast enough runtime for a high-level language. It was possible to write the OS kernel in C, which had been typically done in assembly before that for performance. Second, C is the language of Unix. (And yes, I would put C into the “easily improved upon” category of languages. Null-terminated strings are just a bad design). JavaScript This language has been exclusive in the browsers for quite some time. Java This case I think is the most interesting for the theory. A common explanation for Java’s popularity is “marketing by Sun”, and subsequent introduction of Java into University’s curricula. This doesn’t seem convincing to me. Let’s look at the 90’s popular languages (I am not sure about percentage and relative ranking here, but the composition seems broadly correct to me): On this list, Java is the only non-dynamic cross-platform memory safe language. That is, Java is both memory safe (no manual error-prone memory management) and can be implemented reasonably efficiently (field access is a load and not a dictionary lookup). This seems like a pretty compelling reason to choose Java, irrespective of what the language itself actually looks like. Go One can argue whether focus on simplicity at the expense of everything else is good or bad, but statically linked zero dependency binaries definitely were a reason for Go popularity in the devops sphere. In a sense, Go is an upgrade over “memory safe &amp; reasonably fast” Java runtime, when you no longer need to install JVM separately. Naturally, there are also some things which are not explained by my hypothesis. One is scripting languages. A highly dynamic runtime with eval and ability to easily link C extensions indeed would be a differentiator, so we would expect a popular scripting language. However, it’s unclear why they are Python and PHP, and not Ruby and Perl. Another one is language evolutions: C++ and TypeScript don’t innovate runtime-wise, yet they are still major languages. Finally, let’s make some bold predictions using the theory. First, I expect Rust to become a major language, naturally :) This needs some explanation — on the first blush, Rust is runtime-equivalent to C and C++, so the theory should predict just the opposite. But I would argue that memory safety is a runtime property, despite the fact that it is, uniquely to Rust, achieved exclusively via language machinery. Second, I predict Julia to become more popular. It’s pretty unique, runtime-wise, with its stark rejection of Ousterhout’s Dichotomy and insisting that, yeah, we’ll just JIT highly dynamic language to suuuper fast numeric code at runtime. Third, I wouldn’t be surprised if Dart grows. On the one hand, it’s roughly in the same boat as Go and Java, with memory safe runtime with fixed layout of objects and pervasive dynamic dispatch. But the quality of implementation of the runtimes is staggering: it has first-class JIT, AOT and JS compilers. Moreover, it has top-notch hot-reload support. Nothing here is a breakthrough, but the combination is impressive. Fourth, I predict that Nim, Crystal and Zig (which is very interesting, language design wise) would not become popular. Fifth, I predict that Swift will be pretty popular on Apple hardware due to platform exclusivity, but won’t grow much outside of it, despite being very innovative in language design (generics in Swift are the opposite of the generics in Go). </description>
      <pubDate>25 Feb 21 18:09 EST</pubDate>
      <guid>https://matklad.github.io//2020/09/13/your-language-sucks.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://matklad.github.io/2020/11/11/yde.html</link>
      <description>&lt;a href=&#34;https://matklad.github.io/2020/11/11/yde.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Why an IDE? Nov 11, 2020 Some time ago I wrote a reddit comment explaining the benefits of IDEs. Folks refer to it from time to time, so I decided to edit it into an article form. Enjoy! I think I have a rather balanced perspective on IDEs. I used to be a heavy Emacs user (old config, current config). I worked at JetBrains on IntelliJ Rust for several years. I used evil mode and vim for a bit, and tried tmux and kakoune. Nowadays, I primarily use VS Code to develop rust-analyzer: LSP-based editor-independent IDE backend for Rust. I will be focusing on IntelliJ family of IDEs, as I believe these are the most advanced IDEs today. The main distinguishing feature of IntelliJ is semantic understanding of code. The core of IntelliJ is a compiler which parses, type checks and otherwise understands your code. PostIntelliJ is the canonical post about this. That article also refutes the claim that “Smalltalk IDE is the best we’ve ever had”. Note that “semantic understanding” is mostly unrelated to the traditional interpretation of “IDE” as Integrated Development Environment. I personally don’t feel that the “Integrated” bit is all that important. I commit&amp;push from the command line using Julia scripts, rebase in magit, and do code reviews in a browser. If anything, there’s an ample room for improvement for the integration bits. For me, I in “IDE” stands for “intelligent”, smart. Keep in mind this terminology difference. I feel it is a common source of misunderstanding. “Unix and command line can do anything an IDE can do” is correct about integrated bits, but is wrong about semantical bits. Traditional editors like Vim or Emacs understand programming languages very approximately, mostly via regular expressions. For me, this feels very wrong. It’s common knowledge that HTML shall not be parsed with regex. Yet this is exactly what happens every time one does vim index.html with syntax highlighting on. I sincerely think that almost every syntax highlighter out there is wrong and we, as an industry, should do better. I also understand that this is a tall order, but I do my best to change the status quo here :-) These are mostly theoretical concerns though. The question is, does semantic understanding help in practice? I am pretty sure that it is non-essential, especially for smaller code bases. My first non-trivial Rust program was written in Emacs, and it was fine. Most of rust-analyzer was written using pretty spartan IDE support. There are a lot of insanely-productive folks who are like “sometimes I type vim, sometimes I type vi, they are sufficiently similar”. Regex-based syntax highlighting and regex based fuzzy symbol search (ctags) get you a really long way. However, I do believe that features unlocked by deep understanding of the language help. The funniest example here is extend/shrink selection. This features allows you to extend current selection to the next encompassing syntactic construct. It’s the simplest feature a PostIntelliJ IDE can have, it only needs the parser. But it is sooo helpful when writing code, it just completely blows vim’s text objects out of the water, especially when combined with multiple cursors. In a sense, this is structural editing which works for text. If you add further knowledge of the language into a mix, you’ll get the “assists” system: micro-refactoring which available in a particular context. For example, if the cursor is on a comma in a list of function arguments, you can alt+enter &gt; “swap arguments”, and the order of arguments will be changed in the declaration and on various call-sites as well. (See this post to learn how assists are implemented). These small dwim things add up to a really nice editing experience, where you mostly express the intention, and the IDE deals with boring syntactical aspects of code editing: For larger projects, complex refactors are a huge time-saver. Doing project-wide renames and signature changes automatically and without thinking reduces the cost of keeping the code clean. Another transformative experience is navigation. In IntelliJ, you generally don’t “open a file”. Instead you think directly in terms of functions, types and modules, and navigate to those using file structure, goto symbol, to do definition/implementation/type, etc: When I used Emacs, I really admired its buffer management facilities, because they made opening a file I want a breeze. When I later switched to IntelliJ, I stopped thinking in terms of a set of opened files altogether. I disabled editor tabs and started using editor splits less often — you don’t need bookmarks if you can just find things. For me, there’s one aspect of traditional editors which is typically not matched in IDEs out of the box — basic cursor motion. Using arrow keys for that is slow and flow-breaking, because one needs to move the hand from the home row. Even Emacs&#39; horrific C-p, C-n are a big improvement, and vim’s hjkl go even further. One fix here is to configure each tool to use your favorite shortcuts, but this is a whack-a-mole game. What I do is remapping CapsLock to act as an extra modifier, such that ijkl are arrow keys. (There are also keyboards with hardware support for this). This works in all applications the same way. Easy motion / ace jump functionality for jumping to any visible character is also handy, and usually is available via a plugin. Recent advancements with LSP protocol promise to give one the best of both worlds, where semantic-aware backend and light-weight editor frontend are different processes, which can be mixed and matched. This is nice in theory, but not as nice in practice as IntelliJ yet, mostly because IntelliJ is way more polished. To give a simple example, in IntelliJ for “go to symbol by fuzzy name” functionality, I can filter the search scope by: is this my code/code from a dependency? is this test/production code? is a symbol a type-like thing, or a method-like thing? path to the module where the symbol is defined. VS Code and LSP simply do not have capabilities for such filters yet, they have to be bolted on using hacks. Support for LSP in other editors is even more hit-and-miss. LSP did achieve a significant breakthrough — it made people care about implementing IDE backends. Experience shows that re-engineering an existing compiler to power an IDE is often impossible, or isomorphic to a rewrite. How a compiler talks to an editor is the smaller problem. The hard one is building a compiler that can do IDE stuff in the first place. Check out this post for some of the technical details. Starting with this use-case in mind saves a lot of effort down the road. This I think is a big deal. I hypothesize that the reason why IDEs do not completely dominate tooling landscape is the lack of good IDE backends. If we look at the set of languages fairly popular recently, a significant fraction of them is dynamically typed: PHP, JavaScript, Python, Ruby. The helpfulness of an IDE for dynamically typed languages is severely limited: while approximations and heuristics can get you a long way, you still need humans in the loop to verify IDE’s guesses. There’s C++, but its templates are effectively dynamically typed, with exactly the same issues (and a very complex base language to boot). Curiously, C looks like a language for which implementing a near-perfect IDE is pretty feasible. I don’t know why it didn’t happen before CLion. This leaves C# and Java. Indeed, these languages are dominated by IDEs. There’s a saying that you can’t write Java without an IDE. I think it gets the causation direction backwards: Java is one of the few languages for which it is possible to implement a great IDE without great pain. Supporting evidence here is Go. According to survey results, text editors are stably declining in popularity in favor of IDEs. I think this is because Go actually has good IDEs. This is possible because the language is sufficiently statically typed for an IDE to be a marked improvement. Additionally, the language is very simple, so the amount of work you need to put in to make a decent IDE is much lower than for other languages. If you have something like JavaScript…​ Well, you first need to build an alternative language for which you can actually implement an IDE (TypeScript) and only then you can build the IDE itself (VS Code). </description>
      <pubDate>25 Feb 21 18:10 EST</pubDate>
      <guid>https://matklad.github.io/2020/11/11/yde.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.socher.org/index.php/Main/HomePage</link>
      <description>&lt;a href=&#34;https://www.socher.org/index.php/Main/HomePage&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; We couldn&#39;t find the page you were looking for. This is either because: There is an error in the URL entered into your web browser. Please check the URL and try again. The page you are looking for has been moved or deleted. You can return to our homepage by clicking here, or you can try searching for the content you are seeking by clicking here. </description>
      <pubDate>12 Jun 20 12:17 EDT</pubDate>
      <guid>https://www.socher.org/index.php/Main/HomePage</guid>
    </item>
    <item>
      <title>AR Will Spark the Next Big Tech Platform—Call It Mirrorworld</title>
      <link>https://www.wired.com/story/mirrorworld-ar-next-big-tech-platform/</link>
      <description>&lt;a href=&#34;https://www.wired.com/story/mirrorworld-ar-next-big-tech-platform/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;We are building a 1-to-1 map of almost unimaginable scope. When it&#39;s complete, our physical reality will merge with the digital universe.Illustration: StoryTKEvery December, Adam Savage—star of the TV show MythBusters—releases a video reviewing his “favorite things” from the previous year. In 2018, one of his highlights was a set of Magic Leap augmented reality goggles. After duly noting the hype and backlash that have dogged the product, Savage describes an epiphany he had while trying on the headset at home, upstairs in his office. “I turned it on and I could hear a whale,” he says, “but I couldn’t see it. I’m looking around my office for it. And then it swims by my windows—on the outside of my building! So the glasses scanned my room and it knew that my windows were portals and it rendered the whale as if it were swimming down my street. I actually got choked up.” What Savage encountered on the other side of the glasses was a glimpse of the mirrorworld.</description>
      <pubDate>26 Feb 21 11:27 EST</pubDate>
      <guid>https://www.wired.com/story/mirrorworld-ar-next-big-tech-platform/</guid>
    </item>
    <item>
      <title>Slate Star Codex and Silicon Valley’s War Against the Media</title>
      <link>https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media</link>
      <description>&lt;a href=&#34;https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The mind-set of logical serenity, for all of the rationalists’ talk of “skin in the game” and their inclination to heighten every argument with a proposition bet, only obtains as long as their discussions feel safely confined to the realm of what they regard, consciously or otherwise, as sport. The sheer volume of Alexander’s output can make it hard to say anything overly categorical (epistemic status: treading carefully), but there is some evidence to support the idea that he, like anyone, is wont to sacrifice rigor in moments of passion. (The rationalists might describe the relationship as inversely proportional.) One of Alexander’s most notorious essays was a thirteen-thousand-word screed called “Untitled,” a defense of Scott Aaronson, the Austin computer scientist and rationalist fellow-traveller. Aaronson had written that the charge of “male privilege” obscures and demeans the suffering of nerds in the sexual marketplace, and had been subject to online scorn by some Internet feminists. Alexander, moved to anger by Aaronson’s plight, rebukes the feminists. Although he claims at various points to be largely sympathetic to some of the women he mentions, he mostly deploys the term “feminism” as a tendentious umbrella term for the work of a small handful of online writers with whom he takes particular issue in one discrete instance. Later, Alexander apparently came to a similar conclusion and appended a disclaimer to the top of the post: “EDIT: This is the most controversial post I have ever written in ten years of blogging. I wrote it because I was very angry at a specific incident. I stand by a lot of it, but if somebody links you here saying ‘HERE’S THE SORT OF GUY THIS SCOTT ALEXANDER PERSON IS, READ THIS SO YOU KNOW WHAT HIS BLOG IS REALLY ABOUT’, please read any other post instead.”Since the 2016 Presidential election, a contingent of the media has been increasingly critical of Silicon Valley, charging tech founders, C.E.O.s, venture capitalists, and other technology boosters with an arrogant, naïve, and reckless attitude toward the institutions of a functional democracy, noting their tendency to disguise anticompetitive, extractive behavior as disruptive innovation. Many technologists and their investors believe that media coverage of their domain has become histrionic and punitive, scapegoating tech companies for their inability to solve extremely difficult problems, such as political polarization, that are neither of their own devising nor within their ability to solve. The Valley’s most injured, aggrieved, and single-minded partisans don’t want to be judged by the absurdity of Juicero, the much-ridiculed luxury-juicing startup, or the fraud of Theranos, or the depredations of Uber. As Paul Graham pointed out, in a 2017 tweet, it was unfair to condemn the entirety of the tech sector based on a few bad actors. “Criticizing Juicero is fine,” he wrote. “What’s intellectually dishonest is criticizing SV by claiming Juicero is typical of it.” (The obvious irony—that people like Graham nevertheless feel free to write off the entirety of “the media” on a similarly invidious basis—seems lost on many of them.)Graham’s tweet linked to a Slate Star Codex piece, also from 2017, called “Silicon Valley: A Reality Check,” in which Alexander had collated the most triumphalist dismissals of Juicero and paired them with his own views of what actual technological innovation looked like. “While Deadspin was busy calling Silicon Valley ‘awful nightmare trash parasites’, my girlfriend in Silicon Valley was working for a company developing a structured-light optical engine to manipulate single cells and speed up high-precision biological research,” he writes. Alexander goes on, in the post, to allow that Silicon Valley is not above reproach, acknowledging that “anything remotely good in the world gets invaded by rent-seeking parasites and empty suits,” but argues that journalists at publications such as the former Deadspin do not understand that the “spirit of Silicon Valley” is “a precious thing that needs to be protected.” (Deadspin, in its original form, did not survive the aftermath of Hulk Hogan’s lawsuit against its former parent company, Gawker Media; the lawsuit was underwritten by Peter Thiel, which complicates the issue of who, exactly, needs protection from whom.) He continues, “At its worst, some of their criticism sounds more like a worry that there might still be some weird nerds who think they can climb out of the crab-bucket, and they need to be beaten into submission by empty suits before they can get away.”By then, six months after the election, Alexander had emerged as one of the keenest observers of technologists as a full-fledged social cadre, and of their sharpening class antagonism with an older order—the institutions in New York, Boston, D.C., and Los Angeles that Balaji Srinivasan has disparaged as “the Paper Belt.” (Srinivasan’s Twitter bio reads “not big on credentialism,” a common posture in a place that likes to present itself as the world’s most successful meritocracy, although he provides a link that itemizes his connections to Stanford and M.I.T. “if deemed relevant.”) This new group, Alexander suggested in an earlier beloved essay, “I Can Tolerate Anything Except the Outgroup,” published in 2014, sits at an odd angle to America’s extant tensions. In the essay, he describes our tendency to conceal the degree to which our beliefs and actions are determined by tribal attitudes. It is obvious, Alexander writes, that America is split in recognizable ways. “The Red Tribe is most classically typified by conservative political beliefs, strong evangelical religious beliefs, creationism, opposing gay marriage, owning guns, eating steak, drinking Coca-Cola, driving SUVs, watching lots of TV, enjoying American football, getting conspicuously upset about terrorists and commies, marrying early, divorcing early, shouting ‘USA IS NUMBER ONE!!!’, and listening to country music.” He notes that he himself knows basically none of these people, a sign of how comprehensive our national sorting project has become. “The Blue Tribe,” by contrast, “is most classically typified by liberal political beliefs, vague agnosticism, supporting gay rights, thinking guns are barbaric, eating arugula, drinking fancy bottled water, driving Priuses, reading lots of books, being highly educated, mocking American football, feeling vaguely like they should like soccer but never really being able to get into it, getting conspicuously upset about sexists and bigots, marrying later, constantly pointing out how much more civilized European countries are than America, and listening to ‘everything except country’.” What’s crucial, he emphasizes, is that these are cultural differences rather than political ones—an Ivy League professor might hold right-leaning beliefs, for example, but is nevertheless almost certainly a member of the Blue Tribe.These are caricatures, of course, but Alexander’s crude reductionism is part of his argument, which is that these categories are drawn and redrawn in bad faith, as a way to disavow tribalistic rancor without actually giving it up. When, for example, members of the Blue Tribe censure “America,” they are purporting to implicate themselves in their criticism; in reality, however, they are simply using “America” to mean “Red” America, without making that distinction explicit. What may sound like humility and self-scrutiny is, in fact, actually just a form of thinly disguised tribal retrenchment.He introduces the idea of a third cohort in an aside: “(There is a partly-formed attempt to spin off a Grey Tribe typified by libertarian political beliefs, Dawkins-style atheism, vague annoyance that the question of gay rights even comes up, eating paleo, drinking Soylent, calling in rides on Uber, reading lots of blogs, calling American football ‘sportsball’, getting conspicuously upset about the War on Drugs and the NSA, and listening to filk—but for our current purposes this is a distraction and they can safely be considered part of the Blue Tribe most of the time.)” This is clearly meant as a teasing description of the S.S.C. reader—and, by extension, the Silicon Valley intellectual. Since the post was published, “Grey Tribe” has become a shorthand compliment paid to thinkers who float free of the polarized fiasco of American discourse. But “Except the Outgroup” is not an encomium to the Grey Tribe; it is his gentle reminder that most of its members, most of the time, share a vast portion of their political commitments with the Blue Tribe that they so often censure. He has been very upfront about this in his own case; last year, he wrote, lest there was any confusion, “I am a pro-gay Jew who has dated trans people and votes pretty much straight Democrat.” Any sense of rivalry, he suggests, is likely reducible to the narcissism of minor differences.The division between the Grey and Blue tribes is often rendered in the simplistic terms of a demographic encounter between white, nerdily entitled men in hoodies on one side and diverse, effete, artistic snobs on the other. On this account, one side is generally associated with quantification, libertarianism, speed, scale, automation, science, and unrestricted speech; the other is generally associated with quality, progressivism, precaution, craft, workmanship, the humanities, and respectful language. Alexander, in another widely circulated essay, published in 2018, has popularized an alternative heuristic—a partition between what he calls “mistake theorists” and “conflict theorists.” Mistake theorists, he writes, look at any difference of opinion and conclude that someone must be making an error. They reckon that when the source of the mistake is identified—with more data, more debate, more intelligence, more technical insight—the resolution will be obvious. Conflict theorists are likely to look at the same difference of opinion and assume that no mechanism will provide for a settlement until incompatible desires are brought into alignment. The former tend to believe that after we sort out the problem of means, the question of ends can be left to take care of itself. The latter tend to believe that the preoccupation with means can serve to obscure the real issue of ends. Mistake theorists default to the hope that we just need to fix the bugs in the system. Conflict theorists default to the worry that what look like bugs might be features—and that it’s the system that has to be updated.</description>
      <pubDate>28 Feb 21 09:30 EST</pubDate>
      <guid>https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.yossarian.net/2020/01/23/Anybody-can-write-good-bash-with-a-little-effort</link>
      <description>&lt;a href=&#34;https://blog.yossarian.net/2020/01/23/Anybody-can-write-good-bash-with-a-little-effort&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;ENOSUCHBLOG Programming, philosophy, pedaling. Home Tags Series Favorites Archive Main Site Anybody can write good bash (with a little effort) Jan 23, 2020     Tags: programming, rant     What this post is A gentle admonishment to use shell scripts where appropriate accept that shell scripts will appear in your codebases and to lean heavily on automated tools, modern features, safety rails, and best practices whenever possible. Shell programming is a popular and predictable target of ire in programming communities1: virtually everybody has a horror story about a vintage, broken, or monstrous shell script underpinning a critical component of their development environment or project. Personal favorites include: The ominous run.sh, which regularly: Runs something, somewhere Lacks the executable bit Doesn’t specify its shell with a shebang Expects to be run as a particular user, or runs itself again in a different context Does very bad things if run from the wrong directory May or may not fork May or may not write a pidfile correctly, or at all May or may not check that pidfile, and subsequently clobber itself on the next run All of the above make.sh (or build.sh, or compile.sh, or …), which: Doesn’t understand CC, CXX, CFLAGS or any other standard build environment variable Gets clever and tries to implement its own preprocessor Contains a half-baked -j implementation Contains a half-baked make implementation, including (broken) install and clean targets Decides that it knows better than you where it should be installed Fails if anything, anywhere has a space in it Leaves the build in an indeterminate state if interrupted Happily chugs along after a command fails, leaving the build undiagnosable All of the above test.sh, which: Expects to be run in some kind of virtual environment (a venv, a container, a folder containing a bundle, &amp;c) …tries to install and/or configure and/or load that virtual environment if not run inside it correctly, usually breaking more things Incorrectly detects that is or isn’t in the environment it wants, and tries to do the wrong thing Masks and/or ignores the exit codes of the test runner(s) it invokes internally Swallows and/or clobbers the output of the runner(s) it invokes internally Contains a half-baked unit test implementation that doesn’t clean up intermediates or handle signals correctly Gets really clever with colored output and doesn’t bother to check isatty All of the above env.sh, which: May or may not actually be a shell script May or may evaled into a shell process of indeterminate privilege and state somewhere in your stack May or may not just be split on = in Python by your burnt-out DevOps person All of the above, at different stages and on different machines I’ve experienced all of these, and am personally guilty of a (slight) majority of them2. Despite that (and perhaps because of it) I continue to believe that shell scripts3 have an important (and irreplaceable) niche in my development cycle, and should occupy that same niche in yours. I’ll go through the steps I take to write (reliable, composable) bash below. Basics A bash script (i.e., a bash file that’s meant to be run directly) doesn’t end up in my codebases unless it: Has the executable bit Has a shebang and that shebang is #!/usr/bin/env bash Explanation: not all systems have a (good) version of GNU bash at /bin/bash: macOS infamously supplies an ancient version at that path, and other platforms may use other paths. Has a top level comment that briefly explains its functionality Has set -e (and ideally set -euo pipefail)4 Explanation: set -e, while not perfect, catches and makes fatal many types of (otherwise silent) failure. set -u makes expansions of undefined variables fatal, which catches the classic case of rm -rf &#34;${PERFIX}/usr/bin&#34;. set -o pipefail extends -e by making any failure anywhere in a pipeline fatal, rather than just the last command. Can either: Be run from any directory, or Fail immediately and loudly if it isn’t run from the correct directory I also put two functions in (almost) every script: 1 2 3 4 5 6 7 8 9 10 11 function installed { cmd=$(command -v &#34;${1}&#34;) [[ -n &#34;${cmd}&#34; ]] &amp;&amp; [[ -f &#34;${cmd}&#34; ]] return ${?} } function die { &gt;&amp;2 echo &#34;Fatal: ${@}&#34; exit 1 } Edit: a Redditor has pointed out that this installed function is unnecessarily cautious and verbose. These compose nicely with bash’s conditional tests and operators (and each other) to give me easy sanity checks at the top of my scripts: 1 2 3 4 5 6 [[ &#34;${BASH_VERSINFO[0]}&#34; -lt 4 ]] &amp;&amp; die &#34;Bash &gt;=4 required&#34; deps=(curl nc dig) for dep in &#34;${deps[@]}&#34;; do installed &#34;${dep}&#34; || die &#34;Missing &#39;${dep}&#39;&#34; done Some other niceties: I use shopt -s extglob and shopt -s globstar in some of my scripts, slightly preferring it over (simple) find invocations. Compare this find invocation: 1 items=$(find . -name &#39;foo*&#39; -o -name &#39;bar*&#39;) to the shorter (and process-spawn-free): Linux Journal has a nice extended globbing reference here; globstar is explained in the GNU shopt documentation here. Automated linting and formatting In terms of popularity and functionality, shellcheck reigns supreme. Going by its changelog, shellcheck has been around for a little under 7 years. It’s also available in just about every package manager. As of 0.7.0, shellcheck can even auto-generate (unified-format) patches for some problems: 1 shellcheck -f diff my_script.sh | patch And includes a (sadly optional) check for my personal bugbear: non-mandatory variable braces: 1 2 3 4 5 6 7 # Bad foo=&#34;$bar&#34; stuff=&#34;$# $? $$ $_&#34; # Good foo=&#34;${bar}&#34; stuff=&#34;${#} ${?} ${$} ${_}&#34; shellcheck also doesn’t complain about usage of [ (instead of [[), even when the shell is explicitly GNU bash5. There’s also bashate and mvdan/sh, neither of which I’ve used. Environment variables, not flags In the past, I’ve used the shift and getopt builtins (sometimes at the same time) to do flag parsing. I’ve mostly given up on that, and have switched to the following pattern: Boolean and trivial flags are passed via environment variables: 1 VERBOSE=1 STAMP=$(date +%s) frobulate-website I find this substantially easier to read and remember than flags (did I use -v or -V for verbose in this script?), and allows me to use this nice syntax for defaults: 1 2 VERBOSE=${VERBOSE:-0} STAMP=${STAMP:-$(date +%s)} Where possible stdin, stdout, and stderr are used instead of dedicated positional files: 1 VERBOSE=1 DEBUG=1 frobulate-website &lt; /var/www/index.html &gt; /var/www/index2.html The only parameters are positional ones, and should generally conform to a variable-argument pattern (i.e., program &lt;arg&gt; [arg ...]). -h and -v are only added if the program has non-trivial argument handling and is expected to be (substantially) revised in the future. I generally prefer not to implement -v at all, favoring a line in the header of -h’s output instead. Running the command with no arguments is treated as equivalent to -h. All other kinds of flags, inputs, and mechanisms (including getopt) are only used as a last resort. Compose liberally Don’t be afraid of composing pipes and subshells: 1 2 3 # Combine the outputs of two `stage-run` invocations for # a single pipeline into `stage-two` (stage-one foo &amp;&amp; stage-one bar) | stage-two Or of using code blocks to group operations: 1 2 # Code blocks aren&#39;t subshells, so `exit` works as expected risky-thing || { &gt;&amp;2 echo &#34;risky-thing didn&#39;t work!&#34;; exit 1; } Subshells and blocks can be used in many of the same contexts; which one you use should depend on whether you need an independent temporary shell or not: 1 2 3 4 5 6 7 # Both of these work, but the latter preserves the variables (read line1 &amp;&amp; read line2 &amp;&amp; echo &#34;${line1} vs. ${line2}&#34;) &lt; &#34;${some_input}&#34; # line1 and line2 are undefined { read line1 &amp;&amp; read line2 &amp;&amp; echo &#34;${line1} vs. ${line2}&#34;; } &lt; &#34;${some_input}&#34; # line1 and line2 are defined and contain their last values Note the slight syntactic differences: blocks require spacing and a final semicolon (when on a single line). Use process substitution to avoid temporary file creation and management: Bad: 1 2 3 4 5 6 7 8 9 function cleanup { rm -f /tmp/foo-* } output=$(mktemp -t foo-XXXXXX) trap cleanup EXIT first-stage output second-stage --some-annoying-input-flag output Good: 1 second-stage --some-annoying-input-flag &lt;(first-stage) You can also use them to cleanly process stderr: 1 2 # Drop `big-task`&#39;s stdout and redirect its stderr to a substituted process (big-task &gt; /dev/null) 2&gt; &gt;(sed -ne &#39;/^EMERG: /p&#39;) Roundup The shell is a particularly bad programming language that is particularly easy to write (unsafe, unreadable) code in. It’s also a particularly effective language with idioms and primitives that are hard to (tersely, faithfully) reproduce in objectively better languages. It’s also not going anywhere anytime soon: according to sloccount, kubernetes@e41bb32 has 28055 lines of shell in it6. The moral of the story: shell is going to sneak into your projects. You should be prepared with good practices and good tooling for when it does. If you somehow manage to keep it out of your projects7, people will use shell to deploy your projects or to integrate it into their projects. You should be prepared to justify your project’s behavior and (non-)conformity to the (again, objectively bad) status quo of UNIX-like environments for when they come knocking. </description>
      <pubDate>28 Feb 21 15:59 EST</pubDate>
      <guid>https://blog.yossarian.net/2020/01/23/Anybody-can-write-good-bash-with-a-little-effort</guid>
    </item>
    <item>
      <title></title>
      <link>https://cs.lmu.edu/~ray/notes/types/</link>
      <description>&lt;a href=&#34;https://cs.lmu.edu/~ray/notes/types/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Types Programs manipulate values according to their type. There is more to the study of types than you might think. Why Types? Our very intuition makes the concept of types inescapable. We have an intuition that integers are different from strings which are different from people which are different from books which are different from planets which are, you get the idea. These differences arise from behavior: You can open a socket, but not an integer; you can request the parents of a person, but not a dictionary; you can push an object onto a stack, but not onto character. Therefore: A value’s type constrains the way it may be used in a program. More philosophically: Types impose constraints on what we can and cannot say. More formally: A type consists of a set of values and a set of allowable operations. Examples What are some types you know about from previous programming practice? Here are some you might have seen, Keep in mind, as you go through this list, that types are characterized by their operations (more so than their “values”): void, the empty type; the type of no values at all. boolean, the type containing the values true and false, with operations such as and, or, xor, and not. atom: The type of named symbols. Atoms just have name, and that’s it. You don’t really operate on them (other than testing them for equality perhaps); you just pass them around. They just exist. Numeric types, with operations such as plus, minus, multiply, divide, remainder, modulo, abs, sign, sin, cos, tan, and on and on and on. Numeric types can be integral, fixed-point, floating point, or complex; bounded or unbounded; signed or unsigned. Examples include: number, int8, int16, int32, int64, int128, int, uint8, uint16, uint32, uint64, uint128, uint, bigint, fixed, float32, float64, float128, complex64, complex128, ratio, decimal, bigdecimal. char: The type of “characters”, i.e., units of textual information. Exactly what a character is might differ from language to language. Collection Types, with operations such as size, is_empty, contains, add, remove, retain, clear. Set Types, which are unordered collections of unique elements. Sequence Types. So many of these. Lists, stacks, queues, dequeues, priority queues, blocking queues. Even the famous string is a kind of sequence, but there’s not a lot of agreement about what exactly strings are sequences of. Are they sequences of bytes? character codes? code points? unicode scalars? graphemes? String types are generally broken. You don&#39;t need them. Mapping Types, types that maps keys to values. If the keys are unique we call them maps or dictionaries; if keys do not have to be unique we call them multimaps. Normally the keys are unordered, but orderedmaps or those that are automatically iterated by in key-order (assuming keys are comparable). Singleton types, which are types that only have one value in them. The operations on that individual value vary, of course. Sum Types. The type $T_1 + T_2$ consists of all the values of $T_1$ plus all the values of $T_2$. Sums are often called unions. If the alternatives are labeled (named), we get tagged unions. Product Types, The type $T_1 \times T_2$ consists of all pairs $(x, y)$ where $x$ has type $T_1$ and $y$ has type $T_2$. In general you can have products of any size: $T_1 \times T_2 \times \ldots \times T_n$. If $n=0$ you have what’s sometimes called the unit type: the type of only one value. In general, product types are called tuple types. If the components are labeled (named), we get records. Routines, which can be subroutines or coroutines. Operations on routines include invoke and invokeLater. Other terms for invoke include call and apply (from the idea of function “application” — “applying” a function to its argument(s)). Function types, computational mappings from arguments to results. We can classify functions in many ways: there are predicates, which return booleans, consumers, that accept arguments but do not return results, suppliers, which take no arguments but return results. Processes, Threads, and Tasks, which are things that run asynchronously. Generally these things send messages to each other, but shared memory is sometimes used. They are generally created with a spwan operation. Internally they use send and receive to communicate. The type type, whose members are...types! The type any, which is the type of all values. Exercise: Spend some time thinking about the behaviors of each of the types above. Classifying Types It is tempting to try to put types into a hierarchy, or at least a directed acyclic graph: But it’s actually difficult to this because there are so many other ways to classify type that defy hierarchy. In fact, there’s this whole idea of classifying types into something called typeclasses, which are things like this: A type is equatable, or is an eqtype, if values of the type can be tested for equality (==). In some languages, all types are equatable because most of the types have reference semantics. In Haskell, however, functions, for example, are not equatable. A type is comparable (a.k.a. ordered) if values of the type can be compared with &lt; (and often also with &lt;=, &gt;, and &gt;=, and often &lt;=&gt;). Generally (at least in the vast majority of languages), if you make your own class, it won’t be comparable be default. A type is bounded if there is a minimum value of the type and a maximum value of the type. Fixed-size integers and simple enumerations are bounded. A type is showable, or is a showtype if values of the type can be converted to strings. Functions and binary streams are generally not showable. A type is callable if values of its type can be called on, or applied to, other values. Functions are callable. In Python, so are types! A type is enumerable if it has successor and predecessor operations on its values. Integers are enumerable, as are characters (enumerated via their code points). Note this definition does not apply to languages with very complex enum things, like Swift. Those concepts are completely different. A type is monoidal, or is a monoid, if it has an associative composition operator, and an identity operator for the composition. Concrete example: Functions! They have function composition and the identity x =&gt; x (JS notation). Abstractly: $\exists e, \bullet:\; \forall a, b, c: a \bullet (b \bullet c) = (a \bullet b) \bullet c \wedge a \bullet e = e \bullet a$ A type is monadic, or is a monad, if values of the type “wrap” underlying values in some way (e.g. optional, list), and there is an operator to wrap a value, and an associative composition operator on functions that unwrap a value and do something to it to produce a new wrapped value, such that the wrapper is an identity for the composition. Need a tutorial? Typeclasses in PracticeLanguages vary greatly in their treatment of typeclasses. Haskell has a distinguished typeclass concept (called a class. Swift implements them via protocols, and other languages just use their own interface or mixin construct. Exercise: Find the built-in Swift protocols for equatables and comparables. How do they work? Craft an example that uses them. A Type Algebra We can arrange types by building them up algebraically, similar to how sets are built up, for example: Kind of TypeNotationNotes Empty$\perp$Type with no members at all. A bottom type (meaning it is a subtype of all types). Sometimes called Void. Singleton$x$Type with only a single member. Examples: nulltype containing only the value null truetype containing only the value true &#34;hello&#34;ttype containing only the value &#34;hello&#34; 5type containing only the value 5 bluetype containing only the value blue Sum$T_1\;|\;T_2\;|\,...|\;T_n$or$x_1: T_1\;|\,...|\;x_n: T_n$Also written $T_1 \cup T_2$ or $T_1 + T_2$. If unlabeled, generally called a Union. If labeled, generally called a Tagged Union. Examples: true | false red | green | blue string | null int | bool | float 1 | 2 | 3 | 4 | 5 | 6 circle: float | rectangle: float × float enum { circle: Float; rectangle: (Float, Float) } Product$(T_1, T_2, ... T_n)$or$(x_1: T_1,..., x_n: T_n)$Also written $T_1 \times T_2$. If unlabled, generally called a Tuple. If labeled, generally called a Record or Struct. Examples: bool × int (Bool, Int) () x: int × y: int struct { x: int; y: int; } Sequence$T*$a.k.a. Array or List. Examples: [Int]Swift []float64Go bool listSML List&lt;String&gt;Java Function$T_1 \rightarrow T_2$A function from $T_1$ to $T_2$. Use $\perp$ or a product type for $T_1$ to simulate zero or multiple arguments, and $\perp$ or a product type for $T_2$ to simulate zero or multiple return values. Examples: Int -&gt; Int Bool* -&gt; Int × String Int × Int × String -&gt; Int Float → Void Any$\top$All values are a member of this type. The “type of everything.” A top type (meaning all types are a subtype of it). Examples: anyTypeScript interface{}Go BasicObjectRuby objectPython This arrangement actually works pretty well, as it covers most everything we need: The boolean type is really just true | false for singleton types true and false. A string type is just a sequence of some underlying elements, maybe bytes, maybe character codes, maybe graphemes. An optional type is just T | null for any type T and singleton type null. This is often abbreviated T? or Optional&lt;T&gt;. Are sequence types really primitive? Seems like we can just write $T* = T \cup (T \times T) \cup (T \times T \times T) \cup \ldots$, but the abbreviation is simpler, of course. Be pragmatic. Many statically typed languages have a generic Either type, a tagged union with labels such as left and right, or a similarly structured tagged union called Result with labels success and failure. The latter is commonly used for returning, rather than throwing errors. There is a type called Never, which isn’t really a type, but used in place of a return type in a function that does not return, either because it enters an infinite loop or throws an error. Exercise: In your own words, describe the difference between a sum type and a product type. In case you haven’t seen singleton or union types before, you might not have seen TypeScript. Type Systems In programming language theory, a language’s type system answers questions such as: Is the set of types fixed, or can you create new ones? Are types extra-lingual, or are they themselves values? If values, are they first-class? Also if values, do they themselves belong to one of possibly many types or is there just one type? If not, can they be categorized into different, um, say, typeclasses? What exactly are the types of the language, and how are new types built? How do we infer the type of an expression (gets tricky with variables) How do we know when two types are exactly the same? How do we know when two types are compatible? What do we do when expressions are used in a way inconsistent with their type? Fixed Type Systems Some languages predefine the set of types and do not let you define new ones. These include: JavaScript: The only types are these eight: Undefined, Null, Boolean, String, Number, BigInt, Symbol, Object Lua: The only types are these eight: nil, boolean, number, string, function, thread, userdata, table. Erlang: The only types are: atom, integer, float, binary, ref, pid, port, function, list, tuple, map. JavaScript really does have only eight types, but it kind of feels like it has more. For example: class Dog { bark() { return &#39;woof&#39;; } } class Rat { squeak() { return &#39;peep&#39;; } } const d = new Dog(); const r = new Rat(); d.constructor // Dog r.constructor // Rat typeof d // &#39;object&#39; typeof r // &#39;object&#39; So in JavaScript you can make lots of different kinds of objects (even arrays, functions, dates, and regular expressions are objects), but all of these objects have the same type. They have different constructors, but constructors aren’t types. Are Types Values? In JavaScript, the value 100 has the type “number”, but this “number type” itself is not a JavaScript value. Types in JavaScript are extra-lingual. You can ask for an expression’s type, but you’ll just get back a string: typeof 3 === &#39;number&#39; typeof &#39;hello&#39; === &#39;string&#39; typeof { x: 1, y: 2 } === &#39;object&#39; typeof typeof 3 === &#39;string&#39; Ditto for Lua: type(3) == &#39;number&#39; type(&#39;hello&#39;) == &#39;string&#39; type({ x=1, y=2 }) == &#39;table&#39; type(type(3)) == &#39;string&#39; In Erlang, you can’t even ask for the type, but there are built-in functions to tell you whether or not an expression has a type: is_atom(ten). is_integer($a). is_float(-3.55e-8). is_function(fun (X) -&gt; X*X end). is_reference(make_ref()). is_tuple({dog, &#34;Nika&#34;, 5, &#39;G-SHEP&#39;}). is_list(&#34;a string&#34;). This is probably a good thing, because as we’ll see, values can have multiple types at the same time. So asking for the type of an expression might be an indication of a type system that is less sophisticated than it could be. In C, you can&#39;t even ask or check for a type, but you can see them! The types exist in the code, but they are not values that can be assigned to variables or passed as arguments or returned from functions. Exercise: Try writing C code that uses int as a value. What problems do you run into? In Python, on the other hand, the value 100 has the type int, and int itself is a value in Python. In fact it has the type type. And, you guessed it, type is a value whose type is... yes, type! type(3) == int type(&#39;hello&#39;) == str type({ &#39;x&#39;:1, &#39;y&#39;:2 }) == dict type(int) == type type(type) == type type(type(type(type(3)))) == type Same with Ruby, though in Ruby you tend to see classes more than types: 3.class == Integer &#39;hello&#39;.class == String {x: 1, y: 2}.class == Hash Integer.class == Class Class.class == Class Java, too, has objects representing classes: System.out.println(new Integer(3).getClass()); // class java.lang.Integer System.out.println(int.class); // int System.out.println(&#34;hello&#34;.getClass()); // class java.lang.String System.out.println(new int[]{1, 2, 3}.getClass()); // class [I System.out.println(String.class); // class java.lang.String System.out.println(&#34;hi&#34;.getClass().getClass()); // class java.lang.Class Augh, wait, WTF...we have types and now classes? Types vs. Classes So what is this thing we call a class? A little hard to define, maybe.... Some languages conflate the notion of type and class, but most people agree there is a difference. TYPECLASS Is all about behaviorIs about both structure and behavior An object can have many typesAn object belongs to exactly one class Is about interfacing and usageIs about construction, and things like fields/properties and methods Example: A string object in Java has multiple types, including String, Comparable, and Object. But only one class: String. String s = &#34;hello&#34;; // &#34;hello&#34; ∈ type String Comparable c = s; // &#34;hello&#34; ∈ type Comparable Object o = s; // &#34;hello&#34; ∈ type Object System.out.println(o.getClass().getName()); // THE class of &#34;hello&#34; is String In most languages, declaring a class gives rise to a type. The exceptions to this rule are JavaScript and CoffeeScript, where the word class does not make a type: instead it is a funny way of declaring a function and a prototype. Other languages are similar to Java in having a class construct that makes a type, but they also have ways to make types that “mixin” behaviors, for example Ruby has modules and Swift has protocols. Go has structs for the class-concept and interfaces for the pure behavior-only types. Oversimplying just a bit...A class is an object factory; a type is a behavioral specification. Type Expressions In many languages, you get a set of basic types and mechanisms for making new types. Some examples: Language Basic Types Type formers C char, signed char, unsigned char, short, unsigned short, int, unsigned, long, unsigned long, long long, unsigned long long, float, double, long double, _Bool, float _Complex, double _Complex, long double _Complex, float _Imaginary, double _Imaginary, long double _Imaginary enum, *, [], (), union, struct Java boolean, byte, char, short, int, long, float, double interface, class, enum, [] Standard ML unit, bool, int, word, real, char, string -&gt;, *, list, option, exn, ref, frag, datatype, {} Python NoneType, NotImplementedType, ellipsis, int, bool, float, complex, str, bytes, tuple, list, bytearray, set, frozenset, dict, function, generator, method, classmethod, staticmethod, module, slice, range, type class Go bool, int8 (byte), int16, int32 (rune), int64, int, uint8, uint16, uint32, uint64, uintptr, float32, float64, complex64, complex128, string, error [], map, *, func, struct, interface, chan Rust bool, i8, i16, i32, i64, i128, isize, u8, u16, u32, u64, u128, usize, f32, f64, char, str, ! (T1, T2), [T ; N], [T], struct, enum, union, -&gt;, &amp;, &amp;mut, *const, *mut, trait, impl Swift Bool, Int8, Int16, Int32, Int64, Int, UInt8, UInt16, UInt32 UInt64, UInt, Float, Double, Character, String, (T1, T2), T?, [T], Set&lt;T&gt;, [K:V], T-&gt;U, struct, class Haskell Bool, Int, Integer, Float, Double, Char [a], (a,b), a-&gt;b, data So it looks like one of the most common ways to introduce a new type is to use that class notion we saw above. As in Python, for example: &gt;&gt;&gt; class C: pass ... &gt;&gt;&gt; class D: pass ... &gt;&gt;&gt; c, d = C(), D() &gt;&gt;&gt; type(c), type(d), type(c) == type(d) (&lt;class &#39;__main__.C&#39;&gt;, &lt;class &#39;__main__.D&#39;&gt;, False) But something cooler and more computer sciencey are the type operators of the ML-family of languages, like list (for lists), * (for tuples), and -&gt; (for functions): 7 (* int *) (4, &#34;dog&#34;) (* int * string *) [5, 3, 2, 7] (* int list *) [ [], [3,3,3], [], [1] ] (* int list list *) [(3, 4.0),(1, 5.5)] (* (int * real) list *) (5, [2.2, 1.0E~4]) (* int * real list *) fn x =&gt; x + 8 (* int -&gt; int *) fn x =&gt; fn y =&gt; x ^ Int.toString(y) (* string -&gt; int -&gt; string *) Mmmm, so here’s something interesting. SML has type called int list. Does Python? No, in Python, the type is just called list. So [1, &#34;hello&#34;] is legal in Python but illegal in SML. SML has parameterized types. Parameterized Types In Standard ML, the types int list and string list and ((int * string) -&gt; bool) list are all different, but they are all instances of the parameterized type &#39;a list. Lots of languages are like this. Here are some simple examples: LanguageExample Parameterized TypeExample Instantiated TypeNotes Standard ML&#39;a liststring listType variables are &#39;a, &#39;b, &#39;c and so on. If the instantiating type must admit equality, then we write &#39;&#39;a, &#39;&#39;b, &#39;&#39;c, and so on. &#39;a * &#39;bstring * bool &#39;a -&gt; &#39;b -&gt; &#39;aint-&gt;(bool*int)-&gt;int Haskell[a]a-&gt;bList IntInt-&gt;StringTypes begin with a capital letter and type variables with a lowercase letter. JavaList&lt;T&gt;List&lt;String&gt; SwiftSet&lt;T&gt;Set&lt;String&gt; By the way, type parameters do not have to be just a simple type variable. Type Equivalence When are two types the same? typedef struct { int a; int b; } Point; typedef struct { int a; int b; } Pair; Point x; Pair y; Do x and y have the same type? Should we say “yes” (because they have the same structure), or “no” (because their types have different names, and furthermore appear in different declarations)? These two approaches to determining whether types are the same are structural and named equivalence: Structural Equivalence Name Equivalence Check equivalence by expanding structures all the way down to basic types Strict: Every type declaration defines a new type Loose: Factor out aliases Here is an example from Michael L. Scott: type alink = pointer to cell; subtype blink = alink; p, q : pointer to cell; r : alink; s : blink; t : pointer to cell; u : alink; -- If structural: [p q r s t u] -- If strict name: [p q] [r u] [s] [t] -- If loose name: [p q] [r s u] [t] Loose name equivalence is pretty common: we like to distinguish things like Point and Pair above but want the flexibility of aliasing. In Ada we can do both explicitly: type Height is new Integer; type Weight is new Integer; -- Can’t add heights and weights subtype Height is Integer; subtype Weight is Integer; -- Now you can freely mix them ML seems to use structural equivalence: type pair = int * int; type point = int * int; val x: point = (1, 4); val y: pair = x; type person = {name: string, age: int}; type school = {name: string, age: int}; val p: person = {name=&#34;Alice&#34;, age=22}; val s: school = p; But that’s because type does not define a new type! Only a datatype or abstype declaration creates a new type. abstype person = P of string * int with fun new_person (s, i) = P(s, i) fun name (P(s, i)) = s fun age (P(s, i)) = i end; Exercise: Read this excellent little article that explains name and structural equivalence, with some intersting facts about type equivalence in C. Type Compatibility When can a value of type A be used in a context that expects type B? Possible answers: When A and B are equivalent (the same type) When A is a subtype of B When an A can be coerced to a B So.... In: int a; float b; float c; c = a + b; Is this An error, fixable by writing c = int_to_float(a) + b? Allowable? A language that allows this is said to coerce ints to floats, and we say &#34;int is compatible with float&#34;. Ada, Modula, ML: no coercion Fortran: lots of coercion C: lots of coercion in numeric types C++: user can define coercions, even accidentally! Important definitions: Type Conversion Explicit operation that takes in an object of one type and returns an object of a different type that has the &#34;same value&#34; (not necessarily the same bit pattern). Type Coercion Implicit Non-converting Type Cast &#34;Reinterpret&#34; an object as an object of another type by preserving its bit pattern, regardless of value. Type Inference How do we determine the type of an expression? ML seems to do it the most general way possible. It looks at all the types of the primitive expressions, then at the types of arguments that the subroutines and operators expect, and work your way out to the whole expression. 1 fun fib (n) = 2 let fun fib_helper (f1, f2, i) = 3 if i = n then f2 4 else fib_helper (f2, f1+f2, i+1) 5 in 6 fib_helper (0, 1, 0) 7 end; i is int, because it is added to 1 at line 4 n is int, because it is compared to i at line 3 all three args at line 6 are int consts, and that’s the only use of fib_helper (given scope of let), so f1 and f2 are int also 3rd arg is consistent with known int type of i (good!) and the types of the arguments to the recursive call at line 4 are similarly consistent since fib_helper returns f2 (known to be int) at line 3, the result of the call at line 6 will be int Since fib immediately returns this result as its own result, the return type of fib is int If there’s not enough information to resolve to a known type, ML’s inferencer will bring in type variables. Note how clever it is: fun I x = x; &#39;a -&gt; &#39;a fun p x y = y &#39;a -&gt; &#39;b -&gt; &#39;b fun first (x, y) = x; &#39;a * &#39;b -&gt; &#39;a fun q x = (hd o hd) x; &#39;a list list -&gt; &#39;a fun c x y = if x = y then &#34;y&#34; else &#34;n&#34;; &#39;&#39;a -&gt; &#39;&#39;a -&gt; string In ML, a type variable beginning with two primes can only be instantiated with a type that admits equality. ML’s inferencing is more powerful than that of other languages, Go, Rust, and Scala require that you put types on parameters. scala&gt; var x = 3; x: Int = 3 scala&gt; var y = x + 3; y: Int = 6 scala&gt; def f(x) = x + 3; &lt;console&gt;:1: error: &#39;:&#39; expected but &#39;)&#39; found. def f(x) = x + 3; ^ scala&gt; def f(x:Int) = x + 3; f: (x: Int)Int Go example. Rust example. Exercise: Two things complicate type inference are overloading and coercion. Give concrete examples of complications that arise for each. Type Checking What if you use an expression in a manner inconsistent with its type, like trying to compute the age of a string (instead of a person). Should the evaluation result in an error, or return a &#34;best guess&#34;? Strong Typing vs. Weak Typing Strong TypingWeak Typing Type clashes among unrelated types result in errors. they may be compile time, or even run time (e.g. NoSuchMethodError, or ClassCastException) but they are errors. Type clashes don’t really exist... the language implementation will try to cast the argument into some reasonable (!) type and carry out the operation. Alternate definition: you can’t really circumvent the type system. Example: In Java, even if you explicitly add a cast to cast a string to an int, you get an error. Alternate definition: you can easily circumvent the type system. Example: In C++, you can cast a pointer to an int to a void*, then to a string*, and you get away with it. Some people would argue strong vs. weak typing isn’t a terribly useful thing to worry about. the static/dynamic dimension is a bigger deal. Static Typing vs. Dynamic Typing Static TypingDynamic Typing Type checking done at compile time Once a variable’s type is known, it can only be assigned expressions of that type (including related types that can be coerced to it, of course). Type checking done at run time Because checking is deferred until run-time a variable can be assigned an expression of one type and then later an expression of another type. var x; x = 2; print x + 5; x = &#34;dog&#34;; print concat(x, &#34;house&#34;); It is common for languages to have some static typing and some dynamic typing. Manifest Typing vs. Implicit Typing Manifest TypingImplicit Typing Types of variables must be given in their declarations. void f(int x, list&lt;int&gt; z) { int y = x * x; z.push_back(y); } the types of variables will be inferred from context fun f x z = let y = x * x in z @ [y] end x must be an int because of the &#34;*&#34; operator, so y must be an int as well, and then z must be an int list, and finally f must be int -&gt; int list -&gt; int list. What about some popular languages? Common Lisp: strong, dynamic, implicit Ada: strong, static, manifest Pascal: strong, almost static, manifest Java: strong, some static and some dynamic, manifest ML: strong, static, implicit JavaScript and Perl: weak, dynamic Ruby and Python: strong, dynamic Exercise: Classify Classic Lisp, Smalltalk, Go, and Rust Declarations and Static Typing In order to do static typing, you must sometimes specify types when declaring variables, functions, parameters, etc. How is this done? A comparison of C and Go Exercise: Rewrite the examples in the C and Go comparision article in Rust. Type Checking with Type Variables Suppose f has type &#39;a * int -&gt; &#39;b * &#39;b -&gt; string. then the following expressions are legal f(4, 5)(&#34;sd&#34;,&#34;de&#34;) f(1,1)(2,2) f([],5)([],[4,4]) but these are not f(3,2)(4,&#34;p&#34;) f((3,2),5)([5.5,2.2],[1,6]) Further Reading the literature on strong/weak, static/dynamic, and manifest/implicit typing is enormous, as are the debates and flame wars. Some good reading: Mark Jason Dominus on Strong Typing in Perl (awesome presentation) Wikipedia articles on data types and type systems Dependent Types A dependent type is a type whose definition depends on a value. Examples: Arrays of length n Integers less than 8 Pairs of integers that sum to 21 Does C++ have dependent types? I mean, you can do: template &lt;int n&gt; class BoundedList { public: int contents[n]; } but all types that instantiate this template are created at compile time. Languages that fully support dynamic dependent types can eliminate many logic errors at compile time! (The type system essentially lets you formulate compile-time proofs that a program will have a certain run time behavior, in terms of the values it will produce.) Start your study of dependent types at this Stack Overflow question then go to Wikipedia. Overview of Common Types Numeric Types Lots of variety here: Integer or Floating point or fixed point or ratios Integer: Signed or unsigned Integer: Saturated or unsaturated Integer: Fixed size (8, 16, 32, 64, 128) or unbounded (&#34;BigInt&#34;) Ratios are also called Rationals Some languages have a particular kind of ratio type called Decimal Floats: by size (32, 64, 80, 128) Enumerations In type theory, an enumeration is a type with a fixed number of distinct values, ordered from smallest to largest. Typical examples are: type TrafficLight is (RED, AMBER, GREEN) type Direction is (NORTH, EAST, SOUTH, WEST) The Boolean type is an enum of the values False and True. Many languages have a character type which is essentially an enumeration. Each character’s &#34;value&#34; is its code point. Ranges Some languages allow you to make types such as: range(1, 100) range(3, 900, 5) 1..100 1...100 1..&lt;100 Sum Types and Product Types Suppose we have two types, Boolean and Byte, where: Boolean has the values {false, true} Byte has the values {-128, -127, ... 126, 127} Now we form new types: Boolean + Byte = {false, true, -128, -127, -126, ..., 126, 127} Boolean × Byte = {(false, -128), (false, -127), ... (false, 127), (true, -128), (true, -127), ..., true(true, 127} Note Boolean has 2 values, Byte has 256 values. Boolean + Byte has 2+256 = 258 values, and Boolean × Byte has 2 * 256 = 512 values. Boolean + Byte is a sum type and Boolean × Byte is a product type. Product types are very common; Python calls them tuples. Haskell and ML just let you write the product type name directly, e.g. Int * String. Often sums and products can be tagged. Option Types Option types are one solution to the billion dollar mistake. Basically an object of type T? is pretty much the sum type of T and the type containing a single value representing null. Records, a.k.a. Structs Tagged product types go by many names: record, struct, object, hash, etc. Values of such types are thought of as key-value pairs. Named fields, order may or may not matter Memory layout often contiguous, but may have holes for alignment reasons, with some languages giving you explicit control over this (Ada has pragmas for it) Compilers may rearrange fields Usually you can copy but not compare Copy can be bitwise, shallow, or deep Unions Cheap way to make a type whose instances can have different forms Also known as variant records Generally implemented with overlaying (Fortran uses EQUIVALENCE) Not needed in languages with subclassing (or with a flexible datatype declaration like ML’s) Language design choice: is the variant part &#34;tagged&#34;? If not (C is a good example of this), the whole idea of strong typing goes out the window If it is tagged, several design choices... the tag could be permanent once set Changing the tag could invalidate the object We could prohibit changing just the tag and require a full reassignment (Ada, Algol68) datatype primary_color = Red | Blue | Green Red primary_color (Red, 4) primary_color * int Arrays Usually by array we mean a collection of elements indexed by integers or some other enumerated type, that has constant time access of any element given its index. Contrast this with a list, which needn’t have constant time access, and with a map (a.k.a dictionary or associative array), which can be indexed by anything, not just integers or related enumerated types. Array Bounds Normally, array &#34;types&#34; are unbounded, but array instances have bounds. Is accessing an array outside of its bounds an error? Or does the array object expand to accommodate? C doesn’t really have arrays! C &#34;arrays&#34; are just pointers and don’t know about bounds or not. Use at your own risk, and be careful! If the bounds are not known at compile time, we generally make use of a dope vector. (Note that with dope vectors, like any other indirect storage mechanism, watch out for shallow vs. deep copy and equality testing.) If the bounds are known at compile time, compilers can generate code for element access that uses simple algebraic transforms to compute as much up-front (at compile time) as possible Lifetime and Shape   Static Bounds Bounds Set at Elaboration Bounds Can be Changed(Dynamic Shape) GlobalLifetime C globals LocalLifetime C locals Ada locals ArbitraryLifetime Java Perl, APL Local lifetime / Bounds set at elaboration category is interesting: one can use the alloca syscall to implement these, giving you automatic dealloaction on block exit. Implementation Column Major Row Major (allows 2-D arrays to be the same as arrays of arrays) Row Pointers — necessary for ragged arrays, good on machines with small segments. Slices Several languages (Perl, APL, Fortran 90) have nice syntax for slices. Fortran 90 allows (these examples are from Scott): matrix(3:6, 4:7) columns 3-6, rows 4-7 matrix(6:, 5) columns 6-end, row 5 matrix(:4, 2:8:2) columns 1-4, every other row from 2-8 matrix(:, /2, 5, 9/) all columns, rows 2, 5, and 9 Exercise: Give the equivalent expressions in Julia. Exercise: How are these represented in Python’s numpy library? Good to know: Go uses the term “slice“ to mean something else. Strings Most (all?) languages have them, but care is needed: Are often built-in Usually have easy to use literal forms, with escape characters May be immutable May be interchangeable with character arrays If mutable, are usually resizable (except in old Pascal May or may not allow newlines or control characters Interpolation may or may not be supported Length is ill-defined! (Number of bytes? number of characters? number of code points? number of graphemes? Indexing may also be difficult Sets Mathematically, a set is an unordered collection of unique elements. Usually aren’t built-in but most libraries have them May be mutable or immutable (Python has set and frozenset Implemented with search trees, tries, hashtables or bitsets Pointers A pointer is, more or less, an object through which you reference some other object. They appear in system languages and low level languages. int x = 5; int* p = &amp;x; int* q = NULL; int* r = new int; int* s = new int[100]; cout &lt;&lt; *p &lt;&lt; *r &lt;&lt; s[20]; cout &lt;&lt; *q; // crash Basics Pointers are used for dynamic, linked data structures. Pointers are at a higher level of abstraction than addresses since address computation and error checks can be done on the fly, and in some languages you can overload the dereference operators. In some languages, pointers can refer only to heap-allocated objects. In many languages (Java, LISP, ML) pointers are implicit. Example from Lisp: (R (X () ()) (Y (Z ()()) (W ()()))) And from ML: datatype tree = empty | node of &#39;a * &#39;a tree * &#39;a tree; val x_zw = node (&#39;R&#39;, node (&#39;X&#39;, empty, empty), node (&#39;Y&#39;, node (&#39;Z&#39;, empty, empty), node (&#39;W&#39;, empty, empty))); Reference and Dereference Syntax Can be always explicit, always implicit, or sometimes-implicit (like in Ada). If the pointer is called p (or $p in Perl), then Inthe referent is calledand the field is called C*p(*p).x or p-&gt;x Adap.allp.all.x or p.x Modula-2p^p^.x Javapp.x Perl%$p$$p{x} or $p-&gt;{x} If the referent is called p (or %p in Perl), then Inthe pointer is calledand the field is called C&amp;pp.x Adap&#39;accessp.x Perl\%p$p{x} Memory Leaks and Dangling Pointers In languages like C requiring explicit allocation and explicit deallocation, it is possible to: detach references to heap-allocated objects (memory leak) deallocate an objected through a shared pointer (dangling pointer) How can we prevent such things? Prohibit explicit deallocation and use a garbage collector (yeah but this is a cop out sometimes) Give programmers some nice tools and conventions, like C++ (this does NOT solve the problem) Put in run-time checks to ensure pointers cannot outlive referenced objects (expensive?) Put in compile-time checks to ensure pointers cannot outlive referenced objects (a la Rust) Garbage Collection Covered separately. For now, you can read Wikipedia’s article on garbage collection, and an interesting developerworks article on Java performance and garbage collection. Pointers and Arrays in C Sometimes, these ideas get conflated in C. After all: e1[e2] is the same as *(e1 + e2) For definitions these are completely different things: int *x; /* is totally different from: */ int x[100]; int *a[n]; /* is totally different from: */ int a[n][100]; But for declarations, at least in parameter declarations, you can blur the distinction: void f(int* a) { ... } void g(int b[]) { ... } An array of ints, or pointer to an int, can be passed to either. Streams The term stream is a bit overloaded. Abstractly, a stream: is an object that can be read from and written to, usually sequentially, but sometimes with &#34;random access&#34; is called, if read-only, called a &#34;source&#34;; if write-only, a &#34;sink&#34; is an excellent abstraction; in practice streams are attached to memory buffers, network connections, database connections, files, pipes, whatever.... can be made by pasting two streams together. Sometimes we use the term stream to refer to files. In Java, streams are their own thing. They work like the streams described above (they are processed sequentially), but are in a pretty large subsystem of their own. Regular Expressions Covered separately. Subroutines Subroutines are often objects with their own types. We’ll see these later when we discuss subroutines. Processes and threads Will be covered later when we discuss concurrency. Orthogonality If a type system is orthogonal, then no type is more special and capable than others. The questions you want to ask to see whether a system is orthogonal or not are: Can an object of any type: be represented as a literal? be declared at all? have an initializer? be assigned to? be passed as a parameter? be returned from a function? You might be surprised at how un-orthogonal some language’s type systems are. A related question is: Are types themselves objects? they aren’t in JavaScript, C, C++, Erlang, or Elixir. Python has a type called type, so yes there. Java, Ruby, and many other languages have a class called Class, so yes there too. It’s sometimes undesirable, or maybe even impossible, to get true orthogonality, since: Expression oriented languages need an empty type and other languages want to ignore them. First class subroutines might be difficult to implement or too slow in languages that claim to be fast. Some languages allow arrays of anything; others can store only scalars? Array indexing really can only be done by discrete types. Member, Local, and Anonymous classes in Java don’t act like top-level ones. Should blocks of code be values? Simple languages just take short cuts: did you know, in Pascal, you can’t return complex objects from functions? </description>
      <pubDate>17 Jul 20 22:20 EDT</pubDate>
      <guid>https://cs.lmu.edu/~ray/notes/types/</guid>
    </item>
    <item>
      <title></title>
      <link>https://reasonablypolymorphic.com/blog/writing-technical-posts/</link>
      <description>&lt;a href=&#34;https://reasonablypolymorphic.com/blog/writing-technical-posts/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; One of today’s best bloggers in the functional programming space is Chris Penner. Despite this, Chris is criminally underrated—nobody I talk to seems to know of his work. Go check him out as soon as you finish reading this post! This is reasonably pervasive phenomenon in our subculture; there’s lots of fantastic work being produced, but for one reason or another, it falls between the cracks. I’d propose the biggest obstacle is that most FP writing isn’t optimized to be read. So I’d like to take some time today and talk about common failure modes in technical writing. If you don’t have a checklist you run through before publishing a post, you’ll probably get a lot of benefit from internalizing this advice. The value? You’ll make it easier for people to understand what you’re trying to tell them. Which is why you’re writing in the first place, right? The good news is that none of this is magic—just some simple guidelines for structuring your content. The Guiding Principle Here’s the biggest thing to keep in mind: your reader doesn’t really care what you have to say. You get maybe four sentences to convince them that your essay is worth their time. If you haven’t made your case by then, you’ve probably lost them to the next tab they binge-opened. Even after you’ve convinced them that it’s a valuable read, you need to continue assuring them that you’re not wasting their time. If you take too long to get to the point, or if you make the information too hard to find on a quick skim, you’ve lost them again. As a result, alongside your technical content, you have two primary goals to focus on: Provide strong, concise motivations for everything you want to talk about. Make it easy to skim. If you can do these two things, you’re already most of the way to better engagement. Writing Strong Motivations People care about solutions to problems they have, or expect to have one day. They care about other things too, but it’s not really relevant to us today. With this in mind, you want to tailor your motivations in terms of solutions to problems. Here are some good examples of problems that you’ve probably run into (and links to their solutions for you to read later): Ed Kmett writes shitty documentation I have lots of types that need to evolve at the same time There is no good place to learn type-level programming Bad examples of motivating documents are things that assume you care simply because they exist. This is pretty common of libraries’ tutorials. The same document that convinces you to use a technology should also show you how. As a more general rule, focusing on why is going to be more valuable than on how. Why should someone care, and why your solution is a good one. “How” without a “why” suggests a contrived solution to a made-up problem. In other words, it’s easily read as “who cares?” Understanding How People Read People who spend lots of time reading have good heuristics for skipping lots of text. If you understand these heuristics, you can make it easy for people to find what they’re looking for. And remember: they’re looking for reasons to continue reading. There are two behaviors here. The first is what I call “skimming for concepts”—which is that people will attempt to determine what text is about at a high-level. They’re looking for what you’re trying to tell them, as opposed to what you’re actually saying. When skimming, people are likely to read only the headings and the first two sentences of a paragraph. If they’re convinced they know what you have to say, they’ll skip to the next paragraph. If several paragraphs in a row don’t seem relevant, they’ll skip to the next heading. If the next heading also fails to grab their attention, they’ll probably give up. The solution is to structure your argument as a tree. Headings must provide enough information to let someone know whether or not they care about the following prose. This also means that the first sentence of each paragraph should be sufficient to understand the rest of the paragraph. The remaining sentences are only allowed to reinforce or give details of the first sentence. One paragraph equals one idea. Roughly speaking, the hierarchy of your document should look like this: Document Heading First sentence Rest of paragraph Maybe you feel like it’s hard to know how much knowledge to assume your readership knows. Understanding how people read makes this a non-issue. Present as much information as is necessary to understand your point, but make it easy to skip if people already know it. Those who don’t yet know will learn, those who do can skip past, and both camps will appreciate it. After you’ve convinced your reader that they care what you have to say, they’re more willing to read what you actually have to say. When the reader is ready to dive in, that’s when you can make the finer points of your argument. It’s unlikely that anyone is going to read every word of your essay. It’s even less likely that they’ll read them all in order. Stumbling Blocks Now that you’ve got people ready to listen to you, it’s important to keep them on the same page as you. You really need to stay on top of anything that might break their focus. Use short sentences. If it’s too hard to parse, people won’t. Make sure your spelling and grammar have no egregious problems. You don’t need to have perfect command of the language, but you need to be able to signal that you’re trustworthy-enough to listen to. Don’t underestimate how much credibility you’ll lose from blatantly bad spelling and grammar. This stuff is relatively common-sense. What might be less so is that you also need to stay on top of conceptual stumbling blocks. If your argument makes a jump that feels poorly motivated or references something potentially unfamiliar, expect that your reader will experience vertigo. Most of the ideas we talk about in functional programming are not easy, and it doesn’t do anyone any favors to pretend this isn’t so. Expect that your readers will be pretty similar to you; if you had a problem understanding a piece of your topic, call that out. Point out the obstacle. Point out what they might be thinking, and then very explicitly show them what they should be thinking instead. For example, if your code sample uses a functional dependency, it might be worth a short sentence saying “that funny bar thing is a functional dependency.” Give a tiny summary of its purpose, and provide a link for them to learn more if they need to. Another illustration: if you originally got confused that an “algebra in recursion schemes” is not the same thing as the algebra you learned in high-school, your readers probably will too. The first time you say “algebra” in the new context, say “this is a misleading term. This has nothing to do with solving equations like you did in high-school.” More important than what you’re saying is what you’re not saying. If you’re giving examples of something that fits a pattern, make sure you give examples of things that do not fit the pattern. A concept that is applicable everywhere is useful nowhere. Give lots of examples. Nobody I know learns via mathematical statements, nor do they learn via long, wordy, abstract prose. People learn by seeing lots of examples, and being told explicitly how these examples relate to one another. The mathematical statements are only useful after you have the intuition, so save them for an appropriate time. Conclusion The takeaway advice from this essay is that if you want lots of readers, you must make it easy for them to read. To that end, pay lots of attention to motivation. Why should people care what you have to say? What value does it give them? Focus your energy on the beginnings—both of the essay as a whole, and of each paragraph. People who are unconvinced by your essay’s value will skim their way through it, and they will do that by reading only the beginnings of things. Make your information easy to find. Structure your argument in a tree, and make sure it supports random-access. Nobody is going to read the whole thing from start to finish. Instead they’re going to jump around, ignoring the pieces they already know, and looking for the bits they don’t. Use lots of short paragraphs. A paragraph should correspond to a single idea. Anticipate which parts of your argument will be difficult for your readers, and be proactive in trying to assuage those difficulties. Give lots of examples of what you’re talking about, and more importantly, what you’re not talking about. Point out where you went into the weeds while learning this stuff. Steer readers away from common mistakes and misconceptions. And finally, end on a high note. Leave people with a good feeling, and it will incentivize them to get to the bottom of your next piece. Inspirational calls to action are a good way to go out. Tell them what you’re going to tell them. Then tell them. Finally, tell them what you told them. -Unknown Funny and poignant quotes are too. ← → </description>
      <pubDate>07 Feb 21 15:12 EST</pubDate>
      <guid>https://reasonablypolymorphic.com/blog/writing-technical-posts/</guid>
    </item>
    <item>
      <title></title>
      <link>https://medium.com/clarityhub/low-coupling-high-cohesion-3610e35ac4a6</link>
      <description>&lt;a href=&#34;https://medium.com/clarityhub/low-coupling-high-cohesion-3610e35ac4a6&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Low Coupling, High CohesionThe key to creating maintainable code is adhering to “low coupling, high cohesion”.But what exactly does this mean? At what point is your code loosely coupled and highly cohesive?In this article, I refer to “modules” which represent almost any sort of language construct you have. In Object Oriented languages, this may represent classes. In JavaScript, it may represent actual packages or domains.Low CouplingHow much do your different modules depend on each other?Modules should be as independent as possible from other modules, so that changes to module don’t heavily impact other modules.High coupling would mean that your module knows the way too much about the inner workings of other modules. Modules that know too much about other modules make changes hard to coordinate and make modules brittle. If Module A knows too much about Module B, changes to the internals of Module B may break functionality in Module A.By aiming for low coupling, you can easily make changes to the internals of modules without worrying about their impact on other modules in the system. Low coupling also makes it easier to design, write, and test code since our modules are not interdependent on each other. We also get the benefit of easy to reuse and compose-able modules. Problems are also isolated to small, self-contained units of code.High CohesionWe want to design components that are self-contained: independent, and with a single, well-defined purpose— The Pragmatic ProgrammerCohesion often refers to how the elements of a module belong together. Related code should be close to each other to make it highly cohesive.Easy to maintain code usually has high cohesion. The elements within the module are directly related to the functionality that module is meant to provide. By keeping high cohesion within our code, we end up trying DRY code and reduce duplication of knowledge in our modules. We can easily design, write, and test our code since the code for a module is all located together and works together.Low cohesion would mean that the code that makes up some functionality is spread out all over your code-base. Not only is it hard to discover what code is related to your module, it is difficult to jump between different modules and keep track of all the code in your head.Maintainable CodeWriting maintainable code helps increase productivity for developers. Having highly maintainable code makes it easier to design new features and write code. Modular, component-based, and layered code increases productivity and reduces risk when making changes.By keeping code loosely coupled, we can write code within one module without impacting other modules. And by keeping code cohesive, we make it easier to write DRY code that is easy to work with.A good way to determine how cohesive and coupled your code is, is illistrated by this quote from The Pragmatic Programmer:When you come across a problem, assess how localized the fix is. Do you change just one module, or are the changes scattered throughout the entire system? When you make a change, does it fix everything, or do other problems mysteriously arise?While you are writing and working with your code base, ask yourself:How many modules am I touching to fix this or create this functionality?How many different places does this change need to take place?How hard is it to test my code?Can we improve this by making code more loosely coupled? Can this be improved by making our code more cohesive?🎉 Join Our Weekly NewsletterAt Clarity Hub, we empower developers and the open-source community with our newsletter, articles, and open-source contributions. Join our newsletter to get a weekly dose of hot issues in the open source community, grow your programming skills, and improve your resume!</description>
      <pubDate>08 Mar 21 09:12 EST</pubDate>
      <guid>https://medium.com/clarityhub/low-coupling-high-cohesion-3610e35ac4a6</guid>
    </item>
    <item>
      <title>Never attribute to stupidity that which is adequately explained by opportunity cost</title>
      <link>https://erikbern.com/2020/03/10/never-attribute-to-stupidity-that-which-is-adequately-explained-by-opportunity-cost.html</link>
      <description>&lt;a href=&#34;https://erikbern.com/2020/03/10/never-attribute-to-stupidity-that-which-is-adequately-explained-by-opportunity-cost.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 2020-03-10 Hanlon&#39;s razor is a classic aphorism I&#39;m sure you have heard before: Never attribute to malice that which can be adequately explained by stupidity. I&#39;ve found that neither malice nor stupidity is the most common reason when you don&#39;t understand why something is in a certain way. Instead, the root cause is probably just that they didn&#39;t have time yet. This happens all the time at startups (maybe a bit less at big companies, for reasons I&#39;ll get back to). Some examples of things I hear all the time: I don&#39;t understand why team X isn&#39;t working on feature idea Y. It&#39;s such an obvious thing to do! Why is bug Z still present? Hasn&#39;t it been known for a really long time? I don&#39;t understand why they aren&#39;t fixing it? I don&#39;t get why HR team still doesn&#39;t offer perk W. So many other companies have that! I&#39;ve told my manager that we need a process for thing V but it still hasn&#39;t happened! Of course, why these things never happened is that something else was more important. Quoting Wikipedia on Opportunity cost: When an option is chosen from alternatives, the opportunity cost is the “cost” incurred by not enjoying the benefit associated with the best alternative choice. The New Oxford American Dictionary defines it as “the loss of potential gain from other alternatives when one alternative is chosen.” In simple terms, opportunity cost is the benefit not received as a result of not selecting the next best option. Thus I&#39;ve started telling people: Never attribute to stupidity that which is adequately explained by opportunity cost. Your friends aren&#39;t stupid, just busy It might seem obvious the way I put it that opportunity cost is the thing to blame. But it&#39;s not. Human psychology works in weird ways. People love to conclude that something wasn&#39;t done because they are stupid, or possibly lazy. This happens about 95% of the time when you don&#39;t know a certain person/team. The team that works on the CI/CD system is just a faceless blob in a different office and they must be completely stupid for never fixing the super annoying button in the interface. What are they doing all day! When you know a certain team/person, the percentage drops a bit: maybe stupidity is the first thought only 40% of the time. I&#39;m obviously just making up these numbers, but you get the point. I spent many years working in a satellite office. A lot of the time when I had some deep technical disagreement about something, I&#39;d fly to the main HQ, and go out for dinner with the other team. I wouldn&#39;t even talk about technology, just about random stuff. Once they knew me, and realize I was a human (and not a faceless blob), most technical disagreements tended to go away. People are more likely to assume positive intent and not malice/stupidity/laziness. Ruthless prioritization Erring on the side of assuming opportunity cost as cause precludes any evil/stupid/lazy narrative: the team/person probably actually just had other more important priorities. But how did they pick those? I&#39;m not going to offer any smart advice on how to estimate effort and impact in your JIRA board. There&#39;s a trillion methods and tools and processes for doing that. But what I have come to believe is that: prioritization is the most value creating activity in any company. Generating ideas and executing things is of course also important! But what I&#39;ve seen to set apart great teams from good is a brutal focus on prioritization. This means generating an absurd amount of ideas and throwing 99% of them out of the window, to focus on the 1% that have the highest impact. Ideas will be generated much faster than there&#39;s bandwidth to execute on them, so you&#39;re doing something right if your backlog is growing indefinitely. A negative person on a mediocre team will complain that there&#39;s never time to work on their favorite pet project X. I&#39;ve often heard things like “our backlog of features keeps growing so fast, how are we ever going to have time to invest in paying down tech debt?&#34;. To me this reflects a misunderstanding of how product development should work. Backlogs should be growing indefinitely. What a good team will do is to accept that, and establish a good relationship between product and tech, and make sure you constantly keep reprioritizing. Maybe today it&#39;s shipping a bunch of features the business needs. Maybe tomorrow it&#39;s paying down some tech debt. If you have a shared framework for how to think about value and prioritization, it usually works out. Opportunity cost matters a lot less at bigcos I mentioned in passing that opportunity cost is a likely cause at startups, but maybe less often at big companies. Why? Because a startup is often playing catch up building things that are mostly “obvious”. When you get to a very late stage, and you have a lot of money and a lot of developers, things get a lot more tricky. You can&#39;t do X because it will cannibalize metric Y and upset advertisers. You can&#39;t do Z because that would be inconsistent with how Q works. And so on. Ultra-scientific study of company priorities. Final statement We started with one seemingly innocuous statement, but it quickly led to many corollaries: how to trust people, how to manage the backlog, how prioritization can generate tremendous value. Fun! I often keep quoting the paraphrased quote to people, but people have no clue what I&#39;m talking about. Hopefully going forward, there&#39;s this blog post to refer people to! Edit: this post was on the front page of Hacker News and generated some comments. Tagged with: management, popular </description>
      <pubDate>10 Mar 21 08:30 EST</pubDate>
      <guid>https://erikbern.com/2020/03/10/never-attribute-to-stupidity-that-which-is-adequately-explained-by-opportunity-cost.html</guid>
    </item>
    <item>
      <title>better geometry through graph theory</title>
      <link>https://ideolalia.com/2018/08/28/artifex.html</link>
      <description>&lt;a href=&#34;https://ideolalia.com/2018/08/28/artifex.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; how I spent my summer A few months ago, I decided to implement set operations on curved regions. I had the the canonical textbook on computational geometry, which described approaches for polygons comprised of straight lines, and it seemed like other projects had extended these techniques to parametric curves. I figured it would take a couple of weeks. Unfortunately, the field of computational geometry embodies a fundamental contradiction. In geometry, the angles of a triangle add up to exactly π radians, and if u is clockwise from v then v must be counter-clockwise from u. Computers, on the other hand, use floating point representations which make a mockery of these simple Euclidean truths. The academic literature largely ignores this. Algorithms are proven to be geometrically sound, and robust implementations are left as an exercise for the reader. This is akin to a world in which hash collisions caused unavoidable data loss, and the academic response was to implicitly assume the existence of a perfect hash function. If a paper is predicated on unrealistic assumptions, we cannot evaluate it on its own terms; we must understand, empirically, how well it functions when these assumptions are broken. With this in mind, we can now look at the existing algorithms for polygon clipping, which is the term of art for polygon set operations. Every technique is a variation on a common theme: Given two or more rings, find every point of intersection Segment the rings at the points of intersection Decide whether each segment should be included, based on the operation being performed The dozen or so papers on this subject differ only in the third step. Since our decision to include a segment typically inverts at an intersection point, they describe a variety of approaches for using our decision to about one segment to inform our decision about adjacent segments. My textbook described a method using doubly-connected edge lists, which is a generic geometric data structure. I assumed that meant it could be reused for other problems, so I started my implementation. A month went by. I had finished the implementation my first week, but it wasn’t reliable. A DCEL is a collection of linked loops, which can be incrementally updated. When performing a set operation, we incrementally bisect the original set of loops, and then determine which should and shouldn’t be included. Despite my best efforts, I kept finding new shapes that caused adjacent faces to get tangled together, creating a Möbius strip that is simultaneously inside and outside the expected result. Slowly, I realized the problem wasn’t the data structure, it was the first step that every paper glossed over: finding all the intersections. The DCEL assumes the edges at a vertex have a total ordering: the previous edge is directly clockwise, and the next one is directly counter-clockwise. If we miss an intersection, we might conclude two curves are both clockwise relative to the other, causing everything to fall apart. I began to look for better ways to find intersections, hoping that if I found an approach that was sufficiently accurate, my work on the DCEL could be salvaged. Unfortunately, the approaches I found in the literature and implemented in the wild were no better than what I had been using. My data structure demanded precise inputs, without internal contradictions, and I couldn’t deliver. At that point, I began to wonder if I had missed something fundamental. I thought maybe if I dissected how other, more mature, libraries handled my pathological shapes, I could work backwards to see where I had gone wrong. But when I began to feed these shapes into well-established projects like paper.js, I found they failed just as badly. To find my pathological inputs, I had been using property-based testing. Given a random combination of shapes, I would perform a point-in-region test and compare it to a reference result, generated by querying each shape individually and combining the results according to the operation. Most inputs worked fine, but after a few hundred thousand inputs it would inevitably find some failure. Other projects, it turned out, had been a little less eager to find their own failure modes. Some only had a handful of example-based tests, others had a static suite of a few thousand inputs they used to validate their changes. If I had missed something, it appeared to be that no one else expected these operations to be particularly robust. why is this so hard? Floating point arithmetic is best understood through a simple, maddening fact: a + (b - a) does not necessarily equal b. It might be equal, or it might be off by just a little, where “little” is relative to the larger of the two numbers. This means that when we compare two floating point numbers, we cannot do a precise comparison, we have to ask whether they differ by less than some epsilon value. This epsilon represents the level of numerical uncertainty to which we’ve resigned ourselves. There is vast folk wisdom around how to minimize this uncertainty, but the fact remains that every arithmetic operation injects a bit of uncertainty, and it grows cumulatively with each successive operation. When dealing with small numbers, this uncertainty may dwarf the values themselves. An intersection, in a precise mathematical sense, occurs wherever the distance between the curves is exactly zero: But in a practical sense, it is wherever the distance between the curves is close enough to zero: This has at least one intersection, but we could just as easily return three or ten within that overlapping range. This uncertainty is anathema to the published techniques, which rely on counting these intersections to determine whether we’re inside or outside the other shape. A single spurious intersection may cause the entire result to vanish. If two objects with similar curvature move across each other, the result will tend to flicker in and out of existence. These techniques may suffice for straight lines, which require smaller epsilons, but they are wholly unsuited to the relative imprecision of parametric curves. embracing the uncertainty As the weeks passed, the errors uncovered by my tests went from feeling random to feeling malicious. Floating point arithmetic may be deterministic, but as I watched my screen, waiting for the tests to fail, I imagined a demon in the FPU nudging the values as they flowed past, trying to move them beyond the threshold of self-consistency. One day, I realized this was exactly the narrative behind error-correcting codes; we assume our data has been randomly altered en route, and we want to return it to a consistent state. It didn’t seem like I could get it right the first time, so why not just fix it afterwards? Consider the union of two ellipses: Ostensibly, there should only be three points of intersection, one on the left and two on the right. But for the reasons described above, any intersection routine will likely find multiple points of intersection on the left as the curves converge on each other: The segments on the left are small enough, and thus imprecise enough, that our spatial intuition for the problem will just mislead us. For this reason, it’s better to think of it as a graph: Now we have to decide which edges to include, and which to exclude. Since we’re trying to find the union, we want to keep any segments that are outside the other shape: This is a well-formed result; there is a single cycle, and once we remove that cycle there are no leftover edges. But we can’t rely on getting this lucky; the edges on the left might have succumbed to floating point error and believed they were both inside the other: This is not a well-formed result; there are no cycles, and a bunch of leftover edges. To make this consistent, we need to either close the loop, or remove the leftovers. The cost of these changes is measured by the aggregate length of the edges we are adding or removing. The minimal set of changes is equivalent to the shortest path between the dangling vertices. Having found the path, we then invert the inclusion of every edge it passes through. In this case, it passes through one of the edges we originally excluded, so we add that edge back in, and return the cycle we just created. Alternately, both edges might think they are outside the other: In this case, we have a complete cycle, but once we’ve extracted it there’s a single leftover edge: Here again, we search through all the remaining edges for the shortest path between the two dangling vertices. Since it passes through our leftover edge, we remove it. Every floating point inconsistency will surface as one of these two cases, or some combination thereof. By searching for the shortest path between the dangling edges, we find the smallest possible edit that will yield a consistent result. Of course, a consistent result is not necessarily the correct one, but the fact that floating point errors tend to cluster around the smallest edges makes this a reasonable heuristic. More importantly, it has weathered tens of millions of generative test cases without any issues. A complete implementation of this algorithm can be found here. I’m not sure if this is a novel approach, but at the very least it represents a meaningful improvement on the state of the art in open source. Intuitively, it feels like this might be a means to avoid epsilon hell in a wide range of geometric and numerical algorithms. If anyone is aware of prior art in this vein, I’d be very interested to see it. My work on the Artifex library is ongoing, but I hope it proves useful to others, and look forward to sharing my own projects that it will enable in the near future. Thanks to Alex Engelberg, Elana Hashman, Angus Fletcher, Reid McKenzie, and Zack Maril for feedback on early drafts of this post. </description>
      <pubDate>06 Dec 20 18:48 EST</pubDate>
      <guid>https://ideolalia.com/2018/08/28/artifex.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.documentjournal.com/2021/01/the-internet-didnt-kill-counterculture-you-just-wont-find-it-on-instagram/</link>
      <description>&lt;a href=&#34;https://www.documentjournal.com/2021/01/the-internet-didnt-kill-counterculture-you-just-wont-find-it-on-instagram/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Text by Caroline Busta Photography by Sebastian Lager Posted January 14, 2021 “To be truly countercultural in a time of tech hegemony, one has to, above all, betray the platform which may come in the form of betraying or divesting from your public online self.” Search Google Images for ‘counterculture’ and it overwhelmingly returns black-and-white photos of young people all now over 60. In the pictures, it is so clear what they were countering: The Man, of course, who, with his white collar, white skin, and short hair, singlehandedly symbolized dominant cultural norms. In the age of social media, personal expression has become the most valuable form of currency, yet we still use the term ‘counterculture’ to describe alternatives to the hegemonic forces of yesteryear, as if dressing middle-class, white, and preppy still aligned with the rules of power today. In an era more profoundly organized by Big Tech than our own elected governments, the new culture to be countered isn’t singular or top-down. It’s rhizomatic, nonbinary, and includes all who live within the Google/Apple/Facebook/ Amazon digital ecosystem (aka GAFA stack). With digital platforms transforming legacy countercultural activity into profitable, high-engagement content, being countercultural no longer means being counter-hegemonic. What logic could possibly be upended by punks, goths, gabbers, or neo-pagans when the internet, a massively lucrative space of capitalization, profits off the personal expression and political conflict of its users? As recently as the early ’90s, abjection and extreme profanity still worked pretty well to repel the big social threats of the time: pearl-clutching conservatives with their anti-progressive ideology and market recuperation. Take, for instance, musician GG Allin in an American-flag loincloth, fighting with his audience and shitting on stage before launching into a performance of “I’m Gonna Rape You,” or artist and noise musician Boyd Rice, in what he reports was a prank, joining the founder of the white supremacist group American Front in a 1989 Sassy photo shoot for an article the teen magazine was running on neo-Nazis. In context, these artists (like the psychedelic hippies of yore) were being literally countercultural—using culture against itself to violate the hegemonic push toward, in Allin’s and Rice’s case, neoliberal “responsibilization.” In today’s online space, however, this strategy breaks down. Brought back into the spotlight in 2018 via a NYC gallery exhibition of visually innocuous abstract paintings, Rice quickly found himself at the center of controversy as his decades-old Sassy appearance (among other such stunts) tripped present-day censors. An old punk, he smirked at the outrage. “I’m too dangerous for New York City,” he told Artnet. Yet he wasn’t too dangerous for the internet. High-tension discussion of his work and life and the gallerist’s moral compass raged online, which is to say Rice was attentionally successful online. Despite being informed by billions, this new technological hegemony isn’t democratic; it’s a swarm-led form of para-governance programmed to maximize engagement while obfuscating responsibility for the social and environmental damage it wreaks. Zuckerberg, Bezos, Thiel, and other tech behemoths are quick to remind us that they’re not in charge of public laws or policy; their empires were built according to the “peaceful mechanisms” of free-market capitalism—and that society has adopted their tools and spaces through its own free will. If pressed, they’ll point out how their platforms reflect the countercultural demands of earlier generations: eschewing big government and vertical corporate culture while encouraging personal fulfillment and flat organizational structures. Today you can be a coder and a DJ, an Uber driver and a travel blogger, a Sand Hill Road suit and a Robot Heart Burner. What logic could possibly be upended by punks, goths, gabbers, or neo-pagans when the internet, a massively lucrative space of capitalization, profits off the personal expression and political conflict of its users? Similarly slippery is the new look of power. Far from the parades, palaces, and outsize girths of present-day strongmen like Viktor Orbán, Kim Jong-un, and Donald Trump, the most iconic tells you’ll find among the big tech set are more likely to be a black turtleneck, a Patagonia fleece, and the absence of carrying bags. It’s a flex to be visually indistinguishable from the crowd. The power of today is firmly situated in minimalism, restraint, and ease—it’s only power under threat that turns to physical displays of strength. Actual power is controlling the means by which lesser power can be displayed—i.e., congrats on the 500K likes on your polling numbers, @jack still owns all your tweets. Actual power keeps a low profile; actual power doesn’t need a social media presence, it owns social media. In recent years, users have started to register this shift. Yet the term counterculture still gets used to describe someone like rapper Tekashi 6ix9ine, whose notoriety—first breaking society’s code (sexual abuse and murder conspiracy, among other offenses) and then the omertà code of the streets (snitching on fellow gang members to lessen his own sentence)—propelled him to superstardom: “Gooba,” a track he surprise-dropped upon being surprise-released from prison, made YouTube history by becoming the most-watched rap video in a 24-hour span, frying the platform’s view counter. That same day, 2 million simultaneous users tuned in to his Instagram Live as he confessed into his phone camera: “I snitched, I ratted. But who was I supposed to be loyal to?” And then with a sparkle of VVS diamonds, “I broke the YouTube. I’m at 5 million views in one hour. […] A rat is doin’ more numbers than you. Numbers don’t lie.” But behind 6ix9ine’s self-loyalty is an unwitting loyalty to the platform and, by extension, to the shareholders of Alphabet and Facebook, Inc. And this is where it gets tricky. To be truly countercultural today, in a time of tech hegemony, one has to, above all, betray the platform, which may come in the form of betraying or divesting from your public online self. To be truly countercultural today, in a time of tech hegemony, one has to, above all, betray the platform, which may come in the form of betraying or divesting from your public online self. 6ix9ine is subcultural, but he isn’t countercultural. Someone like Edward Snowden, by comparison, isn’t subcultural but may be the closest we get to a countercultural figure in the postdigital age. A US government subcontractor with access to classified intelligence, Snowden saw Big Tech’s radically scaling power and, in 2013, exposed the NSA’s illegal agreements with major tech platforms to intercept the private e-mail, call records, and cache of “almost anything done on the internet” by users worldwide. Snowden’s whistleblowing targeted a major chakra of the new hegemony, resulting in great personal compromise. But a single individual isn’t an entire counterculture. Counterculture requires a group. Us against the world. And the internet is excellent at bringing groups together around collective dissent. But just like the internet, there is nothing inherently socially progressive about these tools. Extinction Rebellion is countercultural in spirit but so too are QAnon, the armed right-wing libertarian Boogaloo Boys, and Europe’s Reichsbürger, who deny the existence of present-day Germany, claiming to be citizens of the Third Reich (which, they argue, technically never ended). A truth specific to our time is that dissent against one level of authority is now very often driven by a deeper hegemonic force. Perhaps this is why, among many younger people (Greta Thunberg notwithstanding), there is less focus on battling current leaders and more interest in divining counter-futures. Instead of attempting to dismantle the master’s house using the master’s tools, it’s more something like: Let’s pool crypto to book the master’s Airbnb and use the tools we find there to forge a forest utopia that the master could never survive. Central to this counter-future crafting is a strong belief in impending ecological collapse, rendering all the existing systems of control obsolete—which is a logical work-around for thinking about dissent in a time when the socially and ecologically corrosive systems are deemed too sprawling to effectively counter or boycott. Another key factor is Gen Z’s rediscovery of PoliticalCompass.org, a Web 1.0 site that, via six sets of prompts with which a user is asked to dis/-identify, generates an approximate position on the Political Compass’s X/Y axis of Left to Right, Authoritarian to Libertarian. Having spent the past several years intensively studying the development of Gen Z’s online political expression, artist Joshua Citarella points to the emergence of “e-deologies, radical politics as a form of niche personal branding.” In his 2019 report 20 Interviews, Citarella underscores the influence of Political Compass and gaming more generally on ideations of countercultural participation—or what he refers to as a “choose your character / choose your future” mode of “identity play that gained heightened relevance as American politics subsumed all of pop culture” during the mid-2010s. Among the political identities one finds in this space is, for example: “Ted was right” anarcho-primitivism (anprim), which, following Ted Kaczynski’s Industrial Society and Its Future manifesto, promotes a reactionary return to pre-agrarian times where people, reskilled as hunters and gatherers, are no longer alienated from their labor and seek fulfillment through daily survival. If you think this sounds fringe, consider the 10.3 million users currently subscribed to the Primitive Technology channel on YouTube, which has tutorialized building things “in the wild completely from scratch using no modern tools or materials, […] seeing how far you can go without utilizing modern technology”—except, of course, the device you use to stream the video showing you how. The names of these e-deologies tend to be both fantastical and literal. A “post-civilizationist” might focus on what optimal human survival would look like were civilization no longer possible. A “voluntarist post-agrarianist,” meanwhile, might value anarcho-primitivism skills but see them as integral to realizing a civilization sustained through opt-in agrarian communes. Elsewhere on the compass, one finds the likes of “Fully Automated Luxury Gay Space Communism” (where a total embrace of technology delivers humanity from scarcity, ecological volatility, and the reactionary social ills of resource competition) and the defiantly neo-traditionalist “technocratic theocracy,” which puts its faith in a machine-governed future that upholds Christian virtues. E-deologies are further explored on message boards and social media via memes, TikTok posts, and livestreamed Twitch and YouTube debates, all of which can get pretty gnarly (calls for “eco-fash global genocide” and “secession of white ethnostates,” etc.) And maybe here, we do have an aesthetic counter to the wallflower non-style of Big Tech: a raging messy semiotic meltdown of radicalizing (if absurdist) meme culture where the only ideological no-go zone is the liberal center. Key here is that most of this activity is happening under the guise of avatars, pseudonyms, and collectively run social media accounts where direct lines between IRL subjects and online personas are rarely clear. The “niche personal branding” is gamified—push an account to the extreme, see what happens. If the platform shuts you down, start over. While climate change is a shared concern for many younger people, their responses might be more accurately understood as competitive-futurist than countercultural. As the greatly imaginative range of Political Compass positions illustrates, there is little consensus over who or what they are specifically opposing. This is wise in an era when the complexity of global crises makes it exceedingly difficult to effectively isolate responsible parties. How would one even begin to hold, say, Apple accountable for all of the externalities within the life of an iPhone? Who among us could easily give up our connectivity and still be economically and socially okay? It’s as if, having grown up on a fully networked Earth, Gen Z has bypassed counterculture, finding it futile in the face of a hegemonic system that more clearly resembles a Hydra than the monolithic forces that legacy counterculture was rebelling against. Intuiting that any activity directly opposing the system will only make the system stronger, the next generation is instead opting for radical hyperstition: constructing alternative futures that abandon our current infrastructure entirely (the emergence of blockchain-based currencies, for instance, or calls to not merely reform but fully abolish the police). While Citarella’s research focuses on teenagers who began posting online around 2016 (and in 2020 are roughly 18 years old), it nevertheless distills the changing nature of contemporary countercultural activity more broadly. For one, anonymity, or at least pseudonymity, is increasingly important if not fundamental to being active online in counter-hegemonic ways. This is very different from, say, 1990s ideations of IRL counterculture, where there was a premium on unmediated authenticity and “being real” (think MTV Unplugged). Now “selling out” is tying your online identity to your IRL life and real name. In part, this is because one of the biggest impediments to countercultural activity is the fact that the internet doesn’t suppress expression—it forces you to express and then holds you accountable for whatever you say for years. On the platform, silence isn’t an option, at least not if you want the network to remember you exist. This is especially true in the culture sector, where being visible means being kept in mind for gigs and collaborations. There is a reason why 6ix9ine is obsessed with breaking YouTube and why talented young rappers must be equally talented at social media marketing if they ever hope to build a career. It’s as if, having grown up on a fully networked Earth, Gen Z has bypassed counterculture, finding it futile in the face of a hegemonic system that more clearly resembles a Hydra than the monolithic forces that legacy counterculture was rebelling against. We saw this dynamic metastasize in the wake of George Floyd’s murder, when well-intentioned claims of “silence is violence” (recalling the powerful 1987 ACT-UP “Silence = Death” campaign) spiraled into calling out individuals with even a small following who hadn’t come forward with a timely public statement of solidarity or remorse. Yet public posts were subject to popular scrutiny and judged based on sincerity, originality, and tone. Not surprisingly, many people defaulted to posting a somber plain black square. But this generated criticism of its own by clogging the feed with an informational blackout during a moment when community resource sharing was critically important. Amid a chaotic time, the platform functioned exactly as designed: amplification of emotions, uptick in user interaction, growth in platform engagement and data cultivation. Cha-ching, the platform cashes in. What’s really messed up about this is that users, despite understanding that the platform’s mechanics are net-bad, still feel a moral responsibility to obey the platform-enabled-hive-mind’s rules. On the dark edges of the early internet, hackers foresaw the enclosure of the public commons long before the likes of 6ix9ine, Snowden, and teenage Gen Z. These users developed an ethos that valued the radical freedom of a fully anonymous, hyperconnected zone where people could communicate unburdened by their physical bodies and government names. As online activity began to centralize around search engines, such as Netscape, Explorer, and Google, in the late-’90s and early-’00s, the internet bifurcated into what became known as the “clearnet,” which includes all publicly indexed sites (i.e., big social media, commercial platforms, and anything crawled by major search engines) and the “darknet” or “deep web,” which is not publicly indexed (due to being built on anonymized, encrypted networks such as Tor). There were also a number of sites that though officially clearnet, laid the groundwork for a sub-clearnet space that we might think of as a “dark forest” zone—particularly message board forums like Reddit and 4chan, where users can interact without revealing their IRL identity or have this activity impact their real-name SEO. Taken from the title of Chinese sci-fi writer Liu Cixin’s 2008 book, “the dark forest” region of the web is becoming increasingly important as a space of online communication for users of all ages and political persuasions. In part, this is because it is less sociologically stressful than the clearnet zone, where one is subject to peer, employer, and state exposure. It also now includes Discord servers, paid newsletters (e.g., Substack), encrypted group messaging (via Telegram, etc.), gaming communities, podcasts, and other off-clearnet message board forums and social media. One forages for content or shares in what others in the community have retrieved rather than accepting whatever the platform algorithms happen to match to your data profile. Additionally, dark forest spaces are both minimally and straightforwardly commercial. There is typically a small charge for entry, but once you are in, you are free to act and speak without the platform nudging your behavior or extracting further value. It is also interesting to keep in mind that the dark forest shares the same cables and satellite arrays as clearnet channels, is accessed via the same devices, and essentially all of its denizens continue to simultaneously participate in clearnet spaces (as contemporary professional protocol demands). It is therefore not analogous to legacy countercultural notions of going off-grid or “dropping out.” To be sure, none of these spaces are pure, and users are just as vulnerable to echo chambers and radicalization in the dark forest as on pop-stack social media. But in terms of engendering more or less counter-hegemonic potential, the dark forest is more promising because of its relative autonomy from clearnet physics (the gravity, velocity, and traction of content when subject to x algorithm). Unlike influencers and “blue checks,” who rely on clearnet recognition for income, status, and even self-worth, dark forest dwellers build their primary communities out of clearnet range—or offline in actual forests, parks, and gardens (e.g., cottagecore and related eco-social trends)—and then only very selectively or even absurdly/incoherently show themselves under clearnet light. The crux of Liu Cixin’s book is the creed, when called by the clearnet: “Do not answer! Do not answer!! Do not answer!!! But if you do answer, the source will be located right away. Your planet will be invaded. Your world will be conquered.” So what does today’s counter-hegemonic culture look like? It’s not particularly interested in being seen—at least not in person. It gets no thrill out of wearing leather and a mohawk and walking past main-street shops, which are empty now anyway. But it does demonstrate a hunger for freedom—freedom from the attention economy, from atomization, and the extractive logic of mainstream communication. We can imagine collectively held physical spaces reclaimed from empty retail or abandoned venues hosting esoteric local scenes, a proliferation of digital gangs in dark forests who hold secrets dear, and a new desire for scarcity in cultural objects—deeper and closer connections made between people even while rejecting the platform’s compulsion to “like and share.” In the internet era, true counterculture is difficult to see, and even harder to find—but that doesn’t mean it’s not there. </description>
      <pubDate>12 Mar 21 15:35 EST</pubDate>
      <guid>https://www.documentjournal.com/2021/01/the-internet-didnt-kill-counterculture-you-just-wont-find-it-on-instagram/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.nms.ac.uk/explore-our-collections/stories/science-and-technology/the-waste-of-daylight/</link>
      <description>&lt;a href=&#34;https://www.nms.ac.uk/explore-our-collections/stories/science-and-technology/the-waste-of-daylight/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; The Waste of Daylight In 1907 the builder William Willett wrote a pamphlet called The Waste of Daylight (an original copy of which now resides in our Science and Technology collections). The pamphlet campaigned for Daylight Saving in the UK. 9 years later, in 1916, this came into effect in UK law and effectively changed how we experience the passing seasons. Read on to find out more about what Daylight Saving is and why Willett wanted it implemented... What is daylight saving time? Daylight saving time (DST) or summer time is the practice of advancing clocks during summer months so that sunrise and sunset are later each day according to the clock. The current implementation of daylight saving time is to set clocks forward by one hour in the spring (&#34;spring forward&#34;) and set clocks back by one hour in autumn (&#34;fall back&#34;) to return to Greenwich Mean Time (GMT). In other words, there is one 23-hour day in late winter or early spring and one 25-hour day in the autumn. William Willett by Benjamin Stone, 1909, CC BY-NC-ND 3.0, National Portrait Gallery. A builder proposes saving time It was William Willett who was one of the first advocates for Daylight Saving Time (DST) in the United Kingdom. In 1907, he published a pamphlet called The Waste of Daylight, campaigning to advance clocks at the beginning of the spring and summer months and to return to GMT in the autumn. He wanted to encourage people to get out of bed earlier in summer.  Illustration of moving the clocks forward an hour in the spring and back in the autumn. “ Everyone appreciates the long light evenings. Everyone laments their shrinkage as Autumn approaches, and nearly everyone has given utterance to a regret that the clear bright light of early morning during Spring and Summer months, is so seldom seen or used. Nevertheless Standard time remains so fixed, that for nearly half the year the sun shines upon the land, for several hours each day while we are asleep… - William Willett Advantages of being outside Family group walking down the path at the National Museum of Rural Life. “ Now, if some of the hours of wasted sunlight in Spring, Summer and Autumn could be withdrawn from the beginning, and added to the end of the day, how many advantages would be gained by all, in particular by those who spend in the open air… - William Willett The 1908 Daylight Saving Bill was the first attempt in the UK to move clocks forward one hour in summer. The idea was to provide more daylight hours after work for the training of the Territorial Army, to reduce railway accidents, and to reduce lighting expenses. Among the various objections was a concern for lower milk yield: Excerpt from The Waste of Daylight pamphlet  Excerpt from The Waste of Daylight pamphlet (National Museums Scotland owns an original copy) “ That the milk trade will be disorganised and that cows will not yield milk twenty minutes earlier than they had been accustomed to. - William Willett Moving the Ayrshire dairy herd at the National Museum of Rural Life. Willet’s scheme called for a clock change of 20 minutes on each of four Sundays in a row, in order to minimise the disruption to biological clocks, of people or cows, at the expense of remembering four separate clock changes for a total difference between summer and winter time of 80 minutes. The House of Commons eventually rejected the Daylight Saving Bill and sadly William Willett died of the flu in 1915 aged 58 and never lived to see his daylight saving ideas become law. Conserving coal The idea resurfaced during the First World War when the need to conserve coal made the suggestion of daylight saving more crucial. Germany had already introduced a similar scheme when the Summer Time Act was finally passed in the UK on 17th May 1916. The clocks went forward one hour on the following Sunday, 21st May. Fading sunlight over the fields at the National Museum of Rural Life. Double Summer Time During the Second World War, in 1941 Britain adopted British Double Summer Time, which saw clocks being put forward two hours ahead of GMT. The clocks were turned back to GMT at the end of summer 1945. However because of severe fuel shortages resulting from the harsh winter of 1946/47, the UK returned to British Double Summer Time during the summer of 1947. For and against Since its introduction, Daylight Saving Time has had both its advocates and critics. Advocates for the system claim the lighter summer evenings save energy, reduce traffic accidents and get people out and about and more active. This horizontal sundial made by Joseph Williamson, Aberdeen in 1728. It shows how long the summer days are in the north of Scotland. The sundial only works when the sun is up, and this goes from 03:00 to 23:00! (T.1975.171). Critics however claim that if adopted all year round, this would result in darker winter mornings which would be more dangerous for children going to school and for those in the north and Scotland, the sun would not rise until well into the morning leaving farmers working for several hours in the dark each morning in the winter. Some argue that because of this, England and Wales should have their own time zone and Scotland and Northern Ireland, another. Electrical digital clock by Pifco, 1960 - 1976 (T.1976.41).  Time today Today the United Kingdom observes Greenwich Mean Time (GMT) during the winter months and British Summer Time (BST) in the summer months. The country follows the same DST schedule as most of Europe, setting the clocks forward one hour on the last Sunday in March and back again one hour on the last Sunday in October. Header image Working model of a tower clock in the Grand Gallery at the National Museum of Scotland (T.1921.20).  Tags Story Science and Technology Clocks and watches Inventions Transport </description>
      <pubDate>14 Mar 21 23:53 EDT</pubDate>
      <guid>https://www.nms.ac.uk/explore-our-collections/stories/science-and-technology/the-waste-of-daylight/</guid>
    </item>
    <item>
      <title></title>
      <link>https://en.wikipedia.org/wiki/Okapi_BM25</link>
      <description>&lt;a href=&#34;https://en.wikipedia.org/wiki/Okapi_BM25&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Not to be confused with Okapi. In information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others. The name of the actual ranking function is BM25. The fuller name, Okapi BM25, includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London&#39;s City University in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art TF-IDF-like retrieval functions used in document retrieval.[citation needed] The ranking function[edit] BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within the document. It is a family of scoring functions with slightly different components and parameters. One of the most prominent instantiations of the function is as follows. Given a query Q, containing keywords , the BM25 score of a document D is: where is &#39;s term frequency in the document D, is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn. and b are free parameters, usually chosen, in absence of an advanced optimization, as and .[1] is the IDF (inverse document frequency) weight of the query term . It is usually computed as: where N is the total number of documents in the collection, and is the number of documents containing . There are several interpretations for IDF and slight variations on its formula. In the original BM25 derivation, the IDF component is derived from the Binary Independence Model. IDF information theoretic interpretation[edit] Here is an interpretation from information theory. Suppose a query term appears in documents. Then a randomly picked document will contain the term with probability (where is again the cardinality of the set of documents in the collection). Therefore, the information content of the message &#34; contains &#34; is: Now suppose we have two query terms and . If the two terms occur in documents entirely independently of each other, then the probability of seeing both and in a randomly picked document is: and the information content of such an event is: With a small variation, this is exactly what is expressed by the IDF component of BM25. Modifications[edit] References[edit] ^ Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze. An Introduction to Information Retrieval, Cambridge University Press, 2009, p. 233. ^ &#34;The BM25 Weighting Scheme&#34;. ^ Hugo Zaragoza, Nick Craswell, Michael Taylor, Suchi Saria, and Stephen Robertson. Microsoft Cambridge at TREC-13: Web and HARD tracks. In Proceedings of TREC-2004. ^ Stephen Robertson &amp; Hugo Zaragoza (2009). &#34;The Probabilistic Relevance Framework: BM25 and Beyond&#34;. Foundations and Trends in Information Retrieval. 3 (4): 333–389. CiteSeerX 10.1.1.156.5282. doi:10.1561/1500000019. ^ Yuanhua Lv and ChengXiang Zhai. Lower-bounding term frequency normalization. In Proceedings of CIKM&#39;2011, pages 7-16. General references[edit] Stephen E. Robertson; Steve Walker; Susan Jones; Micheline Hancock-Beaulieu &amp; Mike Gatford (November 1994). Okapi at TREC-3. Proceedings of the Third Text REtrieval Conference (TREC 1994). Gaithersburg, USA. Stephen E. Robertson; Steve Walker &amp; Micheline Hancock-Beaulieu (November 1998). Okapi at TREC-7. Proceedings of the Seventh Text REtrieval Conference. Gaithersburg, USA. Spärck Jones, K.; Walker, S.; Robertson, S. E. (2000). &#34;A probabilistic model of information retrieval: Development and comparative experiments: Part 1&#34;. Information Processing &amp; Management. 36 (6): 779–808. CiteSeerX 10.1.1.134.6108. doi:10.1016/S0306-4573(00)00015-7. Spärck Jones, K.; Walker, S.; Robertson, S. E. (2000). &#34;A probabilistic model of information retrieval: Development and comparative experiments: Part 2&#34;. Information Processing &amp; Management. 36 (6): 809–840. doi:10.1016/S0306-4573(00)00016-9. Stephen Robertson &amp; Hugo Zaragoza (2009). &#34;The Probabilistic Relevance Framework: BM25 and Beyond&#34;. Foundations and Trends in Information Retrieval. 3 (4): 333–389. CiteSeerX 10.1.1.156.5282. doi:10.1561/1500000019. External links[edit] Robertson, Stephen; Zaragoza, Hugo (2009). The Probabilistic Relevance Framework: BM25 and Beyond (PDF). NOW Publishers, Inc. ISBN 978-1-60198-308-4. </description>
      <pubDate>17 Mar 21 07:23 EDT</pubDate>
      <guid>https://en.wikipedia.org/wiki/Okapi_BM25</guid>
    </item>
    <item>
      <title></title>
      <link>https://jblevins.org/log/structured-procrastination</link>
      <description>&lt;a href=&#34;https://jblevins.org/log/structured-procrastination&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Structured Procrastination February 10, 2007 Every professional procrastinator knows that when a big project deadline looms in the near future, the time is ripe to work on some other project instead. It usually involves something at least marginally productive, such as cleaning the house, or it might involve starting some entirely new project. My bouts of procrastination usually result in the latter and then I have yet another open project that lingers unfinished. I used to try to fight the urge to procrastinate directly, but I am beginning to believe that the key is not to combat it outright, but to harness it. I know many very bright and productive people who procrastinate just as much as anyone else. In fact, academia is rife with procrastinators yet still manages to plod along somehow. For example, a 1980 article by Gary W. Yohe figures the average time to publication of an article submitted to Econometrica, a top Economics journal, to be 25.9 months. One explanation is that top journals simply examine submitted articles more carefully. Another perhaps more likely reason is that journal referees and editors are procrastinators. The key to harnessing one’s procrastination is to recognize it and channel it away from the marginally productive activities into more highly productive ones. This is the essence of Structured Procrastination, as described in an essay by by John Perry. His first paragraph is an excellent summary: I have been intending to write this essay for months. Why am I finally doing it? Because I finally found some uncommitted time? Wrong. I have papers to grade, textbook orders to fill out, an NSF proposal to referee, dissertation drafts to read. I am working on this essay as a way of not doing all of those things. This is the essence of what I call structured procrastination, an amazing strategy I have discovered that converts procrastinators into effective human beings, respected and admired for all that they can accomplish and the good use they make of time. References Perry, John. “Structured Procrastination”, http://www.structuredprocrastination.com, April 25 1995, retrieved on February 10, 2007. Yohe, Gary W. (1980): “Current Publication Lags in Economics Journals”, Journal of Economic Literature, 18, 1050–1055. </description>
      <pubDate>02 Feb 21 13:33 EST</pubDate>
      <guid>https://jblevins.org/log/structured-procrastination</guid>
    </item>
    <item>
      <title></title>
      <link>https://utcc.utoronto.ca/~cks/space/blog/unix/VimWhatIsAdvanced</link>
      <description>&lt;a href=&#34;https://utcc.utoronto.ca/~cks/space/blog/unix/VimWhatIsAdvanced&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Different views of what are basic and advanced Vim features March 15, 2021 Although vim is now my most regularly used editor (and it really is vim, not vi), I still have lots to learn about it. Thus, every so often I wind up reading guides to Vim, which often label themselves as being for &#39;beginning users&#39; or &#39;intermediate users&#39; or &#39;advanced users&#39;. One of the interesting things in reading these guides is hitting features labeled as basic that I don&#39;t know, or features labeled as advanced that I consider perfectly routine. (My most recent reading is this one, which I haven&#39;t even finished yet.) For some features that are sometimes considered advanced, like address ranges, I think a lot of this has to do with the path I took through various editors on the way to today. For instance, I&#39;ve seriously used a version of Unix ed, and it&#39;s hard to use ed without getting a fairly thorough exposure to address ranges. This was reinforced by both sysadmin use of sed and my long running fondness for the sam editor. Use of ed and then sam left me completely comfortable with the idea of doing editor operations as (Ex) commands. (I don&#39;t know if marks are generally considered an advanced concept, but as a sysadmin who uses less a lot and jumps around in various sorts of files to find things, marks are very familiar to me and I use them all the time. Sam has the idea of a mark but only a single one, which is easier to keep track of, and it&#39;s been so long since I seriously used ed that I can&#39;t remember if I really used marks in it.) When I run into basic vim features that I don&#39;t know or haven&#39;t mastered, I generally remind myself that I basically drifted into vim instead of actively learning it (including vi). I did at one point long ago read through what was then the vi tutorial, but I did it in another editor (which didn&#39;t help get it to stick). This path into vim has left me learning things on a scattershot basis as I stumble over them and decide that they&#39;re worth it (not every vim feature is for me). Part of this is that I use vim primarily as a sysadmin, which is to say that I&#39;m either editing text (such as email) or I&#39;m working with generally small amounts of code, or at least of editing code. My editing sessions tend to be short; I&#39;m definitely not spending all day editing code. If I was, I would probably be more driven to learning vim features that improved that. Plus, right now and likely for the indefinite future, my editor for serious coding is still GNU Emacs for various reasons. (The pattern of vim things that catches my interest is quite driven by sysadmin interests, from windows while modifying related files at once through mass commenting out references to a machine to changing a bunch of files in the same way at once.) </description>
      <pubDate>19 Mar 21 22:36 EDT</pubDate>
      <guid>https://utcc.utoronto.ca/~cks/space/blog/unix/VimWhatIsAdvanced</guid>
    </item>
    <item>
      <title>Studio Moniker on how it’s getting harder for small studios to compete with big tech</title>
      <link>https://www.itsnicethat.com/features/studio-moniker-digital-060520</link>
      <description>&lt;a href=&#34;https://www.itsnicethat.com/features/studio-moniker-digital-060520&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I arrive at Studio Moniker absolutely sopping wet. Soaked through. From head to toe. On the advice of everyone I spoke to, I hired a bike during my (first) time in Amsterdam which was lovely. Until it started bucketing it down. Needless to say, however, my mood was lifted when I stepped inside the surprisingly humble office of Moniker – a studio I have long admired as one of the best – and I was greeted by a small, congenial, intelligent and clearly very passionate team.The space itself is indicative of the group’s personality. An old school building housing many studios, Moniker’s appears small but rises up in a Tardis-like style, with steep staircases leading from one floor to another. On one window is the latest iteration of the studio’s Fungus Series (a crowdsourced sticker installation) and the team of six’s desks sit tessellated with one another. As there are many strings to Moniker’s bow, there are many levels to its studio, it seems. On the second level, a custom wooden almost treehouse structure which juts out over everyone’s desks, is where we sit to have lunch (and dry off) before our interview.AbovePortrait of Studio Moniker, created using neuhaus.worldGalleryFungus SeriesLuna Maurer and Roel Wouters are, as they describe themselves, “the big bad bosses” and they founded the studio in 2012 with Jonathan Puckey, who later left the studio in 2016. An interactive design studio, Luna and Roel, along with their team of Tjerk Woudsma, Jolana Sýkorová, Thomas Boland, and Grischa Erbe, work across a variety of media and with a range of clients, on both commercial projects and artworks for the cultural sector.They’ve created crowdsourced music videos, automated drawing tools, AR painting applications, memorials co-created by thousands of people, and an ever-changing VR collaboration, all of which “explore the social effects of technology – how we use technology and how it influences our daily lives.” Often, there’s an element of participation involved and the resulting works are investigatory, critical, funny and organic. Their results are unexpected, yet are born from the systems and rules Moniker puts in place; they “expand and grow like plants, displaying their inner organisational process.”Luna and Roel met while studying at the Sandberg Instituut, the master programme of the Gerrit Rietveld Academie, and later the pair was employed to teach on the graphic design programme of the latter. They got the gig when Roel was asked to join one day when the programme was looking for a substitute. “I had no idea what teaching was, or how to do it and so I thought, ‘I need to do this with someone else’ and I asked Luna,” he recalls. The pair became quite the “teaching team” for a number of years and “did lots of nice, fun, weird things” with the students on the course, alongside an incredible team which included Experimental Jetset and Julia Born as part of its faculty.AbovePortrait of Studio Moniker, created using neuhaus.worldRoel and Luna, specifically, were in charge of guiding the students’ interaction design practice – something which every graphic design student took part in, in accordance with the Dutch creative education system. They made them “think about screens, the digital, things that are not static, not fixed in the way they were used to (with posters etc),” Luna remembers. “Our assignments weren’t about developing a style,” Roel adds. In a prophetic manner, what they were about was looking at the human aspect of technology, the difference (and sometimes the similarities) between a human and a computer, and what happens when you combine the two. To complete each project, they would organise an event or a party in a club or wherever they felt like it. “And that was not mandatory,” Luna recalls, “it was pretty unique to our class – it was a very special time, and I think the students liked our class!”These kinds of ideas were not new for Roel or Luna as they had been bubbling away throughout their own, separate studies. Luna, who’s originally from Germany, studied there before heading to the Rietveld to do her bachelor’s, during which time she began exploring the notion of the web browser and its philosophical implications. “This whole idea came up with the browser and the fact that it is like a filter, which affects the way you perceive information,” she says. “That started a really big question mark in my head. I was absorbed by the internet and the effect it was having on all of us – it was super interesting.”AboveDo Not Draw a Penis tea towel featuring 5K doodlesHer graduate project, an operating system which turned the interfaces of all the computers in the Rietveld’s computer room to liquid, was a “totally digital and conceptual final project” and her tutors “had no idea what I was doing,” she continues, laughing. It was this misunderstood project that led her to the Sandberg, though, a place that was “very interested in people like me who were interested in digital media.”Roel, on the other hand, had been tinkering with technology and its perceptible results even before heading to university, primarily through an 8-bit computer. “That was very close to my heart – I was very interested in the visual side of technology, how can you create something with technology as a tool, how can you use technology as a tool to express yourself?” When he did make it to art school, though, things didn’t go so smoothly, despite his inherent fascination with creative exploration. “Busy with other things,” and therefore never attending class, Roel was eventually kicked out and never graduated. “But luckily, I met someone who told me about the Sandberg, where, at the time, I could start a master’s without a bachelor’s.”An experimental institution whose graphic design course was interested in technology, it was here that Luna and Roel’s ideas about the rapidly changing world aligned, sowing the seeds of what would eventually become Studio Moniker. This was 2001 and while the rest of the department was interested in technology, Roel and Luna were particularly interested in the human side of technology: interactive storytelling. “You can see it, the roots [of Moniker] are totally there,” says Luna. “It’s been in development for, you could say, 20 years.”AboveFor Play for Feeld, a kind-of “digital foreplay”It was while teaching a few years later, in 2004, that they met Jonathan Puckey, who enrolled on the course. Prior to him attending the institution, Luna and Jonathan had collaborated on a project together and, as Luna tells me, Jonathan was “totally ahead of other students but also the world of digital media.” As it turned out, he was intrigued by the very same topics Luna and Roel had been discussing since meeting each other. “We just clicked,” Luna continues, and so the trio started to work on projects together.It was a different time and the internet was a very different beast back then. The trio spent their time poring over what was then an exciting new breeding ground for creative work, a space filled with optimism and untapped potential. “We were looking at what was happening to the world with the internet, everything was becoming fluid and dynamic,” Luna explains. “The medium didn’t matter to us, it was the mentality. It was the fact that something was happening, something was changing that fascinated us; the relationship between man and machine.” With this change came the potential for the democratisation of knowledge – Wikipedia had been founded in 2001, for example – and for innovative ways of sharing information. “We thought, ‘OK, we need new designers who can shape all this energy that goes into the internet world,’” Roel adds. “We wanted to do something with that – the way we as human beings could give shape to the internet and the possibilities of the internet.”A large part of their conversations was taken up by the issue of defining their practice, or their inability to do so. Their experiments into this newfound world were expressed in, well, whatever way they felt appropriate, resulting in websites but also performances or music videos and even print. At the time, the creative world relied heavily on a tried and tested lexicon of “graphic designer”, “filmmaker”, “performance artist”, “web designer”, etc. But Roel, Luna and Jonathan felt they needed to position themselves differently – they didn’t identify with any of these terms; or rather, they identified with a little bit of each of them.Together with fellow kindred spirit Edo Paulus, they therefore created the Conditional Design Manifesto in 2008, a book outlining a new creative practice. “For us, the method was paramount: to develop, stimulate and visualise processes,” Luna explains. “The power of a conditional designer lies in the ability to define a playing field within which something unexpected or unpredictable can emerge.” Instead of an outcome-focused way of working, conditional design is concerned with developing a strict set of rules (algorithms) in order to delineate a framework in which a process can unfold. Born from their collective frustrations, the book introduced conditional design as an entirely new term, went on to become an international success, and served as an anchor point for Roel, Luna and Jonathan’s work. Following the publication of the Conditional Design Manifesto and an exhibition on the topic, Moniker was officially formed in 2012.Eight years later and Studio Moniker’s portfolio is as absorbing as it is playful. And while conditional design proved to be somewhat of a springboard for the studio’s formalisation, it’s since become known for another approach: participatory design. Put simply, it’s the process by which Roel and Luna produce work, inviting an audience to complete a task or contribute in some way. Take Do Not Touch, for example, an interactive music video which begins with an announcement: “Please note ➝ We are recording your pointer.” Viewers answer questions including “Where are you from?” and complete easy tasks like “Catch the green dot” – doing so using their enlarged cursor, set against a sea of cursors belonging to those who participated before them.Do Not Touch, an ever-changing interactive music videoAnd then there’s Puff Up Club, an installation created for an exhibition on Alexander Calder and Peter Fischli and David Weiss. Mimicking the “fleeting, precarious and exhilarating moment of fragile balance as expressed through the works of Calder and Fischli/Weiss in the early and late-20th Century, respectively”, Puff Up Club enabled visitors to a website to team up in order to puff up a balloon – and burst it. Their progress was streamed live from a physical installation located at Moniker HQ.There’s a tendency to use the term “participation” within design to denote some form of authorship over an outcome, as Luna points out: “The terminology of participation is really worn-out, it’s a problem. Every neighbourhood is talking about participation in the municipality context, meaning you can say whatever you want or at least you can be part of a decision process.” But that is very different from Moniker’s work. Within Moniker’s practice, the participation is very limited, it’s not about expressing any personal ideas, it’s about answering a question, filling in the blank, doing an action, as instructed by Moniker. “We set the framework,” Luna describes, “and they join in the game. There are certain rules and they have to follow (or not follow) the rules. The space for personal expression is rather limited.” She goes on: “This also goes well with the idea of how giving a lot of limitations actually sparks creativity (also a difficult word!). But you don’t get ideas from a blank page. We like this idea and very much embrace it.”It’s a form of creativity which they have learned much about (and from), using each project as an exploration of a particular idea or avenue of curiosity, even referring to some of their works as “social research”. How do you get people to do what you want? How do people respond when told what to do? And how do you know you’ll get the desired outcome? It’s about limiting freedom but providing just enough room for something unexpected to happen. You’ve got to make sure it’s fun, that participants don’t just feel like robots and that there is a clear payoff in return. “So, when making a music video, you ask them to do something and in return, you promise they become part of the video – there’s a very clear reward,” Roel says.It’s also imperative to keep the interaction to a minimum, to “one-click participation” if possible, Luna explains. “It’s also funny, we really like it when people don’t follow the rules,” Roel chips in. “We don’t encourage it, but we leave space for it, a little gap that anyone who is looking out for will find. A hacker who thinks: ‘They’re asking for this, but I will do that.’ I think this is part of the fun.”But why bother asking strangers on the internet to complete tasks using their cursor, when a similar effect could be achieved through video editing? Why does it matter that the interaction actually happened? Because you, as an individual, or even several individuals, could never produce the same results. They are totally and beautifully unpredictable. “The thing that I’m always excited about,” Roel says, “is that it’s not really about us as designers, but there is this moment that is filling up and getting dirty and becoming human and then it just starts producing stuff. Often, you’re slightly disappointed by it but also totally excited about it. It’s always different and it’s never as you anticipate it.”Abovepainted.earth, an AR drawing toolAboveWIP imagery of painted.earth, an AR drawing toolAbovePortrait of Studio Moniker, created using neuhaus.worldPerhaps the perfect example of this is Fungus Series, an ongoing work which has had several iterations since Moniker first produced it in 2010. A generative participatory installation that is executed by a large group of people, the concept is simple: each person receives a sticker sheet containing four stickers and a simple set of instructions that specify how the stickers should be attached to the surface. Then, completely organically, an artwork is produced, sticker by sticker, as more people contribute. “It’s decentralised,” Luna says. “How the crowd organises itself is just fascinating.”Ultimately, this is the crux of Moniker’s practice: humanity. Yes, the majority of its projects are led by technology and, in fact, they often employ some very complicated form of it. But the technology is merely a tool to investigate the friction that occurs when you directly confront humanity and technology, to attempt to understand the role it plays in our lives, what the “social effects” are. It’s interesting to hear that they would rather undertake a project that is “purely human than something purely computational”. It’s the interaction between the two that leads to interesting results. “If everything is ‘human’ and muddy and undefined then that’s also not so interesting,” Luna remarks. “One informs the other.”Nowhere is this clearer than in their project Red Follows Yellow Follows Blue Follows Red. Here each participant received a pair of headphones and a red, blue or yellow cape and was then asked to follow the instructions. These included “follow yellow but avoid blue” told to those wearing red, while participants with a blue cape were asked to “follow red but avoid yellow”. While there is no computer involved, the project highlights what structures do to us, how crowds behave – it’s social programming, and the instructions function like an algorithm.Red Follows Yellow Follows Blue Follows Red, a participatory performanceAbovePortrait of Studio Moniker, created using neuhaus.worldGallerySculpture Cam, a social game in the parkGallerySculpture Cam, a social game in the parkReflecting on the work that Studio Moniker has produced in its eight years of existence, there’s a shift that seems to have occurred. Many of the older projects, such as those mentioned above, have an inherent playfulness to them; some of them are downright funny, in fact. But some of the more recent works are far more critical in their outlook, challenging aspects of society, particularly aspects of technology. Last year, for example, the studio released Do Not Draw a Penis, which was made in response to Google’s Quickdraw data set; “the world’s largest doodling data set”. For obvious reasons, as Moniker outlines, the data set was missing a few specific categories that people enjoy drawing. “This made us at Moniker think about the moral reality big tech companies are imposing on our global community and [about how] most people willingly accept this.” The studio’s response is an automated drawing tool that collects drawings of penises from people “who are not willing to stay within the moral guidelines set by our social network providers”, creating an addendum Quickdraw.There’s also Paperstorm, which mimics the historic practice of spreading political messaging through paper leaflets. In one instance, the project was used to protest The Federal Communications Commission proposed changes to net neutrality which threatened the internet’s values of free speech, choice and innovation. Paperstorm, therefore, asked users to virtually litter The Federal Communications Commission’s headquarters in protest.Something Roel initially points out when our conversation drifts to this topic is that both he and Luna aren’t programmers themselves, and so they are perhaps more reflective on the medium than they were when Jonathan was part of the studio. “Jonathan, being a programmer, played with the substance itself – he produced things that really came from the medium,” Roel continues. What’s more, the pair aren’t actually massive fans of tech – “we like to understand it, that’s true, but I’m not actually that fond of screens, I don’t like them that much!” he laughs.Paperstorm.it, Mozilla’s airborne leaflet propaganda toolThe overriding reason for this shift though is that Studio Moniker began as a project to respond to the changing world, and the world has continued to change. Moniker’s work has a new social context. In the 2000s, the digital world was sanguine, it was a community that saw the emergence of new networks and with it, novel ways of sharing information like “de Digitale Stad, Geocities, Wikipedia, MySpace, Second Life, etc.” Multinationals did not yet understand the web but now, it is 20 years later and as Roel tells me, “startups and multinationals seem to be the only parties that are able to create environments that digital citizens want to use eagerly.” For most people, the internet means WhatsApp, Instagram and Facebook – their apps, not even their websites – and these “capitalise on our behaviour in every possible, radical way,” he continues. “Because of big data, privacy and copyright issues, fake news and automated moderation, the digital domain is becoming increasingly grim.” With this in mind, it would be remiss for the studio to ignore it in its work.What that means is that Moniker is currently left wondering: “Can and should we continue under the same premise – to present the internet as an emancipatory medium and to celebrate connectivity by means of participative and playful projects?”AbovePortrait of Studio Moniker, created using neuhaus.worldThe answer, on the whole, is no. “It’s kind of sad, but it’s true,” says Roel, to which Luna adds: “We cannot just have this super playful, optimistic, let’s all party together outlook.” Realistically, it’s time for Moniker to pivot to, as it always has, responding to the digital world and its effects on human behaviour – just from a slightly different angle. “In the past 15 years, we have built a successful creative practice based on the premise that we as designers should be able to build digital environments that anyone with an internet connection can join,” Luna says. “Slowly we are starting to see the limitations of what is technically possible for an independent studio like ours.” The fact of the matter is, it has become increasingly difficult for a small studio such as Moniker to compete with companies like TikTok and Snapchat, which “are currently sucking up all participation potential on the web.” The field for experimentation when it comes to participation online feels spent, as the post, like and share functions of applications like Instagram is embedded within our muscle memory.So what does the future of studio Moniker look like? This February, Roel and Luna began weekly conversations on such topics; their knowledge of the digital world but also personal fascinations, the world around them and their role within it. They film these discussions and some might form the basis of a project. “Simultaneously, we are developing more speculative, surrealist projects,” Roel remarks. “One concrete project in that regard is a participatory film about the behaviour of the (internet) user – we call it the Perfect User.” Encouragingly, just last week, the pair found themselves having “great laughs with a little film project,” painting Roel’s head yellow and filming him mimicking various emojis. Moniker will, it seems, find a way to continue critiquing and reflecting upon the digital world. And it will, it seems, continue to poke fun at it while doing so.Share ArticleWorkPhotographyNewsDigitalWorkArtWorkIllustration</description>
      <pubDate>20 Mar 21 16:44 EDT</pubDate>
      <guid>https://www.itsnicethat.com/features/studio-moniker-digital-060520</guid>
    </item>
    <item>
      <title></title>
      <link>https://ruder.io/10-tips-for-research-and-a-phd/</link>
      <description>&lt;a href=&#34;https://ruder.io/10-tips-for-research-and-a-phd/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This advice should be most relevant to people studying machine learning (ML) and natural language processing (NLP) as that is what I did in my PhD. Having said that, this advice is not just limited to PhD students. If you are an independent researcher, want to start a PhD in the future or simply want to learn, then you will find most of this advice applicable.Pick and choose.  Everyone is different. You will have the most success if you adapt the particular advice to your situation and do what works for you. TL;DR:Read broadly.Work on two things.Be ambitious.Collaborate.Be proactive.Write a blog.Keep a source of positive energy.Play to your strengths.Intern or visit a university.Play the long game.1) Read broadly.While a PhD encourages you to delve deep into a specific topic, you can add value by making connections between different topics or entirely different fields. The papers that draw such connections can often be insightful. Many ideas in deep learning take inspiration from other fields such as biology (Hinton et al., 2014), neuroscience (Wang et al., 2016), physics (Cohen et al., 2019), and many others.In order to have a rich repertoire to draw on for inspiration, try to cultivate diverse interests. Look beyond your immediate horizon. Attend summer schools in other areas. Connect with people from other labs. Talk to people outside your subarea at conferences. Read papers from different disciplines.ArXiv is a great source of research papers but staying up-to-date with the daily arXiv digest feels like drinking from a firehose. Instead, I use services such as arXiv sanity preserver, arXivist, my Twitter feed, and recommendations from friends to stay up-to-date and to seek out different topics. I also generally prefer to read 10 papers superficially rather than one paper in-depth (as suggested by Jeff Dean). With a search-able paper management system (I use Mendeley) you can always go back and reread the most relevant ones.It can be helpful to dabble in different areas early in your PhD to get a sense for what interests you. Once you have found something, focus on the problems that you deeply care about. Think about the narrative you&#39;d like to tell as part of your thesis.2) Work on two things.While it is good to complete a project before starting a new one, working on a single project has downsides. If the project is not going well, your motivation and well-being may suffer. If you hit a roadblock, you can do nothing but grind until you resolve it. Developing such resilience is important but may at times come at a high mental cost.Instead, I&#39;ve found it useful to work on two projects at once to keep my sanity in check. If you hit a wall on one project, you can spend some time working on the other. This allows you to free up your mind and gain a new perspective, which may help you resolve the problem. If one of the projects is going well, this may also give you a boost to make progress on the other.To minimise context-switching, I generally try to work on one project each day. It is also helpful if both projects are in similar areas so that you can apply what you learn on one project to the other one. 3) Be ambitious.“Shoot for the moon. Even if you miss, you&#39;ll land among the stars.”—Norman Vincent PealeAnother benefit of working on two projects is that it allows you to be more daring. You can work on a relatively safe project and one that is high-risk but also may be more impactful. The safe work ensures that you will graduate. The high-reward work may have a larger impact.Ambitious projects demonstrate that you are creative and can come up with new ideas. Both are extremely valuable qualities. And even if such projects fail, they may lead you to discover unexpected insights that can lead to a publication.Ambitious, however, does not mean that you should cater to the largest possible audience. A high impact can also be concentrated in a small community. A good indication of whether something you are working on is impactful is whether you&#39;d be excited if it was published by someone else. In the end, you want to be known as someone that challenges the status quo and charts their own course.4) Collaborate.The PhD is often painted as a solitary affair, a lone journey on the quest towards knowledge. While you need to show substantial work that is your own in order to graduate, that does not mean that you are all on your own.On the contrary, being able to collaborate is an important skill that you will need later on. Many projects with a large impact in ML and NLP such as AlphaGo or OpenAI Five have been developed by a team. Whether you are part of a larger team or leading a group, you will have to collaborate with others.Collaboration dynamics are more fluid compared to the adviser-PhD relationship, which is typically well defined. Collaborations are about building trust and mutual respect. Successfully navigating collaborations takes practice. In collaborations, particularly if they are remote, it is important to communicate clearly and to set expectations.If you are working on two projects, make one of them a collaboration. Collaborating with someone different from your advisor introduces you to a new perspective and will allow you to learn more than working on your own.If you are based in a lab, collaborating with one of your lab mates is often the easiest choice. However, connecting and collaborating with people in other institutions may often be beneficial long-term.5) Be proactive.This is probably the most important piece of advice. Don&#39;t restrict yourself to the people in your immediate circle.Reach out to people. The main value of conferences is in bringing people together. Before a conference, look up who is going (by checking authors of accepted papers) and email them. Try to be respectful, briefly introduce yourself, and state why you&#39;d like to meet them (a useful mnemonic is Inigo Montoya&#39;s Guide to Networking Success). Most senior people make time for such meetings. Try to talk to many people and particularly seek out those who are not already well-known.Outside of conferences, it is often useful to ask people who have worked in your area for research advice via email. It&#39;s amazing to see how many people in our field are genuinely helpful. Proper email etiquette is important, however and makes it more likely that a busy researcher will respond. In particular, you should make it clear that you&#39;ve done your research and explored alternative solutions before contacting them.Beyond advice, such connections may lead to other opportunities further down the line: job offers, collaborations, mentorship, and even friendship. Many of my collaborations started through such connections—meetings at a conference, a cold email, a Twitter message. The important thing is that they are based on mutual interests and respect. So be conscious of other&#39;s people time. In addition, early career researchers with shared interests will often be much more open to collaborations than senior researchers who already have many commitments.Being proactive also relates to how you view and talk about your research: Make it easy for other people to discover your work by highlighting it on your website, talking about it online, and writing a blog.6) Write a blog.Blogging has many advantages. It allows you to practice writing—and to learn to enjoy it. In order to finish your PhD, you will have to write a thesis, which can be an excruciating process. Blogging provides the training ground that prepares you for the thesis marathon.From a research perspective, it allows you to practice communicating and explaining things clearly. Both are qualities that differentiate the best from mediocre research papers. In fact, clear writing is important both to get your paper accepted and for high impact. In contrast to the hyper-compact format of research papers, a blog allows you to experiment and to find your own voice.A blog can also be a great medium to present and share your work. A great blog post about a paper does not just reiterate its main findings but complements it. A blog can be much more flexible than a paper: You can highlight interesting connections, provide the reader with a broad overview of the background literature and future directions, walk through an illustrative example, highlight code snippets or qualitative examples, show interactive visualisations, or perform an in-depth error analysis.Another great way to start blogging is to discuss what you have just become knowledgeable about. Rachel Thomas puts this as &#34;you are best positioned to help people one step behind you&#34;. If you have just delved into a specialised area, why not save others the time and summarise the work and your insights. Most of my blog posts—from gradient descent to word embeddings—started this way. If you have just learned how to do something cool, tell others about it. Conversely, if you want to learn about a certain topic but cannot find information about it online, consider creating that resource yourself. Starting your own blog has never been easier.Having a blog is the single thing that has led to the most positive interactions throughout my PhD. ML and NLP have become so large that even if you write about a niche area, people will be interested. While I still feel anxious when I publish something, the response has always been worth it. In general, try to ignore unconstructive feedback and remember that the community appreciates genuine and honest voices.7) Keep a source of positive energy.External rewards such as paper acceptances are sparse, so leveraging intrinsic rewards is often necessary.The most natural way to stay positive and energised in research is to work on something that excites you and to follow your curiosity. Depending on your funding or position, you might not be able to choose what you work on. In those cases, try to find a particular angle that excites you. Even an application of an existing algorithm can shed light on new and unsolved questions.A PhD can be draining at the best of times. So it is important to build a support network that you can rely on. Surround yourself with positive people that support your ideas and ambitions. At the same time, find an activity that you can fall back on to give you positive energy when things don&#39;t go as planned. This can be a collaboration, a side project, a hobby, exercise, meditation, or something else. For me, blogging filled this need. Compared to the long stretches of radio silence during peer review, writing, publishing and receiving feedback on a blog all within a couple of days feels liberating.In the end, the most important resource is not the amount of compute you have, but your personal well-being. A crashed GPU can be rebooted; a burnt out GPU can&#39;t be fixed.8) Play to your strengths.“The most value comes from doing something no else can do, or no one else has thought of.”—Sam AltmanWith the increasing interest in ML and NLP, finding a fruitful undisturbed research topic can be challenging. A good strategy is to work on something that you are in the best position to tackle. Your ideal research topic sits at the intersection of work that is impactful, work that you are passionate about, and work that you are uniquely suited for.What makes you uniquely suited can be one of many things: Your background; your knowledge of a particular technology, method, language, or data; your personal preferences. Do you come from a non-CS background? Use this as inspiration for your work. Are you a visually creative person? Supplement your blog and papers with graphs and analyses that will inspire others. Are you a strong coder? Implement technically challenging models. Are you great at maths? Prove your claims mathematically.Another strength can be your network and the diversity of perspectives that you have access to. So locate others that complement your strengths, whether as advisors, mentors, or collaborators.9) Intern or visit a university.The best way to make meaningful connections is to collaborate closely with people and to get to know them in-person. Internships and research visits are both excellent opportunities to expand your network as they enable you to work side-by-side with a group of talented people day-to-day.They also allow you to get a feeling for how research is done in another environment. If you are considering whether to go into academia or industry, seeing first-hand how research is done in industry is an invaluable data point. A research visit or internship can also help you decide whether you would enjoy joining a lab or company at a later point.Lastly, both are amazing learning experiences as you often will need to get familiar with a new tech stack or new research area. Through the guidance of a knowledgeable mentor different from your advisor, you will also be able to focus on different aspects of your personal growth.10) Play the long game.Most of us are where we are because someone took a bet on us early on. My first research visit only happened because my host took a chance on me. So if you get the chance, pay it forward. Maximise not just the expected reward of yourself but of others around you.While being at a big institution gives you access to an initial network, in the long term you want to develop a network of smart people that you can work with. One of the best ways to build a network is by being proactive and helping people as much as you can. This can be through writing blog posts or libraries, publishing tutorials and courses, doing podcasts, reimplementing models, or helping with open-source software. If you do this consistently, you will develop a reputation for being diligent and helpful and people will want to work with you.Generally be kind to others. Assume good intentions. Be generous in giving praise and attribution. Don&#39;t hold grudges. In fact, being nice is one of the best things you can do to be successful (see Paul Graham&#39;s Mean People Fail). Being nice also has a recurring benefit as conferences are effectively—beyond the presentation and exchange of ideas—a yearly reunion of the friends you make along the way. Take care of yourself. Work hard but get enough sleep and exercise. Take the time to learn new things. Work on things that you are not an expert at. In the end, always remind yourself that while a PhD is supposed to culminate in a thesis, the more important outcome of a PhD is a better version of yourself. References and inspirationFinally, here are a few more pieces that served as inspiration:Andrey Karpathy&#39;s A Survival Guide to a PhDThe Frontiers in Natural Language Processing Expert Responses at the Deep Learning Indaba 2018John Schulman&#39;s An Opinionated Guide to ML ResearchAndrey Kurenkov&#39;s Lessons Learned the Hard Way in Grad School (so far)Volkan Cirik&#39;s PhD 101Tim Dettmer&#39;s How to Pick Your Grad SchoolIsabelle Augenstein&#39;s Increasing Well-Being in AcademiaRichard Hamming&#39;s You and Your ResearchFei-Fei Li&#39;s De-Mystifying Good Research and Good PapersSam Altman&#39;s How To Be Successful and associated Twitter threadStuart K. Card&#39;s The PhD Thesis DeconstructedTweets from Himan Abdollahpouri, Chip Huyen, and many others Newsletter If you want to receive regular updates about advances in machine learning and natural language processing, subscribe to my newsletter below. </description>
      <pubDate>23 Mar 21 13:12 EDT</pubDate>
      <guid>https://ruder.io/10-tips-for-research-and-a-phd/</guid>
    </item>
    <item>
      <title>Hammerspoon: A Better, Better Hyper Key</title>
      <link>http://evantravers.com/articles/2020/06/08/hammerspoon-a-better-better-hyper-key/</link>
      <description>&lt;a href=&#34;http://evantravers.com/articles/2020/06/08/hammerspoon-a-better-better-hyper-key/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This all started with Hyper. I talked in the last post about my history with the concept, how I learned from Steve Losh’s post on the topic and borrowed from Brett Terpstra… and I’ve expanded the idea a bit. At the moment, my hyper implementation is contained in a lua module called hyper.lua, with some dependencies on Karabiner-Elements.app. I’m using hs.hotkey.modal to capture an F19 keystroke, and only sending the “hyper chord” of ⌘⌥⇧⌃ if absolutely required. The code isn’t that complex so this post will be focused on the advantages of this approach. Before: hyper chord Traditionally, a Hyper key is implemented by sending to the Operating System “hyper chord” of ⌘⌥⇧⌃ by modifying the keyboard firmware or using Karabiner-elements.app. The user would then use some kind of automation software like Alfred or Keyboard Maestro to listen for the “hyper chord” and fire different automations. You can absolutely do this in Hammerspoon if you want. While it works well, it has its limitations. Using the “hyper chord” as the entire “hyper key”, you can’t add any more modifiers, because it is already all the modifiers. Because of this a lot of hyper key setups are limited to “leader key” style interactions. After: hs.hotkey.modal Using a single keycode as your “hyper” key, and handling the translation at the automation layer is much more expressive. In my case, Hammerspoon becomes a single “router” to all the automation and UI customization on my Mac. I use a single often-unused key (in my case, F19) to trigger a hs.hotkey.modal in Hammerspoon. Instead of having every single application listening to all the keystrokes, I can control it one place. In this as in all things, I am not the first. Brett Terpstra first wrote about this in “A Useful Caps Lock Key” in 2012. Hyper… shifted? One big advantage to using Hammerspoon as a “man-in-the-middle” is using modifiers with your hyper key. Because your “hyper key” is not a cluster of modifier keys, you can actually use it in conjunction with any normal modifiers. -- Press `HYPER+r`, get the Hammerspoon console. hyper:bind({}, &#39;r&#39;, nil, function() hs.console.hswindow():focus() end) -- Because my `HYPER` is actually F19, -- I can press `HYPER+SHIFT+R` -- -- Press `HYPER+⇧+R`, reload Hammerspoon configuration. hyper:bind({&#39;shift&#39;}, &#39;r&#39;, nil, function() hs.reload() end) (from init.lua) I presently only use this in a few bindings, but I’m excited about all kinds of interesting ways to use it: quitting instead of launching an app, automatically choosing a window layout, the options are endless! Truly Global Hotkeys: Local Bindings There is one problem… if you want to use your hyper key to bind to an in-app Preference… F19 is not supported by all applications. Things will not recognize F19 as a keycode, or a modifier. To handle this, when I press F19+. Hammerspoon translates that local binding as if I’m pressing ⌘⇧⌥⌃+.. Many apps give you the ability to set a hotkey in their preference panes for certain “global” tasks. Create to-do’s in Things 3. Open the Quick Capture in Drafts.app. Open the clipboard history, file manager, and main search window of Alfred. To answer this, I have developed a concept of “local bindings.” If you set any local_binding key for the configuration, hyper.lua will intercept the F19+&lt;key&gt; keypress and instead send the traditional ⌘⇧⌥⌃+&lt;key&gt; to the operating system… just like the Traditional model. First, set up the local bindings to the app in Hammerspoon. Then, use your new local bindings in the preference pane of the app of your choice. if app.local_bindings then -- for key in app.local_bindings for _, key in pairs(app.local_bindings) do hyper:bind({}, key, nil, function() [..] -- send hyper chord + key hs.eventtap.keyStroke({&#39;cmd&#39;,&#39;alt&#39;,&#39;shift&#39;,&#39;ctrl&#39;}, key) [..] end) end end (from hyper.lua) Even when the app is closed In addition, I have incorporated an idea from Shawn Blanc’s OopsieThings applescript. If the you trigger a local binding and Hammerspoon sees that app isn’t open, it’ll open it for you and then send the binding. -- if the app is open if hs.application.find(app.bundleID) then -- send hyper chord hs.eventtap.keyStroke({&#39;cmd&#39;,&#39;alt&#39;,&#39;shift&#39;,&#39;ctrl&#39;}, key) else -- launch the app hyper.launch(app) -- wait for it to launch hs.timer.waitWhile( function() return hs.application.find(app.bundleID) == nil end, function() -- then send hyper chord + key hs.eventtap.keyStroke({&#39;cmd&#39;,&#39;alt&#39;,&#39;shift&#39;,&#39;ctrl&#39;}, key) end) end That way my “send a tweet using Tweetbot” or “make a Draft” or especially “make a to-do in Things” buttons always work, regardless of whether the app is accidentally closed. 👍 Configuration Hammerspoon loads an init.lua file by default… so I have a couple of configuration tables declared in there that I pass into the other modules. I used to declare config as a global and just let all the other modules use it, but I wanted to make sure the modules can use whatever anyone wants to use,1 so now each module has a .start() method that takes as an argument a config table. Since lua stores nearly everything as a reference, I’m not worried about blowing out memory. Here is a minimalist example of a config for hyper.lua. I’ve removed some of the other keys and values for some of the other modules for the sake of discussion… just the config to launch Things 3, create local bindings for the capture options, and a quick function to reload the Hammerspoon config file. config = {} config.applications = { [&#39;Things&#39;] = { bundleID = &#39;com.culturedcode.ThingsMac&#39;, hyper_key = &#39;t&#39;, local_bindings = {&#39;,&#39;, &#39;.&#39;} } } -- load as global, it&#39;s going to be used hyper = require(&#39;hyper&#39;) hyper.start(config) hyper:bind({&#39;shift&#39;}, &#39;r&#39;, nil, function() hs.reload() end) It’s very straightforward. My most common use of Hyper.lua is to launch an application, so I have a table of applications that I can define a “hyper key” for, and optionally some local bindings that I bind inside that application to use globally. I use hyper.lua as the “entry point” for nearly all my Hammerspoon based automation… it’s great. All my keybindings are declared in one place, and I know they will never conflict with any new applications that I download. No more discovering that weird app behavior is due to a double-bound keybinding. As this series continues, I’ll list the examples of how I connect hyper.lua to other automations here. If you want to read Hyper.lua at the time of this blog post, it’s available on my GitHub. It’s possible it’s gone through new versions since this post. There’s a few features in there now that I am not covering in this post, but we’ll get to them later in this series. </description>
      <pubDate>07 Feb 21 15:00 EST</pubDate>
      <guid>http://evantravers.com/articles/2020/06/08/hammerspoon-a-better-better-hyper-key/</guid>
    </item>
    <item>
      <title>UBM, and a lament for the software industry</title>
      <link>https://georgestocker.com/2020/09/14/ubm-and-a-lament-for-the-software-industry/</link>
      <description>&lt;a href=&#34;https://georgestocker.com/2020/09/14/ubm-and-a-lament-for-the-software-industry/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I grew up in tech as a UBM fan. Ok, not really a fan, but a “there’s very few voices writing books about ‘how to write code well’, and this person seems to know what they’re doing, so I’m going to listen to him”. I read all his books; attended talks he gave at SCNA, and thought for a while that his way was the right way. That, of course, was a mistake. Much like anyone else who has ideas, most of UBM’s ideas are bad, a very few are good, and then you turn the light on and realize he’s about 10 seconds away from outright fascism. If you’ve read his tweets (don’t), he apparently has a particular affection for law and order in the form of “gosh, we apparently don’t have enough of it since there were shooting deaths over the weekend.” And normally you’d think this would align him with the pro gun-control crowd, but it doesn’t. Instead his answer is to give the police more money, and maybe they’ll find a way to get their clockwork stat of killing 3.2 Americans a day higher. It’s grotesque, and I normally wouldn’t spend any further typing on UBM, but I bring him up because it’s instructive of the tech bullshit that’s holding our industry back. I got into tech because I was the first kid on my block with a computer and a LAN. I wasn’t super competitive at sports in the “I want to win” sense, although growing up in America in the 80s meant that your worth as a human being was tied to whether your team won the game. The teams I was on seemed to lose a lot more than we won, and the few times I was on winning teams I could see the Jock’s kids doing it out of some sense of devotion and attention grabbing from their dads. My dad didn’t really give a crap about sports. He was a reporter. I love him (RIP), but he was an asshole. He liked to find the uncomfortable buttons and push them whether those buttons belonged to politicians or the local school board. It made him a good reporter. He worked for TV, several newspapers, and he was the first reporter on the scene when his eldest son, at the tender age of 7, was hit by a car in the 70s. That was shockingly hard on him for decades to come, and the alcoholism that ensued doomed his first marriage and almost tore his second marriage apart . Ironically the Congestive Heart Failure put a quick stop to the alcoholism. What’s that saying about frying pans and fires again? One of the things my dad used to say repeatedly to me was “it’s not the man with the answers, it’s the man with the questions.” He also liked to say “reporters report the news, journalists make the news.” Ten year old me had no idea what that meant, and 38 year old me has no idea whether he was pro-journalist or not with that statement. One thing he did focus on was being outside of the story. Never make yourself the story, he would say. UBM has a volume of work much greater than most of the modern tech space. Having been working in tech longer than I’ve been alive, it’s understandable. He was also one of the agile manifesto signatories, though with the passage of time it too seems to be a relic of another era and another war. The ire UBM draws seems to be more from his lack of empathy than anything else, and as someone whose father lacked empathy, the signs are all the same. As of this writing, there are 941 examples of police brutality against protestors that have risen up in response to decades of police abuse and the inhumane treatment of black people and people of color that has culminated in George Floyd’s murder at the hands of police. Let us also not forget Breonna Taylor, whom the prosecutors are currently trying to justify her murder by police after the fact by trying to get her boyfriend to sign a plea deal saying she was a part of his “criminal network”. These cases of brutality are well documented, and if you follow Radley Balko’s work, they are just the latest in the decades’ long undeclared war on the black populace by police. It’s during this time, when pain is at its greatest, when UBM tweets, “fund the police.” To which he rightfully got scorned. Only an asshole would say “pour more fuel on the fire” when it had consumed lives, neighborhoods, and our country. This lack of empathy is present in UBMs talks as well; he has had a constant refrain of “we are professionals and professionals do good work.” He also has some shaker woodshop view of software in calling himself a “software craftsman”, which I don’t think you could get any more white or male by referring to the best software developers as “craftsmen”. It’s interesting to note he holds software developers to a higher standard of professionalism than police, but such is the way of the fascist. The rules apply to thee, but not to the state. This is our software industry, and our industry is both responsible for some of today’s woes; but we can also contribute to a economic boom that no other industry can match with the amount of time it takes to become a developer. That’s the power of being in the software industry: You can go from intern to making six figures in less than 5 years, and some are making six figures right out of school. But there’s one thing missing: We’ve not yet conquered the racial and gender issues that hurt black people, people of color, women, transgender, and non-binary people especially. The issues that keep them from being a part of this economic boom. And as is often the case, the people in power are the cause. Whether inadvertently through a lack of empathy or intentionally, white men get to play the tech game on easy mode while everyone else has to jump through the hoops we’ve put in place. I’ve benefited from it, and I look around at the talent that we have in tech in under-represented minorities and I realize that if there were a level playing field; this industry would be far more diverse than it is now. I’ve become rather obsessed with Hamilton, the musical. This in of itself is an odd statement, given that I am (at best) neutral on the idea of a central bank, and harbor a bit of disdain for is historical evolution; but Hamilton the musical warmed me to the understanding of Hamilton the man. A man that previously was the scorn of libertarians everywhere; and this musical humanized him. More so than that, the Musical showed the range and depth of talent that this unique re-telling of history could bring to the table. There’s nothing quite like Hamilton out there, and there’s no way that would have ever seen the light of day with a mostly white cast and white playwright at the helm. Lin-Manuel Miranda’s work has enriched our society; and it’s past time for us to give diversity its due. His work is just a recent example; and there are dozens of others. How many Hamilton’s have been squashed in the software industry due to gatekeeping? How many Lin-Manuel Miranda’s have been pushed away? How much oxygen have white “thought leaders” taken out of the room? Often, when under represented minorities in tech “make it”, it’s in-spite of this climate, not because of it. One of the things we can do is to amplify other people’s voices, and to me that’s one of the problems UBM has: He doesn’t amplify other people’s work. As someone who has spent a lifetime in the software industry and become famous, he should. He has a large following, and using it to raise others up would improve the entire industry. My disappointment with UBM starts with the idea that as a leader he has an obligation to improve the industry; but he can’t see the injustice in front of his own nose. We’ve stopped serving humans with the software we build. We’ve stopped focusing on the humans the software affects in search of basis points of engagement to improve. We have created entire industries to serve at the pleasure of investment returns for venture funds while ignoring the costs associated with serving VCs’ interests. And all the while we’ve done this, we’ve hurt the most vulnerable among us. If you’re Anglo-saxon, there’s good money that you identify as a Christian, and Jesus framed his entire teachings around the idea of lifting up and helping those around you. The irony abounds, and the startup founders cash another venture capital check. We are missing out on so much by gatekeeping; even if it is unintentional. We lose nothing by adding diverse voices to our teams and we gain so much by intentionally making our teams more diverse; from our leaders on down. If we are really supposed to be the leaders of tomorrow, if software is supposed to revolutionize living, then we have to start with who gets a seat at the table, and if that table is filled with mostly white dudes, we’re never going to reach our potential as an industry or as a force for change. We need new leaders, diverse leaders who can help evolve our industry past its roots and make it a force for good for all people, and we’re not going to get there by listening to the people who got us here. I have no answers here, as I have very little experience with this; so I’m going to defer to someone who does. </description>
      <pubDate>29 Mar 21 10:01 EDT</pubDate>
      <guid>https://georgestocker.com/2020/09/14/ubm-and-a-lament-for-the-software-industry/</guid>
    </item>
    <item>
      <title>The power of imagination, and other parenting lessons from Calvin and Hobbes</title>
      <link>https://www.washingtonpost.com/lifestyle/2019/04/05/power-imagination-other-parenting-lessons-calvin-hobbes/</link>
      <description>&lt;a href=&#34;https://www.washingtonpost.com/lifestyle/2019/04/05/power-imagination-other-parenting-lessons-calvin-hobbes/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;My then-7-year-old son hurried into snow pants as he eyed the somersaulting flakes outside the window. “Are you going to build a snowman?” I asked him skeptically. Though only a couple of inches covered the ground, I knew from experience that scant accumulation was no obstacle to a determined child.Otis nodded. “I’m going to cut a big hole in middle of his body. Like he was shot by a cannon. But first,” his voice revved with excitement, “I’m going to build him in the driveway so that when you back the car down, he’ll decapitate!”My son, contrary to appearances, is not a sadistic murderer. He’s a reader. Late the previous night, I had caught him with a flashlight, scouring “Attack of the Deranged Mutant Killer Monster Snow Goons” under the covers.One day recently we had found a stack of Calvin &amp; Hobbes books in the “free” bin outside the local library. It had been years since I had read the books. I couldn’t recall specific story lines, but I remembered the strip as one that reliably made me laugh. I grabbed the entire stack, chuckling at the ingenuity of the titles as I slid them into a cloth bag. “The Revenge of the Baby-Sat.” “Something Under the Bed is Drooling.” “Weirdos from Another Planet.” “Scientific Progress Goes Boink!”“What is baby-sat?” Otis asked.“The ones being watched by the babysitter,” I replied. “You know, you and your sister.”Did I see a glimmer in his eye?Otis picked up “Homicidal Psycho Jungle Cat.”“What does ‘homicidal’ mean?”I snatched the book out of his hands. I definitely remembered how inappropriate some of the content could be. The violence Calvin inflicts on snowmen could alone earn it an R rating. “These are for me,” I said evasively. Classic parenting mistake. The non-answer answer. The disguised “no.” I’d just done the verbal equivalent of cordoning off the evidence with yellow police tape. Do Not Enter! Absolutely Forbidden! What could be a more tantalizing invitation? I wasn’t too worried though. Surely the intimidatingly advanced vocabulary and amorphous philosophical ideas would be a natural barrier for such a young reader.Of course, my son proved me wrong. Within the week, Otis had transferred the pile of books from my nightstand to the cavity under his bed. He walked around, ignoring everyone, splayed book in hand. He read at the breakfast table. He read in the bath. He read in the car on the way to school and then on the way home. “Do you remember what our son’s voice sounds like?” I joked to my husband.In the weeks that followed, I watched as my son’s brain caught fire. The books boosted his vocabulary and pushed him into the highest reading group at school, but, perhaps equally as importantly, they transformed his pretend play. Calvin inspired Otis to dream bigger. Galactically bigger. Otis improved on the many methods Calvin used to murder snowmen.Otis spent days fiddling with string and boxes, constructing and rigging booby traps. He suspended one, a cardboard box filled with confetti, from the roof of the front porch, and laid in wait for someone to ring the bell. The contraption misfired, but I was so impressed with Otis’s industry, I left it hanging. (I would not have blamed the poor postal worker for skipping our house.) Then it was on to the next prank. Unbeknown to me, he had been sneaking around after bedtime, placing bubble wrap in strategic locations. On my way to the bathroom in the middle of the night, I became the first victim. Otis peeked out from his room. The look of satisfaction on his face at the sound of my scream sent chills up my spine.Though Calvin often walks a precocious line in the strip, my child, like many children, clearly got the jokes. “Calvin agrees with you that kids should play outside,” he informed me during a particularly long stretch of days off from school, “But if it’s raining, then the best thing to do is drive your mom crazy.”More and more often, I found myself saying my son’s name in a strangled tone, my patience thin, and I realized that, at some point, my empathy and allegiance had distinctly transferred from Calvin to Calvin’s Mom.One day, I asked Otis what he thought of Hobbes. “What do you believe? Is he real or is he stuffed?” In a tone that connoted my knucklehead status, my son answered, “He’s a real tiger, but for some reason grown-ups think he’s a stuffed animal. I guess they just don’t know any better.”His explanation silenced me. I realized I’d made a grave mistake. I was one of the (stupid) grown-ups. Never once had I considered that Hobbes might be real, because, well, he isn’t. But, belief does not come via verisimilitude. It is a house without struts, a bird without wings, where the former stands and the latter flies. Whether Hobbes was live or stuffed was beside the point. To believe in Hobbes is to believe in the power of imagination.Perhaps a stuffed Hobbes is the symbolic representation of parental limitation and inadequacy, the depth and degree to which we lose sight of what it’s like to be a child. Hobbes is real because children’s feelings are real. Calvin wants to run away to the Yukon when he’s angry or frustrated. He throws apoplectic fits at mealtime because he doesn’t have the power of choice. He daydreams in class because he’s bored. He causes destruction and wreaks havoc because he’s curious. When we, as parents, focus only on the “bad” behavior, we miss the opportunity to understand the motivation and, ultimately, our children. When I asked my son why he liked Calvin so much, he said, simply, “He understands me.”I’ve read my fair share of parenting books, but I’d put “Calvin &amp; Hobbes” at the top of the list of required reading. It reminds me to change my point of view. To listen. To hear not just what my child says with his mouth but also what he conveys through his actions. Again and again, I’m reminded that I was deluded to think parents raise children. In truth, they are the ones raising us.On Halloween this past year, Otis dressed as Calvin. He wore a striped shirt and a pair of shorts. He tucked a stuffed Hobbes under his arm. We ruffled his hair. He carried a pillowcase. On it, he scrawled, in black Sharpie, “Susie Derkins is a Nincompoop.” I accompanied him trick-or-treating dressed as Susie Derkins, in the clothes Otis picked for me. I wore a black top and skirt and carried a bouquet of dead flowers. My husband and I watched our son and daughter scamper up to houses, ring doorbells, then return to where we stood waiting on the sidewalk. We felt it keenly, the tether between children and parents, a piece of gum stretching longer and thinner; at some point, the distance untenable, it must snap: They find independence.That night, at bedtime, while we snuggled, Otis turned to me, looking serious. “I think Hobbes might be stuffed.” My heart shivered with loss, as it had with previous transitions from one phase to another, in life’s irrevocable vector. But I said only, “Is he?”Molly Pascal is a freelance writer living in Pittsburgh. Find her on Twitter @MollyPascal.More reading:</description>
      <pubDate>14 Mar 21 23:53 EDT</pubDate>
      <guid>https://www.washingtonpost.com/lifestyle/2019/04/05/power-imagination-other-parenting-lessons-calvin-hobbes/</guid>
    </item>
    <item>
      <title>Are Deep Neural Networks Dramatically Overfitted</title>
      <link>https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html</link>
      <description>&lt;a href=&#34;https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Mar 14, 2019 by Lilian Weng </description>
      <pubDate>05 Apr 21 13:51 EDT</pubDate>
      <guid>https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html</guid>
    </item>
    <item>
      <title>Blame It on the Phones</title>
      <link>https://blog.nukemberg.com/post/blame-it-on-the-phones/</link>
      <description>&lt;a href=&#34;https://blog.nukemberg.com/post/blame-it-on-the-phones/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Since I’ve abandoned Facebook my primary source of tech news has become Twitter and this week my feed is raging with two seemingly unrelated security/privacy incidents: Zoom’s zero day and Superhuman’s email tracking scandal. I write “seemingly”, because despite these being two very different companies operating in two different markets (Zoom in video conference calls and Superhuman in emails), building very different products (Zoom is all about jump in, jump out - Superhuman is a workspace) these incidents stem from the same fundamental fault: The telephone experience.Not so long ago, when people still used to call each other and conduct phone calls all you had to do to talk to someone was click on the contact (or punch in the phone number), wait a few moments for the other side to answer and violla, start talking. On the receiver end, you got a notification and connected with one click (or picked up the phone). Privacy expectations were pretty clear - either side could record the conversation, mute their side or disconnect and had reasonable expectation that 3rd parties were not involved (other than trusted carriers). Each side knows that they are connected and can reasonably detect the other side is still engaged.This model is so pervasive that almost every digital communication platform tries to mimic it - just pop open your video chat app and look at the controls, have they changed much conceptually? But the click-once-to-connect model, while being easy from a local usability viewpoint, is also fundamentally flawed on a more broad level. Let’s analyze what would happen on a public network where any two parties can connect to each other using this model, remembering how humans actually behave in the real world. One click connection means minimal attention, which on a public system immediately clashes with an identity problem: How do we know who’s on the other side? Not surprisingly, the phone system is plagued with fraud and unsolicited calls. We’ve tried, and failed, to resolve these issues with Caller id and smartphone’s contact management - because such identity mapping system has obvious problems like name take-over, typosquatting etc (DNS anyone?). But still, you have control over when and if a 2nd party was connected, right? turns out it isn’t so good - as pocket calls often remind us. What about trusted carriers and freedom from 3rd parties? also turns out to be problematic as this model doesn’t say anything about ownership - think corporate phones and switchboards. If a 3rd party pays for communication, do you trust it? do you trust the providers it chose to work with? what happens when multiple such parties communicate? who owns the call? the participants or the client who paid?Note that I haven’t wrote a word about the underlying technology - because these “bugs” are in the product experience model, not the code. Thus they will inevitably appear in some form on any system which tries to recreate that experience. What is happening today is a modern version of all the things we hated telephones for. And this was before attention and focus became the scarcest of resources.It’s quite clear how video chats got here. But what does email tracking got to do with it? you see, the original “mail” experience never had this “tracking” feature. You sent a mail, and the only way you knew the recipient got your message was if they chose to respond; mail was strictly asynchronous. But as the one-click model prevailed, various companies tried to make email more phone like - where you knew the other side picked up. The email protocol doesn’t support this of course, and so they hacked it, exploiting email clients along the way - not unlike how Zoom hacked browser security policies to shave user clicks off. And what a surprise, it was abused… much the same way people used call-and-immediately-hangup over phones to detect if you were home.Make no mistake: Zoom and Superhuman incidents are not a code bug; they are product bug.</description>
      <pubDate>07 Feb 21 17:12 EST</pubDate>
      <guid>https://blog.nukemberg.com/post/blame-it-on-the-phones/</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.evjang.com/2021/02/backprop.html</link>
      <description>&lt;a href=&#34;https://blog.evjang.com/2021/02/backprop.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; “Traducción a Español”Biologically Plausible Deep Learning (BPDL) is an active research field at the intersection of Neuroscience and Machine Learning, studying how we can train deep neural networks with a &#34;learning rule&#34; that could conceivably be implemented in the brain.The line of reasoning that typically motivates BPDL is as follows:A Deep Neural Network (DNN) can learn to perform perception tasks that biological brains are capable of (such as detecting and recognizing objects).If activation units and their weights are to DNNs as what neurons and synapses are to biological brains, then what is backprop (the primary method for training deep neural nets) analogous to?If learning rules in brains are not implemented using backprop, then how are they implemented? How can we achieve similar performance to backprop-based update rules while still respecting biological constraints?A nice overview of the ways in which backprop is not biologically plausible can be found here, along with various algorithms that propose fixes.My somewhat contrarian opinion is that designing biologically plausible alternatives to backprop is the wrong question to be asking. The motivating premises of BPDL makes a faulty assumption: that layer activations are neurons and weights are synapses, and therefore learning-via-backprop must have a counterpart or alternative in biological learning.Despite the name and their impressive capabilities on various tasks, DNNs actually have very little to do with biological neural networks. One of the great errors in the field of Machine Learning is that we ascribe too much biological  meaning to our statistical tools and optimal control algorithms. It leads to confusion from newcomers, who ascribe entirely different meaning to &#34;learning&#34;, &#34;evolutionary algorithms&#34;, and so on.DNNs are a sequence of linear operations interspersed with nonlinear operations, applied sequentially to real-valued inputs - nothing more. They are optimized via gradient descent, and gradients are computed efficiently using a dynamic programming scheme known as backprop. Note that I didn&#39;t use the word &#34;learning&#34;!Dynamic programming is the ninth wonder of the world1, and in my opinion one of the top three achievements of Computer Science. Backprop has linear time-complexity in network depth, which makes it extraordinarily hard to beat from a computational cost perspective. Many BPDL algorithms often don&#39;t do better than backprop, because they try to take an efficient optimization scheme and shoehorn in an update mechanism with additional constraints. If the goal is to build a biologically plausible learning mechanism, there&#39;s no reason that units in Deep Neural Networks should be one-to-one with biological neurons. Trying to emulate a DNN with models of biologically neurons feels backwards; like trying to emulate the Windows OS with a human brain. It&#39;s hard and a human brain can&#39;t simulate Windows well.Instead, let&#39;s do the emulation the other way around: optimizing a function approximator to implement a biologically plausible learning rule. The recipe is straightforward:Build a biological plausible model of a neural network with model neurons and synaptic connections. Neurons communicate with each other using spike trains, rate coding, or gradients, and respect whatever constraints you deem to be &#34;sufficiently biologically plausible&#34;. It has parameters that need to be trained.Use computer-aided search to design a biologically plausible learning rule for these model neurons. For instance, each neuron&#39;s feedforward behavior and local update rules can be modeled as a decision from an artificial neural network.Update the function approximator so that the biological model produces the desired learning behavior. We could train the neural networks via backprop. The choice of function approximator we use to find our learning rule is irrelevant - what we care about at the end of the day is answering how a biological brain is able to learn hard tasks like perception, while respecting known constraints like the fact that biological neurons don&#39;t store all activations in memory or only employ local learning rules. We should leverage Deep Learning&#39;s ability to find good function approximators, and direct that towards finding a good biological learning rules.The insight that we should (artificially) learn to (biologically) learn is not a new idea, but it is one that I think is not yet obvious to the neuroscience + AI community. Meta-Learning, or &#34;Learning to Learn&#34;, is a field that has emerged in recent years, which formulates the act of acquiring a system capable of performing learning behavior (potentially superior to gradient descent). If meta-learning can find us more sample efficient or superior or robust learners, why can&#39;t it find us rules that respect biological learning constraints? Indeed, recent work [1, 2, 3, 4, 5] shows this to be the case. You can indeed use backprop to train a separate learning rule superior to naïve backprop.I think the reason that many researchers have not really caught onto this idea (that we should emulate biologically plausible circuits with a meta-learning approach) is that until recently, compute power wasn&#39;t quite strong enough to both train a meta-learner and a learner. It still requires substantial computing power and research infrastructure to set up a meta-optimization scheme, but tools like JAX make it considerably easier now.A true biology purist might argue that finding a learning rule using gradient descent and backprop is not an &#34;evolutionarily plausible learning rule&#34;, because evolution clearly lacks the ability to perform dynamic programming or even gradient computation. But this can be amended by making the meta-learner evolutionarily plausible. For instance, the mechanism with which we select good function approximators does not need rely on backprop at all. Alternatively, we could formulate a meta-meta problem whereby the selection process itself obeys rules of evolutionary selection, but the selection process is found using, once again, backprop.Don&#39;t mess with backprop!Footnotes[1] The eighth wonder being, of course, compound interest. </description>
      <pubDate>15 Feb 21 15:18 EST</pubDate>
      <guid>https://blog.evjang.com/2021/02/backprop.html</guid>
    </item>
    <item>
      <title>DuckDuckGo’s Quest to Prove Online Privacy Is Possible</title>
      <link>https://www.wired.com/story/duckduckgo-quest-prove-online-privacy-possible/</link>
      <description>&lt;a href=&#34;https://www.wired.com/story/duckduckgo-quest-prove-online-privacy-possible/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I was driving up through Pennsylvania last summer, somewhere along US Route 15 between Harrisburg and Williamsport, when I saw a familiar face: a goofy cartoon duck wearing a green bowtie. It was the logo for DuckDuckGo, the privacy-focused search engine, along with a message: “Tired of Being Tracked Online? We Can Help.” The sight of a tech company on a billboard in rural Pennsylvania was surprising enough to lodge in my memory. Highways in and out of Silicon Valley may be lined with billboards advertising startups, where they can be easily spied by VCs and other industry influencers, but the post-industrial communities hugging the Susquehanna River will never be confused with Palo Alto. Far more typical are road signs advertising a fireworks store, a sex shop, or Donald Trump. I found it hard to imagine that the other drivers on the road were really the audience for an internet company that occupies a very specific niche. It turns out DuckDuckGo—itself based in Valley Forge, PA, about 90 miles east of Route 15—knew something I didn’t. According to the company’s market research, just about every demographic wants more data privacy: young, old, male, female, urban, rural. Public polling backs that up, though the results vary based on how the question is asked. One recent survey found that “93 percent of Americans would switch to a company that prioritizes data privacy if given the option.” Another reported that 57 percent of Americans would give up personalization in exchange for privacy. Perhaps most telling are the early returns on Apple’s new App Tracking Transparency system, which prompts iOS users to opt in to being tracked by third-party apps rather than handing over their data by default, as has long been standard. According to some estimates, only a tiny minority of users are choosing to allow tracking. The problem for a company like DuckDuckGo, then, isn’t making people care about privacy; it’s convincing them that privacy is possible. Many consumers, the company has found, have basically thrown up their hands in resignation, concluding that there’s no way out of the modern surveillance economy. It’s easy to see why. Each new story about data privacy, whether it’s about the pervasiveness of tracking, or a huge data breach, or Facebook or Google’s latest violation of user trust, not only underscores the extent of corporate surveillance but also makes it feel increasingly inescapable. DuckDuckGo is on a mission to prove that giving up one’s privacy online is not, in fact, inevitable. Over the past several years, it has expanded far beyond its original search engine to provide a suite of free privacy-centric tools, including a popular browser extension, that plug up the various holes through which ad tech companies and data brokers spy on us as we browse the internet and use our phones. This year it will roll out some major new products and features, including a desktop browser and email privacy protection. And it will spend more money than it ever has on advertising to get the word out. The long-term goal is to turn DuckDuckGo into an all-in-one online privacy shield—what Gabriel Weinberg, the company’s founder and CEO, calls “the ‘easy button’ for privacy.”“People want privacy, but they feel like it’s impossible to get,” Weinberg says. “So our main challenge is to make the idea that you can get simple privacy protection credible.”Whether that mission succeeds could have consequences far beyond DuckDuckGo’s bottom line. DuckDuckGo is operating to some extent in the shadow of Apple, which has already made privacy a core part of its pitch to customers. But DuckDuckGo’s ambition is to provide a suite of protections that are even more extensive and intuitive than Apple’s. And it is offering them to the millions of people who don’t want or can’t afford to use Apple products: Google’s Android operating system accounts for about 50 percent of the mobile market in the US and more than 70 percent worldwide. Perhaps most important, if DuckDuckGo succeeds at bringing simple privacy to the masses, it will mean that the future of privacy might not depend on the relative benevolence of just two corporate overlords. Founded in 2008, DuckDuckGo is best known for its search engine. Which means that it has always been defined as a challenger to Google. It has not shied away from the comparison. In 2011, Weinberg, then the company’s sole employee, took out an ad on a billboard in San Francisco that declared, “Google tracks you. We don’t.” That branding—Google, but private—has served the company well in the years since.“The only way to compete with Google is not to try to compete on search results,” says Brad Burnham, a partner at Union Square Ventures, which gave DuckDuckGo its first and only Series A funding in 2011. When the upstart launched, Google already controlled 90 percent of the market and was spending billions of dollars, and collecting data on billions of users, to make its product even better. DuckDuckGo, however, “offered something that Google couldn’t offer,” Burnham says: “They offered not to track you. And Google’s entire business model is, obviously, built on the ability to do that, so Google couldn’t respond by saying, ‘OK, we won’t track you either.’” Neither DuckDuckGo nor anyone else came close to stopping Google from dominating search. Today, Google’s market share still hovers around the 90 percent range. But the pie is so enormous—advertisers spent $60 billion on search advertising in the US alone last year, according to eMarketer—that there’s quite a bit of money in even a tiny slice. DuckDuckGo has been profitable since 2014. Like Google Search, DuckDuckGo makes money by selling ads on top of people’s search results. The difference is that while the ads you see when searching on Google are generally targeted to you in part based on your past searches, plus what Google knows about your behavior more broadly, DuckDuckGo’s are purely “contextual”—that is, they are based only on the search term. That’s because DuckDuckGo doesn’t know anything about you. It doesn’t assign you an identifier or keep track of your search history in order to personalize your results.This non-creepy approach only protects you, however, while you’re on DuckDuckGo. “You’re anonymous on the search engine, but once you click off, now you’re going to other websites where you’re less anonymous,” Weinberg says. “How can we protect you there?” DuckDuckGo’s first answer to that question rolled out in 2018, with the launch of a desktop browser extension and mobile browser that block third-party trackers by default wherever a user goes on the internet. It was good timing: 2018 was a banner year for raising privacy awareness. Facebook’s Cambridge Analytica scandal broke that spring. The GDPR took effect in Europe, throwing into relief how little the US regulates data collection. That summer, the Associated Press revealed that many Google services were storing your location data even if you explicitly opted out. Data collection and privacy were firmly in the national conversation. Since then, congressional inquiries, antitrust lawsuits, Netflix documentaries, and a growing feud between Apple and Facebook have kept it there. “One of the funny things about DuckDuckGo is that the single best marketing we’ve ever had has been the gaffes that Google and Facebook have made over the years,” says Burnham. “Cambridge Analytica, for instance, was a huge driver of adoption for DuckDuckGo. There is an increasing awareness of how this business model works and what it means—not just in terms of the loss of privacy and agency over our own data, but also what it means for the vibrance and success of an open marketplace.”Information about you, what you buy, where you go, even where you look is the oil that fuels the digital economy.Awareness is one thing, action another. DuckDuckGo was in position to capitalize on the rising tide of scandal because it has a reputation for building products that work. In 2019, for instance, it added a feature to its extension and browser that directs users to encrypted versions of websites whenever possible, preventing would-be hackers or ISPs from, say, looking over your shoulder as you type a password into a web page. While other encryption tools work by manually creating lists of tens of thousands of websites in need of an upgrade, DuckDuckGo crawled the internet to automatically populate a list of more than 12 million sites. The Electronic Frontier Foundation recently announced that it would incorporate DuckDuckGo’s dataset for its own HTTPS Everywhere extension. Similarly, Apple uses DuckDuckGo’s Tracker Radar dataset—a continuously updated, publicly available list of trackers assembled using open-source code—for Safari’s tracking prevention. Weinberg is particularly proud of DuckDuckGo’s tracker prevention. Surveillance is so built into the infrastructure of the web that many sites will stop functioning if you block all cookies. Take Google Analytics, which is found on the vast majority of websites. “If you just straight-up block Google Analytics, you’ll break sites,” Weinberg says. As a result, mainstream browsers with tracking prevention, like Safari and Firefox, allow trackers to load, then try to restrict the data they can gather. “They’re more inclined to err on the side of not breaking websites,” explains Bennett Cyphers, a technologist at the Electronic Frontier Foundation. “They will try and do this middle ground thing where they’ll load resources but restrict what Google can do once it’s in your browser.” The problem is that even allowing a tracker to load in the first place can allow it to gather highly specific data about the user, including their IP address. So DuckDuckGo, like some other privacy extensions, works differently. It simply prevents the cookie from loading at all. To avoid the broken-site problem, it replaces some trackers with a dummy that essentially tricks the site into thinking the cookie has loaded, a technique called “surrogates” pioneered by the ad blocker uBlock Origin. Ultimately, DuckDuckGo probably owes its success less to the technical aspects of its tracker prevention, which very few people are in any position to understand, than to the fact that the company does a pretty good job honoring its slogan: “Privacy, simplified.” Its products don’t require a user to toggle any elaborate settings. They simply include encryption, tracker blocking, and private search automatically. Since their launch, the extension and mobile browser have experienced rapid user growth. According to DuckDuckGo, the extension and browser have together been downloaded more than 100 million times since 2018, and more than half of those downloads took place over the past twelve months. That growth has in turn helped juice the use of the original search engine, which is built into mobile app. The company estimates that its search user count doubled over the past year to between 70 and 100 million. (It’s an estimate because they don’t track users.) According to StatCounter, DuckDuckGo now has the second highest share of the US mobile search market, edging out Bing and Yahoo. (A distant second, that is: 2 percent to Google’s 94 percent.) DuckDuckGo says its annual revenue is over $100 million.This year, the company plans to significantly expand its privacy offerings. It is introducing a desktop browser, incorporating the same features as the existing mobile app. Currently, even someone with the DuckDuckGo privacy extension can’t stop Google from gathering some data on them if they’re using Chrome, for example. DuckDuckGo is also adding two new features to its existing extension and mobile app. The first is email privacy protection. Weinberg says that his company’s researchers found that some 70 percent of emails have some sort of tracker embedded in them. That includes not just corporate promotional emails, but just about any newsletter or fundraising email that’s sent using an automated service. In nearly a third of those cases, Weinberg says, the trackers are sending users’ plaintext email addresses over the internet, potentially exposing them to any number of marketers, data brokers, and shadier actors. The email tool is designed to thwart that by forwarding messages through a DuckDuckGo email address, which will remove the trackers before sending them along to inboxes. It also will allow people to generate random email addresses whenever they have to use email to sign up for something.  (Apple recently announced a similar feature for the Mail app on iOS.) In theory, DuckDuckGo could have created its own email client, but Weinberg recognizes getting users to switch their email providers is prohibitively difficult. “Our goal is simplicity, right?” he says. “We want to make privacy simple and seamless without sacrifice for users.” The final new tech DuckDuckGo is unveiling this year operates on a similar principle. A new feature within its Android app will operate in the background, even when the app itself is not in use, to block third parties from tracking you through any other app on your phone. It does that by using the phone’s VPN permission to route all traffic through DuckDuckGo, so that, as with the email trackers, it can block requests from anyone on its tracker list before they have an opportunity to gather any user data. (Again, this is somewhat analogous to Apple’s App Tracking Transparency on iOS. It will not stop first-party data collection, meaning the app you’re using can still collect your data. But it won’t be able to pass that data through to other companies, including Facebook, which currently tracks users through a vast number of unrelated apps.)Taken together, the new features, which the company says will be available in beta this summer, represent DuckDuckGo’s evolving mission to create what Weinberg calls “the privacy layer of the internet.” “The ideal case for that from a user perspective is, you download DuckDuckGo and you’re just protected wherever you go online,” he says. “We’re obviously not there yet, but that’s the product vision.”So, about those billboards. The company’s reliance on old-school advertising mediums—in addition to billboards, DuckDuckGo is partial to radio ads—is partly of necessity: As a privacy-focused business, it refuses to do any microtargeted online advertising. (Even when it advertises on a social media site like Twitter, Weinberg says it doesn’t set any demographic targeting parameters.) But the strategy also stems from the company’s market research, which has found that precise targeting would be a waste of money anyway. “People who care about privacy, who act on privacy, who would adopt a DuckDuckGo product—they’re actually not a very niche audience,” says Zac Pappis, head of the company’s user insight team. “People who act and care about privacy don’t fall into a particular age group or demographic or have a particular psychographic background, so that makes them easier to reach.”To put it in advertising parlance, this means DuckDuckGo spends its marketing budget on brand awareness. Ordinary people around the country don’t need to be convinced to care about privacy, the theory goes—they just need to learn that a solution exists. “Our current top business priority is to be the household name for simple online privacy protection,” Weinberg says. “So when you think about privacy online, we want you to turn to DuckDuckGo.” To that end, the company is investing in its biggest marketing blitz to date this year, devoting tens of millions to an advertising push—so expect more billboards and more radio ads during those summer road trips. Weinberg believes the time is ripe. He points out the fact that tech giants like Apple, Facebook, and Google have all been raising the salience of privacy through very public battles over their policies and products. Plus, the ongoing antitrust lawsuits against the tech giants will draw more attention to those companies’ business practices, including around user privacy. One of the cases, brought by the Department of Justice, could even give DuckDuckGo a direct boost by preventing Google from being set as the default search engine on phones. DuckDuckGo has competition. Companies like Ghostery offer tracking protection. Brave has a well-regarded privacy browser. The Netherlands-based Startpage offers search without tracking. But in the US, at least, DuckDuckGo has a strong position in the privacy market. In a sector where users have to trust that your product works the way you say it does, a decade-long track record without any privacy scandals establishes important credibility. “They’re probably the biggest name right now, probably because of the popularity of their search engine,” says Jon Callas, director of technology products at the Electronic Frontier Foundation. But being the biggest name among people with a special interest in online privacy still amounts to being a big fish in a small pond. Weinberg believes DuckDuckGo can change that. He is convinced that the pond is actually huge. It just doesn’t know it yet. More Great WIRED Stories📩 The latest on tech, science, and more: Get our newsletters!What really happened when Google ousted Timnit GebruWait, vaccine lotteries actually work?How to turn off Amazon SidewalkThey rage-quit the school system—and they&#39;re not going backApple World&#39;s full scope is coming into focus👁️ Explore AI like never before with our new database🎮 WIRED Games: Get the latest tips, reviews, and more🏃🏽‍♀️ Want the best tools to get healthy? Check out our Gear team’s picks for the best fitness trackers, running gear (including shoes and socks), and best headphones</description>
      <pubDate>16 Jun 21 10:51 EDT</pubDate>
      <guid>https://www.wired.com/story/duckduckgo-quest-prove-online-privacy-possible/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.sajari.com/blog/vectors-versus-hashes</link>
      <description>&lt;a href=&#34;https://www.sajari.com/blog/vectors-versus-hashes&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Artificial intelligence has been built on the back of vector arithmetic. Recent advances show for certain AI applications this can actually be drastically outperformed (memory, speed, etc) by other binary representations (such as neural hashes) without significant accuracy trade off.Once you work with things like neural hashes, it becomes apparent many areas of AI can move away from vectors to hash based structures and trigger an enormous speed up in AI advancement. This article is a brief introduction in to the thinking behind this and why this may well end up being an enormous shift.HashesA hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values, hash codes, digests, or simply hashesYou can read more about hashes here. The example from Wikipedia is illustrated below.Hashes are great for trading off accuracy, data storage size, performance, retrieval speed and more.Importantly they are probabilistic in nature so multiple input items can potentially share the same hashes. This is interesting because at the core the trade off is giving up slower exactness for extremely fast high probability. The analogy here would be the choice between a 1 second flight to somewhere random in the suburb of your choosing in any city in the world vs a 10 hour trip putting you at the exact house you wanted in the city of your choice. The former is almost always better, navigating within a suburb in 10 hours is a piece of cake.When thinking about vectors, floats are the data representation of choice. Although they are more absolute in nature than hashes, they are still not exact either. More on floats below…FloatsTo understand AI you need to understand how computers represent non integer based numbers. If you have not read up on this, you can do here.The problem with floating point numbers is they take up a decent amount of space, are pretty complex to do calculations with and are still an approximation. Watching Rob Pike talk about a bignum calculator was prob the first time I thought about it much. It’s bothered me a lot since. Thanks Rob 😁.The binary representation can also be wildly different for tiny numerical changes (with respect to vector calculations) that have virtually zero impact on model predictions. For example:Take 0.65 vs 0.66 which in float64 (64 bit floating point) binary can be represented by these two binary numbers respectively:1111111110010011001100110011001100110011001100110011001100110111111111100101000111101011100001010001111010111000010100011111It’s not easy to see, but with just that 1% numerical change, almost half (25 of the 64 bits) are different! From a vector perspective in a matrix calculation these two numbers are very, very similar, but in the underlying binary (where all the heavy lifting happens) they are worlds apart.Our brains definitely don’t work like this, so they obviously don’t use floating point binary representations to store numbers. At least it sounds like a stupid thing for neurons to do, except there are people that can remember over 60,000 decimal places of Pi, so maybe I have no idea. But seriously, our brains are visual and visually our brain’s neural networks are great at handling fractional numbers representing intensities and such. But when you think of a half or a quarter, I’ll bet you immediately visualised something like a glass half or quarter full, or a pizza or something else. You likely weren’t thinking of a mantissa and exponent.One idea commonly used to speed float calculations up and use less space is dropping the resolutions to float16 (16 bit) and even float8 (8 bit) which are much faster to compute. The downside here is the obvious loss of resolution.So you’re saying float arithmetic is slow/bad?Not quite. Actually it turns out this is a problem people have spent their careers on. Chip hardware and their instruction sets have been designed to make this more efficient and have more calculations processed in parallel so they can be solved faster. GPUs and TPUs are now also used because they handle mass float based vector arithmetic even faster.You can brute force more speed, but do you need to? You can also give up resolution, but again do you need to? Floats aren’t absolute either anyway. It’s less about being slow here but more about how to go much faster.Neural hashesSo it turns out binary comparisons like XOR on bit sets can be computed much, much faster than float based arithmetic. So what if you could represent the 0.65 and 0.66 in a binary hash space that was locality sensitive? Could that make models much faster in terms of inference?Note: looking at a single number is a contrived example, but for vectors containing many floats the hash can actually also compress the relationship between all the dimensions which is where the magic really happens.Turns out there is a family of hash algorithms to do just this called locality sensitive hashing (LSH). The closer the original items, the closer the bits in their hashes are the same.This concept is nothing new though, except that newer techniques have found added advantages. Historically LSH used techniques like random projections, quantisation and such, but they had the disadvantage of requiring a large hash space to retain precision, so the benefits were somewhat negated.It’s trivial for a single float, but what about vectors with high dimensionality (many floats)?So the new trick with neural hashes (or sometimes called learn-to-hash) is to replace existing LSH techniques with hashes created by neural networks. The resulting hashes can be compared using the very fast Hamming distance calculation to estimate their similarity.This initially sounds complicated, but in reality it isn’t too difficult. The neural network optimizes a hash function that:retains almost perfect information compared to the original vectorproduces hashes much smaller than the original vector sizeis significantly faster for computationsThis means you get the best of both worlds, a smaller binary representation that can be used for very fast logical calculations, with virtually unchanged information resolution.Use casesThe original use case we were investigating was for approximate nearest neighbours (ANN) for dense information retrieval. This process allows us to search information using vector representations, so we can find things that are conceptually similar. Hence why the locality sensitivity in the hash is so important. We’ve taken this much further now and use hashes much more broadly for fast and approximate comparisons of complex data.Dense information retrievalHow many databases can you think of? Likely a lot. How about search indexes? Likely very few and most of those are based on the same old tech anyway. This is largely because historically language was a rules based problem. Tokens, synonyms, stemming, lemmatization and more have occupied very very smart people for their entire careers and they’re still not solved.Larry Page (google founder) has been quoted as saying search won’t be a solved problem in our lifetime. Think about that for a second, the biggest minds of a generation, literally billions of dollars of investment and it’s unlikely to be solved?Search tech has lagged databases mainly due to language problems, yet we’ve seen a revolution in language processing over the last few years and it’s still speeding up! From a tech perspective, we see neural based hashes dropping the barrier for new search and database technology (us at Sajari included!).If this peaked your interest, consider submitting a resume — we&#39;re hiring! If you’re working on hash based neural networks and indexes, I’d love to hear your thoughts on what’s coming next! You can find me on Twitter @hamishogilvy.‍</description>
      <pubDate>01 Oct 21 12:14 EDT</pubDate>
      <guid>https://www.sajari.com/blog/vectors-versus-hashes</guid>
    </item>
    <item>
      <title>Find great things to read</title>
      <link>https://monetize.substack.com/p/open-source-eras</link>
      <description>&lt;a href=&#34;https://monetize.substack.com/p/open-source-eras&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Alphabet SoupWriter Etgar Keret serves up new and previously published stories while playfully experimenting with audio, reader story prompts, and more. Chaoyang TrapThis Substack about everyday life on the Chinese internet explores the marginal subcultures, obsessions, and unexpected connections of contemporary China.Hundreds of paid subscribers · $5/monthA Wink And A NodArtist, writer, and Incubus frontman Brandon Boyd shares his authentic voice through essays, songs, stories, poems, and visual art.By Brandon Boyd · Hundreds of paid subscribers · $6/monthExploding GiraffeBrian K. Vaughan and Niko Henrichon serialize the complete, full-color pages of their not-safe-for-work new graphic novel Spectators.Hundreds of paid subscribers · $7/monthThe ShortcutJournalist Matt Swider brings decades of experience covering gadgets to his Substack, where he recommends what tech to buy and how to save money.By Matt Swider · Hundreds of paid subscribers · $5/monthBrazenfaceWriter, illustrator, and photographer Tatiana Gallardo documents her 2022 journey to courageously tackle the things that terrify her most.By Tatiana Gallardo · Launched 2 months ago</description>
      <pubDate>07 Feb 21 23:24 EST</pubDate>
      <guid>https://monetize.substack.com/p/open-source-eras</guid>
    </item>
    <item>
      <title>An oral history of Bank Python</title>
      <link>https://calpaterson.com/bank-python.html</link>
      <description>&lt;a href=&#34;https://calpaterson.com/bank-python.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; November 2021 The strange world of Python, as used by big investment banks High finance is a foreign country; they do things differently there Today will I take you through the keyhole to look at a group of software systems not well known to the public, which I call &#34;Bank Python&#34;. Bank Python implementations are effectively proprietary forks of the entire Python ecosystem which are in use at many (but not all) of the biggest investment banks. Bank Python differs considerably from the common, or garden-variety Python that most people know and love (or hate). Thousands of people work on - or rather, inside - these systems but there is not a lot about them on the public web. When I&#39;ve tried to explain Bank Python in conversations people have often dismissed what I&#39;ve said as the ravings of a swivel-eyed loon. It all just sounds too bonkers. I will discuss a fictional, amalgamated, imaginary Bank Python system called &#34;Minerva&#34;. The names of subsystems will be changed and though I&#39;ll try to be accurate I will have to stylise some details and - of course: I don&#39;t know every single detail. I might even make the odd mistake. Hopefully I get the broad strokes. Barbara, the great key value store The first thing to know about Minerva is that it is built on a global database of Python objects. import barbara # open a connection to the default database &#34;ring&#34; db = barbara.open() # pull out some bond my_gilt = db[&#34;/Instruments/UKGILT201510yZXhhbXBsZQ==&#34;] # calculate the current value of the bond (according to # the bank&#39;s modellers) current_value: float = my_gilt.value() Barbara is a simple key value store with a hierarchical key space. It&#39;s brutally simple: made just from pickle and zip. Barbara has multiple &#34;rings&#34;, or namespaces, but the default ring is more or less a single, global, object database for the entire bank. From the default ring you can pull out trade data, instrument data (as above), market data and so on. A huge fraction, the majority, of data used day-to-day comes out of Barbara. Applications also commonly store their internal state in Barbara - writing dataclasses straight in and out with only very simple locking and transactions (if any). There is no filesystem available to Minerva scripts and the little bits of data that scripts pick up has to be put into Barbara. Internally, Barbara nodes replicate writes within their rings, a bit like how Dynamo and BigTable work. When you call barbara.open() it connects to the nearest working instance of the default ring. Within that single instance reads and writes are strongly consistent. Reads and writes from other instances turn up quickly, but not straight away. If consistency matters you simply ensure that you are always connecting to a specific instance - a practice which is discouraged if not necessary. Barbara is surprisingly robust, probably because it is so simple. Outright failures are exceptionally rare and degraded states only a little more common. Some example paths from the default ring: Path Description /Instruments Directory for financial instruments (bonds, stocks, etc) /Deals Directory for Deals (trades that happened) /FX Foreign exchange divisions&#39; general area /Equities/XLON/VODA/ Directory for things to do with Vodaphones shar es /MIFID2/TR/20180103/01 Intermediate object from some business process Barbara also has some &#34;overlay&#34; features: # connect to multiple rings: keys are &#39;overlaid&#39; in order of # the provided ring names db = barbara.open(&#34;middleoffice;ficc;default&#34;) # get /Etc/Something from the &#39;middleoffice&#39; ring if it exists there, # otherwise try &#39;ficc&#39; and finally the default ring some_obj = db[&#34;/Etc/Something&#34;] You can list rings in a stack and then each read will try the first ring, and then, if the key is absent there, it will try the second ring, then the third and so on. Writes can either always go to the first ring or to the uppermost ring where that key already exists (determined by configuration that I have not shown). There are some good reasons not to use Barbara. If your dataset is large it may be a good idea to look elsewhere - perhaps a traditional SQL database or kdb+. The soft limit on (compressed) Barbara object sizes is about 16MB. Zipped pickles are pretty small already so this is actually quite a large size. Barbara does feature secondary indices on object attributes but if secondary indices are a very important part of your program, it is also a good idea to look elsewhere. Dagger, a directed, acyclic graph of financial instruments One important thing that investment banks do is estimate the value of financial instruments - &#34;asset pricing&#34;. For example a bond is valued as all the money that you&#39;ll get for owning it, discounted a bit for the danger of the issuer of the bond going bust. Bonds are probably (conceptually!) the simplest instrument going and of much greater interest is the valuation of other, &#34;derivative&#34;, financial instruments, such as credit default swaps, interest rate swaps, and synthetic versions of real instruments. These are all based on an &#34;underlying&#34; instrument but pay out differently somehow. The specifics of how derivatives are valued does not matter, except to say that there are both a lot of specifics and a lot of derivatives. The dependencies between instruments forms a directed, acyclic graph. An example hierarchy for some derivative financial instruments might look like this: Some financial instruments derive their value from others. That makes them derivatives. You can get derivatives of derivatives and some derivatives derive their value from multiple underliers. Dagger is a subsystem in Minerva which serves to help keep these data dependencies straight. You write a class like so: class CreditDefaultSwap(Instrument): &#34;&#34;&#34;A credit default swap pays some money when a bond goes into default&#34;&#34;&#34; def __init__(self, bond: Bond): super().__init__(underliers=[bond]) self.bond = bond def value(self) -&gt; float: # return the (cached) valuation, according to some # asset pricing model return ... Dagger tracks the edges in the graph of underlying instruments and automatically reprices derivatives in Barbara when the value of the underlying instruments changes. If some bad news about a company is published and a credit agency downgrades their credit rating then someone in bonds will update the relevant Bond object via Dagger and Dagger will automatically revalue everything that is affected. That might mean hundreds of other derivative instruments. Credit downgrades can be rather exciting. Individual instruments are composed into positions. The Position class looks a bit like this: class Position: &#34;&#34;&#34;A position is an instrument and how many of it&#34;&#34;&#34; def __init__(self, inst: Instrument, quantity: float): self.inst = inst self.quantity = quantity def value(self) -&gt; float: # return the (cached) valuation, which basically is # self.inst.value() * self.quantity return ... Again, note that a position is something you can also value. It is also something whose value changes when the value of things it contains changes. It it also automatically revalued by Dagger. And a set of positions is called a &#34;book&#34; which is an immensely overloaded word in finance but in this context is just a set of positions: class Book: &#34;&#34;&#34;A book is a set of positions&#34;&#34;&#34; def __init__(self, contents: Set[Valuable]): # the type Valuable is a &#34;protocol&#34; in python terms, # or an &#34;interface&#34; in java terms - anything # with value() self.contents = contents def value(self) -&gt; float: # again, return the (cached) valuation, which is more # or less: sum(p.value() for p in self.contents) return ... Books can contain other books. There is a hierarchy of nested books all the way up the bank from the smallest bond desk to a single book for the entire bank. To value the bank you would execute: # this is the top level book for the whole bank which # recursively contains everything else in the whole bank bank = db[&#34;/Books/BigBankPlc&#34;] # this prints the valuation of the whole bank print(bank.value()) That&#39;s the dream anyway. In reality the CFO probably uses a different system to generate the accounts. Valuations of subsidiary books are still well used though. If you understand excel you will be starting to recognise similarities. In Excel, spreadsheets cells are also updated based on their dependencies, also as a directed acyclic graph. Dagger allows people to put their Excel-style modelling calculations into Python, write tests for them, control their versioning without having to mess around with files like CDS-OF-CDS EURO DESK 20180103 Final (final) (2).xlsx. Dagger is a key technology to get financial models out of Excel, into a programming language and under tests and version control. Dagger doesn&#39;t just handle valuations. It also handles the various &#34;risk metrics&#34; that banks use to try to keep a handle on how exposed they are to various bad things that might happen. For example, Dagger makes it relatively easy to find all positions on, say, Compu-Global-Hyper-Mega-Net Plc, which is rumoured to be going bust. That&#39;s counting all options, futures, credit instruments and all of it &#34;netted out&#34; to find the complete position on that company for the whole bank. Never again be surprised by your exposure to dodgy subprime lenders! Walpole, a bank-wide job runner I&#39;ve said so far that a lot of data is stored in Barbara. Time to drop a bit of a bombshell: the source code is in Barbara too, not on disk. Remain composed. It&#39;s kept in a special Barbara ring called sourcecode. Not keeping the source code on the filesystem breaks a lot of assumptions. How does such a program run? The answer is Walpole, the bankwide job runner. Walpole is a general purpose runner of jobs, like a mega Jenkins combined with a mega systemd. As with many things in Minerva, Walpole is not deployed per-team: there is but one, single, bankwide instance. Walpole is suitable for both long lived-services as well as periodic jobs and is even used for builds. Periodic jobs come up a lot in banks: there are many, many, many end of day or weekly jobs to run to update data, check things, send email digests, etc. Walpole does all the usual stuff you need to run your software. It can restart your software if it crashes and sends out alerts if it keeps crashing. It stores logs. It understands dependencies between jobs (much like systemd does) so if the job that generates the data your job needs fails, you job doesn&#39;t even try starting up but instead fires more alerts. One real advantage is that Walpole considerably lowers the bar for getting your stuff deployed. Anyone can put a job into Walpole - you need only a small ini-style config file explaining what time to run your script, where your main function is and your entire application is deployed with no further negotiation. This is a big deal because negotiating anything in large bank is an exercise in frustration: lead times on hardware can be measured in months. Getting people to agree with you takes of course much longer than that. One of the great drawbacks of &#34;Cloud Native Computing&#34; as it now exists is that it&#39;s really, really complicated. It is often more complicated than the old, non-cloud, sort of computing. In order to deploy your app outside of Minerva you now need to know something about k8s, or Cloud Formation, or Terraform. This is a skillset so distinct from that of a normal programmer (let alone a financial modeller) that there is no overlap. Conversely, anyone can work out an ini-file. MnTable, the ubiquitous table library I always feel that it&#39;s a shame that programming languages rarely, if ever, come with a built-in table datastructure. Programmers have an unfortunate tendency to gravitate towards hash tables - particularly in Python and Javascript where they are used to such extent that it is hard to find anything which is not made out of hash tables. Hash tables have some serious drawbacks. First, most implementations are in-memory only and sit sparsely there, which makes it a pain in the bum to work even with medium sized data sets; a problem Python programs very commonly run into in practice. More importantly they require you to know your access patterns up front and they really had better be by a single primary key. Tables are the reverse: they are memory-dense and easy to spool to and from disk. They can use b-tree indices to allow efficient access by any route; so you never end up having to invert your dictionary in the middle of your program just so that you can access by something other than the key. They can support bulk operations and can make use of lazy evaluation. In open source land the popular library for this is pandas but pandas has some serious drawbacks: It did not exist when Minerva was originally implemented It is less efficient than you might hope, particularly with memory It&#39;s not brilliant with datasets larger than memory (Arguably) it has a baroque API Instead of pandas there is a proprietary table library in Minerva: MnTable. # make a new table with three columns of the types provided t1 = mntable.Table([(&#39;counterparty&#39;, str), (&#39;instrument&#39;, str), (&#39;quantity&#39;, float)]) # put some stuff in the table (in place, tables are # immutable by default) t1.extend( [ [&#39;Cleon Partners&#39;, &#39;xlon:voda&#39;, 1200.0], [&#39;Cleon Partners&#39;, &#39;xlon:spd&#39;, 1200.0], [&#39;Blackpebble&#39;, &#39;xlon:voda&#39;, 1200.0], ], in_place=True) # return a new table (without changing the original) # that only includes vodafone. this is lazy and # won&#39;t get evaluated until you look at it t1.restrict(instrument=&#39;xlon:voda&#39;) MnTable gets used everywhere in Bank Python. Some implementations are lumps of C++ (not atypical of financial software) and some are thin veneers over sqlite3. There are many, many programs which start with an MnTable, apply some list of operations to it and then forward the resulting table somewhere else. This is convenient as data is everywhere in banks and most of it is &#34;medium&#34; sized: in the gigabytes range. A lot is talked about high-frequency traders but the majority of financiers are not looking at tick level or frankly even intra-day level data. &#34;Medium-sized&#34; is big enough that you cannot create an object for every row but not so big that you are going to need some distributed compute cluster thingy. A measure of the pain It would be wrong to imply that working with any financial software is pure and untrammelled joy. Minerva is no different. New starters take an exceptionally long time to get up to speed - and that&#39;s if they don&#39;t resign in fit of pique as soon as they see the special, mandatory, in-house IDE (as I nearly did). Even months in, new starters are still learning quite fundamental new things: there is a lot that is different. Over time the divergence between Bank Python and Open Source Python grows. Technology churns on both sides, much faster outside than in of course, but they do not get closer. The rest of the world is not going to adopt any of Minerva&#39;s ideas, not least because they&#39;ve never heard of them. Minerva is also not adopting many of the ideas from the outside. There is an uncharitable view (sometimes expressed internally too) that Minerva as a whole is a grand exercise in NIH syndrome. By nature, Minerva is holistic and all encompassing. That&#39;s great if you&#39;re inside but if you&#39;re outside, interacting with Minerva is a pain. Occasionally a non-Minerva developer would ask me how he might read some specific piece of data out of Barbara. I would tell him that the best way would be to use the Minerva source code to do that. Ok, he would reply, maybe he could get away with adding a Python script to a cronjob to do that - could I help him get the code? That&#39;s easy, I would reply: just read it out of Barbara. I can just about understand why Minerva has its own IDE - no other IDEs work if you keep your source files in a giant global database. What I can&#39;t understand is why it contains its own web framework. Investment banks have a one-way approach to open source software: (some of) it can come in, but none of it can go out. The github profiles of the bulge bracket investment banks are anaemic compared to those of comparably sized companies in different industries. This highly proprietary attitude has remained even as the Volcker Rule has forced nearly all of the proprietary trading out of investment banks. It is a curse. It could be that the biggest disadvantage is professional. Every year you spend in the Minerva monoculture the skills you need interact with normal software atrophy. By the time I left I had pretty much forgotten how to wrestle pip and virtualenv into shape (essential skills for normal Python). When everything is in the same repo and all code is just an import away, software packaging just does not not come up. What makes it different I haven&#39;t covered everything that&#39;s in a typical Bank Python implementation. For example, I&#39;ve skipped over things like: the proprietary timeseries data-structure the &#34;vouch&#34; system for getting your changes into prod time travel in Dagger the semi-bespoke (non-git) version control system the Prolog-based permission system replay-oriented financial message buses existential ennui arising from prolonged exposure to Windows 7 and MS Outlook 2010 You&#39;ll just have to use your imagination. That said, I hope that I&#39;ve given a view of the most important central parts: Barbara, Dagger, Walpole and MnTable. Of those four subsystems, three pertain to data. (The other can be seen as a database of jobs.) One of the slightly odd things about Minerva is that a lot of it is &#34;data-first&#34;, rather than &#34;code-first&#34;. This is odd because the majority of software engineering is the reverse. For example, in object oriented design the aim is to organise the program around &#34;classes&#34;, which are coherent groupings of behaviour (ie: code), the data is often simply along for the ride. Writing programs with MnTable is different: you group the data into tables and then the code lives separately. These two lenses for organising computations are at the heart of the object relational impedance mismatch which has caused such grief. The force is out of balance: many more programmers can design decent object-oriented classes than can bring a set of tables into third normal form. This is a large part of the reason that that annoying impedance mismatch keeps coming up. The other unusual thing about Minerva is that it opts, in many cases, to have one big something rather than many small somethings. One big codebase. One big database. One big job runner. Clubbing it all together removes a lot of accidental complexity: you already have a language runtime (and the version in prod is the same as on your computer), a basic database and a place for your code to run before you even start. That means it&#39;s possible to sit down, write a script and get it running in prod within the hour, which is a big deal. Minerva is obviously heavily influenced by the technological path dependency of the financial sector, which is another way of saying: there is a lot of MS Excel. Any new software solution is going to be compared with MS Excel and if the result is unfavourable people will often just use continue to use Excel instead. Many, many technologists have taken one look at an existing workflow of spreadsheets, reacted with performative disgust, and proposed the trifecta of microservices, Kubernetes and something called a &#34;service mesh&#34;. This kind of Big Enterprise technology however takes away that basic agency of those Excel users, who no longer understand the business process they run and now have to negotiate with ludicrous technology dweebs for each software change. The previous pliability of the spreadsheets has been completely lost. Using simple Python functions, in a source controlled system, is a better middle ground than the modern-day equivalent of J2EE. Financiers are able to learn Python, and while they may never be amazing at it they can contribute to a much higher level and even make their own changes and get them deployed. Crib ideas from existing systems One thing I regret about software as a field is how little time is spent learning from existing systems and judging what they did well, or badly. There are only a small number of books discussing, in detail, real systems that exist. Even when the public details of systems are available they can still be strangely understudied. Email has been around a long time: it predates the internet by a decade. And in that time it has not changed enormously fast and is still mostly the same as it was in the 80s. Despite that, a lot of programmers are still a hazy about what happens when you click &#34;send&#34;. Some of them, I&#39;m sure, will keep trying to &#34;disrupt&#34; email regardless. This is a shame as foreign systems, like foreign countries, can be mind expanding when experienced firsthand. Their customs can differ so enormously from yours that it can lead you to rethink your own practices. But when you just hear it second hand, it can sound like nonsense. I once described Minerva&#39;s &#34;vouch&#34; system, briefly, to another programmer who had never seen it. I explained that when you had a code change, you just had to convince any one of the code owners for the file in question to sign it off. If the change was very urgent, they might sign off your change sight unseen, based on your reputation alone. As soon as they clicked that &#34;vouch&#34; button - bang - your new change was in prod: after all, there is no such thing as a deployment step when your code is stored in a database. Disbelieving me, he asked who in the world would trust such a bank. The answer is a lot of people. They are a very big bank. You have certainly heard of them. Contact/etc Other notes If you&#39;re curious to try an MnTable-style table library, my friend Sal released a pure-python, API compatible, version called eztable. I&#39;ve mentioned that programmers are far too dismissive of MS Excel. You can achieve a awful lot with Excel: more, even, than some programmers can achieve without it. There exist trading systems in &#34;tier one&#34; investment banks where the way that trades are executed is by clicking on special cells in certain special xlsx files. Even I would accept that that is too far but if you don&#39;t already know Excel it is one of the highest value things you can learn. For programmers the best way to find out what you are missing is Joel Spolsky&#39;s overview talk, aimed directly at programmers. If you decide to take the red pill after that, I&#39;m told that Coursera&#39;s Excel Skills for Business Specialisation is a excellent. One of things that tends to boggle programmer brains is while most software dealing with money uses multiple-precision numbers to make sure the pennies are accurate, financial modelling uses floats instead. This is because clients generally do not ring up about pennies. I&#39;ve mentioned Barbara overlays. They also work for source code. You can tell Walpole to mount your own ring in front of sourcecode when it&#39;s importing code for a job and then you can push source files to that instead of getting them vouched into sourcecode. All manner of crazy, bananas, tutti frutti hacks lie down this dark path. Do it, but only a little. </description>
      <pubDate>04 Nov 21 08:42 EDT</pubDate>
      <guid>https://calpaterson.com/bank-python.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://earthly.dev/blog/awk-examples/</link>
      <description>&lt;a href=&#34;https://earthly.dev/blog/awk-examples/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Understanding AWK 26 minute read     Updated: September 30, 2021 Background I have a confession to make: I don’t know how to use Awk. Or at least I didn’t know how to use it before I started writing this article. I would hear people mention Awk and how often they used it, and I was pretty certain I was missing out on some minor superpower. Like this little off hand comment by Bryan Cantrill: I write three or four Awk programs a day. And these are one-liners. These super quick programs. It turns out Awk is pretty simple. It has only a couple of conventions and only a small amount of syntax. As a result, it’s straightforward to learn, and once you understand it, it will come in handy more often than you’d think. So in this article, I will teach myself, and you, the basics of Awk. If you read through the article and maybe even try an example or two, you should have no problem writing Awk scripts by the end of it. And you probably don’t even need to install anything because Awk is everywhere. The Plan I will be using Awk to look at book reviews and pick my next book to read. I will start with short Awk one-liners and build towards a simple 33 line program that ranks books by the 19 million reviews on Amazon.com. What Is Awk Awk is a record processing tool written by Aho, Kernighan, and Weinberger in 1977. Its name is an acronym of their names. They created it following the success of the line processing tools sed and grep. Awk was initially an experiment by the authors into whether text processing tools could be extended to deal with numbers. If grep lets you search for lines, and sed lets you do replacements in lines then awk was designed to let you do calculations on lines. It will be clear what that means once I take us through some examples. How to Install gawk The biggest reason to learn Awk is that it’s on pretty much every single linux distribution. You might not have perl or Python. You will have Awk. Only the most minimal of minimal linux systems will exclude it. Even busybox includes awk. That’s how essential it’s viewed. cogman10 Awk is part of the Portable Operating System Interface (POSIX). This means it’s already on your MacBook and your Linux server. There are several versions of Awk, but for the basics, whatever Awk you have will do. If you can run this, you can do the rest of this tutorial: GNU Awk 5.1.0, API: 3.0 (GNU MPFR 4.1.0, GNU MP 6.2.1) Copyright (C) 1989, 1991-2020 Free Software Foundation. If you are doing something more involved with Awk, take the time to install GNU Awk (gawk). I did this using Homebrew (brew install gawk). Windows users can get gawk using chocolatey (choco install gawk). If you are on Linux, you already have it. Awk Print By default, Awk expects to receive its input on standard in and output its results to standard out. Thus, the simplest thing you can do in Awk is print a line of input. $ echo &#34;one two three&#34; | awk &#39;{ print }&#39; one two three Note the braces. This syntax will start to make sense after you see a couple of examples. You can selectively choose columns (which Awk calls fields): $ echo &#34;one two three&#34; | awk &#39;{ print $1 }&#39; one $ echo &#34;one two three&#34; | awk &#39;{ print $2 }&#39; two $ echo &#34;one two three&#34; | awk &#39;{ print $3 }&#39; three You may have been expecting the first column to be $0 and not $1, but $0 is something different: $ echo &#34;one two three&#34; | awk &#39;{ print $0 }&#39; It is the entire line! Incidentally, Awk refers to each line as a record and each column as a field. I can do this across multiple lines: $ echo &#34; one two three four five six&#34; \ | awk &#39;{ print $1 }&#39; And I can print more than one column: $ echo &#34; one two three four five six&#34; \ | awk &#39;{ print $1, $2 }&#39; Awk also includes $NF for accessing the last column: $ echo &#34; one two three four five six&#34; \ | awk &#39;{ print $NF }&#39; What I’ve learned: Awk Field Variables Awk creates a variable for each field (column) in a record (line) ($1, $2 … $NF). $0 refers to the whole record. You can print out fields like this: $ awk &#39;{ print $1, $2, $7 }&#39; Awk Sample Data To move beyond simple printing, I need some sample data. I will use the book portion of the amazon product reviews dataset for the rest of this tutorial. This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014. Amazon Review Data Set You can grab the book portion of it like this: $ curl https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_00.tsv.gz | / gunzip -c &gt;&gt; / bookreviews.tsv If you want to follow along with the entire dataset, repeat this for each of the three book files (v1_00, v1_01, v1_02). ❗ Disk Space Warning The above file is over six gigs unzipped. If you grab all three, you will be up to fifteen gigs of disk space. If you don’t have much space, you can play along by grabbing the first ten thousand rows of the first file: $ curl https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_00.tsv.gz \ | gunzip -c \ | head -n 10000 \ &gt; bookreviews.tsv The Book Data Once you’ve grabbed that data, you should have Amazon book review data that looks like this: marketplace customer_id review_id product_id product_parent product_title product_category star_rating helpful_votes total_votes vine verified_purchase review_headline review_body review_date US 22480053 R28HBXXO1UEVJT 0843952016 34858117 The Rising Books 5 0 0 N N Great Twist on Zombie Mythos I&#39;ve known about this one for a long time, but just finally got around to reading it for the first time. I enjoyed it a lot! What I liked the most was how it took a tired premise and breathed new life into it by creating an entirely new twist on the zombie mythos. A definite must read! 2012-05-03 Each row in this file represents the record of one book review. Amazon lays out the fields like this: DATA COLUMNS: 01 marketplace - 2 letter country code of the marketplace where the review was written. 02 customer_id - Random identifier that can be used to aggregate reviews written by a single author. 03 review_id - The unique ID of the review. 04 product_id - The unique Product ID the review pertains to. 05 product_parent - Random identifier that can be used to aggregate reviews for the same product. 06 product_title - Title of the product. 07 product_category - Broad product category that can be used to group reviews 08 star_rating - The 1-5 star rating of the review. 09 helpful_votes - Number of helpful votes. 10 total_votes - Number of total votes the review received. 11 vine - Review was written as part of the Vine program. 12 verified_purchase - The review is on a verified purchase. 13 review_headline - The title of the review. 14 review_body - The review text. 15 review_date - The date the review was written. Printing Book Data I can now test out my field printing skills on a bigger file. I can start by printing fields that I care about, like the marketplace: $ awk &#39;{ print $1 }&#39; bookreviews.tsv | head marketplace US US US US US US US US US Or the customer_id: $ awk &#39;{ print $2 }&#39; bookreviews.tsv | head customer_id 22480053 44244451 20357422 13235208 26301786 27780192 13041546 51692331 23108524 However, when I try to print out the title, things do not go well: $ awk &#39;{ print $6 }&#39; bookreviews.tsv | head product_title The Sticky Black Direction Until Unfinished The Good Patterns To fix this, I need to configure my field separators. Field Separators By default, Awk assumes that the fields in a record are whitespace delimited1. I can change the field separator to use tabs using the awk -F option: $ awk -F &#39;\t&#39; &#39;{ print $6 }&#39; bookreviews.tsv | head product_title The Rising Sticky Faith Teen Curriculum with DVD: 10 Lessons to Nurture Faith Beyond High Black Passenger Yellow Cabs: Of Exile And Excess In Japan Direction and Destiny in the Birth Chart Until the Next Time Unfinished Business The Republican Brain: The Science of Why They Deny Science- and Reality Good Food: 101 Cakes &amp; Bakes Patterns and Quilts (Mathzones) What I’ve learned: Awk Field Separators Awk assumes that the fields in a record are whitespace delimited. You can change this using the -F option like this $ awk -F &#39;\t&#39; &#39;{ print $6 }&#39; I can also work backward from the last position forward by subtracting from NF. $ awk -F &#39;\t&#39; &#39;{ print $NF &#34;\t&#34; $(NF-2)}&#39; bookreviews.tsv | head review_date review_headline 2012-05-03 Great Twist on Zombie Mythos 2012-05-03 Helpful and Practical 2012-05-03 Paul 2012-05-03 Direction and Destiny in the Birth Chart 2012-05-03 This was Okay 2012-05-03 Excellent read!!! 2012-05-03 A must read for science thinkers 2012-05-03 Chocoholic heaven 2012-05-03 Quilt Art Projects for Children Side Note: NF and NR $NF seems like an unusual name for printing the last column, right? But actually, NF is a variable holding the Number of Fields in a record. So I’m just using its value as an index to refer to the last field. I can print the actual value like this: $ awk -F &#39;\t&#39; &#39;{ print NF }&#39; bookreviews.tsv | head 15 15 15 15 15 15 15 15 15 15 Another variable Awk offers up for use is NR, the number of records so far. NR is handy when I need to print line numbers: $ awk -F &#39;\t&#39; &#39;{ print NR &#34; &#34; $(NF-2) }&#39; bookreviews.tsv | head 1 review_headline 2 Great Twist on Zombie Mythos 3 Helpful and Practical 4 Paul 5 Direction and Destiny in the Birth Chart 6 This was Okay 7 Excellent read!!! 8 A must read for science thinkers 9 Chocoholic heaven 10 Quilt Art Projects for Children Awk Pattern Match With Regular Expressions Everything I’ve done so far has applied to every line in our file, but the real power of Awk comes from pattern matching. And you can give Awk a pattern to match each line on like this: $ echo &#34;aa bb cc&#34; | awk &#39;/bb/&#39; bb You could replace grep this way. You can also combine this with the field access and printing we’ve done so far: $ echo &#34;aa 10 bb 20 cc 30&#34; | awk &#39;/bb/ { print $2 }&#39; 20 Using this knowledge, I can easily grab reviews by book title and print the book title($6) and review score($8). The reviews I’m going to focus on today are for the book ‘The Hunger Games’. I’m choosing it because it’s part of a series with many reviews and I recall liking the movie. So I’m wondering if I should read it. $ awk -F &#39;\t&#39; &#39;/Hunger Games/ { print $6, $8 }&#39; bookreviews.tsv | head The Hunger Games (Book 1) 5 The Hunger Games Trilogy Boxed Set 5 The Hunger Games Trilogy: The Hunger Games / Catching Fire / Mockingjay 5 Catching Fire |Hunger Games|2 4 The Hunger Games (Book 1) 5 Catching Fire |Hunger Games|2 5 The Hunger Games Trilogy: The Hunger Games / Catching Fire / Mockingjay 5 Blackout 3 The Hunger Games Trilogy: The Hunger Games / Catching Fire / Mockingjay 4 Tabula Rasa 3 I should be able to pull valuable data from these reviews, but first there is a problem. I’m getting reviews from more than one book here. /Hunger Games/ matches anywhere in the line and I’m getting all kinds of ‘Hunger Games’ books returned. I’m even getting books that mention “Hunger Games” in the review text: $ awk -F &#39;\t&#39; &#39;/Hunger Games/ { print $6 }&#39; bookreviews.tsv | sort | uniq Birthmarked Blood Red Road Catching Fire (The Hunger Games) Divergent Enclave Fire (Graceling Realm Books) Futuretrack 5 Girl in the Arena ... I can fix this by using the product_id field to pattern match on: $ awk -F &#39;\t&#39; &#39;$4 == &#34;0439023483&#34; { print $6 }&#39; bookreviews.tsv | sort | uniq The Hunger Games (The Hunger Games, Book 1) I want to calculate the average review score for ‘The Hunger Games’, but first, let’s take a look at the review_date ($15), the review_headline ($13), and the star_rating ($8) of our Hunger Games reviews, to get a feel for the data: $ awk -F &#39;\t&#39; &#39;$4 == &#34;0439023483&#34; { print $15 &#34;\t&#34; $13 &#34;\t&#34; $8}&#39; bookreviews.tsv | head 2015-08-19 Five Stars 5 2015-08-17 Five Stars 5 2015-07-23 Great read 5 2015-07-05 Awesome 5 2015-06-28 Epic start to an epic series 5 2015-06-21 Five Stars 5 2015-04-19 Five Stars 5 2015-04-12 i lile the book 3 2015-03-28 What a Great Read, i could not out it down 5 2015-03-28 Five Stars 5 Look at those star ratings. Yes, the book is getting many 5-star reviews, but more importantly, the layout of my text table looks horrible: the width of the review titles is breaking the layout. To fix this, I need to switch from using print to using printf. What I’ve learned: Awk Pattern Matching I’ve learned that an awk action, like { print $4}, can be preceded by a pattern, like /regex/. Without the pattern, the action runs on all lines. You can use a simple regular expression for the pattern. In which case it matches anywhere in the line, like grep: $ awk &#39;/hello/ { print &#34;This line contains hello&#34;, $0}&#39; Or you can match within a specific field: $ awk &#39;$4~/hello/ { print &#34;This field contains hello&#34;, $4}&#39; Or you can exact match a field: $ awk &#39;$4 == &#34;hello&#34; { print &#34;This field is hello:&#34;, $4}&#39; Awk printf printf works like it does in the C and uses a format string and a list of values. You can use %s to print the next string value. So my print $15 &#34;\t&#34; $13 &#34;\t&#34; $8 becomes printf &#34;%s \t %s \t %s&#34;, $15, $13, $8. From there I can add right padding and fix my layout by changing %s to %-Ns where N is my desired column width: $ awk -F &#39;\t&#39; &#39;$4 == &#34;0439023483&#34; { printf &#34;%s \t %-20s \t %s \n&#34;, $15, $13, $8}&#39; bookreviews.tsv | head 2015-08-19 Five Stars 5 2015-08-17 Five Stars 5 2015-07-23 Great read 5 2015-07-05 Awesome 5 2015-06-28 Epic start to an epic series 5 2015-06-21 Five Stars 5 2015-04-19 Five Stars 5 2015-04-12 i lile the book 3 2015-03-28 What a Great Read, i could not out it down 5 2015-03-28 Five Stars 5 This table is pretty close to what I’d want. However, some of the titles are just too long. I can shorten them to 20 characters with substr($13,1,20). Putting it all together and I get: $ awk -F &#39;\t&#39; &#39;$4 == &#34;0439023483&#34; { printf &#34;%s \t %-20s \t %s \n&#34;, $15, substr($13,1,20), $8}&#39; bookreviews.tsv | head 2015-08-19 Five Stars 5 2015-08-17 Five Stars 5 2015-07-23 Great read 5 2015-07-05 Awesome 5 2015-06-28 Epic start to an epi 5 2015-06-21 Five Stars 5 2015-04-19 Five Stars 5 2015-04-12 i lile the book 3 2015-03-28 What a Great Read, i 5 2015-03-28 Five Stars 5 Alright, I think at this point, I’m ready to move on to star calculations. What I’ve learned: printf and Built-ins If you need to print out a table, Awk lets you use printf and built-ins like substr to format your output. It ends up looking something like this: $ awk &#39;{ printf &#34;%s \t %-5s&#34;, $1, substr($2,1,5) }&#39; printf works much like C’s printf. You can use %s to insert a string into the format string, and other flags let you the set width or precision. For more information on printf or other built-ins, you can consult an Awk reference document. Awk BEGIN and END Actions I want to calculate the average rating for book reviews in this data set. To do that, I need to use a variable. However, I don’t need to declare the variable or its type. I can just use it: I can add up and print out a running total of review_stars ($8) like this: $ awk -F &#39;\t&#39; &#39;{ total = total + $8; print total }&#39; bookreviews.tsv | head And to turn this into an average, I can use NR to get the total amount of records and END to run an action at the end of the processing. $ awk -F &#39;\t&#39; &#39; { total = total + $8 } END { print &#34;Average book review:&#34;, total/NR, &#34;stars&#34; } &#39; bookreviews.tsv | head Average book review is 4.24361 stars I can also use BEGIN to run an action before Awk starts processing records. $ awk -F &#39;\t&#39; &#39; BEGIN { print &#34;Calculating Average ...&#34; } { total = total + $8 } END { print &#34;Average book review:&#34;, total/NR, &#34;stars&#34; } &#39; bookreviews.tsv Calculating Average ... Average book review is 4.24361 stars What I’ve learned: Awk’s BEGIN, END and Variables Awk provides two special patterns, BEGIN and END. You can use them to run actions before and after processing the records. For example, this is how you would initialize data, print headers and footer, or do any start-up or tear-down stuff in Awk. It ends up looking like this: $ awk &#39; BEGIN { print &#34;start up&#34; } { print &#34;line match&#34; } END { print &#34;tear down&#34; }&#39; You can also easily use variables in Awk. No declaration is needed. $ awk -F &#39;{ total = total + $8 }&#39; Fun Awk One-Liners Before we leave the world of one-liners behind, I reached out to my friends to ask when they use Awk day-to-day. Here are some of the examples I got back. Printing files with a human readable size: $ ls -lh | awk &#39;{ print $5,&#34;\t&#34;, $9 }&#39; 7.8M The_AWK_Programming_Language.pdf 6.2G bookreviews.tsv Getting the containerID of running docker containers: $ docker ps -a | awk &#39;{ print $1 }&#39; CONTAINER 08488f220f76 3b7fc673649f You can combine that last one with a regex to focus on a line you care about. Here I stop postgres, regardless of its tag name: $ docker stop &#34;$(docker ps -a | awk &#39;/postgres/{ print $1 }&#39;)&#34; You get the idea. If you have a table of space-delimited text returned by some tool then Awk can easily slice and dice it. Awk Scripting Examples If you pick your constraints you can make a particular envelope of uses easy and ones you don’t care about hard. Awk’s choice to be a per line processor, with optional sections for processing before all lines and after all lines is self-limiting but it defines a useful envelope of use. Michael Feathers In my mind, once an Awk program spans multiple lines, it’s time to consider putting it into a file. Side Note: Why Awk Scripting Once we move beyond one-liners, a natural question is why. As in ‘Why not use Python? Isn’t it good at this type of thing?’ I have a couple of answers for that. First, Awk is great for writing programs that are, at their core, a glorified for loop over some input. If that is what you are doing, and the control flow is limited, using Awk will be more concise than Python. Second, if you need to rewrite your Awk program into Python at some point, so be it. It’s not going to be more than 100 lines of code, and the translation process will be straightforward. Third, why not? Learning a new tool can be fun. We’ve now crossed over from one-liners into Awk scripting. With Awk, the transition is smooth. I can now embed Awk into a bash script: exec awk -F &#39;\t&#39; &#39; { total = total + $8 } END { print &#34;Average book review is&#34;, total/NR, &#34;stars&#34; } &#39; $1 $ average bookreviews.tsv Average book review is 4.2862 stars Or I can use a shebang (#!): #!/usr/bin/env -S gawk -f BEGIN { FS = &#34;\t&#34; } { total = total + $8 } END { print &#34;Average book $6 review is&#34;, total/NR, &#34;stars&#34; } And run it like this $ ./average.awk bookreviews.tsv Or you can also pass it to awk directly using -f: $ awk -f average.awk bookreviews.tsv Side Note: BEGIN FS If you use a shebang or pass to Awk directly, it’s easiest to set the file separator using FS = &#34;\t&#34; in the BEGIN action. Awk Average Example At this point, I should be ready to start calculating review scores for The Hunger Games: exec awk -F &#39;\t&#39; &#39; $4 == &#34;0439023483&#34; { title=$6; count = count + 1; total = total + $8 } END { print &#34;The Average book review for&#34;, title, &#34;is&#34;, total/count, &#34;stars&#34; } &#39; $1 Now that I’m in a file, I can format this out a bit better so its easier to read: $4 == &#34;0439023483&#34; { title=$6 count = count + 1; total = total + $8 } END { printf &#34;Book: %-5s\n&#34;, title printf &#34;Average Rating: %.2f\n&#34;, total/count } Either way, I get this output: Book: The Hunger Games (The Hunger Games, Book 1) Average Rating: 4.67% What I’ve learned: Calling Awk from a file Once you are beyond a single line, it makes sense to put your Awk script into a file. You can then call you program using the -foption Using a shebang : #!/usr/bin/env -S gawk -f or by using a bash exec command: exec awk -F &#39;\t&#39; &#39;print $0&#39; $1 Awk Arrays I’d like to know if the series stays strong or if it’s a single great book that the author stretched out into a trilogy. If the reviews decline quickly, then that is not a good sign. I should be able to see which book was rated the best and which was the worst. Let’s find out. If I were going to calculate the averages in Python, I would loop over the list of reviews and use a dictionary to track the total stars and total reviews for each. In Awk I can do the same: BEGIN { FS = &#34;\t&#34; } $6~/\(The Hunger Games(, Book 1)?\)$/ { title[$6]=$6 count[$6]= count[$6] + 1 total[$6]= total[$6] + $8 } END { for (i in count) { printf &#34;---------------------------------------\n&#34; printf &#34;%s\n&#34;, title[i] printf &#34;---------------------------------------\n&#34; printf &#34;Ave: %.2f\t Count: %s \n\n&#34;, total[i]/count[i], count[i] } } $ awk -f hungergames.awk bookreviews.tsv --------------------------------------- The Hunger Games (The Hunger Games, Book 1) --------------------------------------- Ave: 4.55 Count: 1497 --------------------------------------- Mockingjay (The Hunger Games) --------------------------------------- Ave: 3.77 Count: 3801 --------------------------------------- Catching Fire (The Hunger Games) --------------------------------------- Ave: 4.52 Count: 2205 --------------------------------------- And look at that, the first book in the series was the most popular. And the last book, Mockingjay was much less popular. So that isn’t a good sign. Let me look at another trilogy to see if this gradual decrease in rankings is common or The Hunger Games specific: BEGIN { FS = &#34;\t&#34; } $6~/\(The Lord of the Rings, Book .\)$/ { # &lt;-- changed this line title[$6]=$6 count[$6]= count[$6] + 1 total[$6]= total[$6] + $8 } END { for (i in title) { printf &#34;---------------------------------------\n&#34; printf &#34;%s\n&#34;, title[i] printf &#34;---------------------------------------\n&#34; printf &#34;Ave: %.2f\t Count: %s \n\n&#34;, total[i]/count[i], count[i] } } --------------------------------------- The Return of the King (The Lord of the Rings, Book 3) --------------------------------------- Ave: 4.79 Count: 38 --------------------------------------- The Two Towers (The Lord of the Rings, Book 2) --------------------------------------- Ave: 4.64 Count: 56 --------------------------------------- The Fellowship of the Ring (The Lord of the Rings, Book 1) --------------------------------------- Ave: 4.60 Count: 93 Lord of the Rings has a different pattern. The books are all in a pretty tight range. The number of reviews is also much smaller, so it’s hard to say for sure that “The Return Of the King” is the best book, but it certainly looks that way. What I’ve learned: Awk Associative Arrays Awk has associative arrays built it, and you can use them in much the same way you would use Python dictionaries. arr[&#34;key1&#34;] = &#34;one&#34; arr[&#34;key2&#34;] = &#34;two&#34; arr[&#34;key3&#34;] = &#34;three&#34; You can then use a for loop to iterate over them: for (i in arr){ print i, arr[i] } key1 one key2 two key3 three Not bad for a language written in 1977! Awk If Else I hate how every book on Amazon has a star rating between 3.0 and 4.5 stars. It makes it hard to judge purely based on numbers. So let’s rescale things in terms of the average. Maybe if I normalize the reviews, it will be easier to determine how good or bad the 3.77 average for Mockingjay is. First, I need to calculate the global average but adding up the total and average for every row: { # Global Average g_count = g_count + 1 g_total = g_total + $8 } Then I calculate the global average: END { g_score = g_total/g_count ... } Once I have this, I can score books based on how higher or lower they are than the average. All I need to do is add some if statements to my END pattern to accomplish this: END { g_score = g_total/g_count for (i in count) { score = total[i]/count[i] printf &#34;%-30s\t&#34;, substr(title[i],1,30) if (score - g_score &gt; .5) printf &#34;👍👍👍&#34; else if (score - g_score &gt; .25) printf &#34;👍👍&#34; else if (score - g_score &gt; 0) printf &#34;👍&#34; else if (g_score - score &gt; 1) printf &#34;👎👎👎&#34; else if (g_score - score &gt; .5) printf &#34;👎👎&#34; else if (g_score - score &gt; 0) printf &#34;👎&#34; printf &#34;\n&#34; } } The values for partitioning are just a guess, but it makes it easier for me to understand the rankings: The Hunger Games (The Hunger G 👍 Catching Fire: The Official Il 👍👍👍 Mockingjay (The Hunger Games) 👎👎 The Two Towers (The Lord of th 👍👍 The Fellowship of the Ring (Th 👍👍 The Return of the King (The Lo 👍👍👍 It looks like Mockingjay, at least on Amazon and in this dataset, was not well received. You can easily modify this script to query for books ad hoc: exec gawk -F &#39;\t&#39; &#39; { # Global Average g_count = g_count + 1 g_total = g_total + $8 PROCINFO[&#34;sorted_in&#34;] = &#34;@val_num_asc&#34; } $6~/^.*&#39;+&#34;$1&#34;+&#39;.*$/ { # &lt;-- Take match as input title[$6]=$6 count[$6]= count[$6] + 1 total[$6]= total[$6] + $8 } END { PROCINFO[&#34;sorted_in&#34;] = &#34;@val_num_desc&#34; g_score = g_total/g_count for (i in count) { score = total[i]/count[i] printf &#34;%-50s\t&#34;, substr(title[i],1,50) if (score - g_score &gt; .4) printf &#34;👍👍👍&#34; else if (score - g_score &gt; .25) printf &#34;👍👍&#34; else if (score - g_score &gt; 0) printf &#34;👍&#34; else if (g_score - score &gt; 1) printf &#34;👎👎👎&#34; else if (g_score - score &gt; .5) printf &#34;👎👎&#34; else if (g_score - score &gt; 0) printf &#34;👎&#34; printf &#34;\n&#34; } } &#39; bookreviews.tsv | head -n 1 And then run it like this: $ ./average &#34;Left Hand of Darkness&#34; The Left Hand of Darkness (Ace Science Fiction) 👎 $ ./average &#34;Neuromancer&#34; Neuromancer 👎👎 $ ./average &#34;The Lifecycle of Software Objects&#34; The Lifecycle of Software Objects 👎 These are all great books, so I’m starting to question the taste of Amazon reviewers. I want to test one more thing, though: how do the most popular books rate? Maybe popular books get lots of reviews, and that pushes them below the overall average? What I’ve learned: Awk If Else Awk has branching using if and else statements. It works exactly like you might expect it to: $ echo &#34;1\n 2\n 3\n 4\n 5\n 6&#34; | awk &#39;{ if (NR % 2) print &#34;odd&#34; else print $0 }&#39; Awk Sort by Values Awk (specifically gawk) allows you easily configure your iteration order using a magic variable called PROCINFO[&#34;sorted_in&#34;]. This means that if I change our program to sort by value and drop the filtering, then I will be able to see the top reviewed books: exec gawk -F &#39;\t&#39; &#39; { # Global Average g_count = g_count + 1 g_total = g_total + $8 title[$6]=$6 count[$6]= count[$6] + 1 total[$6]= total[$6] + $8 } END { PROCINFO[&#34;sorted_in&#34;] = &#34;@val_num_desc&#34; # &lt;-- Print in value order g_score = g_total/g_count for (i in count) { score = total[i]/count[i] printf &#34;%-50s\t&#34;, substr(title[i],1,50) if (score - g_score &gt; .4) printf &#34;👍👍👍&#34; else if (score - g_score &gt; .25) printf &#34;👍👍&#34; else if (score - g_score &gt; 0) printf &#34;👍&#34; else if (g_score - score &gt; 1) printf &#34;👎👎👎&#34; else if (g_score - score &gt; .5) printf &#34;👎👎&#34; else if (g_score - score &gt; 0) printf &#34;👎&#34; printf &#34;\n&#34; } } &#39; bookreviews.tsv Running it: Harry Potter And The Sorcerer&#39;s Stone 👍👍 Fifty Shades of Grey 👎👎 The Hunger Games (The Hunger Games, Book 1) 👍 The Hobbit 👍 Twilight 👎 Jesus Calling: Enjoying Peace in His Presence 👍👍👍 Unbroken: A World War II Story of Survival, Resili 👍👍👍 The Shack: Where Tragedy Confronts Eternity 👎 Divergent 👍 Gone Girl 👎👎 It looks like about half (6 /10) of the most reviewed books were more popular than average. So I can’t blame Mockingjay’s low score on its popularity. I think I’ll have to take a pass on the series or at least that book. Conclusion A good programmer uses the most powerful tool to do a job. A great programmer uses the least powerful tool that does the job. vyuh Awk has more to it than this. There are more built-in variables and built-in functions. It has range patterns and substitution rules, and you can easily use it to modify content, not just add things up. If you want to learn more about Awk, The Awk Programming Language is the definitive book. It covers the language in depth. It also covers how to build a small programming language in Awk, how to build a database in Awk, and some other fun projects. Even Amazon thinks its great: $ ./average &#34;The AWK &#34; The AWK Programming Language 👍👍 Also, if you’re the type of person who’s not afraid to do things on the command line then you might like Earthly: While you’re here: Earthly is a syntax for defining your build. It works with your existing build system. Get repeatable and understandable builds today. What’s your Awk Trick or Book Pick? I hope this introduction gave you enough Awk for 90% of your use-cases though. If you come up with any clever Awk tricks yourself or if you have strong opinions on what I should read next, let me know Twitter @AdamGordonBell: AWK is a Swiss Army knife of text processing.I knew it was powerful but I&#39;d never known how to use it.So I took some time to learn the basics. Here is what you need to know: 🧵 — Adam Gordon Bell 🤓 (@adamgordonbell) September 30, 2021 Sundeep Agarwal pointed out on reddit that Awk does more than this: “By default, awk does more than split the input on spaces. It splits based on one or more sequence of space or tab or newline characters. In addition, any of these three characters at the start or end of input gets trimmed and won’t be part of field contents. Newline characters come into play if the record separator results in newline within the record content.”↩︎ </description>
      <pubDate>01 Oct 21 12:14 EDT</pubDate>
      <guid>https://earthly.dev/blog/awk-examples/</guid>
    </item>
    <item>
      <title></title>
      <link>https://kk.org/thetechnium/class-1-class-2-problems/</link>
      <description>&lt;a href=&#34;https://kk.org/thetechnium/class-1-class-2-problems/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; There are two classes of problems caused by new technology. Class 1 problems are due to it not working perfectly. Class 2 problems are due to it working perfectly. One example: many of the current problems with facial recognition are due to the fact that it is far from perfect. It can have difficulty recognizing dark skin tones; it can be fooled by simple disguises; it can be biased in its gendering. All these are Class 1 problems because this is still a technology in its infancy. Much of the resistance to widely implementing facial recognition stems from its imperfections. But what if it worked perfectly? What if the system was infallible in recognizing a person from just their face? A new set of problems emerge: Class 2 problems. If face recognition worked perfectly, there would be no escaping it, no way to duck out in public. You could be perfectly tracked in public, not only by the public, but by advertisers and governments. “Being in public” would come to have a different meaning than it does now. Perfect facial recognition would probably necessitate some new varieties of public commons, with different levels of disclosure. Furthermore, if someone could hack the system, it’s very trustworthiness would be detrimental. A faked ID could go far. We don’t question perfect tech; when was the last time you questioned the results of a calculator? Another example: Self driving cars. Self-driving cars don’t self-drive very well. They are getting better, but for the next several decades their problems will be Class 1 problems of imperfect function. We will demand near perfect results from robot-drivers (a higher standard than we demand from human drivers), so all the hard problems of detecting edge cases, acts of god, and the weird behavior of human drivers will prevail. Eventually, the tech will be perfected, and then we will encounter its Class 2 problems. In the Class 2 regime, driving a car yourself may be outlawed as too dangerous. The imperfections of human drivers may be incompatible with perfect robot drivers. When the system fails (say from a solar storm) its perfection may not permit it to degrade gracefully to accommodate less-than perfect drivers. A well-functioning robot car infrastructure might lead to more intersections with pedestrians; we might become more comfortable walking alongside silent automobiles that never crashed — until they did. Class 1 problems arise early and they are easy to imagine. Usually market forces will solve them. You could say, most Class 1 problems are solved along the way as they rush to become Class 2 problems. Class 2 problems are much harder to solve because they require more than just the invisible hand of the market to overcome them. Take cell phones. The first versions of consumer cell phones were too big, they only worked in some places, they had frustratingly short battery life, and their rings and talking on them were disruptive. Most importantly only the rich could afford them, in a new inequality. At the time many saw these problems as inherent in the technology. Yet years of intense market forces fixed most of those problems, making smartphones that silently vibrated, and had quiet text, and became so cheap and ubiquitous every adult on the planet has one. Unlike computers, they rarely crash, are easy to operate, and are extremely reliable. They just work. The cell phone quickly jumped into Class 2 problems. Whereas once the problem was “not everyone has this technology that doesn’t work very well” now the problem is “everyone has this technology that works very well.” We now contend with a technology that is present everywhere, all the time. Billions of people around the globe are connected 24/7, which allows all kinds of information, ideas, as well as rumors and disinformation to ricochet and touch everyone in an intimate way. The technology can suggest, recommend and “guide” us through the billion-eyed cacophony of everyone talking at once. Mob fears and beliefs can take over. Whispers are amplified and distorted as they cascade through friends of friends. The difference between Class 1 and Class 2 problems is that Class 2 problems cannot be solved by the market alone. Entrepreneurial spirit and the profit-mode are perfectly capable of solving most Class 1 problems. But Class 2 tech has already been perfected, and is ubiquitous — it works and everyone has it. What can the market do in this case? Making it better and selling more aren’t options anymore; those are saturated. What can the market do if facial recognition works perfectly and is everywhere? If robot drivers are the default? If everyone is connected to everyone all the time? These kind of system challenges require a suite of extra-market levers, such emerging cultural norms, smart regulation, broad education, and reframing of the problem. These are soft, slower moving forces that are currently not given the attention they deserve. To deal with ubiquitous accurate facial recognition when it comes (and it will come) requires a societal consensus on what it means to have a face that is both personal and public, to re-evaluate what public or private even means, to ensure symmetry between watchers and the watched, and to encourage expansive ideas around the very notions of identity of any type. A lot of this work is beyond the realm of dollars, and will take place in schools, courts, forums, communities, tweets, congresses, books, and late at night. When technologies reach the state that they work extremely well and become ubiquitous, their problem domain shifts from the realm of quick cycles powered by money, to the slower cycles of cultural imagination. To solve the problem of perfect facial recognition demands an expanded imagination, society wide, with new and different ideas about our face and identity. The latest fashionable tech is crypto. While the math behind blockchain is utterly reliable the implementations so far have many Class 1 problems. Crypto is hard to use, easy to trip up, biased to early adopters, an energy hog, and of marginal utility except to make money. But all these problems will be overcome by entrepreneurs. Someday blockchain will be ubiquitous and boring. It will be perfected and its wide-spread adoption will enable many thousands of new types of organizations and relationships that we can’t even imagine today. Blockchain tech could unleash collaborations of several million members working on one project in real time, or orgs that are far more leaderless than today. When crypto succeeds that way, it will graduate to Class 2 problems. At that point, entrepreneurs alone won’t solve those. These new problems will require a social imagination to revision what orgs do and what they are for, to re-imagine what transparency in a group means, to re-evaluate the role of money, or even the meaning of money. These in turn will launch new social expectations and norms of behavior, and in turn as a consensus forms, new legislation to codify the norms. Class 1 problems are caused by technology that is not perfect, and are solved by the marketplace. Class 2 problems are caused by technology that is perfect, and must be solved by extra-market forces such as cultural norms, regulation, and social imagination. </description>
      <pubDate>08 Nov 21 12:34 EST</pubDate>
      <guid>https://kk.org/thetechnium/class-1-class-2-problems/</guid>
    </item>
    <item>
      <title>The Hermit and The Magician</title>
      <link>https://notebook.wesleyac.com/hermit-magician/</link>
      <description>&lt;a href=&#34;https://notebook.wesleyac.com/hermit-magician/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Today’s cards are The Hermit and The Magician.The Hermit represents introspection and searching, while The Magician represents conscious action and concentration on a single task. I often find myself oscillating back and forth between these two modes of operation — first searching for interesting ideas, pulling in possibilities and examining them in my own mind, one by one, then, once I’ve found an idea worthy of pursuit, devoting all of my energy and attention to it.This of course begs the question — how do you know when to do one or the other? When have you searched enough, and found the thing worthy of putting your attention towards? I don’t have an answer to this question (I’ve been searching for one for a long time), but I have some ideas that I’ve seen:Wait for External ConditionsWaiting for external conditions to changeor for my knowledge of external conditions to change to take action is very much my default mode, and it has advantages and disadvantages — while it frequently results in spending a non-optimal amount of time searching (since one can change conditions ones self, if one wants), it has the advantage of being simple and more consistent than you might expect. If you take this approach, you don’t need to worry whether you’re doing the right thing or not — simply have faith that some change will come at some point to shock you into action.Schedule TimeAnother approach to deciding which mode to operate in is to schedule time — if you default to action, schedule time for reading and introspection. If you default to introspection, schedule time to build something (anything), and see if it feels like the right thing once you’ve started.Find Opposite CollaboratorsInstead of taking it upon yourself to decide when the time is right to switch modes, you can seek out collaborators who are predisposed to operate in the opposite mode that you are. This has two advantages — first, just by being around people who are operating in a different mode, you realize that that is a possibility for you as well, should you choose to pursue it. Second, if you’re working together with someone who has a different tendency from you, you can have faith that they are covering the part that you aren’t, allowing you to focus on the parts that you’re inclined towards.I’m sure that other approaches exist (as well as simply trusting your intuition, which is an often underrated approach) — for now, I’ll continue searching, collecting, gathering, and thinking over the possibilities. And maybe some day, I’ll know exactly which path to take.☙</description>
      <pubDate>10 Feb 21 11:45 EST</pubDate>
      <guid>https://notebook.wesleyac.com/hermit-magician/</guid>
    </item>
    <item>
      <title>Scattered Thoughts on Why I Waste My Own Time</title>
      <link>https://mbuffett.com/posts/wasting_time/</link>
      <description>&lt;a href=&#34;https://mbuffett.com/posts/wasting_time/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I spend way too much time on reddit, hacker news, twitter, feedly, coinbase, robinhood, email, etc. I’d like to spend time on things that are more meaningful - chess, digital painting, side projects, reading, video games, exercising, meditating, etc. This has been a problem for years. This post is only slightly adapted from my personal notes, so excuse the lack of structure; I didn’t want to try to twist this jumble of thoughts into a narrative. Ascribing meaning Some might object to the “meaningful” items in that list, like video games. Why does playing Wild Rift, which is often very toxic, get a passing mark? By what am I judging a given 10 minutes as being spent wisely? Progress on a skill? Enjoyment in the moment? Creation over consumption? Strengthening relationships? Giving to a greater cause? It’s some mix of the above and others, but the point is that I can look at two activities and place them in relation to each other. Studying chess is more meaningful than playing Wild Rift, which is more meaningful than reading hacker news articles, which is more meaningful than reading the comments. The source of this meaning is less of the concern of this post. The point is that this is something I’ve thought about as a potential reason, like “maybe I don’t really believe that studying chess is a better use of time than reading /r/programming, after all what’s even the point of being good at chess?”. But realistically I think this is at most a small part of the issue. There have been times where some task has seemed so important, that it was able to overshadow the time-wasting apps. When I was going to Hack Reactor and felt a pressing need to get a job so I didn’t have to go back to USC, I didn’t spend my free time wastefully. When I was trying to create a startup with a couple friends, I didn’t have any problem eliminating the time-wasters from my day. But getting in these modes doesn’t seem like a long-term solution. I’d like to choose chess over reddit for the rest of my life, but it doesn’t overpower the pull of reddit by it being more meaningful alone. Absence of more meaningful activities For a while, part of my problem with wasting time was that I didn’t have many alternative things to do. Apart from work, my only hobby was reading, so if I’d burned myself out on the few genres I liked, I’d inevitably fall into a Reddit-shaped hole. This is no longer a problem, I’ve got plenty of things I can use my time on. So I think this is a failure mode that can cause the same time-wasting pattern, but it’s not my problem currently. Top shelf theory From here, the theory is that as long as you have access to things that are more immediately satisfying, you’re making everything else seem worse by comparison. For example, if you have the option of eating Blue Bell dutch chocolate ice cream, eating some berries seems worse by comparison; a comparison you won’t be able to help making. If you didn’t have the option of eating Blue Bell Dutch chocolate ice cream (this blog brought to you by the Blue Bell marketing department), then you could just enjoy the berries. This rang true when I read through it the first time. However, it doesn’t match my own experience in getting to a stage where I can easily eat healthy. I don’t eat ice cream any more, but I do have a ranking for snacks, with berries on the “top shelf”. So I will generally eat berries over other snacks when I have them, but not exclusively, and when I eat beans I’m not thinking of the berries I could be eating. I think the way the top shelf theory falls apart is when you’re not just talking about a couple super-satisfying options. To go back to time-wasting instead of food, it’s not like the only issue is that Twitter and Reddit are these irresistible temptations, and if they didn’t exist I would be spending my time optimally. They may have caused the issue, it’s hard to say, but what I have now is more general, in that I have a pull toward easy satisfaction, that I find hard to resist. If I eliminate Reddit/Twitter specifically, I start watching YouTube videos or reading trash fiction. It’s a gradient, and even if I could magically set some strict boundaries around my behavior, I’d be attracted to the tasks that just about made it inside the boundary. I’d end up bikeshedding my vim config instead of doing real work on my side project. Junk food and junk time You can compare time-wasting to eating junk food, like the top shelf article does. In contrast to my free-time habits, my eating habits are flawless. There’s zero disconnect between “what I wish I would eat” and “what I eat”. I read a book on gut health and went from a steak every day to a whole food, plant-based diet. I recently read another book on plant foods and replaced all my refined grains with whole grains. I’ll pick up brown rice now instead of white rice with no hesitation, even though I know I like the taste of white rice better. Overall, this feels like an identity thing. Caring about my own health is a very strong part of my identity. It would feel wrong to eat any other way; I’d be betraying a core part of myself. So what would that look like in the context of wasting time? If I was as good at spending my time as I am at eating? I would identify strongly as someone that doesn’t waste their own time, to the point where going on reddit wouldn’t even occur to me, the way that drinking my girlfriend’s diet coke doesn’t occur to me when I go to the kitchen. If I did find myself wasting time, I’d be able to cut it out without effort, the way I could cut out white rice as soon as I learned it was missing most of the nutrients that brown rice has. The problem is I don’t know how to bridge that gap, but I have tried. Attempt #42: screen time When the screen time limits feature came around, I set some reasonable limits on my problem apps. I’ve tried this approach many times, and it has the “new todo-app effect”; it works for a while because I’m excited about the new system, but ultimately doesn’t change my behavior. There came times when I was exhausted, or waiting for something, or there was a legitimate reason to use one of my screen time apps, and I’d tap “another 15 minutes”. Each time that happens it weakens the strength of that popup, until eventually it’s almost second-nature to tap that sweet “another 15 minutes” prompt. At that point it was still probably better than nothing, but not by a whole lot. Attempt #74: screen time, throw away the key edition At one point I set some strict screen time limits, then gave my roommate the keys to the kingdom. I could no longer ask for 15 more minutes, because I didn’t have the pin. This worked pretty well, strictly speaking. I spent way less time on reddit, hacker news, and twitter. The voice in my mind telling me to seek out immediate pleasure didn’t change though, so I found myself discovering entertaining YouTube channels, playing stupid mobile games, etc. Also I wasn’t immune to the pull of reddit and twitter on my laptop, while waiting for my code to compile. Success story #1: puzzle storm Somewhat recently I had a month or two of success. I managed to stay away from twitter and reddit enough that I didn’t view them as a problem anymore. I had been studying chess for a long time, and discovered a new mode on Lichess, puzzle storm. It’s a timed mode where you solve as many puzzles as you can in about 3 minutes. It was fun and lined up with a skill I was learning, so it ended up winning in whatever calculus my subconscious does to decide how I’ll spend my free time. This was a nice fluke, but there’s no chance that coding on a side project, for example, will ever win out in in an immediate gratification battle with the time-wasters. So I don’t see how it could be the path to a general solution. Mindfulness I’ve been picking up meditation again, and it feels like there may be a benefit in there. In the beginning, you’ll often forget the breath completely, but as you meditate more and more, it won’t go so far as to totally forget the breath. You train yourself to acknowledge when this is happening, to stop yourself before you get to the “forgot to focus on the breath at all” stage. Mind wandering during meditation seems somewhat related to attention wandering while I go about my day. If, instead of letting myself be lured by twitter’s siren’s call while staging is deploying, maybe I could recognize the pull and bring my attention back to the task at hand. If I was really good at this I could eliminate a certain class of time-wasting. This hasn’t actually happened yet, it just feels similar in some ill-defined way, the feeling of being pulled into reddit and the feeling of being distracted from focusing on the breath. Motivation bursts Occasionally I’ll get fired up by something, usually a book but sometimes a YouTube video, and I’ll get the motivation needed to do the more meaningful things. For example I read “The War of Art”, and for a few weeks I viewed the war for my attention as just that, and the gung-ho mentality that the book instilled in me was enough to win some battles. But this is more like someone on a SAD diet reading fad diet books every few weeks, riding on motivation for a week then crashing back to baseline. It helps, I’ve finished a couple side projects on waves of motivation like this, but it’s a band-aid. Attention span As much fear-mongering as there is online about how social media is eroding our attention span, I haven’t actually seen this effect in practice. When I need or want to, I can still focus on stuff for the entire day. When I was close to getting to 1800 elo in chess, I could spend the whole day studying. When Will Wight’s latest book, Reaper, came out, I read it in a day, probably never visiting twitter or reddit. Maybe it would actually be better if I saw my attention span eroding from doomscrolling, because then I’d have more motivation to quit. Lack of evidence I think if I saw some really compelling studies about how twitter/reddit/etc affect my health and mental function, it would be easier to avoid them. Going back to the food analogy for the 100th time, it’s easy for me to avoid dairy and red meat, after seeing some studies. Unfortunately, for time-wasting the best argument I can come up with is “scrolling through that much crap just, like, can’t be good for you”, and that doesn’t have the same motivating factor as something like “populations eating a primarily vegetarian diet have a 40% lower incidence of cancer”. The value of my own time Wasting these vast swathes of time seems to suggest that at some level, I don’t value that time too highly. Which seems crazy, I’d say I value my time higher than most everyone. But the proof seems to be right there in my screen time report, every week. “This is how much you value your time”, it says, “you wasted three and a half hours every day, on absolutely nothing of value”. Potentially, if I valued my time more, I would be less inclined to read reddit, scroll twitter, and watch guess the elo on youtube. Certainly if I knew I only had a few weeks left, I wouldn’t be hitting the kind of numbers on my screen time report that I am now. What sort of steps would I take, if valuing my time incorrectly is the problem? Memento mori is the practice of reflecting on your own death, specifically in how much time you have left. Maybe engaging in that practice would help highlight how little time I have to do things that truly matter to me. Or maybe a gratitude practice. I haven’t explored these, but intend to. The value of your time Breaking out of first person, this is getting really long. If you’ve figured out the secret to using your time well, let me know. Thanks for reading! If you have any questions, comments, or just want to say hi, please email me at me@mbuffett.com. I&#39;m not very active on twitter, but you can choose to follow me in case that changes. If you&#39;re into chess, I&#39;ve made a visualization trainer. The visualization helper is novel, there are 500,000+ puzzles, and you can select the difficulty both of the puzzles and the visualization. Samar Haroon, my girlfriend, has started a podcast where she talks about the South Asian community, from the perspective of a psychotherapist. Go check it out! She&#39;s also on instagram. Looking for a personal trainer/nutritionist? Highly recommend Conrad&#39;s Coaching. Not an affiliate link or anything, just a friend that does a great job and really knows her stuff. </description>
      <pubDate>12 Nov 21 08:38 EST</pubDate>
      <guid>https://mbuffett.com/posts/wasting_time/</guid>
    </item>
    <item>
      <title></title>
      <link>https://mikemcquaid.com/2021/07/21/how-i-get-things-done/</link>
      <description>&lt;a href=&#34;https://mikemcquaid.com/2021/07/21/how-i-get-things-done/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; How I Get Things Done 21 July 2021 The first thing you need to accept is: your memory sucks. If you have tasks you want or need to do in life (for yourself or for others) chances are you can’t remember them all. That’s why organised people don’t rely on their own memory and instead have a system to track their commitments. In this post I’ll explain my system and why it is the way it is. Firstly, when someone asks me to do something I immediately write it down in my Apple’s Notes app. This doesn’t have any mechanism for storing dates or times but just lets me enter text. This is good because it’s available and synced everywhere. If I have time (or when I next do) I then sort into lists based on topic matter. Some of my long-running lists are: GitHub (my current employer), Open Source (encompassing Homebrew and open-source projects I work on) and Random (a short dump for other tasks). Every time I do work for GitHub, open-source or myself I look on the relevant location to see what I should do. I try my long-running notes structured into sections headed: Urgent - tasks that need done soon Don’t Do - the sort of tasks I shouldn’t be doing Todo - tasks that need to be done eventually Blocked - tasks I can’t do until something happens Notes - a dumping ground for anything that doesn’t fit above When there’s a task with a time or date I put it into either Apple’s Reminders or Calendar app. Reminders gets anything that needs done on a particular date or repeatedly but doesn’t require me to go anywhere. Calendar is generally when I need to go somewhere or if it’s e.g. an online meeting that has a specific start and end time. These apps will pop up alerts on my Apple devices with a noise or vibration so I actually remember them. The above was sufficient in 2015 when I first wrote this post but since I got promoted to Staff Engineer at GitHub I’ve had to get a lot better at keeping track of what I plan to work on in future, am working on right now and have completed. I’ve found a personal GitHub Projects board to be a good fit for this. I have columns headed: Backlog - tasks I may or may not ever do but want to keep track of Up Next - tasks I plan to work on in the near future WIP (Work In Progress) - tasks I’m working on right now Blocked - tasks I can’t do until something happens Done This Week - tasks I’ve completed this week; I’ll report on these and archive If you have any familiarity with Getting Things Done then some of the above may seem familiar. I tried to follow it strictly in the past but found the structure overkill for me personally. I tend to keep my various TODO lists short enough that I can quickly scan and mentally prioritise them daily. So far it’s relatively straightforward but I’ve omitted the most common productivity nightmare: email. I’ve heard the main problem with email being that it’s basically a TODO list that anyone can add to at any time. This is a problem. First thing to do with your email is trying to keep Inbox Zero. In short: the default state of your inbox should be to have no emails in it and when you check your email you should end up with an empty inbox afterwards. This sounds like a bit of a pipe dream so let me tell you how I try to do this. You probably get a bunch of email that you expect. Someone commented on your GitHub issue. Your company’s weekly newsletter got sent round. That person in your office who sends you only junk. These all patterns and patterns are good because a computer can handle them trivially for us. What I do is set up filters (in Gmail, iCloud, Outlook, etc.) which sorts each type of email into a folder (“label and archive” in Gmail). My long-running personal email folders are Bills, GitHub, News, Social (all social media notifications) and Software. This immediately let’s me prioritise these groups. Anything in these folders is never urgent during work time. If I’m emailed by someone I didn’t expect (e.g. my dog) it may be urgent so it ends up in the inbox instead of a folder. I also keep my personal, Homebrew and GitHub emails in separate email accounts with different filtering rules. This makes it easier for me to ignore Homebrew and GitHub emails when I’m not working or on my phone. If you’re interested in my GitHub filtering rules, I wrote a post on the GitHub blog about managing large numbers of notifications. It’s important to ask yourself when you get a new email: was this expected, somewhat urgent/important or even desirable? If not, filter it into a folder, unsubscribe from the mailing list, tweak your social media notifications or setup a filter to just automatically mark it as read and archive/delete it. Stop wasting your time repeatedly ignoring the same emails. The tricky thing with the above system: what do you do if you don’t have the time to reply immediately to an email or it’s generally something you want to deal with later? I used to enjoy the Mailbox (an email client from DropBox) “Later” feature which could remind me of an email at a later point. Mailbox is no longer around and I’ve been burned too many times by my email clients going away that I feel I can no longer rely on this functionality (until it’s in Apple Mail or Gmail). I now have returned to will leaving email in my inbox if it needs actioned imminently (i.e. this month) or flag (“star” in Gmail) if I need it at some point in future. Both Apple’s Mail app and particularly Gmail provide good enough search that I can always find what I need (as I always archive and never delete emails). Slack has become a bigger part of my life for GitHub and Homebrew since I originally wrote this post in 2015. I tend to treat it pretty similarly to email: it can be a bit of a TODO list but I don’t want others to control it. Thankfully, it does have a pretty decent reminders feature that gets me back what I lost from Mailbox. I hope you found this post to be useful. To summarise: immediately write down tasks you need to do on your phone/computer, triage them regularly, assign times/dates when possible and automate your email for easier prioritisation. Finally, a pet hate: if you commit to doing something for someone please immediately make a note of this and actually do it in a timely fashion or let the person know if you can’t. This alone will make you seem like A Productive Person. Don’t try to just remember (because you won’t) or write it on a scrap of paper because you’ll lose it. Good luck! </description>
      <pubDate>17 Nov 21 11:14 EST</pubDate>
      <guid>https://mikemcquaid.com/2021/07/21/how-i-get-things-done/</guid>
    </item>
    <item>
      <title>The genius of John von Neumann</title>
      <link>https://unherd.com/2021/11/the-genius-of-john-von-neumann/</link>
      <description>&lt;a href=&#34;https://unherd.com/2021/11/the-genius-of-john-von-neumann/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In 1956, shortly before his early death from bone cancer, John von Neumann received a letter from Kurt Gödel, the Austrian logician. After a paragraph of half-hearted inquiries into von Neumann’s health, Gödel finally got to the point: he had found an interesting new mathematical puzzle. And in the Fifties, if you found an interesting new mathematical puzzle, you sent it to John von Neumann. The puzzle that Gödel was describing would come to be known as P vs NP. To oversimplify, it asks: can every mathematical question which can be checked quickly also be solved quickly? For instance: you are given a half-complete Sudoku puzzle. Is there a legal solution? If someone were to show you a solution, you could quickly verify whether it was legal. If you used a larger grid, the solution would take longer to check, but not exponentially so1. But establishing that there is a legal solution is much slower. There might be quintillions of possible ways of filling it out; the number grows exponentially with the size of the grid. Checking them all one by one might take millions of years even on a powerful computer, if the grid is large enough. What Gödel wanted to know was: is there some algorithm that could solve the Sudoku (or similar problems) as quickly as we could check a solution? P vs NP is one of the great outstanding questions of mathematics: it has profound implications, but no one has been able to prove it, one way or the other. The Man from the Future, Ananyo Bhattacharya’s fascinating, fast-moving intellectual biography of von Neumann, made me think of P vs NP. Not because von Neumann solved it; but because von Neumann, in Bhattacharya’s telling, provided solutions to many other previously unsolved problems, in dozens of different fields; others simply had to check them, and expand on them. There is, I think, some discomfort about calling people “geniuses” these days, or in admitting that intelligence is a real thing or that it shapes history – but von Neumann was a genius, and his extraordinary intelligence shaped the modern world. He was not an economist, but he developed the use of fixed-point theorems in economics in a paper which the historian Roy Weintraub calls “the single most important article in mathematical economics”, and which inspired “half a dozen” Nobel laureates. His work on game theory – he invented the field, and coined the term “zero-sum game” – inspired at least half a dozen more. Game theory also transformed the study of evolution, inspiring the work of Bill Hamilton, John Maynard Smith, and Richard Dawkins. He developed utility theory, the basis of modern economics. In 2011 Daniel Kahneman, another economics Nobel laureate (who won his Nobel partly for building on von Neumann’s game-theory ideas), called it“ the most important theory in the social sciences”. Some of his last work, with Stanislaw Ulam on “cellular automata” – grids of squares that turn on and off according to simple rules – shaped modern computer science in thousands of ways, notably inspiring John McCarthy, who would go on to coin the term “artificial intelligence”. Von Neumann’s genius was apparent early. In 1915, at the age of 11, he had gone to the famous gymnasium school in his native Budapest; the “legendary” maths teacher, László Rátz, immediately realised that von Neumann was beyond his ability to teach, and sent him for extra tuition at the local university. There he was mentored by Gábor Szegö, later head of Stanford’s maths department, who was “moved to tears” by his brilliance. At 17, still at high school, he partly rescued Cantor’s set theory, the basis of much mathematical theory, from a crippling paradox. A couple of years later, he helped reconcile Werner Heisenberg and Erwin Schrödinger’s rival models of quantum mechanics. In the early Thirties, he met the astronomer Subrahmanyan Chandrasekhar, and worked with him on general relativity and the behaviour of stellar clusters. Chandrasekhar would later tell an interviewer, “If I say, ‘He reminds me of von Neumann,’ that’s about the best compliment I can give anyone.” Von Neumann read some Alan Turing research which imagined a hypothetical computing machine, and saw how to build a working computer. The paper he produced building on Turing’s ideas is considered “the birth certificate of modern computers”, according to the computer scientist Wolfgang Coy. With his wife Kläri, and Ulam, he pioneered Monte Carlo simulations, vital now in climate modelling and a million other fields. In almost every sphere of scientific inquiry – physics, biology, maths, economics, the social sciences, computing – you find von Neumann’s fingerprints. There is a Wikipedia page of “List of things named after John von Neumann.” Were it not for him, our understanding of the world would be decades behind where it is. What created this genius? Bhattacharya does not speculate a great deal, but there are things worth considering. First, simple genetics: his family was high-achieving. His father was a doctor of law and an economic adviser to the Hungarian government; his uneducated maternal grandfather apparently could “add or multiply numbers into the millions” in his head instantly, a trick von Neumann emulated. The family was “puzzled” by their son’s inability to play the piano properly at the age of five, suggesting rather higher expectations than most. But it turned out to be because he “had taken to propping up books on his music stand so he could read while ‘practising’”. He also grew up in a fertile environment. Around the turn of the 20th century, the Budapest Jewish community of which he was part produced an astonishing number of great thinkers. Near-contemporaries included Dennis Gabor, “who won the Nobel Prize in physics in 1971 for inventing the hologram”; Theodore von Kármán, after whom the “Kármán line” is named, denoting the boundary between the Earth’s atmosphere and space; and Eugene Wigner, Edward Teller, and Leo Szilard, three of the greatest minds behind the Manhattan Project. The atomic bomb has been described as a “Hungarian high school science fair project”. The Hungarians who worked on America’s atomic weapons programme in the Thirties and Forties were known as “the Martians” by the other physicists – the joke being that the only way of explaining them was that super-intelligent aliens must have come to Budapest in the late 19th century and had babies with the locals. Von Neumann was the most alien of the lot. But there was some accident of history that meant that European university departments at that time were disproportionately Jewish, and Belle Epoque Budapest, which was going through a less than usually antisemitic period, had a large and well-integrated Jewish population. Von Neumann himself speculated that insecurity drove this Jewish success – they recognised that Hungary’s tolerance might evaporate at any moment, and that they faced “the necessity to produce the unusual or face extinction”. The tolerance did evaporate, in Hungary and elsewhere. Von Neumann, along with Teller, Wigner and the rest, had already left for Princeton, but Nazi persecution of the Jews in Germany devastated their universities: 15% of physicists and 19% of mathematicians were dismissed, including 20 who had won or would win Nobel prizes. Ironically, this may have lost Germany the war: analysis suggests that the loss of Jewish scientists damaged German science for decades. Werner Heisenberg, a German quantum physicist, was branded a “white Jew” for believing in Einstein’s theories, despite being a nationalist. He later said that it was not worth Germany pursuing nuclear weapons, because they wouldn’t be ready in time to affect the war: he believed this because he thought Germany’s nuclear research was well ahead of other nations. “As it was,” says Bhattacharya. “Until 1933.” So von Neumann, along with Wigner and others, ended up in Princeton — and then at the next-door Institute for Advanced Study, a sort of intellectual all-star team, where great brains were enticed from around the world with vast salaries, no undergrads to teach, and the promise that they could just think big thoughts. Einstein was there, along with Gödel, Robert Oppenheimer, Freeman Dyson, Ulam, and a host of others. It was an environment made for a certain kind of hard-to-pigeonhole genius, able to wander from subject to subject simply by walking around campus, surrounded by brilliant weirdos. Von Neumann is compelling evidence, I think, that individual genius is important and influential. Yes, he worked in a series of collaborations; yes, he built on the work of others. But he seems to have pushed on, or sometimes simply created, entire subfields of science, into areas that no one else realised could exist. The economist Oskar Morgernstern remembers him rapidly devising utility theory, immediately overturning economic orthodoxy: “But didn’t anyone see that?”, von Neumann asked. It’s fashionable to say that intelligence isn’t real, or that we can’t define it, or that it’s a Western colonial construct. But the word points to a real thing: there is some quality which rocks don’t have, and which mice have a bit of, and which chimpanzees have more of, and humans have a lot of; and which is something like problem-solving ability or ability to achieve goals. Calling it intelligence seems as good as anything. It is this ability which has allowed humanity to shape the world, and it is this ability that some people – von Neumann among them – seem to have in unusually large measure. Via scientific and technological progress, intelligence has made human life better. But in itself, intelligence is morally neutral: it can serve any end, good or ill, to which it is put. Von Neumann is a case in point. He developed computers partly to better predict the behaviour of explosive shockwaves and ballistic shells; he designed two different kinds of nuclear weapon, including the plutonium implosion bomb that was dropped on Nagasaki; his game-theoretic ideas led him to suggest using atom bombs in a first strike on Russia, and he was part of the inspiration for Doctor Strangelove. He believed that all this was in the interest of America, his adopted country, and no doubt of humanity; but not everyone would agree with him. First and foremost he wanted to solve puzzles. I put the book down wondering if it is still possible to encourage and harness genius. Perhaps it’s as simple as putting lots of clever people together and letting them think weird thoughts — and Von Neumann and his colleagues were often weird people. Or perhaps it is a true accident, and the only lesson is randomness. Perhaps the proposed new field of research into “progress studies” will yield some ideas as to how to recreate that environment in which Von Neumann and his fellow weirdos flourished. I wondered, too, if John von Neumann was well enough to understand the P vs NP puzzle when he received that letter from Gödel. For it is a wonderful metaphor for genius. I can dimly understand, for instance, Turing’s solution to the “Halting Problem”, or Gödel’s incompleteness theorem, or Russell’s set paradox that undermined mathematics. (They’re all based on the “liar paradox” – the statement “this statement is a lie”, which is false if true or true if false.) But it often takes no great brilliance to understand an idea once it has been brought forth: checking the Sudoku solution is relatively straightforward. Finding that idea in the space of possible ideas, though — solving the great sprawling Sudokus of science and maths, as Von Neumann did time and again, that takes genius. </description>
      <pubDate>18 Nov 21 09:49 EST</pubDate>
      <guid>https://unherd.com/2021/11/the-genius-of-john-von-neumann/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.gamasutra.com/view/feature/3938/the_pacman_dossier.php?print=1</link>
      <description>&lt;a href=&#34;https://www.gamasutra.com/view/feature/3938/the_pacman_dossier.php?print=1&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; The Pac-Man Dossier By Jamey Pittman [What design and AI lessons can we learn from Namco&#39;s seminal Pac-Man? From history through behavior, Gamasutra presents a comprehensive Jamey Pittman-authored guide to the classic game.] In 1999, Billy Mitchell of Hollywood, Florida became the first person to obtain a perfect score of 3,333,360 at Pac-Man, eating every possible dot, energizer, ghost, and bonus on every level without losing a single life in the process. But perhaps what is most amazing is the fact he can play without using any memorized routines widely known as &#34;patterns&#34;. Instead, he relies on his familiarity with how each ghost behaves as it moves through the maze, using that knowledge to keep Pac-Man one step ahead of his enemies at all times. Unlike Mitchell, most players are only able to rack up high scores with the aid of multiple patterns that take advantage of the game&#39;s deterministic nature. These patterns require perfect memorization and recall to be of any real use - a single hesitation or wrong turn during execution can make the remainder of a pattern useless. Not surprisingly, an over-reliance on these routines leaves many a player clueless as to how to effectively avoid the ghosts and finish off the remaining dots in the higher levels once a mistake occurs. Most Pac-Man strategy guides available on the internets today are very similar in content to the books that used to be sold back in the early 80s A summary of gameplay and scoring is provided first, followed by a list of patterns to be memorized by the reader, but very little insight is offered on how the game works or how the ghosts make decisions. Therefore, the purpose of this guide is to give the player and game designers a better design understanding of Pac-Man by taking a closer look at gameplay, maze logic, ghost personalities, and the mysterious &#34;split screen&#34; level. All information provided has been extracted from or verified with the disassembly output from the Midway Pac-Man arcade ROMs along with controlled observations of actual gameplay. As such, I have a high confidence in its accuracy. That said, if you notice an error or omission, please contact me so it can be corrected as soon as possible. I hope you find the information just as interesting and useful as I did for gaining a better understanding of this classic game. Special thanks to Don Hodges (www.donhodges.com) whose invaluable contributions to this guide can be found in every chapter. Chapter 1: Welcome to the Machine &#34;I don&#39;t have any particular interest in [computers]. I&#39;m interested in creating images that communicate with people. A computer is not the only medium that uses images; I could use the movies or television or any other visual medium. It just so happens I use the computer.&#34;-Toru Iwatani It was 1977 when a self-taught, capable young man named Toru Iwatani came to work for Namco Limited, a Tokyo-based amusement manufacturer whose main product lines at the time were projection-based amusement rides and light gun shooting galleries. He was just 22 years old with no formal training in computers, visual arts, or graphic design, but his creativity and aptitude for game design were obvious to the Namco executives that met with Iwatani. They offered to hire him-with assurances they would find a place for him in the company-and he accepted. Iwatani eventually found his place designing titles for Namco&#39;s new video games division. His limited computer skills necessitated his being paired with a programmer who would write the actual code while Iwatani took on the role of game designer for the project. This was a new job for the game industry in 1977 when most games were designed by the programmers who coded them. In addition to a programmer, Iwatani&#39;s team would usually include a hardware engineer to develop the various devices and components, a graphic artist to realize his visual ideas, and a music composer for any music and sound effects needed in the game. Iwatani had initially wanted to work on pinball machines, but Namco had no interest in the pinball business. Perhaps as a concession, his first game design, called Gee Bee, was a paddle game similar to Atari&#39;s Breakout but with a decidedly pinball-inspired slant to the gameplay. Released in 1978, it was Namco&#39;s first original video game-they had only ported existing Atari games to the Japanese market up to this point-and it enjoyed moderate success in the arcades. But the paddle games were losing ground fast to a new genre. The unprecedented success of Taito&#39;s Space Invaders in 1978 caused an industry-wide shift toward space-themed, shoot-&#39;em-up games (as well as a national coin shortage in Japan). Game manufacturers scrambled to match Taito&#39;s success with space shooters of their own. Namco was quick to follow suit, assigning a team to start work on a Space Invaders clone at once. It was around this time that Toru Iwatani began thinking about designing a different kind of game. He felt the shoot-&#39;em-up craze was destined to fade away like the paddle games before them. Rather than make another space shooter, Toru wanted to take his game design in a completely new direction that did not focus on violence or conflict, and would appeal to both male and female audiences. He took inspiration from a children&#39;s story about a creature that protected children from monsters by eating them. One of Iwatani&#39;s design methods included taking key words associated with a story to aid in developing his ideas. The kanji word taberu (&#34;to eat&#34;), became the premise for the game. The word kuchi (&#34;mouth&#34;) has a square shape for its kanji symbol and provided the inspiration for the game&#39;s main character-the better-known legend of Iwatani receiving his inspiration from a pizza pie with a slice missing was, by his own admission, not entirely correct: &#34;Well, it&#39;s half true. In Japanese the character for mouth (kuchi) is a square shape. It&#39;s not circular like the pizza, but I decided to round it out. There was the temptation to make the Pac-Man shape less simple. While I was designing this game, someone suggested we add eyes. But we eventually discarded that idea because once we added eyes, we would want to add glasses and maybe a moustache. There would just be no end to it. Food is the other part of the basic concept. In my initial design, I had put the player in the midst of food all over the screen. As I thought about it, I realized the player wouldn&#39;t know exactly what to do: the purpose of the game would be obscure. So I created a maze and put the food in it. Then whoever played the game would have some structure by moving through the maze. The Japanese have a slang word-paku paku-they use to describe the motion of the mouth opening and closing while one eats. The name Puck-Man came from that word.&#34; -Toru Iwatani The monsters from the children&#39;s story were included as four ghosts that chase the player through the maze, providing an element of tension. Attacks on the player were designed to come in waves (similar to Space Invaders) as opposed to an endless assault, and each ghost was given an unique personality and character. The children&#39;s story also included the concept of kokoro (&#34;spirit&#34;) or a life force used by the creature that allowed him to eat the monsters. Toru incorporated this aspect of the story as four edible power pellets in the maze that turn the tables on the ghosts, making them vulnerable to being eaten by the player. With a name and a basic design in place, Iwatani was ready to begin work. The team Namco assigned Iwatani to bring Puck-Man to life included a programmer (Shigeo Funaki), a hardware engineer, a cabinet designer, and a music composer (Toshio Kai). Development got underway in early 1979. In the course of that year, two new pinball-themed designs from Iwatani-Bomb Bee and Cutie Q-were both released during Puck-Man&#39;s development cycle. Both games were similar to Gee Bee but with stronger gameplay and improved visuals. The Namco team working on the Space Invaders clone for the past several months had just achieved a technological coup for Namco: the first game to use a true, multi-colored, RGB display instead of the monochrome monitors with colored cellophane tape so prevalent at the time. Thanks to the breakthrough of the other team, Iwatani now had the new promise of color to enhance his design. Mindful that he wanted the game to appeal to women, he immediately decided to use it on the ghosts, choosing pastel shades for the bodies and adding expressive, blue eyes. Dark blue was used for the maze itself, while Puck-Man was drenched in a brilliant yellow. The look and feel of Puck-Man continued to evolve for over a year. A large amount of time and effort was put into developing the ghosts unique movement patterns through the maze and tweaking the game difficulty variables as boards were cleared. Bonus symbols (including the Galaxian flagship) were added into the mix at some point, and the ghosts were finally given names: Akabei, Pinky, Aosuke, and Guzuta. Sound effects and music were some of the final touches added as development neared an end along with constant tweaking of the ghosts&#39; behavior. Puck-Man&#39;s creation was a year and five months in the making-the longest ever for a video game to that point. Finally, on May 22nd, 1980, it was released to arcades in Japan. Initially, the game did moderately well, but was no overnight sensation. In fact, Namco&#39;s multi-colored Space Invaders clone, called Galaxian, was much more popular with the gaming public-the predominately male, game-playing audience in Japan was unsure what to make of Puck-Man with its cartoon-like characters, maze, and pastel colors, whereas Galaxian was more immediately familiar to them with its shoot-&#39;em-up space theme. Midway was a distributor of coin-operated video games in the U.S. that was always looking for the next big hit from Japan to license and bring to America. They opted for both Puck-Man and Galaxian, modifying the cabinets and artwork to make them easier to manufacture as well as providing a more American look and feel. Puck-Man went through the majority of the changes: the cabinet was modified slightly, changing the color from white to a bright yellow to make it stand out in the arcade. The detailed, multi-colored cabinet artwork was replaced with cheaper-to-produce, three-color artwork illustrating an iconic representation of Puck-Man (now drawn with eyes and feet) and one blue ghost. English names were given to the ghosts (Blinky, Pinky, Inky, and Clyde), and the Namco title was changed to Midway. The most significant change to Puck-Man was the name. Midway feared it would be too easy for nasty-minded vandals to change the P in Puck-Man to an F, creating an unsavory epithet. Not wanting their product associated with this word, Midway renamed the game Pac-Man before releasing it to American arcades in October 1980. But the situation in America was reversed from Japan for these two titles. Galaxian got lost in the shuffle of the shoot-&#39;em-up craze that blanketed America&#39;s arcades and, by the fall of 1980, it was already competing with more advanced video games like Defender. In the end, Galaxian enjoyed moderate success in America and in Japan, but was never the smash hit the original Space Invaders was. Pac-Man was another story. There were no games to compare it to-it was in a genre all by itself. The bright yellow cabinet, visuals, and sounds drew a great deal of attention. No one had seen a game quite like this before. The addictive gameplay and challenge of increasing levels of difficulty kept the die-hard gamers more than happy, while the simplicity of the game appealed to younger children. The lack of war-like motifs and violence did as Iwatani had hoped and attracted a sizable female audience-a first for a video game. Even the parents wary of the violence-themed arcade games had no problem with their kids playing as cute and innocuous a game as Pac-Man. Pac-Man went on to capture the world&#39;s imagination like nothing before or since. It was a genuine phenomenon on a global scale, selling over 100,000 machines in its first year alone. Easy to learn but notoriously difficult to master, everyone from school children to Wall Street executives dropped quarter after quarter into an ever-increasing number of waiting Pac-Man machines. By 1982, Pac-Man merchandise was literally everywhere: t-shirts, hats, keychains, wrist bands, bedsheets, air fresheners, wall clocks, drinking glasses, trading cards, stickers, cereal boxes, comic books-even a Saturday morning cartoon. A novelty song called &#34;Pac-Man Fever&#34; received significant radio play, reaching number nine on the U.S. Billboard charts. Many books were written offering tips and tricks used by the best players to achieve high scores-the first-ever strategy guides published for a video game. Fast-forward to nearly thirty years later: Pac-Man remains the best-selling coin-operated video game in history. Still considered the most widely-recognized video game character in the U.S., his likeness has been licensed to over 250 companies for over 400 products. His namesake has been adopted by the business world to describe a way to defend against a hostile takeover (the defending company swallows up the larger company instead in a move known as the &#34;Pac-Man defense&#34;). There is even an upright Pac-Man machine on display at the Smithsonian Institution in Washington, D.C. Unlike the majority of his early-80s contemporaries, new Pac-Man games are still in development today. Most recently, Pac-Man Championship Edition was released in 2007 for the X-Box 360 console with the aid of Namco game designer Toru Iwatani. Interest in the original coin-op title has never completely faded, thankfully. Thanks to Namco&#39;s re-release of Pac-Man and other arcade classics for modern home consoles, new generations of Pac-addicts have worn their hands out playing a game often older than themselves. Many classic titles are also kept alive thanks to the advent of high-quality arcade emulators available for the home computer (like MAME) that use a software copy of the arcade ROM chips to recreate the game with 100% accuracy. Several web pages with information about the original Pac-Man arcade game can be found online including Wikipedia and the Killer List Of Video Games. Chapter 2: Gameplay Details &#34;As Pac-Man was originally conceived to appeal to women players, it is a very easy and approachable game. I believe that is an ingredient in the longevity of the game.&#34;-Toru Iwatani, creator of Pac-Man The Basics The premise of Pac-Man is delightfully simple: using a four-way joystick, the player guides Pac-Man-up, down, left, and right-through a maze filled with dots for him to gobble up. Four ghost monsters are also in the maze and chase after our hero, trying to capture and kill him. The goal is to clear the maze of dots while avoiding the deadly ghosts. Each round starts with the ghosts in the &#34;monster pen&#34; at the center of the maze, emerging from it to join in the chase. If Pac-Man is captured by a ghost, a life is lost, the ghosts are returned to their pen, and a new Pac-Man is placed at the starting position before play continues. When the maze is cleared of all dots, the board is reset, and a new round begins. If Pac-Man gets caught by a ghost when he has no extra lives, the game is over. There are 244 dots in the maze, and Pac-Man must eat them all in order to proceed to the next round. The 240 small dots are worth ten points each, and the four large, flashing dots - best known as energizers - are worth 50 points each. This yields a total of 2,600 points for clearing the maze of dots each round. Players have two ways to increase their score beyond what is earned from eating dots: The first way to increase your score each round is by turning the tables on your enemies by making them your prey. Whenever Pac-Man eats one of the four energizer dots located in the corners of the maze, the ghosts reverse their direction and, in early levels, turn the same shade of blue for a short period of time before returning to normal. While blue, they are vulnerable to Pac-Man and can be gobbled up for extra points providing they are caught before the time expires. After being eaten, a ghost&#39;s eyes will return to the monster pen where it is resurrected, exiting to chase Pac-Man once again. The first ghost captured after an energizer has been eaten is always worth 200 points. Each additional ghost captured from the same energizer will then be worth twice as many points as the one before it-400, 800, and 1,600 points, respectively. If all four ghosts are captured at all four energizers, an additional 12,000 points can be earned on these earlier levels. This should not prove too terribly difficult to achieve for the first few rounds as the ghosts initially remain blue for several seconds. Soon after, however, the ghosts&#39; &#34;blue time&#34; will get reduced to one or two seconds at the most, making it much more problematic to capture all four before time runs out on these boards. By level 19, the ghosts stop turning blue altogether and can no longer be eaten for additional points. The second way to increase your score each round is by eating the bonus symbols (commonly known as fruit) that appear directly below the monster pen twice each round for additional points. The first bonus fruit appears after 70 dots have been cleared from the maze; the second one appears after 170 dots are cleared. Each fruit is worth anywhere from 100 to 5,000 points, depending on what level the player is currently on. Whenever a fruit appears, the amount of time it stays on the screen before disappearing is always between nine and ten seconds. The exact duration (i.e., 9.3333 seconds, 10.0 seconds, 9.75 seconds, etc.) is variable and does not become predictable with the use of patterns. In other words, executing the same pattern on the same level twice is no guarantee for how long the bonus fruit will stay onscreen each time. This usually goes unnoticed given that the majority of patterns are designed to eat the bonus fruit as quickly as possible after it has been triggered to appear. The symbols used for the last six rounds completed, plus the current round are also shown along the bottom edge of the screen (often called the fruit counter or level counter). See Table A.1 in the appendices for all bonus fruit and scoring values, per level. Ghosts have three mutually-exclusive modes of behavior they can be in during play: chase, scatter, and frightened. Each mode has a different objective/goal to be carried out: CHASE - A ghost&#39;s objective in chase mode is to find and capture Pac-Man by hunting him down through the maze. Each ghost exhibits unique behavior when chasing Pac-Man, giving them their different personalities: Blinky (red) is very aggressive and hard to shake once he gets behind you, Pinky (pink) tends to get in front of you and cut you off, Inky (light blue) is the least predictable of the bunch, and Clyde (orange) seems to do his own thing and stay out of the way. SCATTER - In scatter mode, the ghosts give up the chase for a few seconds and head for their respective home corners. It is a welcome but brief rest-soon enough, they will revert to chase mode and be after Pac-Man again. FRIGHTENED - Ghosts enter frightened mode whenever Pac-Man eats one of the four energizers located in the far corners of the maze. During the early levels, the ghosts will all turn dark blue (meaning they are vulnerable) and aimlessly wander the maze for a few seconds. They will flash moments before returning to their previous mode of behavior. Reversal Of Fortune In all three modes of behavior, the ghosts are prohibited from reversing their direction of travel. As such, they can only choose between continuing on their current course or turning off to one side or the other at the next intersection. Thus, once a ghost chooses which way to go at a maze intersection, it has no option but to continue forward on that path until the next intersection is reached. Of course, if you&#39;ve spent any time playing Pac-Man, you already know the ghosts will reverse direction at certain times. But how can this be if they are expressly prohibited from doing so on their own? The answer is: when changing modes, the system can override the ghosts&#39; normal behavior, forcing them to go the opposite way. Whenever this happens, it is a visual indicator of their behavior changing from one mode to another. Ghosts are forced to reverse direction by the system anytime the mode changes from: chase-to-scatter, chase-to-frightened, scatter-to-chase, and scatter-to-frightened. Ghosts do not reverse direction when changing back from frightened to chase or scatter modes. When the system forces the ghosts to reverse course, they do not necessarily change direction simultaneously; some ghosts may continue forward for a fraction of a second before turning around. The delay between when the system signals a reversal and when a ghost actually responds depends on how long it takes the ghost to enter the next game tile along its present course after the reversal signal is given (more on tiles in Chapter 3). Once the ghost enters a new tile, it will obey the reversal signal and turn around. Scatter, Chase, Repeat... Ghosts alternate between scatter and chase modes during gameplay at predetermined intervals. These mode changes are easy to spot as the ghosts simultaneously reverse direction when they occur. Scatter modes happen four times per level before the ghosts stay in chase mode indefinitely. Good players will take full advantage of the scatter periods by using the brief moment when the ghosts are not chasing Pac-Man to clear dots from the more dangerous areas of the maze. The scatter/chase timer gets reset whenever a life is lost or a level is completed. At the start of a level or after losing a life, ghosts emerge from the ghost pen already in the first of the four scatter modes. For the first four levels, the first two scatter periods last for seven seconds each. They change to five seconds each for level five and beyond. The third scatter mode is always set to five seconds. The fourth scatter period lasts for five seconds on level one, but then is only 1/60th of a second for the rest of play. When this occurs, it appears as a simple reversal of direction by the ghosts. The first and second chase periods last for 20 seconds each. The third chase period is 20 seconds on level one but then balloons to 1,033 seconds for levels two through four, and 1,037 seconds for all levels beyond-lasting over 17 minutes! If the ghosts enter frightened mode, the scatter/chase timer is paused. When time runs out, they return to the mode they were in before being frightened and the scatter/chase timer resumes. This information is summarized in the following table (all values are in seconds): Mode Level 1 Levels 2-4 Levels 5+ Scatter 7 7 5 Chase 20 20 20 Scatter 7 7 5 Chase 20 20 20 Scatter 5 5 5 Chase 20 1033 1037 Scatter 5 1/60 1/60 Chase indefinite indefinite indefinite Frightening Behavior Anytime Pac-Man eats one of the four energizers on the level, the ghosts reverse direction and, on earlier levels, go into frightened mode. Frightened ghosts turn dark blue and wander about the maze for a few moments, flashing briefly as a warning before returning to normal. Ghosts use a pseudo-random number generator (PRNG) to pick a way to turn at each intersection when frightened. The PRNG generates an pseudo-random memory address to read the last few bits from. These bits are translated into the direction a frightened ghost must first try. If a wall blocks the chosen direction, the ghost then attempts the remaining directions in this order: up, left, down, and right, until a passable direction is found. The PRNG gets reset with an identical seed value every new level and every new life, causing predictable results. This is why the frightened ghosts&#39; movements are repeatable in the context of a memorized pattern or sequence. As the levels progress, the time ghosts spend in frightened mode grows shorter until eventually they no longer turn blue at all (they still reverse direction). Refer to Table A.1 in the appendices for the frightened time in seconds and number of flashes, per level. Speed The game starts with Pac-Man at 80% of his maximum speed. By the fifth level, Pac-Man is moving at full speed and will continue to do so until the 21st level. At that point, he slows back down to 90% and holds this speed for the remainder of the game. Every time Pac-Man eats a regular dot, he stops moving for one frame (1/60th of a second), slowing his progress by roughly ten percent-just enough for a following ghost to overtake him. Eating an energizer dot causes Pac-Man to stop moving for three frames. The normal speed maintained by the ghosts is a little slower than Pac-Man&#39;s until the 21st level when they start moving faster than he does. If a ghost enters a side tunnel, however, its speed is cut nearly in half. When frightened, ghosts move at a much slower rate of speed than normal and, for levels one through four, Pac-Man also speeds up. The table below summarizes the speed data for both Pac-Man and the ghosts, per level. This information is also contained in Table A.1 in the appendices. PAC-MAN SPEED GHOST SPEED LEVEL NORM NORM DOTS FRIGHT FRIGHT DOTS NORM FRIGHT TUNNEL 1 80% 71% 90% 79% 75% 50% 40% 2 - 4 90% 79% 95% 83% 85% 55% 45% 5 - 20 100% 87% 100% 87% 95% 60% 50% 21+ 90% 79% - - 95% - 50% Cornering Pac-Man is able to navigate the turns in the maze faster than his enemies. He does not have to wait until he reaches the middle of a turn to change direction as the ghosts do (see picture below). Instead, he may start turning several pixels before he reaches the center of a turn and for several pixels after passing it. Turns taken one or more pixels before reaching the center are &#34;pre-turns&#34;; turns taken one or more pixels after are &#34;post-turns&#34;. Players learn to consistently move the joystick in the direction Pac-Man should go well before arriving at the center of a turn, ensuring each pre-turn is started as many pixels away from center as possible. This technique is known as cornering and is one of the first skills a new Pac-Man player should master. For every successful pre-turn maneuver, Pac-Man puts a little more distance between himself and any ghosts following close behind. Such a small gain in distance may not seem terribly significant at first, but cornering through a quick series of turns will shake off even the most determined pursuer. It is a vital tool for survival in the higher levels of the game. Whenever Pac-Man makes a pre-turn or post-turn, his orientation changes, and he starts to move one pixel in his new direction for every pixel traveled in his old direction, effectively doubling his speed as he moves at a 45 degree angle. Once he reaches the centerline of the new direction&#39;s path, he starts moving purely in that direction and his speed returns to normal. The greatest distance advantage is thereby gained by making the earliest pre-turn possible. The illustration below shows the layout of pre-turn pixels (shown in green), center point pixels (shown in yellow), and post-turn pixels (shown in red) for each of the four possible directions a turn can be approached. Each example shows Pac-Man entering the same four-way intersection from a different direction. When entering from the left, there are three pre-turn pixels before the center of the turn, and four post-turn pixels. Conversely, entering the same intersection from the right yields four pre-turn pixels and three post-turn ones. Entering from the top as opposed to the bottom exhibits the same property. For any turn that is made later than the earliest possible pre-turn, Pac-Man will be one frame behind where he would be for every pixel of &#34;lateness&#34; in the turn. Basically, it pays to move the joystick well before reaching a turn to maximize your speed. (click image for full size) Turning at the earliest pre-turns possible is also required to successfully execute most any pattern. Patterns are meant to be played with perfect cornering because it removes the human element of uncertainty as to when Pac-Man will turn. Without cornering, it would be nigh-impossible to reproduce the exact timing of every turn as made by the pattern&#39;s author, thereby increasing the possibility of unpredictable ghost behavior due to Pac-Man not being in the exact same tile at the exact same time anymore. Typically, the most popular patterns have been those that tend to &#34;hold together&#34; well when small input timing flaws occur (turning three pixels away from center instead of four when approaching a turn from the right is a timing flaw, for example). Other patterns-especially those that bring Pac-Man very close to the ghosts late in the sequence-tend to &#34;fall apart&#34; unless every turn is perfectly cornered. During a long Pac-Man session, even the best players will make occasional timing mistakes during a fast series of turns and have to deal with the possible consequences. As such, one should aim for perfect cornering at all times but remain alert for unexpected ghost behavior from subtle input timing flaws creeping into the pattern. Home Sweet Home Commonly referred to as the ghost house or monster pen, this cordoned-off area in the center of the maze is the domain of the four ghosts and off-limits to Pac-Man. Whenever a level is completed or a life is lost, the ghosts are returned to their starting positions in and around the ghost house before play continues-Blinky is always located just above and outside, while the other three are placed inside: Inky on the left, Pinky in the middle, and Clyde on the right. The pink door on top is used by the ghosts to enter or exit the house. Once a ghost leaves, however, it cannot reenter unless it is first captured by Pac-Man-then the disembodied eyes can return home to be revived. Since Blinky is already on the outside after a level is completed or a life is lost, the only time he can get inside the ghost house is after Pac-Man captures him, and he immediately turns around to leave once revived. That&#39;s about all there is to know about Blinky&#39;s behavior in terms of the ghost house, but determining when the other three ghosts leave home is an involved process based on several variables and conditions. The rest of this section will deal with them exclusively. Accordingly, any mention of &#34;the ghosts&#34; below refers to Pinky, Inky, and Clyde, but not Blinky. The first control used to evaluate when the ghosts leave home is a personal counter each ghost retains for tracking the number of dots Pac-Man eats. Each ghost&#39;s &#34;dot counter&#34; is reset to zero when a level begins and can only be active when inside the ghost house, but only one ghost&#39;s counter can be active at any given time regardless of how many ghosts are inside. The order of preference for choosing which ghost&#39;s counter to activate is: Pinky, then Inky, and then Clyde. For every dot Pac-Man eats, the preferred ghost in the house (if any) gets its dot counter increased by one. Each ghost also has a &#34;dot limit&#34; associated with his counter, per level. If the preferred ghost reaches or exceeds his dot limit, it immediately exits the house and its dot counter is deactivated (but not reset). The most-preferred ghost still waiting inside the house (if any) activates its timer at this point and begins counting dots. Pinky&#39;s dot limit is always set to zero, causing him to leave home immediately when every level begins. For the first level, Inky has a limit of 30 dots, and Clyde has a limit of 60. This results in Pinky exiting immediately which, in turn, activates Inky&#39;s dot counter. His counter must then reach or exceed 30 dots before he can leave the house. Once Inky starts to leave, Clyde&#39;s counter (which is still at zero) is activated and starts counting dots. When his counter reaches or exceeds 60, he may exit. On the second level, Inky&#39;s dot limit is changed from 30 to zero, while Clyde&#39;s is changed from 60 to 50. Inky will exit the house as soon as the level begins from now on. Starting at level three, all the ghosts have a dot limit of zero for the remainder of the game and will leave the ghost house immediately at the start of every level. Whenever a life is lost, the system disables (but does not reset) the ghosts&#39; individual dot counters and uses a global dot counter instead. This counter is enabled and reset to zero after a life is lost, counting the number of dots eaten from that point forward. The three ghosts inside the house must wait for this special counter to tell them when to leave. Pinky is released when the counter value is equal to 7 and Inky is released when it equals 17. The only way to deactivate the counter is for Clyde to be inside the house when the counter equals 32; otherwise, it will keep counting dots even after the ghost house is empty. If Clyde is present at the appropriate time, the global counter is reset to zero and deactivated, and the ghosts&#39; personal dot limits are re-enabled and used as before for determining when to leave the house (including Clyde who is still in the house at this time). If dot counters were the only control, Pac-Man could simply stop eating dots early on and keep the ghosts trapped inside the house forever. Consequently, a separate timer control was implemented to handle this case by tracking the amount of time elapsed since Pac-Man has last eaten a dot. This timer is always running but gets reset to zero each time a dot is eaten. Anytime Pac-Man avoids eating dots long enough for the timer to reach its limit, the most-preferred ghost waiting in the ghost house (if any) is forced to leave immediately, and the timer is reset to zero. The same order of preference described above is used by this control as well. The game begins with an initial timer limit of four seconds, but lowers to it to three seconds starting with level five. The more astute reader may have already noticed there is subtle flaw in this system resulting in a way to keep Pinky, Inky, and Clyde inside the ghost house for a very long time after eating them. The trick involves having to sacrifice a life in order to reset and enable the global dot counter, and then making sure Clyde exits the house before that counter is equal to 32. This is accomplished by avoiding eating dots and waiting for the timer limit to force Clyde out. Once Clyde is moving for the exit, start eating dots again until at least 32 dots have been consumed since the life was lost. Now head for an energizer and gobble up some ghosts. Blinky will leave the house immediately as usual, but the other three ghosts will remain &#34;stuck&#34; inside as long as Pac-Man continues eating dots with sufficient frequency as not to trigger the control timer. Why does this happen? The key lies in how the global dot counter works-it cannot be deactivated if Clyde is outside the house when the counter has a value of 32. By letting the timer force Clyde out before 32 dots are eaten, the global dot counter will keep counting dots instead of deactivating when it reaches 32. Now when the ghosts are eaten by Pac-Man and return home, they will still be using the global dot counter to determine when to leave. As previously described, however, this counter&#39;s logic only checks for three values: 7, 17, and 32, and once those numbers are exceeded, the counter has no way to release the ghosts associated with them. The only control left to release the ghosts is the timer which can be easily avoided by eating a dot every so often to reset it. Click on the YouTube video below to see a demonstration of this curious behavior: The last thing to mention about the ghost house is how to determine whether a ghost will move right or left after exiting the home. Ghosts typically move to the left once they get outside, but if the system changes modes one or more times when a ghost is inside, that ghost will move to the right instead of the left upon leaving the house. Areas To Exploit The illustration above highlights four special &#34;zones&#34; in the maze where ghost behavior is limited by certain conditions which can be exploited to the player&#39;s advantage. The two red zones represent the areas where ghosts are forbidden to make upward turns. Once a ghost enters either of these two zones, it may only travel from right-to-left or left-to-right until exiting the area. Thus, only Pac-Man has access to these four, upward-facing tunnel entrances. It will serve the player well to remember the ghosts can still access these tunnels from the other end! The red zone restrictions are enforced during both scatter and chase modes, but in frightened mode the red zones are ignored temporarily, allowing the ghosts to turn upwards if they so choose. The pink zones are in the two halves of the connecting side-tunnel. As mentioned previously, any ghost that enters the tunnel will suffer an immediate speed penalty until leaving the zone. This slow-down rule is always enforced and applies to ghosts only-Pac-Man is immune. Chapter 3: Maze Logic 101 We need to take a look at how ghosts are able to move through the maze in pursuit of a goal. All pathfinding logic described in this chapter is shared by the four ghosts - it is important to understand what they have in common before we get into what makes them different. Before we proceed, let&#39;s see how the game tracks the location of Pac-Man and the four ghosts (herein referred to as actors for brevity&#39;s sake). The visible game screen should be thought of as a regular grid of tiles, each eight pixels square. The actual pixel dimensions of the screen are 224 x 288, so dividing each value by eight yields a grid that is 28 x 36 tiles in size. Each tile is either in legal space or dead space. In the picture above, legal space is shown as the gray-colored tiles; all other tiles are considered dead space. Actors only travel between the tiles in legal space. Each dot sits in the center of a tile, meaning they are exactly eight pixels (one tile) apart-this is useful for estimating distances during gameplay: What Tile Am I In? As the actors move through the maze, the game keeps track of the tile each one occupies. An actor is only associated with a single tile at a time, although its graphic will overlap into the surrounding tiles. The location of the actor&#39;s center point is what determines the tile it occupies at any given time. As the actors can move at pixel-level precision, they are often not centered directly on top of the tile they are in. Consider the following example: (click image for full size) The transparent red ghost is moving left-to-right across a row of tiles in legal space. In frame one, its occupied tile (shown in bright red) is near the left side of the picture. It does not matter that some of the ghost&#39;s graphic is not in the tile-what matters is that the ghost&#39;s center point is in the tile. By frame two, it has moved far enough for its center point to be in the adjacent tile to the right and its occupied tile is updated accordingly. The ghost continues to be associated with the same tile until frame six where its center point has now crossed over into the next one. The underlying concept of tiles is essential for understanding the ghosts&#39; pathfinding logic as it only cares about the tile an actor occupies-not its per-pixel location within that tile. To the logic routines, the five actors look very much like the picture below. Each actor is defined by the tile it presently occupies along with its current direction of travel. Distances between actors are also measured in tiles (the pink ghost is five tiles away from Pac-Man horizontally and one tile away vertically, for example). Just Passing Through It wasn&#39;t too long after the release of Pac-Man when word began to spread of players occasionally passing straight through a ghost unharmed, seemingly at random. This rumor turned out to be completely true as most die-hard Pac-Man players can attest. If you play the game long enough, you will eventually see Pac-Man run into one of the ghosts and come out unscathed on the other side-it doesn&#39;t happen very often so enjoy it when it does! Some players have even gone so far as to incorporate this mysterious pass-through oddity into their patterns. The root cause of this elusive peculiarity lies in the way the game detects collisions between Pac-Man and the four ghosts. Any time Pac-Man occupies the same tile as a ghost, he is considered to have collided with that ghost and a life is lost. It is irrelevant whether the ghost moved into Pac-Man&#39;s tile or Pac-Man into the ghost&#39;s-the result is the same either way. This logic proves sufficient for handling collisions more than 99% of the time during gameplay, but does not account for one very special case: The above picture illustrates the conditions necessary to produce this curious behavior. There are five consecutive frames showing Blinky and Pac-Man passing through each other. Below each frame is the same scene represented by the tiles they currently occupy and the per-pixel location of their center points. Pac-Man and Blinky are at just the right position and speed relative to one another to cause them to swap tiles with each other simultaneously. In other words, Pac-Man&#39;s center point moves upwards into Blinky&#39;s tile in the same 1/60th of a second that Blinky&#39;s center point moves downwards into Pac-Man&#39;s tile, resulting in them moving past each other without colliding. Note that Pac-Man&#39;s origin point is centered on the top edge of his tile in frame four; this is still considered to be inside the bottom tile, but moving up one more pixel will push him over the edge into the next one. Pac-Man and Blinky have now swapped tiles with each other in frame five, and Pac-Man can go on his merry way because he never &#34;collided&#34; (i.e., shared the same tile) with Blinky at all! Click on the YouTube video below to see an example of the pass-through bug (it happens 40 seconds after playback begins): Target Tiles Whenever a ghost is in chase or scatter mode, it is trying to reach a target tile somewhere on (or off) the screen. A target tile is merely a way to describe the tile a ghost would like to occupy at any given moment. This tile can be fixed in place or change location frequently. Whenever the ghosts scatter to the corners of the maze, for example, each ghost is striving to reach a fixed target tile located somewhere near its home corner. In chase mode, the target tile is usually (but not always) related to Pac-Man&#39;s current tile which changes often. Although it may not be obvious at first, the only difference between chase and scatter mode to a ghost is where its target tile is located. The same pathfinding logic applies in either case. Looking Ahead Ghosts are always thinking one step into the future as they move through the maze. Whenever a ghost enters a new tile, it looks ahead to the next tile along its current direction of travel and decides which way it will go when it gets there. When it eventually reaches that tile, it will change its direction of travel to whatever it had decided on a tile beforehand. The process is then repeated, looking ahead into the next tile along its new direction of travel and making its next decision on which way to go. When a ghost looks ahead into the upcoming tile, it must examine the possible exits from that tile to determine a way to proceed. In the picture below, the red ghost has just arrived at tile A and is moving right-to-left. It immediately looks ahead to tile B (the next tile along its direction of travel). Each tile has four potential exits to be considered: right, left, up, and down. In the case of tile B, the up and down exits are blocked by walls and must be discarded as potential candidates. The right exit is also discounted because it would only take the ghost back to tile A again, and ghosts never voluntarily reverse direction. With three of the four possible exits eliminated from tile B, moving left is the only remaining choice. This example is the most simple to explain as the ghost has but one way it can legally move. As such, we did not have to worry about where its target tile was located. The majority of game tiles in legal space are similar to this one, but things get more interesting when a ghost approaches a tile with more potential exits to choose from. Intersections When a ghost arrives one tile away from an upcoming intersection, it must choose between several possible directions in which to proceed. Consider the following example: (click image for full size) In the first picture, the red ghost has just reached tile A and is seeking its target (shown as the green tile). It immediately looks ahead to the subsequent tile along its present direction of travel (up). In this case, that tile is a four-way intersection. As this intersection tile has no walls blocking off any of the exits, the ghost can only discard his reverse direction (down), leaving three exits open for travel. It looks one tile beyond the intersection in each of the three remaining directions, collecting &#34;test tiles&#34; (shown as the tiles with dashed, white lines). In the middle picture, the ghost triangulates the distance from each of these test tiles to its target tile. Whichever direction&#39;s test tile has the shortest distance to the target becomes the direction the ghost will take upon reaching the intersection tile. In this case, the right test tile has the shortest distance to the target, and the ghost updates its chosen direction for the intersection tile accordingly. Sometimes a ghost is presented with two or more test tiles that have the same distance to the target tile. In the example below, the red ghost must choose between moving down or left at the upcoming intersection tile. Unfortunately, both test tiles have the same distance to the target (bottom left). To break the tie, the ghost prefers directions in this order: up, left, down, right. Up is the most preferred direction; right is the least. Therefore, the ghost chooses to go left at the intersection because left precedes down in the preference list. Although it may seem obvious to a person that going down was the better choice to reach the target, ghosts are not that smart. They cannot see more than a few tiles ahead and, as a consequence, cannot recognize the disparity between these two options. Scatter Targets As mentioned before, each ghost has a fixed target tile it is trying to reach in scatter mode. The picture below shows the actual tile used by each ghost. Notice each target tile is in dead space on either the top or bottom edge of the screen. As such, the ghosts will never be able to reach them. Luckily, a ghost does not care if its goal is attainable or not-the A.I. routines are very short-sighted. All a ghost cares about is following the pathfinding logic described above to make the best choice it can on which way to turn at the next tile. As a result, it will simply make circles in the area of the maze nearest its target tile until the target is set to some other location. That&#39;s all scatter mode really is. The only reason a ghost has a &#34;favorite corner&#34; of the maze at all is due to the location of a target tile it will never reach. Chapter 4: Meet the Ghosts &#34;First, you&#39;ve got to learn how to control the monsters. See how the red, pink and blue are grouped together? It&#39;s easier to control two monsters than four.&#34;-Billy Mitchell, champion Pac-Man player In the last chapter, we learned how a ghost follows a target tile through the maze. Now we will take a closer look at Blinky, Pinky, Inky, and Clyde to better understand why they behave so differently when in chase mode. They all share the same pathfinding logic for chasing a target tile, so how is it each one behaves differently when following Pac-Man? The answer is delightfully simple: Pac-Man&#39;s tile is not always the target. Every ghost has a distinct method for calculating its target tile in chase mode, resulting in their unique personalities. Some of the ghosts use Pac-Man&#39;s actual tile as the target; others only use it as an intermediate step to find another tile. Sometimes a ghost is targeting a tile that has absolutely nothing to do with Pac-Man at all! Regardless of where a ghost&#39;s target tile is at the time, Pac-Man will still be killed if he gets in that ghost&#39;s way. Rumor has it Toru Iwatani and his team spent months doing nothing but tweaking and refining the ghost A.I. routines before releasing Pac-Man to the world. Their efforts show in the final product: Itawani&#39;s team created the illusion of complex pathfinding by using very simple logic and very little code. Blinky: The red ghost&#39;s character is aptly described as that of a shadow and is best-known as &#34;Blinky&#34;. In Japan, his character is represented by the word oikake, which means &#34;to run down or pursue&#34;. Blinky seems to always be the first of the ghosts to track Pac-Man down in the maze. He is by far the most aggressive of the four and will doggedly pursue Pac-Man once behind him. Of all the ghosts&#39; targeting schemes for chase mode, Blinky&#39;s is the most simple and direct, using Pac-Man&#39;s current tile as his target. In the pictures above, we can see Blinky&#39;s target tile is the same as Pac-Man&#39;s currently occupied tile. Targeting Pac-Man directly in this way results in a very determined and tenacious ghost who is tough to shake when he&#39;s right behind you. All ghosts move at the same rate of speed when a level begins, but Blinky will increase his rate of speed twice each round based on the number of dots remaining in the maze. While in this accelerated state, Blinky is commonly called &#34;Cruise Elroy&#34;, yet no one seems to know where this custom was originated or what it means. On the first level, for example, Blinky becomes Elroy when there are 20 dots remaining in the maze, accelerating to be at least as fast as Pac-Man. More importantly, his scatter mode behavior is also modified to target Pac-Man&#39;s tile in lieu of his typical fixed target tile for any remaining scatter periods in the level. This causes Elroy to chase Pac-Man while the other three ghosts continue to scatter as normal. As if that weren&#39;t bad enough, when only 10 dots remain, Elroy speeds up again to the point where he is now perceptibly faster than Pac-Man. If a life is lost any time after Blinky has become Elroy, he will revert back to his normal behavior and speed when play resumes, heading for his home corner during the initial scatter period. But once the last ghost (Clyde) has left the ghost house in the middle of the board, he will turn back into Elroy again. Keep in mind: he is still in scatter mode the entire time. All that has changed is the target tile-he will still reverse direction when entering and exiting scatter mode as before. As the levels progress, Blinky will turn into Elroy with more dots remaining in the maze than in previous rounds. Refer to Table A.1 in the appendices for dot counts and speeds for both Elroy changes, per level. Pinky: Nicknamed &#34;Pinky&#34;, the pink ghost&#39;s character is described as one who is speedy. In Japan, he is characterized as machibuse, meaning &#34;to perform an ambush&#34;, perhaps because Pinky always seems to be able to get ahead of you and cut you off when you least expect it. He always moves at the same speed as Inky and Clyde, however, which suggests speedy is a poor translation of the more appropriate machibuse. Pinky and Blinky often seem to be working in concert to box Pac-Man in, leaving him with nowhere to run. In chase mode, Pinky behaves as he does because he does not target Pac-Man&#39;s tile directly. Instead, he selects an offset four tiles away from Pac-Man in the direction Pac-Man is currently moving (with one exception). The pictures below illustrate the four possible offsets Pinky will use to determine his target tile based on Pac-Man&#39;s orientation: If Pac-Man is moving left, Pinky&#39;s target tile will be four game tiles to the left of Pac-Man&#39;s current tile. If Pac-Man is moving right, Pinky&#39;s tile will be four tiles to the right. If Pac-Man is moving down, Pinky&#39;s target is four tiles below. Finally, if Pac-Man is moving up, Pinky&#39;s target tile will be four tiles up and four tiles to the left. This interesting outcome is due to a subtle error in the logic code that calculates Pinky&#39;s offset from Pac-Man. This piece of code works properly for the other three cases but, when Pac-Man is moving upwards, triggers an overflow bug that mistakenly includes a left offset equal in distance to the expected up offset (we will see this same issue in Inky&#39;s logic later). Don Hodges&#39; website has an excellent article giving a thorough, code-level analysis of this bug, including the source code and a proposed fix-click here to go there now. Pinky is the easiest ghost to exert control over thanks to his targeting scheme. By changing direction, you can dictate where Pinky will turn next when he is nearby (see above picture). If you are facing off closely with Pinky, he will turn before he reaches you if he can. This happens due to the fact Pac-Man has come close enough to Pinky for Pinky&#39;s target tile to now be behind him. In the picture above, Pinky chooses to turn up at the intersection because moving left would have taken him further away from his target tile. The longest-lived example of this is the technique known as &#34;head faking&#34;. This is where the player shakes the joystick to cause Pac-Man to rapidly change direction back and forth, hopefully causing a ghost to change course in the process. As it turns out, the shaking is not necessary-one well-timed, quick reversal of direction towards Pinky just before he decides what to do at an upcoming intersection is all that is needed to get him off your tail. Inky: The light-blue ghost is nicknamed &#34;Inky&#34; and his character is described as one who is bashful. In Japan, he is portrayed as kimagure, meaning &#34;a fickle, moody, or uneven temper&#34;. Perhaps not surprisingly, Inky is the least predictable of the ghosts. Sometimes he chases Pac-Man aggressively like Blinky; other times he jumps ahead of Pac-Man as Pinky would. He might even wander off like Clyde on occasion! In fact, Inky may be the most dangerous ghost of all due to his erratic behavior. Bashful is not a very good translation of kimagure, and misleads the player to assume Inky will shy away from Pac-Man when he gets close which is not always the case. Inky uses the most complex targeting scheme of the four ghosts in chase mode. He needs Pac-Man&#39;s current tile/orientation and Blinky&#39;s current tile to calculate his final target. To envision Inky&#39;s target, imagine an intermediate offset two tiles away from Pac-Man&#39;s tile in the direction Pac-Man is moving (shown as the dashed, green tile in the picture above), then draw a line from Blinky&#39;s tile to that offset. Now double the line length by extending the line out just as far again, and you will have Inky&#39;s target tile as shown above. For the same reasons already discussed in Pinky&#39;s case, Inky&#39;s offset calculation from Pac-Man is two tiles up and two tiles left when Pac-Man is moving up (shown above). The other three orientations have the expected offset of two tiles in the direction Pac-Man is moving. Inky&#39;s targeting logic will keep him away from Pac-Man when Blinky is far away from Pac-Man, but as Blinky draws closer, so will Inky&#39;s target tile. This explains why Inky&#39;s behavior seems more variable as Pac-Man moves away from Blinky. Like Pinky, Inky&#39;s course can often be altered by Pac-Man changing direction or &#34;head-faking&#34;. How much or how little effect this will have on Inky&#39;s decisions is directly related to where Blinky is at the time. Clyde: The orange ghost is nicknamed &#34;Clyde&#34; and is characterized as one who is pokey. In Japan, his character is described as otoboke, meaning &#34;pretending ignorance&#34;, and his nickname is &#34;Guzuta&#34;, meaning &#34;one who lags behind&#34;. In reality, Clyde moves at the same speed as Inky and Pinky so his character description is a bit misleading. Clyde is the last ghost to leave the pen and tends to separate himself from the other ghosts by shying away from Pac-Man and doing his own thing when he isn&#39;t patrolling his corner of the maze. Although not nearly as dangerous as the other three ghosts, his behavior can seem unpredictable at times and should still be considered a threat. In chase mode, Clyde&#39;s target differs based on his proximity to Pac-Man. When more than eight tiles away, he uses Pac-Man&#39;s tile as his target (shown as the yellow target above). If Clyde is closer than eight tiles away, he switches to his scatter mode target instead, and starts heading for his corner until he is far enough away to target Pac-Man again. In the picture above, Clyde is stuck in an endless loop thanks to his targeting scheme. Outside of the dashed area, Clyde acts exactly as Blinky would, heading straight for Pac-Man, but upon entering the dashed area, Clyde will change his mind and head for his scatter target instead. Leaving the eight tile perimeter surrounding Pac-Man causes his target to change back to Pac-Man&#39;s tile and results in Clyde circling the island indefinitely until Pac-Man moves elsewhere or a mode change occurs. Clyde&#39;s targeting method results in him not being particularly dangerous unless you get in his way as he runs back to his corner or before he can reach an intersection to turn away. Extra care should be taken when Pac-Man is in Clyde&#39;s home corner as Clyde is less likely to get out of the way. Chapter 5: Deciphering The Split Screen &#34;This is the way the world ends This is the way the world ends This is the way the world ends Not with a bang but with a whimper.&#34; —T. S. Eliot On The Edge Of Forever Pac-Man was always meant to be a game with no ending. The developers at Namco mistakenly assumed the game&#39;s increasing difficulty was sufficient to prevent anyone from playing indefinitely. Of course, within a few years of Pac-Man&#39;s release, players had discovered that every level beyond the 21st was identical. Patterns were quickly created to exploit this fact and, for any player able to get past the first 20 levels, the game now became a test of endurance to see how many points you could rack up before losing focus and making a mistake. High scores soared into the millions and most players agreed the game simply went on forever. Eventually, a few highly-skilled players were able to complete 255 consecutive levels of play (scoring over three million points and taking several hours to accomplish) and found a surprise waiting for them on level 256. It was a surprise no one knew about-not even the developers at Namco. The 256th level displays the left half of the maze correctly, but the right half is a jumbled mess of randomly colored letters, numbers, and symbols. Notice the bonus counter in the lower-right of the screen is also malfunctioning. The left side of the maze plays normally, but the right side is a different story. Although both the player and the ghosts can navigate through the right half of the screen, the original maze walls no longer apply. Instead, Pac-Man must be guided through a confusing series of open areas, tunnels, one-way intersections, lone walls, and pass-throughs-all invisible to the player-while four ghosts are in hot pursuit. Why does this broken level happen in the first place? The culprit is the routine responsible for drawing the bonus symbols along the bottom edge of the screen. Here&#39;s what happens: when level 256 is reached, the internal level counter is incremented to 255 (the level counter starts at zero - not one) and the routine for drawing the bonus symbols is called. The routine loads the current level counter value (255) into a CPU register and increments that register by one. Unfortunately, 255 is the largest number that can fit in a single byte which is the size of the Z-80 CPU registers, so when the value is incremented the overflow is discarded leaving a zero in the register instead of the expected value of 256. This zero value leads the routine to believe this is an early level since its value is less than seven. The routine starts drawing bonus symbols using the confused register as a counter. At the end of every drawing loop, the register is decreased by one and then checked to see if it is zero (the signal for the routine to stop drawing symbols). Since the register already has a zero in it to start, the first decrement will roll the value back to 255. It will keep decrementing the register and drawing symbols until the register is reduced to zero again, causing the loop to run a total of 256 times. This means that memory locations outside the bounds of the bonus symbol table are drawn to the screen at increasing locations in video memory. This half-broken level was named the &#34;split screen&#34; by players; developers refer to it as a &#34;kill screen&#34;. Playing The Level There are 114 dots on the left half of the screen, nine dots on the right, and one bonus key, totaling 6,310 points. When all of the dots have been cleared, nothing happens. The game does not consider a level to be completed until 244 dots have been eaten, so there is nothing left to do but sacrifice Pac-Man to a hungry ghost. Interestingly, every time a life is lost, the nine dots on the right half of the screen get reset and can be eaten again, resulting in an additional 90 points per extra man. In the best-case scenario (five extra men), 6,760 points is the maximum score possible, but only 168 dots can be harvested-not enough to change levels-so we are stuck. There are no more dots to gobble or energizers to eat. There is no final victory waiting for Pac-Man, only an empty half-maze full of ghosts. The game has an ending after all-just not a very happy or exciting one. Four of the nine dots on the right half of the screen are invisible, but can be heard when eaten. The picture on the left shows all nine dot locations. Dots 1, 5, 6, and 9 are invisible; the rest can be seen but some are a different color than normal. Anyone reaching this level quickly realized: to safely map out the right side of the screen something had to be done about the ghosts. After much tinkering, it was discovered that a ghost would get &#34;trapped&#34; on the right edge of the screen if he got too close to it. Once trapped, a ghost can only move up or down but never right or left again. By leading ghosts near the edge of the screen, a skilled player could eventually get the ghosts out of the way and concentrate on exploring the right half of the maze and collecting the dots. There are many methods for trapping the ghosts. One of the easiest ways to trap the three important ghosts is shown in the picture to the right. The yellow line shows Pac-Man&#39;s path from the start of the level to a spot near the bottom-right. The exact instructions are as follows: begin by going right until you reach a blue letter &#39;N&#39;, then go down. Keep going down until you reach a blue letter &#39;F&#39;, then go right. Keep going right until you reach a yellow &#39;B&#39;, then go down again. When executed properly, Pac-Man will hit an invisible wall almost immediately after the last turn is made. Now we wait. The red ghost will get stuck first. The pink ghost follows a few seconds later. The blue ghost will continue to move freely for several moments until the next scatter mode occurs. At that point, it will try to reach some location near the right edge of the screen and get stuck with the pink and red ghost instead. Now the orange ghost is the only one still on the loose (bottom-right). Clyde is no real threat, however, since he runs to his corner whenever Pac-Man gets close (see Chapter 4), making it relatively easy to clean up all the dots. Be sure to take care around the lower-left corner of the maze-the orange ghost will have nowhere left to run to and will be much more aggressive. Click on the YouTube video below to watch this ghost-trapping method in action: Believe It Or Not Some versions of the Pac-Man ROMs have a &#34;rack test&#34; feature, allowing the cabinet owner to skip ahead to the next level of play whenever they want. To date, the only known way to legitimately get past level 256 is by using the rack test switch inside these machines. The result is that the game loops back around to the first board, but with the score intact and the ghosts still behaving as though it were level 21 or above. Many of the Pac-Man ROMs available for use with the MAME emulator also have this rack test feature, making it relatively easy for those without an arcade version handy to quickly get to the split-screen and beyond. For decades, Pac-Man enthusiasts worldwide have heard the whispers about a &#34;secret trick&#34; allowing a player to get past level 256 and continue playing without using the aforementioned rack test. Several players have boasted having acquired this holy grail of Pac-Man knowledge over the years, but no one has been able to make good on their claims by actually proving it. This topic became so hotly debated in the upper echelons of the arcade gaming community that Billy Mitchell-who was convinced it was impossible-offered a $100,000 cash prize to the first player to prove they could legitimately get past level 256, leaving the challenge open for a full year. The prize money went unclaimed. In spite of the evidence against there being a way to get past level 256, rumors still persist and can occasionally be found in classic gaming forums online, yet no one has been able to back up their words with indisputable proof. It&#39;s hard to imagine why anyone who could legitimately get past the level did not collect Mr. Mitchell&#39;s prize money to be sure. Still the occasional whispers can be heard. Perhaps it is simply natural for people to want to believe in the possibility as opposed to not-like Santa Claus or the Easter Bunny. Then again, maybe there is some middle-aged Pac-Man junkie out there who is withholding secrets to a 30 year-old amusement device for his or her own unfathomable reasons. Stranger things have happened. You be the judge. References and Further Reading Splitting Apart The Split Screen, Don Hodges Pac-Man&#39;s Ghost Behavior Analyzed And Fixed, Don Hodges Pac-Man Source Code With Comments, Chris Lomont Pac-Man Emulation Guide, Chris Lomont The Virtual Pac-Man Museum, Chuck and Vicki Gill Pacman: The Phenomenon - Part 1, Marty Goldberg The History of Pac-Man, Doug Trueman Pac-Man Entry At Arcade History, Alexis Bousiges Pac-Man ghost behavior revealed, Twin Galaxies forum topic Pac-Man ghost AI question, AtariAge forum topic Reawakening The Sleeping Giant: The Pac-Man CE Interview, Christian Nutt Appendix A: Reference Tables (click image for full size) Table A.2 - Difficulty Specifications There is a small spot on the Pac-Man PCB where you can solder two pads together to set the game to &#34;hard&#34; difficulty. The only difference in hard difficulty is that five of the levels (1, 3, 6, 19, and 20) are eliminated from play. The system does not eliminate any of the bonus symbols, however, causing much confusion as to what level you&#39;re really playing. Level two is the first board in hard difficulty for example, but the cherry symbol is used instead of the strawberry. It&#39;s still level two in terms of gameplay, but with cherry symbols in place of the usual strawberries. Also, the bonus point values are changed to match the current symbol being used. You can determine the difficulty setting of a machine by observing which ghost kills Pac-Man during the attract mode demo game. In normal difficulty, Pac-Man gets captured by Inky in the lower-left area of the maze. If the difficulty jumper has been connected, however, he is captured by Clyde near the same location. Normal Normal Bonus Hard Hard Bonus 1 Cherries - - 2 Strawberry 2 Cherries 3 Peach 1 - - 4 Peach 2 4 Strawberry 5 Apple 1 5 Peach 1 6 Apple 2 - - 7 Grapes 1 7 Peach 2 8 Grapes 2 8 Apple 1 9 Galaxian 1 9 Apple 2 10 Galaxian 2 10 Grapes 1 11 Bell 1 11 Grapes 2 12 Bell 2 12 Galaxian 1 13 Key 1 13 Galaxian 2 14 Key 2 14 Bell 1 15 Key 3 15 Bell 2 16 Key 4 16 Key 1 17 Key 5 17 Key 2 18 Key 6 18 Key 3 19 Key 7 - - 20 Key 8 - - 21+ Key 9 21+ Key 4+ APPENDIX B: Easter Eggs &amp; Tricks This section is meant to contain not only the easter egg below, but also tricks-interesting ways players have found to create unexpected behavior in Pac-Man. I have seen a few of these curious abuses of a Pac-Man machine before, but I don&#39;t know how to recreate them. As such, I am asking for any help the readers of this guide can provide towards expanding this section. Please send any Pac-Man tricks you may know me for inclusion in the guide; full credit will be given for your altruism and ingenuity. NAMCO Easter Egg There is a secret message hidden in Pac-Man by the developers at Namco. To see it, put the machine into service mode and wait for the settings screen to appear. Now quickly toggle service mode off and on (an alignment grid will appear on the screen). While holding down the player 1 and player 2 buttons, toggle service mode off and on again very quickly (if done properly, the grid will stay on the screen), and then push the joystick in the following directions: UP x 4, LEFT x 4, RIGHT x 4, DOWN x 4. The message &#34;MADE BY NAMCO&#34; will appear sideways on the screen, spelled out using energizers. APPENDIX C: Hardware Information Specifications: Platform - NAMCO 8-bit PCB CPU - Z80A at 3.072 MHz ROM - 16K in four, 4K chips RAM - Almost 2K Display - Raster Orientation - Vertical Resolution - 224x288 Colors - 16 Attributes - Eight 16x16 hardware sprites Refresh rate - 60.61 Hz Sound - Custom monophonic 3-voice waveform sound generator chip Controls - One 4-way leaf joystick, 1P/2P buttons Models - Upright, Mini, and Cocktail Midway Operator&#39;s Reference Books: Pac-Man Operator&#39;s Manual Troubleshooting Logic Board Part I for Pac-Man and Ms. Pac-Man APPENDIX D: Vintage Guides Break A Million At Pac-Man - Ernest Zavisca, Ph.D. and Gary Beltowski. How To Win At Pac-Man - Penguin Books. Glossary apple: The bonus symbol for levels five and six. Worth 700 points. bell: The bonus symbol for levels eleven and twelve. Worth 3,000 points. Blinky: The red ghost. Also known as “Akabei” or “Macky” in Puck-Man. bonus symbol: The often fruit-related symbol that appears twice per level below the ghost house and can be eaten for additional scoring. The point-value depends on the specific symbol and can range anywhere from 100 to 5,000 points each. Also known as fruit. cherries: The bonus symbol for the first round of play. Worth 100 points. Clyde: The orange ghost. Also known as “Guzuta” or “Mocky” in Puck-Man. cornering: The technique of moving the joystick in the direction one wishes to go well before reaching the center of a turn, ensuring Pac-Man will take the turn as quickly as possible. Cruise Elroy: When a certain number of dots are all that remain in a level, Blinky (red ghost) will change “gears”, speeding up as well as chasing Pac-Man even in scatter mode. He speeds up yet again when half the dots remain from the first change. dots: The 244 objects in the maze Pac-Man must eat to move on to the next round. There are 240 small dots worth 10 points each, and 4 energizer dots worth 50 points each. Also known as pills. energizer: One of four, large, flashing dots located near the corners of the maze worth 50 points each. When Pac-Man eats an energizer, the ghosts simultaneously reverse direction and, on earlier levels, turn dark blue. Pac-Man can then eat the blue ghosts for additional points, scoring more for each consecutive ghost eaten from one energizer: 200, 400, 800, and 1,600 points respectively. Also known as a power pills, fuel tanks, and vitamins. flipping: “Flipping the machine” refers to when a player earns one million points. The game is unable to display a score larger than 999,999, so the score readout “flips” over to zero and keeps counting. Also known as rolling the machine. fruit: See bonus symbol. galaxian: The bonus symbol for levels nine and ten. Also known as a tulip or a thunderbird. Worth 2,000 points. ghost house: The rectangular area in the middle of the maze where the ghosts start each new level and new life, returning to the house whenever they are captured by Pac-Man. Also known as the monster pen. ghosts: Pac-Man&#39;s four enemies in the maze are typically referred to as ghosts or monsters. grapes: The bonus symbol for levels seven and eight. Also known as a grenade. Worth 1,000 points. grenade: See grapes. head faking: Changing Pac-Man&#39;s direction back and forth in quick succession in an attempt to affect the turning logic of one or more ghosts in play. Blinky and Clyde do not use Pac-Man&#39;s current direction in their chase logic, so they are unaffected by head faking. Inky: The blue ghost. Also known as “Aosuke” or “Mucky” in Puck-Man. intersection: Anywhere pathways in the maze intersect, yielding more than one option on which way to proceed. key: The bonus symbol for levels 13 and above. Worth 5,000 points. monster pen: See ghost house. monsters: See ghosts. orange: See peach. pattern: A memorized series of turns associated with a particular level or levels that, when repeated, clears the maze of dots without getting Pac-Man captured by any of the ghosts. Also known as a routine. peach: The bonus symbol for levels three and four. Also known as an orange. Worth 500 points. pills: See dots. Pinky: The pink ghost. Also known as “Micky” in Puck-Man. power pill: See energizer. routine: See pattern. side tunnel: The connecting tunnel between the right and left edges of the screen. Entering this tunnel will “wrap” the player around to the other side of the screen. The monsters always suffer a speed penalty while in the tunnel while Pac-Man does not. Also known as The Tube, The Time Warp, and The Scoot. split screen: The 256th level of the game, where the right half of the screen is filled with garbage instead of the usual maze. strawberry: The bonus symbol for level two. Worth 300 points. thunderbird: See galaxian. tulip: See galaxian. Frequently Asked Questions Q: How do I get past the split screen? A: The only known way to get past the split screen is via the “rack test” feature available on some Pac-Man ROMs (see Chapter 5). Q: What is a “perfect score”? A: A perfect score in Pac-Man is 3,333,360 points, which can only be attained by playing a perfect game. This requires catching all four ghosts at every energizer, gobbling down every bonus fruit, and never once losing a life for 256 consectutive levels of play. All extra lives are needed once the split screen is reached to eat the nine dots hidden on the right side of the screen the maximum number of times—they respawn every time a life is lost. This was first achieved by Billy Mitchell of Hollywood, Florida in 1999. Q: Is it true that some of the ghost A.I. routines examine the joystick directly to make decisions? A: This is false. The memory-mapped IN0 joystick port is completely removed from the pathfinding and logic routines in the code. Q: What other games from the Pac-Man family will this ghost logic work with? A: Pac-Man Plus and Ms. Pac-Man both use the same basic pathfinding/targeting logic as the original Pac-Man. Many popular bootleg ROMs like the Atlantic City Chip and Hanglyman also use this logic. Q: Why are all of the bonus symbols food-related except for the galaxian, the bell, and the key? A: The galaxian was added as a nod to the Namco space-shooter title, Galaxian, which was under development at the same time as Pac-Man. No one knows why Toru Iwatani chose a bell and a key for the final two bonus symbols. It has been theorized the bell may actually be some sort of food like an Asian cashew or even a blancmange dessert, which would make it consistent with the food theme. The bell at least has the possibility of being something else—no one has any theories on the key being anything but ... well ... a key. Q: Why are some members of the NAMCO development team listed by name but not others in Chapter 1? A: I have not been able to find out these persons&#39; names—I&#39;m sure they are documented somewhere but I have yet to find that information. If you know of any reliable sources for the names of the full NAMCO development team, please let me know ([email protected]). [NOTE: The latest version of the Pac-Man Dossier is available at Jamey Pittman&#39;s website, and he can be contacted at [email protected] with questions, comments, and updates. Gamasutra will be working with Jamey on new dossiers for other games to be published in the future.] Return to the full version of this article Copyright © UBM Tech, All rights reserved </description>
      <pubDate>14 Feb 21 16:12 EST</pubDate>
      <guid>https://www.gamasutra.com/view/feature/3938/the_pacman_dossier.php?print=1</guid>
    </item>
    <item>
      <title></title>
      <link>https://filipnikolovski.com/posts/thoughts-on-microservices/</link>
      <description>&lt;a href=&#34;https://filipnikolovski.com/posts/thoughts-on-microservices/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I know that the topic of microservices has been discussed over and over again, just wanted to add my two cents to the pile, based on my experience with this approach of designing web apps:A lot of people believe that the microservices architecture solve software problems that are of scaling and performance nature. But the most important problem that they solve, is an organizational one.Conway’s law is always in play. When you think about how the software that you build should look like, you need to think how your organizational structure should look like. They always go hand in hand.If you are a single team, then a design involving multiple moving pieces does not make a lot of sense from that perspective. Who should take ownership over each component? How can the services be truly independent from one another? It is inevitable to entangle them just because it’s more convenient that way and end up with something that resembles a microservice architecture, but in reality it is more of a “distributed monolith”. Starting with a microservices architecture is the wrong play that a lot of people seem to make. The structure of the software always ends up looking like the structure of the organization. It is inevitable.Never start with a microservice architecture if you have a single team.As the organization scales though, and more people are added to the team, then it is the right time to raise the question for the current design of the architecture.As the team grows, it will become more and more difficult to manage it. Breaking this big team apart into multiple, smaller, independent teams is a natural step going forward. Then the question that we should ask is: how can those teams take full ownership over the parts of the product that they are responsible for? How can we make the teams take control over their own “destinies”?For a team to be truly independent, it needs full decision making power in every layer of the stack: from the UI/UX, the APIs that the backend is going to expose, all the way down to the infrastructure that willl power the whole thing. Doing this as a single monolithic application is certainly feasible, but then the teams will need to be synchronized in their development process, especially in the deployment phase. They will step on each other’s toes constantly. Thus, a need will arise to create an architecture that will mirror the organizational one. Microservices solve this exact problem - scaling the teams.The services need to be composable, and play well with each other, just as you would create composable modules in a monolith. Breaking it apart and simply sticking a web server in front of it won’t save you.For features that span multiple domains, a clear ownership of the data, as well as clear and consistent APIs are a must, otherwise you risk complicating the relationship between the services that are involved. Defining these boundaries is the responsibility of the teams that will develop this feature. The communication between the services should reflect the communication of the teams.</description>
      <pubDate>01 Dec 21 11:19 EST</pubDate>
      <guid>https://filipnikolovski.com/posts/thoughts-on-microservices/</guid>
    </item>
    <item>
      <title></title>
      <link>https://santurcesoftware.com/making-web-sites-small</link>
      <description>&lt;a href=&#34;https://santurcesoftware.com/making-web-sites-small&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Sitting on my development workstation with fiber internet it&#39;s easy to forget that there is a large number of users that are browsing the same websites as I am, however they are using an underpowered mobile device over a 3G network. Navigating to a local legal firm&#39;s website which displays basic information and marketing material, opening up a network tab shows over 19MB of resources being pulled down to render. It&#39;s eye-opening to fire up the dev tools and with the network tab use throttle to emulate a 3G connection and watch the dozens of seconds pass before a website in 2021 can render some contact information for a straightforward service near me.I encourage you to type in &#34;Dentist near me&#34;, &#34;Lawyer near me&#34;, &#34;Pizza near me&#34;, &#34;CPA near me&#34;, etc, into google for local services, and as you click into a few - watch the network tab as it pulls in enough data to have my unlimited mobile plan charge overage fees. While it&#39;s fairly common to see extreme examples in most modern social media sites where advertising and user interaction is the focus, where you can open up your network tab and watch as your network goes until infinity or you finally exit out (or quit social media). It&#39;s a bigger tragedy that local services and company sites are settling for a solution to displaying services offered, contact information, and marketing material that requires potential customers to be loading a dozen or more megabytes of resources.Most places will probably not have to go any further than optimizing their images to shrink a site by an order of magnitude.There&#39;s a million ways to skin the cat of image resizing, whether you&#39;re using photoshop, gimp or a command line utility. We like to use imagemagick when ever possible.Now that we have images that are the correct size, or at least closer to the target display size, we can address the type of image we&#39;re using. Except for animated images, we use webp (`cwebp`) to accomplish a reduction in the size of our images. sudo apt-get install webp cd /mywebapp/static/img/ mkdir webp for file in *.{jpg,png,jpeg}; do cwebp -q 60 ${file} -o webp/${file}.webp; done; We&#39;ll need to make sure our img tags are updated to point to /webp/filename.jpg.webp now, but depending on how aggregious our file type was we have saved ourselves a ton of bandwidth. A local dentist office near me renders a 900x1000 PNG file that is originally 1500x1750 and weighing in at 3MB! The webp version of the same size is ~79KB. Resizing our image to 1000x1152 shrinks our PNG down to 1.5MB, and finally running cwebp to convert it to a webp we&#39;re down to 49KB.The last bit in your control is to ask yourself if your design needs a 1000px image or an image at all. The answer to the latter part is often yes, but the size has an impact on the weight of your website and is often no. You don&#39;t need to see a 3MB-&gt;50KB reduction for this to make a difference, shaving off a few hundred KB per image when you have a handful of them to load can amount to a meaningful impact to your users in aggregate.CSS, Fonts and Javascript ImportsThere are a lot of hard realities of customer requirements that may get in the way of knocking down the number of dependencies and resources you find yourself loading, but there are a few opportunities to reduce the bandwidth it brings. The examples below are commonly found issues, but some may apply to your use cases more than others.Do you really need 150KB of Font resources to display two 16x16 icons?Do we need 500KB of javascript to display a static view of our business on google maps?Is everything minified?Do you like a metric or do you need a metric?Are we using gzip?Do we really need more than a MB of CSS to render a web site (Looking at you wordpress)?There are a lot of times when using things like `material icons extended` is used by habit when an application requires one or two icons, loading a 150KB font file to display 1KB of images can be avoided with simply supplying the icon as a static image.If you have your company located on a static google map frame, you&#39;re loading a javascript file that is probably a few times larger than all of your sites pertinent content and information in order to avoid creating an image of this with a clickable link to jump your user out to google maps or Open Street Map. So create a small (webp) of your desired map and shave off a few hundred KB and some processing power that can be better spent elsewhere.Minify your files! Use Minified files! Gzip your bits over the wire! All of these things should be able to be done quickly and if you find yourself relying on large 3rd party files that aren&#39;t minified, consider retrieving them to be minified and served from your server.If you are using gtag for google analytics, facebook signal, or whatever else front end tracking mechanism you&#39;re using you are making me download hundreds of KB of javascript for something that I don&#39;t want and quite possibly you actually don&#39;t either. What are you tracking? Do you want to see page views? Do you care how many times you showed up in google or bing search results this week? Are you tracking how long a user is on your page before going to the shopping cart? What facebook demographic is the person visiting my local dentist&#39;s office? Which pages are users leaving our site without a completed transaction? According to facebook, what is my target audience for a local pizza place? There are probably a million more metrics that we can ask about, but for the most common that small businesses and local services want to know, how many times am I coming up on search results, how many people are visiting the page, and which promotions are directing traffic to their site.Google Search Console will provide insights into performance on google search without any front end dependencies. Bing web master Provides similar functionality and pretty performance charts. Both of their UIs are intuitive enough for non technical users to access and make sense of.Server side analytics are usually more than enough to get a fairly accurate look into URLs visited to find out overall page views and which atrocious utm_* codes were visited the most. The technology stack you have is going to greatly impact the implementation details for server side analytics, Go Access provides a rich dashboard for server side analytics, page views and query values or look into Prometheus and Grafana.There are also bad habits out there of including all of jquery in place of using a few document.getElementById or document.querySelector calls, grabbing all of bootstrap in order to get row and column css classes instead of just grabbing bootstrap-grid.min.css or just what they actually need or using wordpress. Use the Coverage Tool to find javascript we&#39;re making clients download that we aren&#39;t actually utilizing.Chrome dev-tools now have a CSS Overview which you can use to find unused declarations that you can simply remove from your page.Less IS MoreFor lots of people, we learned this from navigating to a myspace page with an mp3, two videos, and a dozen flashing gifs that took as long to load as it did for it to become responsive enough to figure out how the hell to turn them off. This phenomenon still applies today when you navigate to a business page and find yourself greeted with a 75vh image carousel, social media links and banners, a subscribe card, a &#34;Chat Now!&#34; banner with a GPT bot typing, xyz.com wants to know you location, or a prompt for cookies just to scroll, click and grumble through to read the services, hours, contact information, pricing and location.Having a site that conveys the services and information instantly on any device under poor network conditions allows more of your message to be seen by users and potential customers quicker. There is an endless number of clever things you can do to decrease the size of your page loads, but making a direct and lightweight initial design will always yield the best results.Uncluttering your resources and your design also has a noticeable effect on accessibility. When pertinent information on the site is more distilled it becomes much easier to lay out the idea and information using properly formed and organized html tags to attain a perfect accessibility score using devtools Light house and folks with disabilities that benefit from this use and need local service and businesses like anyone else.Hopefully your next project shows up on The 1MB ClubHappy Hacking!</description>
      <pubDate>01 Dec 21 17:02 EST</pubDate>
      <guid>https://santurcesoftware.com/making-web-sites-small</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.jessesquires.com/blog/2021/12/01/my-worst-tech-interview-experience/</link>
      <description>&lt;a href=&#34;https://www.jessesquires.com/blog/2021/12/01/my-worst-tech-interview-experience/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Everyone in tech seems to have a “terrible tech interview” story. The topic quietly orbits in the ether of our industry and periodically bursts through the atmosphere in the form of a tweet or blog post that goes viral. Despite universal loathing of our industry’s impetuous and heedless interview processes, few seem willing to improve the current standard. A recent tweet in my timeline reminded me of a story that I’ve never told. It was sometime around 2015. I was interviewing at various places, and I got an interview with the Apple Watch team. After a full morning, I was on (maybe?) my 4th interview. The interviewer asked me to write merge sort in C on a whiteboard. Of course, I was familiar with the algorithm and could explain how it works conceptually. But accurately reproducing it on a whiteboard in a programming language that I didn’t use daily was, as you might expect, intimidating. Not to mention, I was generally nervous and anxious, because interviews. After only a few lines, the interviewer called out from behind me while comfortably sitting in his chair to make a joke that “you must must be writing a lot of Swift” because I had accidentally omitted a few semicolons. That comment threw me off for the rest of our time, not only because I was in the middle of a ridiculous whiteboard exercise that would demonstrate nothing, but because it also pissed me off and I had to just be cool about it. At the time, Swift was only a couple of years old and I was not writing it at my current job. It was the early days when Swift was rough. My team was avoiding it. I only experimented with Swift for fun at home. I was writing Objective-C all day, every day — which does have semicolons. But if your whiteboard code doesn’t compile, you must be an idiot. As you’ve likely concluded, the rest of the hour or so in that room did not turn out well. * * * However, there’s a special irony to this story. This interview took place only a year or two after my Apples To Apples blog post series (Part 1, Part 2, Part 3) in which I implemented merge sort along with various other sorting algorithms in Swift, Objective-C, and C, and then compared the results. (Swift used to be really slow.) But I didn’t write any of those sorting algorithms from memory. I referenced the infinite number of resources on the Internet to write them and ensure they were correct. I certainly didn’t internalize any of them. That wasn’t the point, nor is that a good use of time — even now. Those posts were a quick exercise for fun and experimentation, and literally no one has ever written merge sort in production. Fast forward a few years and I ended up taking a job at Facebook (big mistake, but that’s another post). Surprisingly, a few members of the early Swift team ended up leaving Apple and joining Facebook to work on HHVM or something similar, not sure. Anyway, one of those people was Nadav Rotem, who had worked on the compiler optimizations team. We met for a coffee break after he sent me a message saying he recognized my name from the Apples To Apples blog posts, which had been fireballed and linked in iOS Dev Weekly. I also remember getting a message from Chris Lattner thanking me for writing the posts at the time. I learned that those posts had been circulated internally — all the way up to Craig Federighi, in fact. Apple was eager to showcase Swift’s success in those early days. Nadav’s team ended up using the code I wrote to improve optimizations in the compiler. * * * Given that, I suppose it’s pretty funny that I failed an interview dealing with academic sorting algorithms that no one will ever use in their day-to-day work. But how was I supposed to know that the underpinnings of watchOS were built on top of merge sort? It was very early in my career and I was nervous, I somewhat panicked in the moment, and I had to deal with an asshole making jokes. Also, writing code on a whiteboard is fucking stupid — so there’s that. Even though I did not get an offer, my recruiter was eager to get me interviews with other teams. I’ve heard all teams are very different within Apple. However, I declined and proceeded with interviews at other companies instead. Another strange thing about this experience is that over the years, I’ve watched some of the people from my interview loop give talks at WWDC. * * * Since that experience, I’ve been on both sides of the interview process dozens and dozens of times at various companies. I’ve learned that a great interviewer knows how to ask great questions that give her a lot of signal about the candidate’s skills while ensuring the candidate always feels comfortable and confident. That’s how I interview folks. Trick questions and esoteric puzzles give the interviewer no useful information about the interviewee — her skills, her strengths, her weaknesses, her potential for growth. Those are the things a great interviewer should be seeking out. That means you give hints when the candidate gets stuck and you don’t interrupt them with asinine comments. Whatever problem you give a candidate to solve, your goal as an interviewer should be to get them through as much of that problem as possible, to get as much signal as possible. The added benefit is that the further you get, the better and more confident the candidate feels, which also helps prepare them to do their best for the next session. Less experienced interviewees often need more help, and that’s ok. The goal should never be to trick the interviewee — that’s a waste of everyone’s time. And my interview experience with that team was just that — a complete waste of my time and theirs. </description>
      <pubDate>01 Dec 21 18:54 EST</pubDate>
      <guid>https://www.jessesquires.com/blog/2021/12/01/my-worst-tech-interview-experience/</guid>
    </item>
    <item>
      <title></title>
      <link>https://craigwritescode.medium.com/user-engagement-is-code-for-addiction-a2f50d36d7ac</link>
      <description>&lt;a href=&#34;https://craigwritescode.medium.com/user-engagement-is-code-for-addiction-a2f50d36d7ac&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Error410This account is under investigation or was found in violation of the Medium Rules. There are thousands of stories to read on Medium. Visit our homepage to find one that’s right for you.Take me to Medium</description>
      <pubDate>16 Feb 21 09:40 EST</pubDate>
      <guid>https://craigwritescode.medium.com/user-engagement-is-code-for-addiction-a2f50d36d7ac</guid>
    </item>
    <item>
      <title>Good design for a real problem.</title>
      <link>https://fromthearchives.blogspot.com/2006/09/good-design-for-real-problem_28.html</link>
      <description>&lt;a href=&#34;https://fromthearchives.blogspot.com/2006/09/good-design-for-real-problem_28.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Good design for a real problem. Flood fighting class was great. There was a disappointing lack of jargon, but a ton of technique. You know, it was all technique that would have occurred to me after my third wave protection plastic tarp washed away, like facing the seams and folds downstream instead of into the current. But honestly, it would have taken me the first hundred sandbags to learn to throw the first shovelful on the lip of the bag to anchor it before throwing the next three shovelfuls in, from a safe distance from your partner’s hand. There is so much technique to moving in the physical world. When you tie off a sandbag, would you have thought to hold it by one corner and spin it until the cloth doubles over and becomes easy to tie? Again, after doing it the hard way for a hundred sandbags, I would have immediately recognized the better way when I saw it. But I don’t know when I would have thought of it.Some interesting things from the class:Incompetent and bungling as we were, sandbagging goes shockingly fast. It must be astonishing to watch a crew with a few days’ practice; like fast motion film, but not. If water is channeling through a levee, it will come out the other side in a boil. A boil running clear is not a problem. Levees can seep or even tunnel like that for decades. If the boil is carrying material, which you might see as a little volcano building around the boil, it is scouring the inside of the levee. That’ll grow until the levee collapses.So how do you stop a dirty boil? Again, I would never have thought of this; I would have filled a sandbag and plugged the boil. But no, you never stop a running boil; the water inside the levee will find a new exit. Instead, you put a ring of sandbags around it and let the water fill that. Increase the height of your sandbag ring to decrease the difference in head between waters on the two sides of the levee. Stop when the water from the boil runs clear but before you stop the running water entirely. Leave a small spillway for water to drain from your sandbag ring.Environmental damage from the gold mining days is still causing trouble. The bed of the Sacramento River was raised several feet for much of its length. Levees built on sands washed down from hydraulic mining are being eaten from the bottom. 140 year old decisions are active problems.A scary thing from the class:The guys who taught the class are old school engineers and maintenance guys. They have bellies and they can drive heavy machinery and they make jokes about the enviros. They are tremendously competent at things that impress me, like driving stakes or tying fast hitches. They are pragmatic and know their landscapes and they are largely unimaginative. I absolutely trust them to run crews in a cold rain in the middle of the night, there to save me and mine. Those guys? They are scared and angry at the new development going up in California. They say things like “That levee at that bend is gonna fail everytime. It’ll never hold, but now there’re six thousand new houses behind it.” “I don’t know what we’re gonna do, ‘cause last time we could break out a spillway there, but now… all the new houses… .” “Where we’re gonna get the machinery to protect all those places at once, I don’t know. But we’re gonna have to, if it isn’t farmland any more.” They are not Democrats and they are not environmentalists. But they hate the sprawl behind levees and they make me scared too. </description>
      <pubDate>17 Feb 21 13:31 EST</pubDate>
      <guid>https://fromthearchives.blogspot.com/2006/09/good-design-for-real-problem_28.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://eighteenthelephant.com/2021/11/29/pushed-around-by-stars/</link>
      <description>&lt;a href=&#34;https://eighteenthelephant.com/2021/11/29/pushed-around-by-stars/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A terrible question to ask in science is unfortunately quite common: Does something affect something else? It seems innocuous enough — after all, one might want to know whether exposure to sunlight affects one’s risk of skin cancer, or whether the location of the moons of Jupiter affects the stock market. The answer, however, is always yes, and not realizing this is part of the plague of null hypothesis significance testing, p-values, and related ills. Everything affects everything else, and the correct questions to ask are how much? and Do I care about the magnitude of the effect? Skeptical? Here’s an example dating back over a century. In 1914, the French mathematician and physicist Émile Borel noted that moving a gram of matter located far from our solar system by just one centimeter will sufficiently perturb the motion of gas molecules on Earth, due to the slightly shifted gravitational field, such that well within a second the molecules’ trajectories will be completely different than they would have been if the distant mass hadn’t moved. If we had perfect knowledge of all the positions and velocities of the gas molecules beforehand and could exactly predict their subsequent molecular motions, the motion of the distant matter would rapidly render our predictions useless. (Quantum mechanics makes this classical statement about predictability even worse.) Do little bits of stuff light years away affect gas molecules on Earth? Yes, and it doesn’t even take astronomical timescales, just fractions of a second. (Technically, it would take years for the gravitational perturbation to reach the Earth, which Borel was unaware of, but that’s irrelevant for the sub-second timescale of our gas being stirred up by the distant mass. Or put another way, the motion of stardust a few years ago suffices to make long-range prediction of molecular trajectories impossible.) I’ll give a non-hypothetical example of astronomical motions detectably tugging on Earth-bound objects later, after tackling Borel. In addition to highlighting the lack of isolation of any real system, Borel’s example points out the necessity and profundity of statistical mechanics. The details of nearly every system we care about, whether gas in a bottle, proteins in a cell, or electrons in a superconductor, are irrelevant and unknowable; it’s the statistical properties that matter. In fact, my undergraduate statistical mechanics course started off noting Borel’s work [1], and I’ve done the same in an undergraduate statistical mechanics course I’ve taught. However, I’ve been annoyed for over a decade by how unclear Borel’s explanation is. Other writers have referred to it, perhaps most notably L. Brillouin in the 1960s [2], but as far as I’ve been able to find, they don’t actually derive Borel’s result. Every few years I think to search again, and I am always unsuccessful. I decided, therefore, to try to sketch a simple version of the calculation myself. You can feel free to skip to the numbers at the end, but in case you drop out completely, dear reader, I’ll include here my now customary brief advertisement of my upcoming pop-science biophysics book. You can read about it here or at the publisher’s site! Reconstructing Borel’s Argument Borel makes his statement in the book Le Hasard (1914). Not surprisingly, it’s in French. Despite watching a lot of old French crime movies, I have not picked up enough French to read Borel’s work, perhaps because the films involve long scenes of people silently digging their way out of prison rather than discussing anything. The miraculous Google Translate comes to my rescue, though, and twice — in 2007 and 2017 — I entered into it the relevant passages from Borel. He is verbose. An excerpt: “Let us try to evaluate, from Newton’s law on universal attraction, the deviation which a molecule would undergo in the interval between two shocks, the displacement of an extremely small mass situated at a great distance… Let us now transport this miniscule sphere beyond the extremities of the visible universe, at a point from which the light takes billions of years to reach us… … the effect of shock disperses very rapidly the bundles of trajectories supposedly infinitely untied, and the problem of the subsequent motion of the molecules becomes, of seconds, very indeterminate, in the sense that a colossally large number of different possibilities are a priori equally probable. … The only form in which the problem can be solved and solved is therefore the statistical form.” (I posted PDFs of Borel’s text and Google’s translation; link below.) As others have also noted [3], Borel doesn’t place the moving mass at a distant star; this appears to be due to Leon Brillouin, decades later. In another book, Introduction géométrique à quelques théories physiques (1914) Borel provides a longer argument that (at least in crude translation) is still not at all transparent, but that involves volumes in phase space and their transformation due to shifting forces — a sensible approach. I only stumbled on this work this year (2021). I’m again awed by the wonders of the internet and modern technology: the book is accessible on the Internet Archive (link) and Google Lens can be used to translate pieces from the images themselves. The relevant parts are in “NOTE II. Statistical Mechanics and Irreversibility,” pages 94-98. Rather than slog through this, I made my own crude estimate of the effects of a distant shifted mass on the predictability of molecular trajectories. I suggest skipping the following paragraphs — the fun part is to see what approach you can think up yourself! How big is the change in trajectories induced by the changed gravitational field? What does it mean for the gas molecules trajectories to be “predictable” or not, and what is a criterion that distinguishes these two cases? See what you come up with… The calculation We imagine the usual sort of gas on Earth — what we might find in a balloon, for example — molecules of mass m, at temperature T and pressure P. The molecules zip around, and we can calculate their mean free path (the average distance between collisions) and the mean time between collisions as usual [4]: where T is temperature, P is pressure, and kB is Boltzmann’s constant. Following Borel and Brillouin, we imagine M = 1 gram of matter far away; let’s say 4 light years (D = 4 x 1016 m), which moves away from us by = 1 cm. This motion changes the magnitude of the gravitational force exerted on a local gas molecule by the distant mass, since its distance has increased. Specifically, the change in the force is Now let’s make some leaps. I’m going to imagine perfect, classical, billiard-ball trajectories, and consider the system to have been rendered “unpredictable” if collisions between molecules that would have occurred do not occur, now that the distant mass has moved. This isn’t the only, or the best, way to think about predictability, but it’s simple and intuitive: if I can’t predict what molecules are going to collide, I can’t make (microscopic) predictions about the system. (Note that macroscopic behavior is still predictable!) Imagine a gas molecule moving perpendicular to the vector pointing to the distant matter. Left alone, it would collide head-on with another molecule, let’s say. The change in force nudges it along that vector, perpendicular to its original path, so that by the time it collides it’s shifted by relative to its original position. How large is , and will it be sufficient to avoid the collision entirely? You might object that this setup of perpendicular trajectories is the “best case” for a large effect. That’s true, but that’s ok — as we’ll see, we can relax this by diminishing the likelihood of an avoided collision by a factor of hundreds or thousands, with no change to the overall argument. Ignoring factors of two, our , by elementary kinematics. Rather than hitting our (spherical” target molecule head-on, we hit at an angle , where r is the radius of the molecule. Our trajectory upon reflection is therefore shifted by from what it would be. Now let’s think about the next collision. Even if we ignore the further gravitational effects of the distant mass, we get cascading effects from our initial shift. By the time it reaches the next molecule it collides with, our original molecule is displaced perpendicular to its original path by . This causes a change in angle , which causes another displacement , and so on. After n collisions, we have . Does this matter? What’s relevant is its magnitude compared to the radius of a molecule: if it’s much larger, the collision that would have happened will have been prevented. Let’s define a dimensionless factor . Plugging in all the above expressions, including those for our mean free path and collision time, If you’re following along, dear reader, now is a great time for dimensional analysis, making sure this is dimensionless. I’ve grouped together some dimensionless factors. Even before plugging in numbers, we can see why our distant matter may perturb us: its effects multiply with every collision, scaling as a power of n. This will rapidly grow. A problem with this argument. [Added later] My very clever colleague Eric Corwin points out a problem with this argument: “But shouldn’t you consider the differential change in force between one side of the sample and the other, instead? If everyone feels the same change in force then it just puts the whole system into a different accelerating frame and doesn’t do much about the internal collisions.” I had been imagining the effect of collisions by considering the “target” particle at rest relative to the moving one, a convenient setup for figuring out things like the mean free path, but Eric is correct that this won’t work here. Calculating the differential change in force across some distance would be fine, and I think everything would work similarly, perhaps extending the timescale of predictability, but I’m not certain. I’ve spent too long on this, so I will leave it as an exercise for the reader! (As far as I can tell, Borel doesn’t deal with this, either.) Update Nov. 29, 2021: I couldn’t stop myself from thinking about the differential force issue. The relevant “” gets an additional L / D factor, where L is the size of the system. The relevant size should be the mean free path, , i.e. we care about a differential force across two colliding particles. The punchline is that all this has a negligible effect on our overall conclusions, as updated below in “numbers.” See the PDF linked immediately below for my sketch of the differential force calculation. Now back to the original argument. Now for some numbers For molecular nitrogen, with a mass of m = 28 a.m.u. and a “kinetic radius” of 0.2 nm, and standard temperature and pressure, we find that by ! This corresponds to a time , which is about 5 nanoseconds, given that the mean time between collisions for our gas molecules is about 1.6 ns. If we revisit our perhaps optimistic geometry and state that only 1 in 1000 collisions are perturbed by the amount we’ve calculated — a factor I’m recklessly making up, as being roughly the ratio of molecular radius to mean free path — our time to reach increases by a factor of 1000, but it still very small: 5 microseconds. In other words, the 1-cm motion of a gram of matter 4 light years away makes prediction of molecular trajectories impossible beyond a timescale of a few microseconds. Update Nov. 29, 2021: If I correctly account for the differential force across , as described above, crosses 1 at about rather than 30, or about 6 ns rather than 5 ns. With our fudge-factor of 1000, it’s still only 6 microseconds until unpredictability. The exponential scaling with n wins over any prefactor! Imagine the effects of displaced materials at a much closer distance — the asteroid belt, or the room next door! Can we detect it? The answer to the question “Does X affect Y” is always, trivially, yes. This calculation is, perhaps, an entertaining example. Moreover, it’s yes with a guaranteed “p &lt; 0.05” if one looks at enough samples. It’s perhaps worth keeping in mind concrete examples like the one above that highlight how essentially every effect size is nonzero, so that it’s clearer that the only meaningful questions are “how big is the effect?” and “how well can I estimate it?” You may think this is example is silly since we’re not trying to calculate molecular trajectories, and in fact the lesson of all sorts of calculations like Borel’s is such a task is hopeless (even with computers far beyond any foreseeable capabilities) — we instead have to be clever, and realize that statistical properties are what matter, and these can be remarkably robust. Nonetheless, it’s worth noting that these days we do observe the effects on Earth of the motion of distant objects. A stunning example is LIGO (the Laser Interferometer Gravitational-Wave Observatory), which uses interferometry to detect the tiny motion hanging mirrors causes by gravitational perturbations from far-off objects. How tiny? LIGO detects shifts in the mirror’s (average) position of about 10-18 meters , from stellar events hundreds of millions of light years away! [5] Watching a molecule be pushed a few nanometers by a few-light-year-distant chunk of rock doesn’t seem so implausible any more. Today’s illustration. I painted this based on a photo I took at a glacier in Iceland in September. It didn’t turn out as well as I had hoped, but it’s not as bad as I feared when I was halfway into it. — Raghuveer Parthasarathy; November 28, 2021 NOTES and References [1] That was Physics 112 at UC Berkeley, taught by the late Eugene Commins — an excellent course! [2] e.g. L. Brillouin, Poincare’s theorem and uncertainty in classical mechanics. Information and Control. 5, 223–245 (1962). [3] Bob Doyle, https://www.informationphilosopher.com/solutions/scientists/borel/ [4] See, for example, the first chapter of Schroeder’s very good textbook, “An Introduction to Thermal Physics”. [5] On LIGO sensitivity, see e.g. the plot at https://ligo.org/science/Publication-S6CBCRangeDoc/index.php . On the distances of the events detected so far, see https://en.wikipedia.org/wiki/List_of_gravitational_wave_observations. </description>
      <pubDate>01 Dec 21 18:53 EST</pubDate>
      <guid>https://eighteenthelephant.com/2021/11/29/pushed-around-by-stars/</guid>
    </item>
    <item>
      <title>Why Facts Don’t Change Our Minds</title>
      <link>https://jamesclear.com/why-facts-dont-change-minds</link>
      <description>&lt;a href=&#34;https://jamesclear.com/why-facts-dont-change-minds&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; The economist J.K. Galbraith once wrote, “Faced with a choice between changing one’s mind and proving there is no need to do so, almost everyone gets busy with the proof.” Leo Tolstoy was even bolder: “The most difficult subjects can be explained to the most slow-witted man if he has not formed any idea of them already; but the simplest thing cannot be made clear to the most intelligent man if he is firmly persuaded that he knows already, without a shadow of doubt, what is laid before him.” What&#39;s going on here? Why don&#39;t facts change our minds? And why would someone continue to believe a false or inaccurate idea anyway? How do such behaviors serve us? The Logic of False Beliefs Humans need a reasonably accurate view of the world in order to survive. If your model of reality is wildly different from the actual world, then you struggle to take effective actions each day. 1 However, truth and accuracy are not the only things that matter to the human mind. Humans also seem to have a deep desire to belong. In Atomic Habits, I wrote, “Humans are herd animals. We want to fit in, to bond with others, and to earn the respect and approval of our peers. Such inclinations are essential to our survival. For most of our evolutionary history, our ancestors lived in tribes. Becoming separated from the tribe—or worse, being cast out—was a death sentence.” Understanding the truth of a situation is important, but so is remaining part of a tribe. While these two desires often work well together, they occasionally come into conflict. In many circumstances, social connection is actually more helpful to your daily life than understanding the truth of a particular fact or idea. The Harvard psychologist Steven Pinker put it this way, “People are embraced or condemned according to their beliefs, so one function of the mind may be to hold beliefs that bring the belief-holder the greatest number of allies, protectors, or disciples, rather than beliefs that are most likely to be true.” 2 We don&#39;t always believe things because they are correct. Sometimes we believe things because they make us look good to the people we care about. I thought Kevin Simler put it well when he wrote, “If a brain anticipates that it will be rewarded for adopting a particular belief, it&#39;s perfectly happy to do so, and doesn&#39;t much care where the reward comes from — whether it&#39;s pragmatic (better outcomes resulting from better decisions), social (better treatment from one&#39;s peers), or some mix of the two.” 3 False beliefs can be useful in a social sense even if they are not useful in a factual sense. For lack of a better phrase, we might call this approach “factually false, but socially accurate.” 4 When we have to choose between the two, people often select friends and family over facts. This insight not only explains why we might hold our tongue at a dinner party or look the other way when our parents say something offensive, but also reveals a better way to change the minds of others. Facts Don&#39;t Change Our Minds. Friendship Does. Convincing someone to change their mind is really the process of convincing them to change their tribe. If they abandon their beliefs, they run the risk of losing social ties. You can’t expect someone to change their mind if you take away their community too. You have to give them somewhere to go. Nobody wants their worldview torn apart if loneliness is the outcome. The way to change people’s minds is to become friends with them, to integrate them into your tribe, to bring them into your circle. Now, they can change their beliefs without the risk of being abandoned socially. The British philosopher Alain de Botton suggests that we simply share meals with those who disagree with us: “Sitting down at a table with a group of strangers has the incomparable and odd benefit of making it a little more difficult to hate them with impunity. Prejudice and ethnic strife feed off abstraction. However, the proximity required by a meal – something about handing dishes around, unfurling napkins at the same moment, even asking a stranger to pass the salt – disrupts our ability to cling to the belief that the outsiders who wear unusual clothes and speak in distinctive accents deserve to be sent home or assaulted. For all the large-scale political solutions which have been proposed to salve ethnic conflict, there are few more effective ways to promote tolerance between suspicious neighbours than to force them to eat supper together.” 5 Perhaps it is not difference, but distance that breeds tribalism and hostility. As proximity increases, so does understanding. I am reminded of Abraham Lincoln&#39;s quote, “I don&#39;t like that man. I must get to know him better.” Facts don&#39;t change our minds. Friendship does. The Spectrum of Beliefs Years ago, Ben Casnocha mentioned an idea to me that I haven&#39;t been able to shake: The people who are most likely to change our minds are the ones we agree with on 98 percent of topics. If someone you know, like, and trust believes a radical idea, you are more likely to give it merit, weight, or consideration. You already agree with them in most areas of life. Maybe you should change your mind on this one too. But if someone wildly different than you proposes the same radical idea, well, it&#39;s easy to dismiss them as a crackpot. One way to visualize this distinction is by mapping beliefs on a spectrum. If you divide this spectrum into 10 units and you find yourself at Position 7, then there is little sense in trying to convince someone at Position 1. The gap is too wide. When you&#39;re at Position 7, your time is better spent connecting with people who are at Positions 6 and 8, gradually pulling them in your direction. The most heated arguments often occur between people on opposite ends of the spectrum, but the most frequent learning occurs from people who are nearby. The closer you are to someone, the more likely it becomes that the one or two beliefs you don&#39;t share will bleed over into your own mind and shape your thinking. The further away an idea is from your current position, the more likely you are to reject it outright. When it comes to changing people&#39;s minds, it is very difficult to jump from one side to another. You can&#39;t jump down the spectrum. You have to slide down it. Any idea that is sufficiently different from your current worldview will feel threatening. And the best place to ponder a threatening idea is in a non-threatening environment. As a result, books are often a better vehicle for transforming beliefs than conversations or debates. In conversation, people have to carefully consider their status and appearance. They want to save face and avoid looking stupid. When confronted with an uncomfortable set of facts, the tendency is often to double down on their current position rather than publicly admit to being wrong. Books resolve this tension. With a book, the conversation takes place inside someone&#39;s head and without the risk of being judged by others. It&#39;s easier to be open-minded when you aren&#39;t feeling defensive. Arguments are like a full frontal attack on a person&#39;s identity. Reading a book is like slipping the seed of an idea into a person&#39;s brain and letting it grow on their own terms. There&#39;s enough wrestling going on in someone&#39;s head when they are overcoming a pre-existing belief. They don&#39;t need to wrestle with you too. Why False Ideas Persist There is another reason bad ideas continue to live on, which is that people continue to talk about them. Silence is death for any idea. An idea that is never spoken or written down dies with the person who conceived it. Ideas can only be remembered when they are repeated. They can only be believed when they are repeated. I have already pointed out that people repeat ideas to signal they are part of the same social group. But here&#39;s a crucial point most people miss: People also repeat bad ideas when they complain about them. Before you can criticize an idea, you have to reference that idea. You end up repeating the ideas you’re hoping people will forget—but, of course, people can’t forget them because you keep talking about them. The more you repeat a bad idea, the more likely people are to believe it. 6 Let&#39;s call this phenomenon Clear&#39;s Law of Recurrence: The number of people who believe an idea is directly proportional to the number of times it has been repeated during the last year—even if the idea is false. 7 Each time you attack a bad idea, you are feeding the very monster you are trying to destroy. As one Twitter employee wrote, “Every time you retweet or quote tweet someone you’re angry with, it helps them. It disseminates their BS. Hell for the ideas you deplore is silence. Have the discipline to give it to them.” 8 Your time is better spent championing good ideas than tearing down bad ones. Don&#39;t waste time explaining why bad ideas are bad. You are simply fanning the flame of ignorance and stupidity. The best thing that can happen to a bad idea is that it is forgotten. The best thing that can happen to a good idea is that it is shared. It makes me think of Tyler Cowen&#39;s quote, “Spend as little time as possible talking about how other people are wrong.” Feed the good ideas and let bad ideas die of starvation. The Intellectual Soldier I know what you might be thinking. “James, are you serious right now? I&#39;m just supposed to let these idiots get away with this?” Let me be clear. I&#39;m not saying it&#39;s never useful to point out an error or criticize a bad idea. But you have to ask yourself, “What is the goal?” Why do you want to criticize bad ideas in the first place? Presumably, you want to criticize bad ideas because you think the world would be better off if fewer people believed them. In other words, you think the world would improve if people changed their minds on a few important topics. If the goal is to actually change minds, then I don&#39;t believe criticizing the other side is the best approach. Most people argue to win, not to learn. As Julia Galef so aptly puts it: people often act like soldiers rather than scouts. Soldiers are on the intellectual attack, looking to defeat the people who differ from them. Victory is the operative emotion. Scouts, meanwhile, are like intellectual explorers, slowly trying to map the terrain with others. Curiosity is the driving force. 9 If you want people to adopt your beliefs, you need to act more like a scout and less like a soldier. At the center of this approach is a question Tiago Forte poses beautifully, “Are you willing to not win in order to keep the conversation going?” Be Kind First, Be Right Later The brilliant Japanese writer Haruki Murakami once wrote, “Always remember that to argue, and win, is to break down the reality of the person you are arguing against. It is painful to lose your reality, so be kind, even if you are right.”10 When we are in the moment, we can easily forget that the goal is to connect with the other side, collaborate with them, befriend them, and integrate them into our tribe. We are so caught up in winning that we forget about connecting. It&#39;s easy to spend your energy labeling people rather than working with them. The word “kind” originated from the word “kin.” When you are kind to someone it means you are treating them like family. This, I think, is a good method for actually changing someone&#39;s mind. Develop a friendship. Share a meal. Gift a book. Be kind first, be right later. 11 </description>
      <pubDate>10 Dec 21 08:52 EST</pubDate>
      <guid>https://jamesclear.com/why-facts-dont-change-minds</guid>
    </item>
    <item>
      <title>Uncle Bob and Silver Bullets</title>
      <link>https://www.hillelwayne.com/uncle-bob/</link>
      <description>&lt;a href=&#34;https://www.hillelwayne.com/uncle-bob/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A while back I wrote that Robert Martin was ruining software by being too good at programming. That was supposed to be a joke. Since then he’s done his damndest to actually ruin software by telling people they’re doing it wrong. His most recent response where he yells at software correctness was the breaking point for me, so I’m going to go ahead and say what many of us have been thinking: Uncle Bob gives terrible advice. Following it will make your code worse. He begins Tools are not the Answer by listing some of the “new” tools that are not “the answer”: Light Table, Model Driven Engineering, and TLA+. Now, I’m pretty biased here, what with writing a TLA+ manual and all. But I agree with what (I thought) was the core critique: there is no silver bullet. TLA+ has shown some astounded successes at Amazon and Microsoft, but it only verifies specs, not code. While it’s incredible for designing systems, you should combine it with other correctness techniques, like type systems and tests. A pretty good argument. But it turns out that argument was only in my head, because he follows it with this: The solution to the software apocalypse is not more tools. The solution is better programming discipline. Just what is “discipline”, anyway? Uncle Bob says that means not “doing a half-assed job.” Uncle Bob is saying the solution for people writing bad code… is to not write bad code. Our programs would be perfect if it weren’t for the programmers! One of the core assumptions of modern systems engineering1 is that there’s a constant flow of defects: that people make mistakes. You can’t rely on people to not fuck up on their own: after all, the US still has 30,000 auto deaths a year. Rather, the best way to reduce the volume and severity of mistakes is to adjust the system itself. Either make them harder to do, make them easier to catch, or make them cause less damage when they do happen. Don’t just blame the drivers, give them safe roads! Give them seatbelts! One way of doing this is to add a bureaucratic process, such as code review. If your code doesn’t conform to requirements (it lacks tests, you named your variables x and x2, etc), the code will be rejected. That, on a systems level, reduces bugs.2 When we adopt mechanical tools, like tests and IDEs, all we are doing is automating those processes. We use the way we create code, and the kind of code we create, to check our work. This is the vast field of software correctness, and spans everything from type systems to language design. Uncle Bob is okay with software correctness: after all, he uses the phrase “unit testing” like a smurf uses “smurf”. But what about every other correctness technique? End-to-End Tests: ‘Don’t test through UIs’. Safer Languages: The ‘Dark Path’. Data Integrity: Just ‘a detail’. Type Systems: ‘You don’t need static type checking if you have 100% unit test coverage’. Formal Methods: Only a ‘shiny’. Property-Based Testing: Probably hasn’t heard of it. In other words, any correctness technique that isn’t unit tests can be dismissed.3 But unit tests don’t give you much confidence in your code. That’s because humans make mistakes, and we can’t always guarantee that the mistakes we make are the nicely unit testable ones. For example, here’s a ceiling function I wrote. Quick, what numbers would you test it with? function ceiling(num) { if (num == (num | 0)) { return num; } return Math.round(num + 0.5); } Did you try -1e-100? You’d have seen that ceiling(-1e-100) == 1 when it should be 0. That’s because of how floating point works: 0.5 - 1e-100 == 0.5. I’d be shocked if many people remembered to check that, if they even knew that floating point has quirks at all. But a property-based test catches it easily. Okay, function two: function clamp(min, x, max) { return Math.max(Math.min(max, x), min); } The function is perfectly fine. The bug isn’t in the function at all! It’s that, in our 50 kLoC codebase, there is a single path that eventually ends with calling clamp with a null value. Are you going to test every possible path? Is that really superior to using a type system? Okay, last one: function append_to_body(type, text) { var d = document.createElement(type); d.innerHTML = text; document.body.appendChild(d); } The function works fine, except you’ve now opened up an XSS vector. That’s why we have static analysis. These aren’t just toy examples. These are topics with plenty of research, plenty of development, and plenty of history.4 We’ve learned what they’re good for and what their limitations are. We use these tool because they work. Exactly the same reason we have unit tests and TDD. But unit tests are not enough. Type systems are not enough. Contracts are not enough, formal specs are not enough, code review isn’t enough, nothing is enough. We have to use everything we have to even hope of writing correct code, because there’s only one way a program is right and infinite ways a program can be wrong, and we can’t assume that any tool we use will prevent more than a narrow slice of all those wrong ways. That’s what makes Bob’s advice so toxic. By being so dismissive of everything but unit tests, he’s actively discouraging us from using our whole range of techniques. He demands we run blind and blames us for tripping. Uncle Bob can say whatever he likes. We don’t have to listen to him. He’s welcome to keep shouting that tools won’t help, better languages won’t help, better process won’t help, the only thing that’ll help is lots and lots of unit tests. Meanwhile, we’ll continue to struggle with our bugs, curse our model checkers, and make software engineering just a little bit better. We don’t believe in silver bullets. </description>
      <pubDate>16 Feb 21 13:23 EST</pubDate>
      <guid>https://www.hillelwayne.com/uncle-bob/</guid>
    </item>
    <item>
      <title></title>
      <link>https://liamp.substack.com/p/i-dont-want-to-be-old</link>
      <description>&lt;a href=&#34;https://liamp.substack.com/p/i-dont-want-to-be-old&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;This post is also available on my new website liamporr.com you will start receiving emails from there as well instead of substack. Me at 18 vs me at 22There comes a point where birthdays are no longer exciting. Everyone looks forward to the big milestones— On my 16th birthday I recieved freedom in exchange for taking a drivers test. On my 18th birthday we went camping in Big Bend National Park with my cousins. On my 21st birthday, we were all sent home because of Covid, but I got one great night in at the bars and got on a 7am flight home to Texas with no sleep.On my 22nd birthday, one week ago, I got my Covid vaccine. Best present ever.But now, the birthdays will come more quickly and with less celebration. In other words, I&#39;m entering adulthood.I’m a harsh self-critic, and I’ve always taken myself way too seriously. Throughout highschool and college I wanted to accomplish a lot as fast as possible, so others would take me more seriously. I wanted to reach the “serious accomplished adult” phase as quickly as possible. The younger the better.I think I felt this way because I wasn’t very good at the “be a fun kid” phase. I was geeky, unathletic, and awkward. So, instead of trying to fight that loosing battle, I decided to try and beat everyone to the next phase. Then I’ll be on top and we’ll see who’s laughing.For my first job, I worked as a landscaper for my dad’s friend. It was not a glamorous job. Working outside in the South Texas summer is about as shitty as it gets. The heat and humidity are brutal, but hey- you get payed a whole dollar above minimum wage, for your troubles. In other words, it was the perfect place to start my career.Fast forward to my first summer in college. I came to California to become a software engineer, and I manage to land my first real internship. I’m getting payed over 4x the salary of my first job as a landscaper, and I’m working in Silicon Valley. Nice as it was though, things didn’t work out.There’s something very unsettling about working at a large tech company in the valley. It felt like one of those office cubicle nightmares you see on TV shows. The people I worked with in the landscaping company had.. troubled backgrounds for the most part, but damn if they weren’t way more entertaining than the people I was working with now. One of the guys I worked with back home once told me he was working on a cargo ship before he started landscaping. While docked in Columbia, he smoked so much weed with a guy in a hut that he forgot the ship was leaving that afternoon, and nearly got stranded in a foreign country. I don’t think anyone at this new place could’ve told me as good a lie as that one.But hey, this is just one company. Let’s do another internship and maybe it’ll turn out better.Nope, nope, and nope. This one was even worse. Bigger company, same problem. I didn’t admire my bosses or mentors. I even met the CEO at one point, and felt equally disappointed, thinking “this guy is the head of a massive company, the physical embodiment of a Silicon Valley success story, and I have zero desire to be like him”. They just seemed very.. different from me.ExpectationsAs an adult you have certain expectations— get a job, save up for retirement, get a spouse, make more money, get a house. These expectations are totally made up of course. Nobody is forcing you to do any of these things. The more we play into these artifical games the more they suck us in. Suddenly, it becomes about living in the right neighborhood and having the right interior design, making your kids go to the right schools.These expectations are all inherently restrictive. For every “expectation” you seek to satisfy, the ability live uniquely in your own life decreases.I dont think this is news to people, but I also don’t think people are trying to restrict their lives on purpose. Nobody wakes up and thinks “ah I cant wait to keep allowing societal expectations implicitly dictate the way I live, time to go get a keto bagel”. Its something that happens because of our instincts as social creatures. To survive we must fit in.Social media makes this even worse. Pick your poison: compulsively use instagram and mope because you’re not a world-traveling fitness health influencer, or use twitter and start telling people ten things you learned about being a total tool (a thread) :down point:.Sorry that twitter one was a little mean. I like twitter. But, like all social media, it hijacks your social instincts—tricking you into believing that everyone lives a particular way and you have to conform to survive. But, in reality those people live unrealistic lifestyles, and you dont see what happens behind the scenesI think people do this because it’s the easiest thing to do. When you’re young, its easier to ignore those expecations because you’re in the process of changing, your personality is more fluid. Once you start to settle into an identity though, you begin to restrict your goals, aspriations, hobbies, basically your entire life in ways that satisfy that identityI think this is why we admire youth so much. Young people have the fluidity and courage that we loose as adults.Courage is a finicky thing. It’s important and essential, but it’s bolstered by ignorance. I think when you’re more ignorant you tend to be more couragious. Because you don’t have a firm grasp on the consequences or the risks involved in your actions, there’s very little to restrict you from acting. As adults we understand the consequences and we have more to loose, so we tend to take fewer bold actions. But, we want to take bold actions, and we admire those who do.It seems like the wisest people tend to embody characteristics of youth. It’s almost as if they’ve circled back: they no longer feel the need to take themselves seriously and impress people. They understand the consequences of bold actions. In fact, they understand the consequences so well that they know that the negative parts are mostly meaningless.We spend our adulthood in the process of accumulating things: wealth, reputation, a career. We avoid anything that could risk loosing those things. Contrast this with the brazen 22 year old, with nothing to loose and the world at their fingertips.For the last couple years I wanted to accumulate those things quickly. Proximity to Silicon Valley did not help in this regard either. Everyone at Berkeley wants to be young and rich. You either flipped sneakers and made thousands of dollars in high school or wanted to create a startup and be the next Zuckerberg.You either had the right internships at Facebook and Google or worked in IB and made boatloads of money working 100+ hours a week during summer “vacation”. Are you in the right consulting or CS clubs? Machine learning at Berkeley has a two-phase interview process and probably accepts around ~2.5% of applicants. Blockchain at Berkeley has made so much money from consulting jobs that they created their own startup accelerator (https://www.xcelerator.berkeley.edu/). They charge upwards of 50k per gig (https://www.theinformation.com/articles/meet-the-hottest-blockchain-consultancy-a-berkeley-student-group).I’m not here to tell you money wont make you happy. Anyone with money could tell you that. Many students here seek accomplishment more than money. They want people to know their reputation and admire their skills. So, they work just as hard (if not more) than those who just want to be rich.We jump into these 40 hour a week jobs right after college because we want to be taken seriously and thats what serious people do. Then when we realize its not what we expected, but we stay because leaving is scary and we might loose the comfortable lifestyle we’ve come to know. So instead, we suffer through our desperation quietly.I think we can learn a thing or two from young people. We cant live ignorant and free of responsibility forever, but there’s no reason why we have to so eagerly toss them aside.As long as possible live free and uncommitted. It makes but little difference whether you are committed to a farm or the county jail. - Thoreau</description>
      <pubDate>22 Dec 21 12:54 EST</pubDate>
      <guid>https://liamp.substack.com/p/i-dont-want-to-be-old</guid>
    </item>
    <item>
      <title>Understanding UUIDs, ULIDs and String Representations</title>
      <link>https://sudhir.io/uuids-ulids</link>
      <description>&lt;a href=&#34;https://sudhir.io/uuids-ulids&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;What UUIDs and ULIDs are under the hood, and how to encode and use them.Database and data storage systems need identifiers for each piece of information they store. Using numbers as identifiers is a one way to do it, but that has limitations — and we have alternatives in the form of UUIDs and ULIDs.NumbersThe easiest and most intuitive way to generate IDs is to use numbers. They&#39;re small, efficient and simple. In SQL databases like PostgreSQL, MySQL, SQL Server or Oracle, the numeric primary key is normally called SERIAL with an explicit or implicit size. The common sizes are 4 bytes, which corresponds to a 32 digit (or 32 bit) binary number, capable of representing numbers from 0 to 2,147,483,647; and a bigger version (BIGSERIAL) at 8 bytes, or 64 bits, which can go from 0 to 9,223,372,036,854,775,807.That&#39;s the first problem right there. When using a numeric primary key, you need to be sure the size of key you&#39;re using is big enough. 2,147,483,647 looks like a big number, but in the context of modern web applications it&#39;s not that large. An application that serves a few hundred thousand people can quickly blow through that number, especially if it&#39;s being used to identify individual items, changes, or events.Using 8 bytes gives you more headroom to grow — 9,223,372,036,854,775,807 of anything is pretty hard to hit. Most frameworks will / should set this as the primary key default when creating databases for you, but it always helps to double-check. I&#39;ve experienced and heard of many times where tables have run out of 32-bit space and engineers had to run upgrades and migrations at the worst possible time. How does a numeric ID work, though? And what are its limitations? The first thing to remember is that if a number is being used as an identifier it needs to be unique — which means some entity needs to keep track of which numbers have already been used and make sure they&#39;re never used again. In most database systems, this is done via the use of a sequence object. When adding a new piece of data, this sequence object is asked for a new number — it checks its storage to get last number it gave out, increments it, durably updates the new number in storage, and then gives it out. The most common type of sequence is a monotonically increasing sequence, which means that each time you ask the sequence object for a number it&#39;ll give you the previous number it gave out plus one.You might have noticed that when you asked for a number the sequence object stored the number first and then gave it out to you. Why would it do that? Why not give you the number and then store it? This is an important distinction — if the sequence didn&#39;t store the number first before giving it out to you, and it crashed before storing that number, it would give the same number out again the next time someone asked for one. This is a catastrophic failure — multiple consumers will have received the same number out of a sequence whose main job is to make sure no two consumers get the same number. This means that the sequence implementation must err on the side of caution — it must update the number first before it gives it out to you.But there&#39;s a downside to this approach — what if you crash before using the number that you got out of the sequence generator? When you restart and try your work again, you&#39;ll get a new number, incremented even if no other consumer is using this sequence. That number that you first pulled out and didn&#39;t use is lost forever. This is a common error with sequences — you can never assume that the highest number you see as an ID is implicitly the count of the number of items or rows. This may the case if nothing has ever gone wrong during the lifetime of the system, but that&#39;s pretty unlikely.Numeric sequences also have the disadvantage of being a separate entity from the datasets that they&#39;re being used in, making them implicitly coupled. You can copy a table over to a new database and forget to copy over the sequence generator object and its state, setting yourself up for an unexpected blowup. You might make the mistake of resetting the sequence generator to a number that&#39;s already been used (usually back to zero). Or your database backup/restore script might make the same mistakes. When you have coupling like this, there&#39;s a lot that can go wrong.Then there&#39;s scalability — this is something you don&#39;t often need, but if and when you do need it it&#39;s always too late for simple fixes. Having a single place where identifiers are generated means that you can add data only as fast as your sequence generator can reliably generate IDs. If you suddenly have a million people who want to buy things on your store, you can&#39;t ask them to wait because your sequence generator can&#39;t number their order line items fast enough. And because a sequence must store each number to disk before giving it out, your entire system is bottle-necked by the speed of rewriting a number on one SSD or hard disk — no matter how many servers you have.On a scaling-related note, numeric IDs limit your sharding options — if you&#39;re trying to split your workloads across multiple databases, you&#39;ll need to be careful to make sure that sequences can&#39;t overlap. You might make one shard&#39;s sequence run in even numbers, and another one run in odd numbers, for example; or more complex schemes when you have more shards.So numeric IDs are simple and work great in many situations, but remember that they&#39;re separate objects from the data you&#39;re storing but still always need to managed in tandem with your data; choosing a small size is likely to cause a crash when you least expect it; they look like counters but they&#39;re not; and they&#39;ll limit how fast you can add new data by their nature.UUIDsWith the rise of distributed systems it became more and more important to have identifiers that could be created on multiple computers simultaneously, without forcing them to communicate with each other. This is necessary either because the systems are geographically independent, like databases that take orders in different countries and then merge them for global sales reporting, or because the rate of identifier creation required was greater than what could be supported by one sequence generator.The most common type of UUID or GUID in use today is a 128-bit (16-byte) number, generated at random. This means that under the RFC 4122 specification, where 122 bits are generated randomly, it can represent a number up to to 5.3×10^36. That&#39;s a lot of zeroes, and makes for a very large number — so large we consider it practically unreachable under any single application, or even the combination of all human applications. The new internet protocol address format, IPv6, is also essentially a 128 bit number given as a unique address to every possible internet connected object.Most database systems support UUIDs, and do so efficiently. The human readable standard representation of a UUID looks like xxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx with x being the hexadecimal representation of a random byte and M and N representing version information. This looks like a long string, but most databases will internally support an efficient binary UUID type that maps onto a 16-byte array. It&#39;s also possible to use all the bits for randomness, if you don&#39;t care about conformance to the specification.This entire idea of using random IDs assumes that your computers can generate random numbers that are random and unpredictable — this isn&#39;t an easy problem, and there&#39;s a lot of research being done in the field. The Cloudflare lava lamp project is an interesting look into how to tap into a real-world source of randomness. Studies into the nature of random number generators in modern computers and operating systems is its own topic, and for now we&#39;ll just assume that all our systems are using state-of-the-art cryptographic random number generators.Using UUIDs then allows us to rise above some of the problems we saw with numeric IDs. Since there&#39;s no single authority or sequence generator in charge of a particular identifier, it becomes possible to generate identifiers for anything at a practically unlimited rate. There&#39;s also no problems with generating identifiers at multiple places simultaneously — there&#39;s no need for them to coordinate, so if you need more identifier generation capacity you can just add more hardware to handle the problem. Because these identifiers are independent, it&#39;s also possible to merge them into a single dataset later (asynchronously) without reasonably expecting any problems.That last sentence might have raised your eyebrows, though. What does “without reasonably expecting any problems” mean?. The issue with random IDs that it is remotely possible that the same ID may be generated more than once, even by the same system. If you&#39;re using a single system it may be possible to check the data and reject a duplicate ID, but if you&#39;re using multiple systems this won&#39;t be possible. The chances of this happening are extremely remote, though — similar to your chances of winning the lottery multiple times and immediately being struck by lightning each time — but they&#39;re not strictly zero. There&#39;s a mathematical treatment on the Wikipedia page, but the basic premise is that the more IDs you generate they more your probability of a duplicate ID, or a collision, increases. This probability is actually more than just the naive assumption that it&#39;s 1 / TOTAL_POSSIBLE_IDS because of an interesting concept called the birthday paradox. The probability that a your birthday is the same as mine is 1/365, but in a room of 30 people the probability that any two people have the same birthday isn&#39;t 1/365, because the universe now has many more chances to get a match. The same things applies with UUIDs, because each UUID you generate has the chance to collide with every UUID ever generated in the history of your dataset. In practice, though, this isn&#39;t a problem. Most applications will ignore the these odds, and some may have a contingency in place to handle a collision, but will almost never actually experience one. The bigger problem with UUIDs is they have no concept of locality. With numeric IDs, we could examine the ID and reasonably assume that higher numbers were generated later than lower ones, and the highest ID is the last one generated. This also helps optimize indexes in some databases — additions are made only on one side of the index tree, so it&#39;s possible to optimize for that use case. In a completely random UUID system, though, IDs are being inserted with no concept of location at all. Any ID has an equal chance of being inserted anywhere in the spectrum of possibilities. This means that indexes become wasteful — they&#39;re usually made to order items neatly, but if you put UUIDs in them you&#39;re wasting any ordering capability, and mostly causing extra bloat and book-keeping that won&#39;t be used. Alternate indexes based on hashing are excellent for UUIDs, though, but they&#39;re not as popular or optimized because they only offer fast lookup and no sorting.ULIDs To The RescueSo given the two major problems of UUIDs, which are 1) collision possibilities across the history of all generated IDs, and 2) complete loss of locality, can we do better? Yes, we can!Instead of using all the 128 bits for randomness, what if we use the first 48 bits for a timestamp? 48 bits is enough to represent a millisecond-precision Unix timestamp (the number of milliseconds since an epoch at the beginning of Jan 1, 1970) till the year 10889 AD. Given the way we&#39;re going, humanity in its present form isn&#39;t likely to exist them, so when this becomes an issue it&#39;ll be somebody else&#39;s problem. More likely something else&#39;s problem.The remaining 80 bits are available for randomness, which means they still represent a pretty large number: ~1,208,925,820,000,000,000,000,000. You get to generate this many IDs inside a single millisecond.At one stroke, this solves both the problems we have. An ID generated at a particular millisecond in the past can never collide with one generated in the future, so we only need to worry about collisions inside the same millisecond — which is to say the amount of worrying we need to do is a lot closer to zero. It also introduces locality into the ID, which means that IDs generated later will have higher byte values than those generated earlier. Assuming they&#39;re encoded with the proper alphabet, they&#39;ll also sort correctly. We&#39;ll come back to encoding later.This time-based ULID system continues to give us all the advantages of a UUID system—it can be distributed, because there&#39;s no single sequence authority, the IDs can be merged into a single dataset later, and there&#39;s an even lower chance of global collision, because collisions are only possible inside the same millisecond. And because they sort based on time, we can use them in append-only settings, where we can keep adding them to our dataset without having to re-balance indexes. The ULID format also allows continuous archiving of our data — because we know we&#39;re not going to receive any more data with identifiers lower than those representing a particular time, we can just archive all data in permanent storage each hour or day or week if we wanted to. It would still be possible to quickly find the data we need based on the timestamp prefix.Words MatterSpeaking of prefixes, the way we represent our IDs makes a big difference. SQL database systems will usually have an optimized 16-byte representation internally, but most No-SQL systems work on strings. Strings also come into play when sending the IDs over JSON, XML or any other format over the wire. Let&#39;s look at a few common encoding options:To start with, we&#39;re working with a 128 bit binary number that looks like this:11111111011110001010111001101011011100010111011001000110111010011000000111000110010010111110111001100000101110101000000110111011 Encodings work with bytes instead of bits, so let&#39;s see what our ID looks like when split into 8-bit chunks, or bytes:[11111111, 01111000, 10101110, 01101011, 01110001, 01110110, 01000110, 11101001, 10000001, 11000110, 01001011, 11101110, 01100000, 10111010, 10000001, 10111011] In decimal, or base10, this is really just a sequence of numbers between 0 and 255:[255, 120, 174, 107, 113, 118, 70, 233, 129, 198, 75, 238, 96, 186, 129, 187] The UUID spec uses hexadecimal characters to represent IDs. Each hexadecimal character represents 4 bits, so it can represent the binary number 0000 (decimal 0) to 1111 (decimal 16). Since we&#39;re dealing with a byte, we&#39;ll need two hexadecimal characters for each byte in the sequence:[&#34;ff&#34;, &#34;78&#34;, &#34;ae&#34;, &#34;6b&#34;, &#34;71&#34;, &#34;76&#34;, &#34;46&#34;, &#34;e9&#34;, &#34;81&#34;, &#34;c6&#34;, &#34;4b&#34;, &#34;ee&#34;, &#34;60&#34;, &#34;ba&#34;, &#34;81&#34;, &#34;bb&#34;] If we smash them all together we get ff78ae6b717646e981c64bee60ba81bb, and if we insert hyphens according to the RFC 4122 spec we get:ff78ae6b-7176-46e9-81c6-4bee60ba81bb But of course this isn&#39;t the only way to do it. We had to use two characters for each byte because the hexadecimal alphabet has only 16 characters, which are 0123456789abcdef. But what if we use more characters? What if we exactly double our available alphabet, to 0123456789ABCDEFGHJKMNPQRSTVWXYZ? Then we wind up with a more compact representation:7ZF2Q6PWBP8VMR3HJBXSGBN0DV This alphabet, 0123456789ABCDEFGHJKMNPQRSTVWXYZ, is Douglas Crockford&#39;s Base32, chosen for human readability and being able to call it out over a phone if required. You&#39;ll see that ambiguous letters like I and L have been omitted because they can be confused with 1 — along with other considerations.This Base32 is what&#39;s used in the ULID format. In a ULID the first 48 bits are the timestamp, so here&#39;s what a ULID that I generated looks like:01EWW6K6EXQDX5JV0E9CAHPXG5 In binary, that&#39;s1011101110011100001101001100110011101110110111011011110100101100101101100000011100100101100010101000110110111011000000101 or 1948255503464693300277367552865957381 if we&#39;re just using decimal.Using the process above, we can always just turn this back into the UUID format:01773869-99dd-bb7a-596c-0e4b151b7605 If you remember, the first 48 bits it a timestamp, and 48 bits is 6 bytes, and since each byte is written with two hexadecimal characters, the first 12 characters are the timestamp component: 0177386999dd. Convert that to to decimal and you get 1611559180765 — which is the number of UTC milliseconds between Jan 1, 1970 and the time I generated this ID.All this is to say that UUIDs and ULIDs are really just big numbers. We can write them however we want, making their string representations as small or as big as we want.We could use base64, for instance, and write the ID as AXc4aZndu3pZbAAADksVGw. Or we could go all in on UTF-8 and use a 1,112,064 character alphabet, including emoji and symbols.My personal favourite alphabet is the lexicographic base62.: 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz This contains every number and every uppercase and lowercase English alphabet, and no symbols. This means it&#39;s safe for URLs, and results in fairly short identifiers — but most importantly, it sorts correctly under ASCII and UTF-8 sorting. This means that you can encode ULIDs with this alphabet, and as long as you make sure all your IDs are the same length (you can left pad to a standard length with a zero), they&#39;ll string sort correctly.This is fantastic for NoSQL databases — your primary identifier can now act as a timestamp and natural sort, allowing you to do time range queries on a UUID primary key.This is one of my pet subjects, so here are a few tools I&#39;ve built to help work with UUIDs and ULIDs:https://github.com/sudhirj/uulid.go — easily move between UUIDs and ULIDs in Gohttps://github.com/sudhirj/shortuuid.rb — UUID encoding into any alphabet for Rubyhttps://github.com/sudhirj/shortuuid.go — UUID encoding into any alphabet for GoReferenceshttps://github.com/ulid/spec — the ULID spechttp://www.crockford.com/base32.html — Douglas Crockford&#39;s Base32Other FormatsI wrote about UUIDs and ULIDs because they&#39;re compatible with each other and widely supported, but there some other options if you just need strings: https://github.com/segmentio/ksuidTwitter&#39;s Snowflakehttps://github.com/ericelliott/cuidPSNew UUID formats are also being proposed: https://datatracker.ietf.org/doc/html/draft-peabody-dispatch-new-uuid-format-01There&#39;s a Hacker News discussion here: https://news.ycombinator.com/item?id=29794186</description>
      <pubDate>05 Jan 22 12:55 EST</pubDate>
      <guid>https://sudhir.io/uuids-ulids</guid>
    </item>
    <item>
      <title>Two Years Is Long Enough</title>
      <link>https://www.theatlantic.com/ideas/archive/2022/01/i-want-my-life-back-fear-covid/621214/</link>
      <description>&lt;a href=&#34;https://www.theatlantic.com/ideas/archive/2022/01/i-want-my-life-back-fear-covid/621214/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;After multiple lockdowns, three vaccines, and one bout of COVID, I want my life back.Getty; The AtlanticAbout the author: Helen Lewis is a London-based staff writer at The Atlantic and the author of Difficult Women: A History of Feminism in 11 Fights. I got my COVID-19 booster shot last week, on the first day I was eligible. My shot was delayed because I caught COVID in early December, an experience that was low-key grim: two days of shotgun sneezing, no taste or smell for a week, and a constant fatigue that didn’t abate until the holidays. I was very glad to face the coronavirus with two Pfizer doses already in my arm, and even more grateful that my parents and 91 percent of Britons in their age group are triple-jabbed.Immunity builds to a peak in the fortnight after vaccination, and so next week I will be about the most protected a human can realistically expect to be against COVID. That reflection has inevitably led to another one: I want my life back. Thank you, coronavirus. Next. Avoiding the virus is no longer an option; Omicron has seen to that. Almost everyone is likely to catch the variant eventually. Over Christmas, one in 10 of my fellow Londoners—one in 10!—had COVID. Thanks to Britain’s solid vaccination rates, particularly among vulnerable groups, this tsunami of infections has so far led to a daily death toll less than a fifth the size of the one we had last winter. In the United States, the picture looks bleaker, with overwhelmed hospitals and 1,500 deaths a day. Because the vaccinated can still spread the disease, Americans should probably lie low for a few more weeks, until this wave subsides. Personally, I don’t need an immediate license to party like it’s February 2020, but I want some indication from lawmakers and medical experts that restrictions won’t last forever. For any country without the discipline, collectivism, and surveillance technology of China, the zero-COVID dream is over. Two years is long enough to put our lives on hold.Read: Omicron is forcing us to rethink mild COVIDAnd that really is how it feels here in a big city like London. Prime Minister Boris Johnson’s government wants to slow the spread of Omicron in England, but not enough to impose a new lockdown or provide the financial support that goes with that. (Tougher restrictions are in place in Wales and Scotland, which have more left-leaning administrations.) The English are expected “merely” to work from home if possible, and to wear masks in shops, restaurants, and other indoor spaces. Nonetheless, on a weeknight last week, the center of Soho was eerily quiet. Many of the big theater shows have suffered closures because of the self-isolation rules. As soon as the news of Omicron reached Britain, Tube journeys fell by 38 percent. No one I know had an office Christmas party.In Britain, the era of formal business closures has been supplanted by something equally stressful: a nationwide guilt trip designed to ensure that we regulate our own behavior without the state handing out more cash. It reminds me of the constant nagging advice given to pregnant women. Of course you could have a drink, but why risk it? You could have some soft cheese, but why risk it? Why not just stay indoors, forever, until you die of old age and no one can say “I told you so”? In the pandemic, too, moral judgments are being passed off as medical ones. Why put the National Health Service in danger for the sake of your social life? (In America, the likelihood of a guilt trip varies by geography. You might not get dirty looks for shopping maskless in Florida, but the story is different in the bluest cities.)When I caught COVID at a completely legal restaurant dinner, I was surprised that I felt ashamed. Where did that sensation come from? Some leftish pundits seem to think that demanding anything beyond the essentials of life is immoral, and that any willingness to tolerate some risk (personal and society-wide) is reckless. Recently, a prominent San Francisco doctor wrote a long Twitter thread about his otherwise healthy, triple-vaccinated 28-year-old son, whose case of COVID was no worse than a nasty cold and was apparently caught from watching a movie with a vaccinated friend. To me, his apprehensive rundown of potential treatments and his admonishment that “even low risk stuff—things that were safe last [month]—may now be risky” felt like a dispatch from an alternate universe. How can people live with this level of fear?Yet to publicly question the current level of restrictions is to invite accusations from your more COVID-averse friends, or even strangers on social media, that you hate doctors, reject science, and actively want people to die. I am not anti-lockdown; I lived through three of them without a single illicit wine-and-cheese party or Christmas quiz, unlike senior members of the British government. I got my vaccines the minute I was allowed to. I wear a mask whenever doing so is mandated. But I’m done, profoundly done. Or at least I’m what my colleague Derek Thompson has called “Vaxxed and Cautious Until Omicron Burns Through and Then Prepared to Be Done.”And that’s because I am desperate for a party. So desperate I would attend an enemy’s book launch. So desperate I would attend an improv-comedy night. So desperate I would see an amateur production of Shakespeare. All of these things are technically legal and available in Britain, but the hassle of masks and the worry of last-minute dropouts mean that, in practice, most of my friends have cleared their diary indefinitely. And so I work and binge-watch, and work and doomscroll, and work and see the occasional friend for coffee or dinner, when what I really want to do is freely move about in the world again—even if that means plotting my escape from a man with mild halitosis telling me his plan to reinvigorate the great American novel, or standing in line for the ladies’ room and feeling annoyed. I want to see the people I see only twice a year in normal times. I want to gossip about acquaintances. I want to wear something with a real waistband.Perhaps you think this is shallow. You are right! Life should be shallow sometimes. I used the first lockdown to create a perfect cross-stitch replica of Hokusai’s Great Wave Off Kanagawa, and the third to read and discuss several of Shakespeare’s history plays with like-minded friends on Zoom. I cannot honestly say whether these activities made me a better person.Read: Omicron is pushing America into soft lockdownIf the health-care system needs more investment to deal with future winter COVID waves, hit me up for the taxes needed. In the meantime, we should simplify our public-health rules and wean ourselves off hygiene theater. Outdoor masking—which is still mandated in some European countries, such as Spain—is absurd. End it. The recent travel bans effectively punished South Africa for responsibly monitoring new variants, and should not be repeated. Because “‘post-vax COVID’ is a new disease,” isolation periods should be shortened to five days for the vaccinated, which would help hospitals and schools struggling with staff absences. Politicians should explain their planned endgame: Will we test and isolate forever, or will COVID one day be treated like the flu? Oh, and people should stop terrifying their children. Your kids will get COVID, if they haven’t had it already. Statistically, they will be fine. They are in more danger on the drive to school than from COVID once they get there.As for the vaccine-hesitant, I sympathize with those who are afraid of needles and with pregnant and breastfeeding women, for whom the advice has been inconsistent, but less so with those who think that the vaccines are untested or “experimental”—a plausible concern last year but not now—and those who object to “putting chemicals in their body,” unless they eat only homegrown food and wash their hair with soap nuts. Public policy should make life more irritating for the unvaxxed—or rather, consistently less irritating for the vaxxed. (Ten years of writing about technology and privacy has shown me that most people value convenience more highly than principles, although they won’t admit it.) On vaccines for health workers, I am implacable: You have no business working in medicine if you don’t believe in science. The American and European COVID vaccines went through rigorous clinical trials, before being used on hundreds of millions of people. If you can’t accept that evidence, no other evidence will sway you.The past two years have been tough, but the world has emerged from them with a formidable medical arsenal, as well as the knowledge that humanity has to live with COVID. Putting normal life into suspended animation back in the spring of 2020 was the right decision, but now I want to hear more about the thaw. There will be trade-offs, and there will be casualties, but you can’t remove all risk from human existence. Like most of us, I’ve followed the rules. I will continue to do so. But I really do want my life back.</description>
      <pubDate>12 Jan 22 09:42 EST</pubDate>
      <guid>https://www.theatlantic.com/ideas/archive/2022/01/i-want-my-life-back-fear-covid/621214/</guid>
    </item>
    <item>
      <title>How Do We Trust Our Science Code?</title>
      <link>https://www.hillelwayne.com/how-do-we-trust-science-code/</link>
      <description>&lt;a href=&#34;https://www.hillelwayne.com/how-do-we-trust-science-code/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In 2010 Carmen Reinhart and Kenneth Rogoff published Growth in a Time of Debt. It’s arguably one of the most influential economics papers of the decade, convincing the IMF to push austerity measures in the European debt crisis. It was a very, very big deal. In 2013 they shared their code with another team, who quickly found a bug. Once corrected, the results disappeared. Greece took on austerity because of a software bug. That’s pretty fucked up. Programming is now essential to scientific research. Pretty much every policy, econ, and sociology grad student I know has used Python or Excel at least once, to say nothing of the hard scientists out there. For most them coding is not interesting. Their job is research, not fighting weird computer bullshit. Knowing shell arcana is as little a part of their world as grantwriting is to me. They want to hammer out some code, get some results, and get back to their work. In 2015 a well-known fintech company accidentally introduced hundreds of mock applicants into their production database. For months their credit models were completely wrong and nobody noticed. This was a disciplined engineering team who documented carefully, practiced TDD, and ran rigorous code reviews. Even with all of that, serious bugs can slip through. Software engineers know this. How many bugs happen when you don’t use any of that? Hell, how many bugs happen when you don’t use functions, descriptive variables, or comments? So how do we fix this? No clue. Obviously we have the technical tools: tests, code reviews, etc. But how do you convince everyone to make a boring, frustrating part of their work more boring and frustrating for such a nebulous benefit? Most researchers don’t worry about this. Neither did Reinhart. This is a social problem, not a technical one. Which is why I don’t have a good answer. Watching engineers try to solve social problems is either tragic or hilarious, depending on how far you are from the explosion. The only thing I can think of is to have all papers include their source code. That way other people can help you find any bugs. Not a great solution, but at least it’s something. This is something a lot of researchers want and many are quite happy to share their code. There’s no way to do it simply and at scale, though. Every journal has different means of distributing code, if they require open code at all. I don’t think this would be an easy fix, or even a painless one.1 If you have any ideas, feel free tell me about them. Being paranoid about science isn’t fun. Thanks to Daiyi, Elliot Abrams, and Sasha Ayvazov for their feedback. </description>
      <pubDate>16 Feb 21 13:27 EST</pubDate>
      <guid>https://www.hillelwayne.com/how-do-we-trust-science-code/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/</link>
      <description>&lt;a href=&#34;https://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Quantized ColumnsWhat Does It Mean for AI to Understand?It’s simple enough for AI to seem to comprehend data, but devising a true test of a machine’s knowledge has proved difficult.Maggie Chiang for Quanta MagazineRemember IBM’s Watson, the AI Jeopardy! champion? A 2010 promotion proclaimed, “Watson understands natural language with all its ambiguity and complexity.” However, as we saw when Watson subsequently failed spectacularly in its quest to “revolutionize medicine with artificial intelligence,” a veneer of linguistic facility is not the same as actually comprehending human language. Natural language understanding has long been a major goal of AI research. At first, researchers tried to manually program everything a machine would need to make sense of news stories, fiction or anything else humans might write. This approach, as Watson showed, was futile — it’s impossible to write down all the unwritten facts, rules and assumptions required for understanding text. More recently, a new paradigm has been established: Instead of building in explicit knowledge, we let machines learn to understand language on their own, simply by ingesting vast amounts of written text and learning to predict words. The result is what researchers call a language model. When based on large neural networks, like OpenAI’s GPT-3, such models can generate uncannily humanlike prose (and poetry!) and seemingly perform sophisticated linguistic reasoning. But has GPT-3 — trained on text from thousands of websites, books and encyclopedias — transcended Watson’s veneer? Does it really understand the language it generates and ostensibly reasons about? This is a topic of stark disagreement in the AI research community. Such discussions used to be the purview of philosophers, but in the past decade AI has burst out of its academic bubble into the real world, and its lack of understanding of that world can have real and sometimes devastating consequences. In one study, IBM’s Watson was found to propose “multiple examples of unsafe and incorrect treatment recommendations.” Another study showed that Google’s machine translation system made significant errors when used to translate medical instructions for non-English-speaking patients. How can we determine in practice whether a machine can understand? In 1950, the computing pioneer Alan Turing tried to answer this question with his famous “imitation game,” now called the Turing test. A machine and a human, both hidden from view, would compete to convince a human judge of their humanness using only conversation. If the judge couldn’t tell which one was the human, then, Turing asserted, we should consider the machine to be thinking — and, in effect, understanding. Unfortunately, Turing underestimated the propensity of humans to be fooled by machines. Even simple chatbots, such as Joseph Weizenbaum’s 1960s ersatz psychotherapist Eliza, have fooled people into believing they were conversing with an understanding being, even when they knew that their conversation partner was a machine. In a 2012 paper, the computer scientists Hector Levesque, Ernest Davis and Leora Morgenstern proposed a more objective test, which they called the Winograd schema challenge. This test has since been adopted in the AI language community as one way, and perhaps the best way, to assess machine understanding — though as we’ll see, it is not perfect. A Winograd schema, named for the language researcher Terry Winograd, consists of a pair of sentences, differing by exactly one word, each followed by a question. Here are two examples: Sentence 1: I poured water from the bottle into the cup until it was full. Question: What was full, the bottle or the cup? Sentence 2: I poured water from the bottle into the cup until it was empty. Question: What was empty, the bottle or the cup? Sentence 1: Joe’s uncle can still beat him at tennis, even though he is 30 years older. Question: Who is older, Joe or Joe’s uncle? Sentence 2: Joe’s uncle can still beat him at tennis, even though he is 30 years younger. Question: Who is younger, Joe or Joe’s uncle? In each sentence pair, the one-word difference can change which thing or person a pronoun refers to. Answering these questions correctly seems to require commonsense understanding. Winograd schemas are designed precisely to test this kind of understanding, alleviating the Turing test’s vulnerability to unreliable human judges or chatbot tricks. In particular, the authors designed a few hundred schemas that they believed were “Google-proof”: A machine shouldn’t be able to use a Google search (or anything like it) to answer the questions correctly. These schemas were the subject of a competition held in 2016 in which the winning program was correct on only 58% of the sentences — hardly a better result than if it had guessed. Oren Etzioni, a leading AI researcher, quipped, “When AI can’t determine what ‘it’ refers to in a sentence, it’s hard to believe that it will take over the world.” However, the ability of AI programs to solve Winograd schemas rose quickly due to the advent of large neural network language models. A 2020 paper from OpenAI reported that GPT-3 was correct on nearly 90% of the sentences in a benchmark set of Winograd schemas. Other language models have performed even better after training specifically on these tasks. At the time of this writing, neural network language models have achieved about 97% accuracy on a particular set of Winograd schemas that are part of an AI language-understanding competition known as SuperGLUE. This accuracy roughly equals human performance. Does this mean that neural network language models have attained humanlike understanding? Not necessarily. Despite the creators’ best efforts, those Winograd schemas were not actually Google-proof. These challenges, like many other current tests of AI language understanding, sometimes permit shortcuts that allow neural networks to perform well without understanding. For example, consider the sentences “The sports car passed the mail truck because it was going faster” and “The sports car passed the mail truck because it was going slower.” A language model trained on a huge corpus of English sentences will have absorbed the correlation between “sports car” and “fast,” and between “mail truck” and “slow,” and so it can answer correctly based on those correlations alone rather than by drawing on any understanding. It turns out that many of the Winograd schemas in the SuperGLUE competition allow for these kinds of statistical correlations. Rather than give up on the Winograd schemas as a test of understanding, a group of researchers from the Allen Institute for Artificial Intelligence decided instead to try to fix some of their problems. In 2019 they created WinoGrande, a much larger set of Winograd schemas. Instead of several hundred examples, WinoGrande contains a whopping 44,000 sentences. To obtain that many examples, the researchers turned to Amazon Mechanical Turk, a popular platform for crowdsourcing work. Each (human) worker was asked to write several sentence pairs, with some constraints to ensure that the collection would contain diverse topics, though now the sentences in each pair could differ by more than one word. The researchers then attempted to eliminate sentences that could allow statistical shortcuts by applying a relatively unsophisticated AI method to each sentence and discarding any that were too easily solved. As expected, the remaining sentences presented a much harder challenge for machines than the original Winograd schema collection. While humans still scored very high, neural network language models that had matched human performance on the original set scored much lower on the WinoGrande set. This new challenge seemed to redeem Winograd schemas as a test for commonsense understanding — as long as the sentences were carefully screened to ensure that they were Google-proof. However, another surprise was in store. In the almost two years since the WinoGrande collection was published, neural network language models have grown ever larger, and the larger they get, the better they seem to score on this new challenge. At the time of this writing, the current best programs — which have been trained on terabytes of text and then further trained on thousands of WinoGrande examples — get close to 90% correct (humans get about 94% correct). This increase in performance is due almost entirely to the increased size of the neural network language models and their training data. Have these ever larger networks finally attained humanlike commonsense understanding? Again, it’s not likely. The WinoGrande results come with some important caveats. For example, because the sentences relied on Amazon Mechanical Turk workers, the quality and coherence of the writing is quite uneven. Also, the “unsophisticated” AI method used to weed out “non-Google-proof” sentences may have been too unsophisticated to spot all possible statistical shortcuts available to a huge neural network, and it only applied to individual sentences, so some of the remaining sentences ended up losing their “twin.” One follow-up study showed that neural network language models tested on twin sentences only — and required to be correct on both — are much less accurate than humans, showing that the earlier 90% result is less significant than it seemed. So, what to make of the Winograd saga? The main lesson is that it is often hard to determine from their performance on a given challenge if AI systems truly understand the language (or other data) that they process. We now know that neural networks often use statistical shortcuts — instead of actually demonstrating humanlike understanding — to obtain high performance on the Winograd schemas as well as many of the most popular “general language understanding” benchmarks. The crux of the problem, in my view, is that understanding language requires understanding the world, and a machine exposed only to language cannot gain such an understanding. Consider what it means to understand “The sports car passed the mail truck because it was going slower.” You need to know what sports cars and mail trucks are, that cars can “pass” one another, and, at an even more basic level, that vehicles are objects that exist and interact in the world, driven by humans with their own agendas. All this is knowledge that we humans take for granted, but it’s not built into machines or likely to be explicitly written down in any of a language model’s training text. Some cognitive scientists have argued that humans rely on innate, pre-linguistic core knowledge of space, time and many other essential properties of the world in order to learn and understand language. If we want machines to similarly master human language, we will need to first endow them with the primordial principles humans are born with. And to assess machines’ understanding, we should start by assessing their grasp of these principles, which one might call “infant metaphysics.” Training and evaluating machines for baby-level intelligence may seem like a giant step backward compared to the prodigious feats of AI systems like Watson and GPT-3. But if true and trustworthy understanding is the goal, this may be the only path to machines that can genuinely comprehend what “it” refers to in a sentence, and everything else that understanding “it” entails. </description>
      <pubDate>16 Dec 21 12:01 EST</pubDate>
      <guid>https://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/</guid>
    </item>
    <item>
      <title></title>
      <link>https://jakobgreenfeld.com/essays/</link>
      <description>&lt;a href=&#34;https://jakobgreenfeld.com/essays/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; What really matters is getting into the right state of mind It is futile to tell people to mimic outward manifestations. Read More I need to stop sabotaging myself Each of these self-sabotaging actions is just a symptom, not the root cause. And to really solve the problem, I need to tackle this root cause, not just fight the symptoms. Read More Playing the entrepreneurial game with cheat codes Right now a seismic shift is happening. The wall that divides influencers and entrepreneurs comes tumbling down and a whole new spectrum between these two extremes starts to emerge. Read More Overcoming limiting scripts through agency expanding experiments The mind, once stretched by a new idea, never returns to its original dimensions. Read More The simple system I’m using to stay in touch with hundreds of people Staying in touch with people is one of these asymmetric habits that require little effort, time and resources but has an unlimited upside. Read More How to win the metagame (in real life) This is how you stay ahead of the curve and ultimately win. Read More It&#39;s time to win Step 1 was to leave the sidelines, put myself out there and join the fun. But what’s Step 2? Read More Effortless personal productivity (or how I learned to love my monkey mind) I recently discovered a simple step-by-step process that significantly increased my personal productivity and made me happier along the way. Read More How to add a LemonSqueezy checkout overlay to any Carrd site First we need the LemonSqueezy overlay code. We can find it by clicking on “Share” next to our product’s name. Read More Why I write If you don’t use tools to make sure your actions are aligned with your long-term goals, you’ll get off course in no time. Sure, you’ll still be moving. But you won’t make any progress towards goals you care about. Read More Why I don&#39;t think where I work Unlike in a normal job, the hours I sit at my desk don’t matter. All that counts is my output. Read More Don&#39;t read this Why did you open this essay? Do you even have a reason? Did you even think about it? I bet the answer is no. And that, my friend, is a problem. Read More Breaking Mimetic Chains I know zero people personally who want to change the world. I don’t know anyone in real life who is on a non-standard path. No one who wants more out of life than anyone else and is fighting tooth and nails for their dream. Read More Write raw Here’s a dirty, little secret: my essays are all really just stream-of-consciousness journal entries. Read More I&#39;m awake I’ve been asleep. Drifting. Pushing buttons for 10 hours every day but not accomplishing anything. Read More Build a business, not an audience If you’re reading this, I’m pretty sure you’ve seen the following pattern over and over again: Read More How management by metrics leads us astray Let’s say my goal this year is to get to 10k monthly recurring revenue (MRR). Read More What I really learned at university You know the “And All I Got Was This Lousy T-Shirt” meme right? My personal variant is: Read More What’s the point of free if you have nothing to sell Two days ago Corey Haines shared the following framework. Read More How I created a #2 Product of the Day in 90 minutes I know, the headline would be much better if it said Product of the Day. But while What to Tweet was at the #1 spot on Product Hunt almost all day long, it was eventually pushed to the second spot near the end of the day when Almanack got hundreds of additional upvotes, while my project only gained upvotes at a steady rate. Read More How a tiny badge led to dozens of sales and hundreds of new followers Oh boy, am I glad that I trusted my gut feeling. Read More What I learned from Sam Parr about cold outreach Sam Parr isn’t that big of a deal. But still, when he shared his screen it showed almost a thousand unread emails and hundreds of Twitter notifications. So this is what you’re competing with if you want to connect with people like him. It’s a noisy world out there. Read More 6 Lessons I Learned from Nathan Latka I consider Nathan to be one of my mentors even though I’ve never talked to him. Read More The Grand Unified Theory of Product Ideation In short, effective methods to come up with product ideas can be categorized along two dimensions: Read More Review and Summary of Start Small, Stay Small by Rob Walling Most of the books that I want to read during my Bootstrap MBA experiment focus on just one specific topic. This makes sense because book that cover too many topics usually don’t go deep enough to be useful. However, I thought that it wouldn’t hurt to read at least one “general purpose” book that covers a lot of ground and thus can give me some orientation at the beginning of my journey. Initially, I decided to read Pieter Level’s book MAKE which is subtitled “The Bootstrapper’s Handbook” and has gathered a lot of praise in the maker community. Read More Review and Summary of MAKE by Pieter Levels One of the first books I read on bootstrapping is MAKE by Pieter Levels. Pieter promises to share the techniques that allowed him to build two highly successful products (Nomad List and RemoteOK) that bring in more than $80.000 each month. In addition to sharing actionable advice he wants to encourage others to try the bootstrapped way of building businesses. In his words: Read More Making sense of Ruby on Rails Part 1 - Routes, Actions, Controllers, and Views I recently decided that I want to learn Rails. Since everyone keeps raving about it, I used Michael Hartl’s Rails Tutorial. While the tutorial was helpful, I felt as if something immensely important was missing from it. Read More Choosing a tech stack and making sense of the web devlopment landscape (from a bootstrap entrepreneurial perspective) I’ve wanted to learn how to develop proper web apps for several years now. But what has stopped me so far is that there are so many options, and I was never sure what exactly I should learn. In particular, what I’ve been missing is a proper overview of the various players in the field and how they interact. Read More Build your personal self-invention machine Most people treat their personal blogs with a wrong attitude. If you think of it as a self-promotion machine, chances are high that you’ll quickly be demotivated because not enough people read your stuff and most readers won’t enjoy reading your articles. Read More I&#39;ve decided to pursue a Bootstrap MBA My next twelve months will be dedicated to a single meta-framework that I call the Bootstrap MBA. This name makes sense for two reasons: Read More Coming up with a learning challenge as an aspiring entrepreneur A huge challenge everyone who decides to be self-employed needs to face is that there are suddenly no constraints. At school, university, or work, there are always deadlines and you had to carry pre-specified work using a pre-defined toolbox. But if you’re self employed, you can build anything, work on any project, using any tool, framework, or programming language. Read More The Insight Epidemic Information overload is not a real problem. Information is just prettified data and only slightly less boring. Hence, almost no one really cares about information and whether or not there’s too much of it. Read More The Broken Window Theory of Productivity The broken window theory: “Social psychologists and police officers tend to agree that if a window in a building is broken and is left unrepaired, all the rest of the windows will soon be broken. [Such visible signs of crime] create an urban environment that encourages further crime and disorder, including serious crimes.” Read More Bet on Trends As I’ve mentioned in my humble business ideation article, one of the best strategies for humble entrepreneurs without a huge marketing budget is to bet on trends. Big companies are typically too slow to move quickly enough. And if a market is still small when you enter it, it will be much easier to get the attention of customers. When the market then explodes, your business will grow accordingly. This way, “the market you’re in will determine most of your growth,” to quote Sahil Lavingia. Read More Sell pickax to gold miners and heads and handles to the pickax sellers The easiest way to convince someone to pay for your product is by arguing how much more money they’ll earn or save by using your solution. If your product costs $30 per month and each customers makes or saves on average $300 through your product, the buying decision becomes a no-brainer. Read More Need business ideas? Watch out for unbundlings Society moves in waves. It always starts with lots of separate things. Then they get bundled. After a while, more and more people start to realize that the bundled solution is too bloated and not specific enough for their needs. This is when an unbundling happens. Read More Follow the Money and Turn Costs into Revenue Don’t look for clever ideas and don’t ask people what they want. Instead, just follow the money. Read More Code and design are no longer bottlenecks The number one reason that’s stopping aspiring software entrepreneurs is the thought that they’ll eventually have to build the product they envision and have no idea how to make it happen. Read More My Principles and Practices of Personal Education One area of my life I struggled with for a long time is education. I love to learn but what I’ve been doing for a long time was far from systematic. I picked books at random. I mostly restricted myself to purely passive reading. I didn’t interact with other people that are interested in the same topic. Read More The different kinds of humble businesses you can build Let’s say you’ve successfully found an idea to solve a painful problem in a healthy market with some but not too much competition. What’s next? Read More Humble Businesses vs. Startups Before we can generate and evaluate business ideas, we need to answer the question: what type of business do you want to build? This question is important because many ideas are not bad per se. Instead, they’re just bad if you want to build a particular kind of business. Read More </description>
      <pubDate>14 Feb 22 12:02 EST</pubDate>
      <guid>https://jakobgreenfeld.com/essays/</guid>
    </item>
    <item>
      <title>Against an Increasingly User-Hostile Web</title>
      <link>https://neustadt.fr/essays/against-a-user-hostile-web/</link>
      <description>&lt;a href=&#34;https://neustadt.fr/essays/against-a-user-hostile-web/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; We&#39;re quietly replacing an open web that connects and empowers with one that restricts and commoditizes people. We need to stop it. - Parimal Satyal, 2 november 2017. See also: my Paris Web 2020 talk based on this article and the Hacker News discussion threads from 2017 and from 2020. I quit Facebook seven months ago. Despite its undeniable value, I think Facebook is at odds with the open web that I love and defend. This essay is my attempt to explain not only why I quit Facebook but why I believe we&#39;re slowly replacing a web that empowers with one that restricts and commoditizes people. And why we should, at the very least, stop and think about the consequences of that shift. The Web: Backstory (If you want, you can skip the backstory and jump directly to the table of contents). I love the web. I don&#39;t mean that in the way that someone might say that they love pizza. For many of us in the early 2000s, the web was magical. You connected a phone line to your computer, let it make a funny noise and suddenly you had access to a seemingly-unending repository of thoughts and ideas from people around the world. It might not seem like much now, but what that noise represented was the stuff of science fiction at the time: near-instantaneous communication at a planetary scale. It was a big deal. I was an average student at school. Despite well-meaning and often wonderful teachers, I didn&#39;t thrive much in a school system that valued test performance and fact-retention over genuine curiosity. Had it not been for the web, I might have convinced myself that I was a poor learner; instead, I realized that learning is one of my great passions in life. What remains of my fan site for German powermetal band Gamma Ray from 2001, archived thanks to the wonderful folks over at Archive.org I was 11 when I set up my first website. Growing up in Nepal, this was magical. Almost everything I love today—design, aviation, cosmology, metal music, computation, foreign languages, philosophy—I discovered through the many pages that found their way to my web browser. All I needed were curiosity, a phone line and that strange little electrical song. And good old Netscape Navigator. Netscape Navigator 4.04, source: A Visual Browser History, from Netscape 4 to Mozilla Firefox The web enabled that. It&#39;s one of humanity&#39;s greatest inventions. And now, we the architects of the modern web—web designers, UX designers, developers, creative directors, social media managers, data scientists, product managers, start-up people, strategists—are destroying it. We&#39;re very good at talking about immersive experiences, personalized content, growth hacking, responsive strategy, user centered design, social media activation, retargeting, CMS and user experience. But behind all this jargon lurks the uncomfortable idea that we might be accomplices in the destruction of a platform that was meant to empower and bring people together; the possibility that we are instead building a machine that surveils, subverts, manipulates, overwhelms and exploits people. It all comes down a simple but very dangerous shift: the major websites of today&#39;s web are not built for the visitor, but as means of using her. Our visitor has become a data point, a customer profile, a potential lead -- a proverbial fly in the spider&#39;s web. In the guise of user-centered design, we&#39;re building an increasingly user-hostile web. If you work in the design/communication industry, consider this essay introspective soul-searching by one of your own. If you&#39;re a regular web user, consider this an appeal to demand a better web, one that respects you instead of abusing and exploiting you. Note: The entire essay is rather long so feel free to skip to individual parts: The Web was Born Open: a very brief history of the web The Modern Web (of Deception): the disturbing state of the web today Track the Trackers, an Experiment: with whom websites are sharing your information Gated Communities: recentralization and closed platforms The Way Forward: open tools, technologies and services for a better web The Web was Born Open It all began in the early 90s. The Internet—the physical network that allowed computers around the world to communicate—was already in place but it remained inaccessible to most people. You had to know how to use a local client to connect to a remote FTP, Usenet, Gopher or an email server. This was before the days of ubiquitous graphical user interfaces so you had to type funny commands into a terminal, one of those black screens with green text that that hackers supposedly use to do Bad Things. Usenet Archives from 1981 on gopher server Quux.org, accessed 31 October 2017 via lynx Meanwhile, Tim Berners-Lee was working as an independent contractor at CERN in Geneva. Frustrated with how difficult it was to find, organize and update technical documentation, he proposed a solution that involved &#34;global computer networked information system&#34; that &#34;presented users with a web of interlinked documents&#34;, called Mesh. Pretty soon it became apparent that WWW—World Wide Web, as it came to be known—could do more than just link technical documents. The world&#39;s first website, accessed 31 October 2017 via lynx On April 30 1993, CERN made a bold decision. It decided to release WWW into the public domain. It renounced all intellectual property rights and essentially invited anyone at all, anywhere in the world, to play with it. Later, the director of CERN who approved the decision said that he was inspired by Richard Stallman&#39;s vision of free, open software. Had CERN decided otherwise and patented the technology to then license it for money, the web would arguably not have taken off the way it did. It might have died out like the Minitel did in France. The web as we know it was born of a vision to create an open system that brought people and ideas together, with documents that &#34;may reside on any computer supported by that web&#34;. Advances in the hyper-text transfer protocol (HTTP), network infrastructure, web browsers and standards, consumer Internet access, accessible hosting and blogging platforms led to a massive democratization and adoption of the web. Soon, anyone could put a document on the web and any document could link to any other. It created a completely open platform where a writer in Nepal could freely share her ideas with a dancer in Denmark. A climate science student in Nairobi could access data from the McMurdo weather station in Antarctica. You could start reading about logical fallacies and end up on a website about optical illusions. Read about the history of time-keeping and end up learning about Einstein&#39;s special theory of relativity. All interests were catered to. Information could truly be free: transverse borders, cultures and politics. That is the web at its best. My own journey from designing that first website as an 11-year old &#34;webmaster&#34; in Nepal to writing this article as a UX Consultant in France has its origin in that 1993 decision by CERN. The Modern Web (of Deception) The modern web is different. It&#39;s naturally different from a technological standpoint: we have faster connections, better browser standards, tighter security and new media formats. But it is also different in the values it espouses. Today, we are so far from that initial vision of linking documents to share knowledge that it&#39;s hard to simply browse the web for information without constantly being asked to buy something, like something, follow someone, share the page on Facebook or sign up to some newsletter. All the while being tracked and profiled. Almost every website you go to today reports your activities to third parties that you most likely neither know nor trust. They record where you come from, what pages you visit, how long you stay on each, where you click and where you go next. In fact, since so many websites report to the same third parties, these companies can essentially have your web history on file as you go from link-to-link, website to website. Like an omnipotent eye embedded on Sir Berners-Lee&#39;s global system of interlinked documents, noting down everything you do and reporting to private entities who then sell this information for profit. These companies build profiles, anonymous at first, with your interests and navigational behavior. These profiles can then get increasingly personal: they might include your email addresses, home address, income, educational history, political affiliation, information on your family. Over time, they can cross-reference all this information with your location data to figure out where you work, which restaurants you go to, where your gym is. Recently, we even learned that Google was able to associate your offline purchases with your online ad viewing history (albeit anonymously, it would appear). Once they have that, they can look into your behavior and psychology: what kind of ads do you tend to click on? What kind of messages resonate most with you? What are the best strategies to influence your opinion? Screenshot of Mr. Alexander Nix presenting the work of Cambridge Analytica, video The Power of Big Data and Psychographics on Youtube The Leave campaign responsible for Brexit in the United Kingdom and Donald Trump&#39;s 2016 presidential campaign both bought the services of a certain Cambridge Analytica, a company that boasts a gigantic database containing personal details amounting to &#34;close to four or five thousand data points on every adult in the United States&#34; (their own words). The goal? Craft hyper-personalized messages to change voting behavior based on your individual personalities, and by extension, your attitudes, opinions and fears. So if you are identified as a dad of three young kids in rural Texas, the message is nuanced to suggest that only a certain candidate will be able to protect your family against real or imagined threats. If you are identified as a patriot who&#39;s previously posted comments about gun rights and the second amendment, it might be about crime rates and how the opposition is trying to take your constitutional rights away from you. You become a manipulable data point at the mercy of big corporations who sell their ability to manipulate you based on the data you volunteer. This is the equivalent of someone following you in real life as you go about your everyday business, like a private eye who notes down with whom you meet, what you talk about, what you spend time looking at in stores. A private eye who takes notes and then sells it to the highest bidder. But you got to enter the store for free, so you should be so glad. The stores might also justify it. &#34;Sure it&#39;s a bit invasive, but we&#39;ll be able to give you better recommendations if we know what you like&#34;. But how do they get all this personal information -- where you live, who your friends are, what your religion and ethnicity are, where you were last night, what you bought on Monday? Most of it you volunteer yourself on social platforms like Facebook, Twitter and Instagram. The little share buttons you see on websites aren&#39;t just there to make it easy for you to post a link to Facebook; they also allow Facebook to be present and gather information about you from pretty much any website. But how can you know that any of this is true? Track the Trackers: An Experiment Perhaps you think I&#39;m being a tad too dramatic. In your defense, all of this does sound like some dystopian fantasy. But I&#39;m not that great a fiction writer quite yet. Let me illustrate my point with a little experiment. We&#39;ll pick a major website that you might visit regularly and identify third parties it shares your information with. We&#39;ll need a few things: a test website Webbkoll, a web privacy check tool by Dataskydd.net, a Swedish association for data protection and privacy (of which I&#39;m a proud member) and A web inspector Let&#39;s take an article that was published around the time I first started working on this article (which is last year; I&#39;m a slow writer): Astronomie : la sonde Juno s’est mise en orbite autour de Jupiter (Astronomy: space probe Juno put in orbit around Jupiter). Le Monde article Astronomie : la sonde Juno s’est mise en orbite autour de Jupiter If you run this URL through Dataskydd&#39;s Webbkoll and a web inspector tool (I used Chromium&#39;s web inspector), you learn a few interesting things: the page is 3.1 MB in size, makes about 460 HTTP requests of which 430 are third-party requests (outside of its parent domain) and takes 20 seconds to fully load on a fast 3G connection (from Paris, France). It also stores 100 cookies (these are little pieces of text stored on your computer by websites other than lemonde.fr; cookies are normally used to save session information but can also be used to identify and track you) and contacts 118 third-parties. And if all this weren&#39;t enough, your connection to LeMonde and the majority of third-party connections are over unsecure HTTP protocol (instead of the more secure HTTPS, which should be a basic requirement). That&#39;s a lot of big numbers for an article of 1500 words, three images and one video. Now let&#39;s look at some of the third parties that the page connects to when you load it: Weborama: advertising platform for analytics, digital marketing and behavioral targeting Visual Revenue: predictive analytics platform AppNexus: multimedia content monetization service Outbrain: &#34;online advertiser specializing in presenting sponsored website links&#34; (Wikipedia) Facebook: a social network and micro-targeted advertising platform Cedexis: a multi-CDN application delivery platform Note: In an earlier version of the article, I had mistakenly identified Cedexis as an &#34;ad-delivery platform&#34;, which it is not. My apologies to Cedexis for the error. Some of these are simply tools to manage content delivery but many are advertising or content monetization platforms. Companies like Weborama make money by selling information about you. When people say, &#34;you&#39;re the product,&#34; it isn&#39;t just some analogy, it accurately reflects the business propositions of many such companies. What&#39;s surprising is that the bulk of the information transferred between LeMonde and you doesn&#39;t even concern the actual article. If you were to isolate the actual content—the words, images and video—and put it in an HTML file, it would weigh considerably less than 3.1 MB and would make a lot fewer requests. If fact, I did just that and made three versions : Version A: With the original text (including comments, images and video) Version B: With the original text (including comments, images) but no video Version C: With just the original text (including comments), no images or video Some numbers: Original (LeMonde.fr) Version A Version B Version C Page Size 3,1 MB 1 MB (32%) 183 KB (5,8%) 17 KB (0,54%) Load Time 20,9 s 4,6 s (19,4%) 2,8 s (9,6%) 662 ms (3,2%) Requests (total) 459 108 (23,5%) 5 (1%) 1 (0,2%) Requests (third-party) 436 64 (14,7%) 4 (0,9%) 0 Third Parties Contacted 118 17 (14,4%) 2 (11,8%) 0 Cookies (total) 100 16 (16%) 0 0 Cookies (third-party) 73 16 (21,9%) 0 0 Text (% of Page Size) 0,5 % 1,7 % 9,5 % 100 % Text + Images (% of Page Size) 5,8 % 17,9 % 100 % Text + Images + Video (% of Page Size) 32,3 % 100 % Note: Data on the number of requests (first- and third-party) and cookies (first- and third-party) comes from Dataskydd Webbkoll. The rest of the data comes from Chromium&#39;s built-in web inspector. All connections were made from Paris, France with cacheing disabled and the bandwidth throttled to simulate a &#34;fast 3G&#34; connection. You can run these numbers yourself; they should vary only nominally depending on where you are. If you find errors, please let me know. Those are some very interesting figures. Some observations: The actual article (text and three images, version B) makes up less than 6% of the total size of the page on LeMonde.fr. This means that 94% of the data transferred between you and LeMonde.fr has nothing to do with the article. What about the video, you ask? Before you even play it, that one video adds over a 100 requests (60 of which are to 15 additional third parties) and 16 third-party cookies. It also adds over 800 KB of data. Again, this is before you even decide to play the video. The video might be related to the content, but it’s doing a lot more than that. Even compared to the version with the video (Version A), the LeMonde article makes about 450 additional third party requests, of which 370 are to about 100 additional third parties, storing 100 additional cookies (55 of which are third party cookies). It also adds over 2 MB to the page. All that is data that has nothing do with and is completely unnecessary to load the article you&#39;re reading. The text + image version (Version B) is able to load the entire text and the 3 images with only 5 requests and no cookies whatsoever. Adding a video should reasonably add one or two more requests and maybe one cookie, not 450 requests and 100 cookies, the majority of which are on behalf of companies you neither know nor trust, including those who track and sell your data for profit. The Le Monde page will continue to periodically transfer data and make additional requests even after it has completely loaded and as you scroll and interact with the page. If you monitor network traffic, a lot of this data is going to third-party tracking scripts. For example, a request is made to Xiti.com (a web analytics company) every few seconds. If you don&#39;t use a content blocker, you will notice that in just a matter of minutes, over 30 MB of data will be transfered between your browser and the 100+ third parties. The number of requests will go into the thousands. This will continue to rise as long as you leave your browser open. Essentially, this means that about 94% of the data being transferred and 99% of the requests being made have nothing to do with the article itself. Le Monde might principally be a newspaper in its printed version, but the online version is an invasive, insecure advertising platform with good content (in that order). If you&#39;re curious, try using Webbkoll on other websites you visit to see how privacy-friendly and respectful these websites are. We&#39;ll get into how to protect yourself from these third-party trackers later on in the article. All this might not be illegal (although there&#39;s some doubt, especially now that in the context of up the upcoming European General Regulation on Data Protection), but it is rather disrespectful towards the user. Not only are these websites breaking my trust—when I visit your website, I entered into contact with you, not 80 other websites—but they are loading content from websites neither know nor trust. Some of which have been know to spread malware. Using an ad/content-blocker isn&#39;t cheating the system; it&#39;s taking very basic precautions that websites like Le Monde can&#39;t be bothered to take to protect you. For me, it&#39;s a basic necessity in the modern web. If you&#39;re reading this and are wondering what to do to protect yourself, skip ahead to the The Way Forward section. If you run a website and you put official share buttons on your website, use intrusive analytics platforms, serve ads through a third-party ad network or use pervasive cookies to share and sell data on your users, you&#39;re contributing to a user-hostile web. You&#39;re using free and open-source tools created by thousands of collaborators around the world, over an open web and in the spirit of sharing, to subvert users. Gated Communities One of the most impressive things about the Internet (and consequently also the web) is that it is decentralized. No central authority gets to decide which page is more important than others and you don&#39;t have to play by anyone else&#39;s terms to publish and read what you want. There isn&#39;t anything like a main server that stores the code that runs the Internet; it&#39;s just a protocol on a physical backbone (of undersea cables). You could buy a Raspberry Pi Zero today for less than 10€, connect it to the Internet, set up a chat server on it, give it a public address and the world would be able to connect to it and talk to one other. Sure, it might not perform too well and no one might actually use it, but it is technically possible. But most of the time we spend on the web today is no longer on the open Internet - it&#39;s on private services like Facebook, Twitter and LinkedIn. While Facebook provides a valuable service, it is also a for-profit, company. Their source of revenue is advertising. It is the epitome of centralized. Francisco Goya&#39;s The Naked Maja (1800) Try posting a picture of the Francisco de Goya&#39;s &#34;The Naked Maja&#34; or your naked breasts (if you&#39;re a woman) on Facebook; it&#39;ll almost certainly be removed. It&#39;s against their terms of use. To use their platform, you have to agree to whatever conditions they set, however absurd. If you replace the open web with Facebook, you&#39;re giving up your right to publish and share on your terms. The data that you post there does not belong to you; you&#39;re putting it in a closed system. If one day Facebook decides to shut down—unlikely as that might seem today—your data goes with it. Sure, you might be able to download parts of it, but then what? Tumblr Blog Our Incredible Journey, &#34;cataloging the thrilling opportunities start-ups are offered when their incredible journey continues by being bought by an exciting company. However, as a user of the start-up’s service, your own incredible journey must end, because all of your photos and writing and checkins and messages and relationships must now be deleted&#34;. This works because they know you&#39;ll agree to it. You&#39;ll say you don&#39;t have a choice, because your friends are all there—the infamous &#34;network effect&#34;. This is Facebook&#39;s currency, its source of strength but also a crucial dependency. And this is what we often fail to realize: without its users—without you— Facebook would be nothing. But without Facebook, you would only be inconvenienced. Facebook needs you more than you need it. And they do their best to keep you on their website as long as possible. Your attention is worth a lot to a lot of companies who are convinced that traditional advertising is dead and that micro-targeted campaigns work better. (And they mostly do, from their point of view). This drives them to come up with absurd techniques to create addiction: wish your friend happy birthday, wish your colleague a happy work anniversary (who does that?), here&#39;s a video we made about you, three friends are going to an event near you, continue watching the video you started even as you scroll, be the first to comment, react to this photo, tell everyone what you&#39;re up to. The longer you stay, the more information you give, the more valuable your profile—and the platform—is to advertisers. I&#39;m not saying that what Facebook is doing is entirely unethical. It has to make money to make up for the resources it employs to keep the website running and it does so by advertising. Every time you choose to use a free service like Instagram, LinkedIn, Gmail or Snapchat, you are paying for the convenience with your eyes, your data and your attention. There&#39;s nothing inherently wrong as long you as you understand and consent to this exchange of value. But do you? Does your daughter? Your dad? What I&#39;m against is the centralization of services; Facebook and Google are virtually everywhere today. Through share buttons, free services, mobile applications, login gateways and analytics, they are able to be present on virtually every website you visit. This gives them immense power and control. They get to unilaterally make decisions that affect our collective behavior, our expectations and our well-being. You&#39;re either with them or out. Well, I chose out. You see, the web wasn&#39;t meant to be a gated community. It&#39;s actually pretty simple. A web server, a public address and an HTML file are all that you need to share your thoughts (or indeed, art, sound or software) with anyone in the world. No authority from which to seek approval, no editorial board, no publisher. No content policy, no dependence on a third party startup that might fold in three years to begin a new adventure. A website on Doom level design on Geocities from 1999, accessed October 31, 2017 via Archive.org That&#39;s what the web makes possible. It&#39;s friendship over hyperlink, knowledge over the network, romance over HTTP. In fact, the browser you&#39;re reading this on (Chrome, Firefox, lynx, whatever), the web server that&#39;s hosting this website (Nginx), the operating system that this server runs on (Ubuntu), the programming tools used to make it all work (python, gcc, node.js...) -- all of these things were created collectively by contributors all around the world, brought together by HTTP. And given away for free in the spirit of sharing. The web is open by design and built to empower people. This is the web we&#39;re breaking and replacing with one that subverts, manipulates and creates new needs and addiction. The Way Forward If you want to protect yourself (as a user) from predatory web marketing companies and defend the open web, there a few things you can do today at an individual level. If you&#39;re a web professional (a designer, UX consultant, strategist, programmer...), there are a number of considerations for better respecting your users and protecting their privacy (and your integrity). Here&#39;s a basic list: For end users (you, dear reader) If you use Chrome as your main browser, consider switching to the open-source version called Chromium. Better yet, switch to Mozilla Firefox, developed by the not-for-profit Mozilla Foundation that has a solid record of defending your privacy. Consider minimalist browsers like Min (and choose to block all ads, trackers and scripts) to browse news websites. Install a content/ad blocker for your browser: I recommend uBlock Origin (available for Firefox, Chrome and Safari on most platforms). You can also complement this with the Electronic Frontier Foundation&#39;s Privacy Badger tool that protects you from invasive ads and third-party tracking. Install HTTPS Everywhere for your browser; this forces your information through secure, encrypted channels (HTTPS vs HTTP one) if possible. It can also be configured to only allow connections to HTTPS websites. Think about how much information/details you provide to social media platforms like Facebook, LinkedIn, Twitter and Instagram. They already have quite a lot (including the ability to recognize you by name on photographs), but what other information are you volunteering? Where you are, whom you&#39;re with, information about your friends? Consider quitting social networks, especially Facebook (but download your data first!). What would you miss the most? Are there alternatives? Consider alternatives to free services provided by the likes of Google and Facebook. Today, if both of these companies shut down (or implement policies I don&#39;t like), I would mostly be fine because my contact with them is limited. I use DuckDuckGo and Startpage for search (free); FastMail for email and calendar (less than 40€ a year) ; HERE WeGo for maps (free); Signal, email and IRC for messaging (free, along with iMessage, Whatsapp and Twitter); Digital Ocean for web hosting (about 5€ per month). Pay for services and content that you like, if you are able. If you like reading The Guardian, for example, consider subscribing. If your favourite YouTube channel is on Patreon, consider pledging a small amount per video. If you like services like Pinboard.in that charge in return for a useful service, buy it. There&#39;s mutual respect when both the user and the service provider know what basic service they are buying/selling. At the very least, consider that the platforms you use need you more than you need them. You have power over them (unfortunately, in numbers) and they know it. If enough people care about privacy and respect for their data and time, platforms will have to adapt to stay relevant. For web professionals (you, fellow industry colleague) Consider not putting share buttons everywhere. They&#39;re visual noise and make third party connections every time the page is loaded (adding to load time). If you have to, create your own instead of using ones provided by Facebook and co. (so that a click is needed before a request is made to their servers) Support HTTPS. It&#39;s super easy (and free!) with Let&#39;s Encrypt so you don&#39;t have an excuse to not respect your users&#39; privacy Think about accessibility also in terms of page size, load times and tech requirements: will your website work without Javascript? What percentage of the total weight of your page is actual information? How many third party requests are you making? How long would it take to load on a 56.6k dial-up or on EDGE? How does it render for speech readers? Can it be read via a text-based browser? (It&#39;s a fun experiment; try visiting your website with a text-based browser like lynx or Links). Refuse client requests to implement hyper-invasive technologies like canvas fingerprinting. Consider replacing Google Analytics with a more privacy-respecting analytics software like Piwik. Even better if you can host it yourself! Minimize third-party dependencies like Google Fonts (you can self-host them instead). Avoid ad networks (like the plague!) if possible. Serve your own ads by selling ad space the old school way if you&#39;re able. If not, explore privacy-respecting methods of serving ads, including developments powered by the blockchain (like the Basic Attention Token). Respect Do Not Track. Carefully consider the benefits of hyper personalisation and retargeting. The benefits are debatable but the long term consequences might be disastrous. Ask yourself: would you be okay with a company collecting as much data (as you seek to collect) on your teenage daughter, your nephew in college, your husband or your grand-mother? Consider business models where you actually respect your clients and your website visitors instead of using them. If you can&#39;t be honest about your business model with your client, maybe you need to ask questions. Thoughts and feedback It all comes down to one simple question: what do we want the web to be? Do we want the web to be open, accessible, empowering and collaborative? Free, in the spirit of CERN’s decision in 1993 or the open source tools it&#39;s built on? Or do we want it to be just another means of endless consumption, where people become eyeballs, targets and profiles? Where companies use your data to control your behaviour and which enables a surveillance society—what do we want? For me, the choice is clear. And it&#39;s something worth fighting for. I hope this article has been interesting. If you have thoughts—you agree, disagree, have reservations, other ideas or a suggestion—I&#39;d love to hear them! This article is on GitHub; if you&#39;d like you can send a pull request with edit suggestions (like Anders and many others did, thank you!). You can also get in touch via email (userhostileweb—at—neustadt.fr) or, if you&#39;re on Hacker News or Reddit, share your thoughts there. — ← back home Against an Increasingly User-Hostile Web, written by Parimal Satyal on 7 November 2017 and published on Neustadt.fr. This text is in the public domain with a CC0 1.0 Universal license; you are free to do whatever you want with it (obviously doesn&#39;t apply to the photos or examples I&#39;ve included). A link back is nice but not required. </description>
      <pubDate>15 Feb 22 12:56 EST</pubDate>
      <guid>https://neustadt.fr/essays/against-a-user-hostile-web/</guid>
    </item>
    <item>
      <title></title>
      <link>https://zeynep.substack.com/p/critical-thinking-isnt-just-a-process</link>
      <description>&lt;a href=&#34;https://zeynep.substack.com/p/critical-thinking-isnt-just-a-process&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;After my last post expanding on some of the ways to best evaluate information and ways of knowing, people asked for more such discussions. So let’s talk more about metaepistemology.One of the things I noticed throughout the past year has been that a lot of my friends who had grown up in authoritarian or poor countries had a much easier time adjusting to our new pandemic reality. My childhood was intermittently full of shortages of various things. We developed a corresponding reflex for stocking up on things when they were available, anticipating what might be gone soon. That was quite useful for the pandemic. So was trying to read between the lines of official statements—what was said and what was not, who was sitting with whom on the TV, and evaluating what the rumor networks brought in. It turns out those are really useful skills when authorities are lying at all levels.I commented on this:Others with similar backgrounds started reporting a similar “authoritarian muscle memory.”When Trump got sick with COVID last October, there was a lot of speculation about how sick he was. We knew he was taken to the hospital, but that may have been out of an abundance of caution. He was, after all, the president, and there was indeed footage of him walking to the helicopter. When his medical team held a press conference, one detail stood out: he had been given dexamethasone—a steroid that has been shown to greatly reduce mortality, but only when the patient was severely ill. In the early stages of the disease, the result was the opposite: it increased risk and negative outcomes. The reason for this flip is that the disease has two phases: the early phase is dominated by the “virus pathology,” and in the latter phase, it’s the immune system response that’s doing the damage.There is a growing movement to approach COVID-19 as a two-phase disease: in the early phase, virus pathology dominates; and in the later phase, immunopathology drives disease. In thinking about COVID-19 this way, perhaps it is not surprising that dexamethasone offered no benefit to patients in the RECOVERY trial whose disease had not progressed to a stage necessitating respiratory support; indeed, the immunosuppressive effects of glucocorticoids at this stage of disease might hamper antiviral responses. It is in the later, hyperinflammatory phase of COVID-19 that the immunomodulatory effects of glucocorticoids are beneficial, perhaps by breaking the inflammatory feedforward loop, at least in some patients.Dexamethasone is a synthetic glucocorticoid that’s used to suppress the immune system—a very useful drug if it’s the immune system response that’s driving the disease, but obviously terrible if it’s in the early phase, where we want the immune system to work hard in suppressing the virus replication. Hence, the guidelines recommend against using it for patients who do not require supplemental oxygen—something that happens around low 90s percentage oxygen saturation.So what had actually happened to Trump? One obvious hypothesis was that he was given dexa because he was sicker than his doctors were saying, and it was an appropriate medical response.The alternative idea was that he was a victim of “VIP medicine” syndrome where important personalities get aggressive “kitchen-sink” treatments: everything gets thrown at them, even if it&#39;s not to their medical advantage. Many raised this possibility on Twitter:And the New York Times story about Trump’s COVID suggested this possibility as well:Some experts raised an additional possibility: that the president is directing his own care, and demanding intense treatment despite risks he may not fully understand. The pattern even has a name: V.I.P. syndrome, which describes prominent figures who receive poor medical care because doctors are too zealous in treating them — or defer too readily to their instructions.“You think you’re helping,” said Dr. Céline Gounder, a clinical assistant professor of medicine and infectious diseases at the N.Y.U. Grossman School of Medicine. “But this is really a data-free zone, and you just don’t know that.”Still, based on the doctors’ account, Mr. Trump’s symptoms appear to have rapidly progressed since he announced early Friday morning that he had tested positive for the coronavirus.Other outletsraised the same idea.So which was it?I listened to the news conference delivered by the doctors. Here’s what they said:So here’s the text again:Q: Was Trump&#39;s oxygen level ever below 90?CONLEY: We don&#39;t have any recordings here of that.Q: But was it ever below 90, here or at the White House?CONLEY: No, it was below 94 percent. It wasn&#39;t down in the low 80s or anything.So, here’s what I deduced at the time from the information we had:Notice the pattern. He clearly adds the word “here” to answer whether Trump’s oxygen level had fallen below 90. So it must have fallen below that somewhere. He’s then asked whether it fell below that at the White House. He says that it was below 94 percent but &#34;never reached low 80s.&#34;Metaepistemology may be a fancy term, but it’s actually a mundane skill. Let’s think through its applications. The statements made by the doctor in charge of Trump’s treatment contained more information than he claimed--if one read them correctly.,  If one interpreted them by “reading between the lines” in a way that took into account who the person was, what their motivations were, and what their incentives and “borders” were.A principle that’s often useful in these situations is that most deliberate misinformation from authorities—especially in places that are mid-range in terms of institutional trust and strict licensing—comes from omission, not saying the truth, rather than outright lying. That offers a way to get at the truth by trying to detect a picture, and looking at the parts that have been obscured, to make out the actual shape.It’s like looking at this picture (which is actually an optical illusion from the San Francisco Exploratorium Museum) and using the columns to figure out that the relevant information: the people. In other words, the “Baghdad Bob” syndrome—where a high-level official will outright lie about something that almost anyone can see for themselves is an obvious lie—is not just rare, it is not the best way to lie:Sahaf&#39;s nickname, &#34;Baghdad Bob,&#34; now denotes someone who confidently declares what everyone else can see is false--someone so wrong, it&#39;s funny. But when read beside the eventual cost of America&#39;s decade in Iraq, &#34;Baghdad Bob&#34; isn&#39;t so funny anymore.And perhaps a key point here is that the difference between lies of omission—misleading by skipping relevant information—and lies of commission—outright lying—is not just that the latter is weak, it’s also that it’s harder for the person doing the misleading. It deprives them of their self-respect. And in countries like the United States, it’s not easy for a medical doctor at a respectable institution to be outright lying.In fact, whenever cornered by reporters, where he could not directly mislead, Dr. Conley started invoking HIPAA—the law that protects patient privacy. This was another obvious tell: privacy rules did not prevent Dr. Conley from sharing positive impressions of the president’s health, but were invoked when it came to questions that would have revealed the extent of the severity of the illness.Hence, I concluded that the most likely explanation was not “VIP syndrome” but that, indeed, the President had faced severe illness.Yesterday, we finally got actual reporting on this. Here’s the New York Times:The people familiar with Mr. Trump’s health said he was found to have lung infiltrates, which occur when the lungs are inflamed and contain substances such as fluid or bacteria. Their presence, especially when a patient is exhibiting other symptoms, can be a sign of an acute case of the disease. They can be easily spotted on an X-ray or scan, when parts of the lungs appear opaque, or white.Mr. Trump’s blood oxygen level alone was cause for extreme concern, dipping into the 80s, according to the people familiar with his evaluation. The disease is considered severe when the blood oxygen level falls to the low 90s.Emphasis mine: and so there it was. Indeed, it was in the 80s.There is often talk of teaching people “critical thinking” thinking skills, and that’s certainly something worth doing. A mistake, though, is to think that such critical thinking skills are independent of knowledge: that there is a recipe, or a way of interrogating conclusions, that can turn into “critical thinking.”  In reality, the process by itself isn’t where the magic happens. These do not seem complicated skills in some sense—and especially not in retrospect, once the actual answer is known.  But they require more than parsing of words. The institutional operation, and the status and psychological incentives of the people, matter greatly to discerning the truth. Like most knowledge, this is more than “word games.”  It is a mixture of sociology and psychology—if we are putting them into fields—but also involve probability: what’s the most likely outcome? What types of evidence would help tip the balance in which direction? How do these institutions operate? What are the personal and professional incentives of this particular person? And so on. Critical thinking is not just formulas to be taught but knowledge and experience to be acquired and tested and re-examined, along with habits and skills that can be demonstrated and practiced. But there is no separating the “process” from the “substance”. It may be a privilege to live in a society that does not always need official statements to be interrogated as such.But if the past few years have shown anything, that privilege is not something to be taken for granted.</description>
      <pubDate>17 Feb 21 09:19 EST</pubDate>
      <guid>https://zeynep.substack.com/p/critical-thinking-isnt-just-a-process</guid>
    </item>
    <item>
      <title>It is exactly rocket scienceSpaceX’s monstrous, dirt-cheap Starship may transform space travel</title>
      <link>https://www.economist.com/science-and-technology/2022/02/19/spacexs-monstrous-dirt-cheap-starship-may-transform-space-travel</link>
      <description>&lt;a href=&#34;https://www.economist.com/science-and-technology/2022/02/19/spacexs-monstrous-dirt-cheap-starship-may-transform-space-travel&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;WHEN IT COMES to size and spectacle, the peak of the Space Age passed in 1973, with the final flight of the Saturn V rocket that had carried the Apollo astronauts to the moon. Taller than the Statue of Liberty, the Saturn V could lug 140 tonnes into orbit. Its first flight, in 1967, provoked Walter Cronkite, an American news anchor reporting far from the pad, to exclaim: “My God, our building’s shaking here!” as ceiling tiles fell around him. Half a century later, nothing as powerful has reached orbit since (see chart 1).Listen to this story. Enjoy more audio and podcasts on iOS or Android.Your browser does not support the &lt;audio&gt; element.Not far from Boca Chica, a Texan hamlet a couple of miles from the Mexican border, SpaceX, a rocketry firm founded by Elon Musk, is developing a machine that it hopes will change that. Built from gleaming stainless steel, with its nose adorned with fins and ten metres taller than even the Saturn V, Starship looks like something from the cover of a 1950s pulp science-fiction magazine. Its planned payload of up to 150 tonnes means that five Starship flights could put more stuff into space than the rest of the world managed with 135 rocket launches in 2021. Its upper stage contains more pressurised volume than the International Space Station, which took a decade, dozens of launches and perhaps $100bn to assemble.But it is not just the size that matters. When a Saturn V took off to send men to the Moon, the only bit of the 2,800 tonnes of hardware which came back was a cramped five-tonne capsule with three men inside. Each new mission meant a new Saturn V. With Starship, the idea is that all the hardware will come back: the massive booster stage almost immediately, the second, orbital stage after fulfilling whatever mission it had been sent on.At a press event on February 10th to show off an assembled rocket Mr Musk reiterated his reasons for founding SpaceX: to buy humanity an insurance policy against existential risks by establishing a colony on Mars. Starship is designed to transport the million tonnes of supplies he thinks is needed for that job—roughly 100 times more mass than has been launched since the start of the Space Age. To that end, it is designed to be not only the biggest rocket ever built, but also the cheapest. Existing rockets cost tens to hundreds of millions of dollars per launch (the Saturn V may have cost over $1bn in today’s money). Despite Starship’s size, SpaceX hopes to cut that to the low millions.Mars colonies, if they ever come, remain a long way off. But Starship’s unprecedented combination of size and frugality could upend the economics of the space business closer to Earth, too. An industry used to shaving grams of mass and cramming complicated payloads into small cargo bays will see those restrictions lifted. Some scientists are already imagining extravagant space missions that would make full use of the rocket’s huge capacity. NASA intends to use it to land astronauts on the Moon; America’s soldiers are eyeing it up, too. And Starship is vital to the future of SpaceX itself, which was valued recently at more than $100bn (see chart 2).But first the rocket needs to fly. A series of test flights of Starship’s upper stage (which, in isolation, is rather confusingly also called “Starship”) have ended in crash-landings and explosions. A successful flight came on May 5th last year, when an upper stage flew 10km into the air before landing safely back on its pad. A full-fledged orbital test of the two-stage form of the rocket, with one Starship upper stage sitting atop a Super Heavy booster, had been due in January.That orbital flight, though, needs approval from regulators, who were deluged with thousands of public comments. Officials have promised a decision within weeks. But broader environmental issues could yet force the firm to suspend work at Boca Chica entirely. An internal memo leaked last year revealed serious problems with the Raptor engines intended to power Starship. In his press conference, Mr Musk left himself a fair amount of wriggle room. An orbital flight, he said, might come in “a couple of months”—though it could also slip to the end of the year.Zero gravitasSomething like Starship has been in development at SpaceX for over a decade, under names such as MCT (Mars Colonial Transporter), ITS (Interplanetary Transport System), and BFR (Big Fucking Rocket). Earlier versions were huger still: the ITS had a 300-tonne payload at one point. But all versions had one thing in common: they are designed to be entirely reusable.SpaceX already flies partially reusable rockets: the first stages of its Falcon 9 machines fly back to Earth under their own power. Once refurbished and refuelled, they can fly again, spreading their construction cost over many launches. But their second stages, which end up much higher and moving at orbital speeds, remain expendable.With Starship, SpaceX plans to recover both parts. Its Super Heavy first stage, like the Falcon 9’s, is designed to fly back to the ground shortly after launch. SpaceX plans to catch it in mid-air with a pair of robotic “chopsticks” attached to the launch tower from which it took off.Recovering the upper stage requires more drama. Starship will fall belly-first from space, relying on atmospheric drag to shed most of its speed. It will use its stubby fins for control, “rather like how skydivers use their hands and feet”, says Scott Manley, a physicist and programmer who runs a popular rocketry-focused YouTube channel. When it is within a few hundred metres of the ground it will flip itself upright, relight some of its engines and make a rocket-powered landing of its own.Several test flights have practised this flipping manoeuvre already, though not after a descent from orbit. Mr Musk (whose bold visions sometimes work, and sometimes do not) hopes that each Super Heavy booster could be ready to fly again within an hour. Since the rocket’s upper stages would have to complete at least one orbit before returning to Earth, he hopes they might one day manage three flights a day. (The minimum re-use time for a Falcon first stage is about a month.)Starship’s Raptor engines are also designed with reusability in mind, says Mr Manley. They use a sophisticated, highly efficient design pioneered—but never flown—in the Soviet Union in the 1960s. Somewhat unusually, they run on methane rather than kerosene, a more-commonly used rocket fuel. Methane produces very little soot, which helps keep the engine’s internals clean—another boon for an engine intended to fly again and again. And both methane and the oxygen necessary to burn it can be made from Mars’s thin carbon-dioxide atmosphere with the help of some straightforward industrial chemistry. SpaceX hopes that could, one day, allow Mars-bound Starships to refuel for a return trip to Earth.But high-level design decisions are not the only reason Starship is cheap. SpaceX has an iterative, rapid-fire, startup-style culture very different from that of older aerospace firms (hence all the crash-landings and explosions). Mr Musk’s development philosophy is that “if things are not failing, you aren’t innovating enough.” In a speech in November to America’s National Academies of Sciences, Engineering and Medicine he spoke of running a dozen test flights in 2022. The firm mixes high-tech, bespoke design in some areas (such as the Raptor engines) with a make-do-and-mend attitude elsewhere (some Super Heavy prototypes have fins controlled by electric motors taken from cars made by Tesla, another of Mr Musk’s businesses).One good example is the rocket’s stainless-steel construction. Starship was originally going to be built from high-tech carbon-fibre composites, which are both very strong and very light. But in 2019, despite having produced several big components, SpaceX went back to the drawing board. Carbon composites, it turns out, have several disadvantages. They are porous, fiddly to work with, and need to be cured in an autoclave—not easy when making rocket-body segments that are nine metres across. And, at around $130 per kilogram, composites are expensive.Stainless steel, by contrast, is strong but heavy and therefore not an obvious choice for rocket-building. Some steel alloys, though, get significantly stronger as they cool down, meaning less is required for a given strength. And since Starship uses cryogenic propellant, cooling is in abundant supply. Steel is tougher, too, which can save weight elsewhere. SpaceX hopes to get away with applying a heat shield to only the windward part of the upper stage, which feels the full force of re-entry heating, leaving the leeward side as bare metal and saving mass. Stainless steel does not need painting, which reduces weight. It is much easier to work with, and costs mere dollars per kilogram. For a company that intends to mass-produce its rocket, says Simon Potter at BryceTech, a firm of space-industry analysts, that matters.That may sound like a risky approach when it comes to something as unforgiving as rocket science. But it has served SpaceX well so far. It has pulled off 111 Falcon 9 launches in a row without failure, making it one of the most reliable rockets ever flown. Some Falcon 9 first stages have already been launched ten times.A cheap, big, reusable rocket has been a dream of space cadets for decades. On paper, at least, Starship fulfils it. “You almost get to a point where launch costs would go away entirely as a consideration,” says Mr Potter. Mr Musk has talked of eventually building a fleet of Starships. If each were indeed launching several times a day, that would give SpaceX the ability to lug a million tonnes of stuff into orbit each year. BryceTech reckons that, in 2021, the world managed 750 tonnes. What you might do with all that capacity (other than supplying a future Mars colony) is another question.Jonathan McDowell, an astrophysicist and rocket enthusiast at the Harvard-Smithsonian Centre for Astrophysics, notes that Starship’s colossal size might go unused in the commercial-satellite market, at least for the foreseeable future. “There just isn’t currently a market for large numbers of enormous payloads,” he says. SpaceX’s Falcon Heavy, with a payload capacity of 64 tonnes, is the most powerful rocket currently flying. Its first launch was in 2018, but it has only flown twice since.The satellite industry might adapt, in time. In any case, Mr Musk has indicated that Starship, thanks to its cheapness, will replace SpaceX’s smaller Falcon rockets, which already have a market share of around 50%. If he sticks to that plan, then early commercial launches of Starship could fly with their holds mostly empty.Congenital optimistOne medium-term option might be space tourism, says Mr Potter. Existing rockets from Blue Origin or Virgin Galactic can already carry a handful of thrillseekers into space—though not to orbit. Starship could take perhaps 100 people on an orbital trip, or a smaller number even further and in greater luxury.On February 14th Jared Isaacman, an American billionaire who has already flown into orbit with SpaceX announced that he had ordered three further flights from the firm. The first two will use SpaceX’s existing Falcon rockets—but the third, said Mr Isaacman, should mark Starship’s first crewed flight. Meanwhile Yusaku Maezawa, a Japanese billionaire, has contracted with SpaceX to send himself and up to a dozen companions on a six-day jaunt around the Moon and back.Jennifer Heldmann, a planetary scientist at NASA’s Ames Research Centre who has written a paper about what Starship could do for science, is more excited. Starship’s upper stage is designed to be refuelled in orbit, with extra fuel brought up in the cargo bay of other Starships. A full refill would require several extra flights. But the pay-off, says Dr Heldmann, would be the ability to deposit 100 tonnes or more of cargo on the surface of almost any body in the solar system. (The Perseverance rover that landed on Mars last year had a total mass, with its lander, of about four tonnes.)Cheap launches might not be immediately revolutionary. Science missions are expensive, and even pricey launches make up only a small chunk of the overall budget. But Dr Heldmann points out that Starship would enable much more ambitious missions, getting scientists more bang for their buck. One option, she says, would be to fly larger quantities of cheaper kit. “All that payload capacity means you could use off-the-shelf components rather than having to custom-make and miniaturise things,” she says.Another option would simply be to go big. Perseverance, which cost $2.7bn, carries a drill that can excavate a few inches of Martian regolith. Starship, says Dr Heldmann, could carry a full-sized drilling rig that could bore kilometres deep.And it could also open up access to the outer planets, which have historically been tricky to send missions to. In recent years the watery moons of Saturn and Jupiter have overtaken Mars as the most promising places to search for alien life. One group of scientists has drawn up a plan to use Starship to explore Neptune, which has been visited just once before, in 1989, when the American Voyager 2 probe zoomed by on its way out of the solar system. Such a space craft could weigh tens of tonnes, compared with just 722kg for Voyager 2.America’s government is another potential customer. The country’s newly minted Space Force is looking into Starship for its Rocket Cargo programme, which is designed to explore whether the rocket could be used to deliver equipment rapidly to anywhere on the planet. And with space a vital part of warfighting, America’s armed forces would welcome the ability to replenish shot-down satellites quickly and cheaply.NASA, meanwhile, has chosen a modified version of Starship’s upper stage to ferry astronauts to the lunar surface as part of its ambitious Artemis programme. Most of Artemis is designed to use the Space Launch System (SLS), another jumbo-sized rocket that NASA is developing as a successor to the Space Shuttle. But the SLS has a lower cargo capacity than Starship does, and a launch cost projected at $2bn a time. If Starship works, NASA could come under pressure to scrap the SLS entirely.SpaceX, for its part, knows exactly what it wants to do with Starship, even before it starts thinking about Mars. Its Starlink project aims to use swarms of thousands of low-flying satellites to beam high-speed internet to anywhere on Earth’s surface. Gwynne Shotwell, SpaceX’s chief executive, has noted that the global telecommunications market is worth perhaps $1trn a year. SpaceX thinks it might reasonably aspire to about 3-4% of it.Because low-flying satellites can see only a small portion of the Earth’s surface, Starlink requires enormous numbers of them. The firm already has about 1,655 in orbit, about a third of the total number of active satellites in space. It has permission from American regulators to fly 12,000, and is trying to obtain a licence for 30,000.But first, SpaceX has to make the rocket work. In his press conference Mr Musk was at pains to play down the probability of the orbital test—when it happens—going smoothly. Even if it did, plenty more testing would be needed before the rocket would be ready to fly real cargo.Regulatory battles may be looming, too. The firm’s Boca Chica facility was built on the understanding that it would be used for the Falcon Heavy, a much smaller rocket than Starship. Explosions from failed flight tests have scattered debris over a wide area, says Mr Manley, while road closures annoy locals. Environmental regulators are reportedly unhappy, and pushing for a full review of the firm’s licence. Mr Musk has said that, in the worst case scenario, SpaceX would have to move Starship development to Cape Canaveral in Florida, which would delay things for months.Nervous energyEven then, Starship’s capabilities could go unused. The true size of the market for Starlink remains unknown. As for his grandest ambition, it is not at all clear how many people would volunteer to live on Mars. The sales pitch, said Mr Musk, is that “it’s going to be cramped, dangerous, difficult, very hard work [and] you might die.”Despite the technical challenges ahead, it would take a bold person to bet against SpaceX. In 2008, after the first three launches of its tiny Falcon 1 rocket had failed, the firm almost went under. But the fourth launch worked. The Falcon 9’s impressive failure-free run was preceded by more than a dozen unsuccessful attempts to land its first stage. Mr Musk, for his part, is confident. “[Starship] will work,” he said. “There’ll be a few bumps along the road, but it’ll work.” ■To enjoy more of our mind-expanding science coverage, sign up to Simply Science, our weekly newsletter.This article appeared in the Science &amp; technology section of the print edition under the headline &#34;Ad astra, on the cheap&#34;</description>
      <pubDate>18 Feb 22 08:52 EST</pubDate>
      <guid>https://www.economist.com/science-and-technology/2022/02/19/spacexs-monstrous-dirt-cheap-starship-may-transform-space-travel</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.raptitude.com/2022/02/what-i-learned-during-my-three-days-offline/</link>
      <description>&lt;a href=&#34;https://www.raptitude.com/2022/02/what-i-learned-during-my-three-days-offline/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; As most of you know, I just took three days completely offline so that I could discover what would be difficult about it. I have so much to say about that three days, and first thing I would like to report is that there was almost nothing difficult about it. To my surprise, I didn’t crave the internet at all. I wasn’t dying to check email, judge people on Twitter, or figure out the day’s Wordle. Instead I did my daily work — very little of which requires the internet, I discovered — and simply lived life in the physical world.   This simplicity was disorienting in a way. Many times a day I would finish whatever activity I was doing, and realize there was nothing to do but consciously choose another activity and then do that. This is how I made my first bombshell discovery: I take out my phone every time I finish doing basically anything, knowing there will be new emails or mentions or some other dopaminergic prize to collect. I have been inserting an open-ended period of pointless dithering after every intentional task. With my phone parked in a cardboard pouch taped to my kitchen wall, this ritual was unavailable, so I again and again found myself hitting a kind of intentionless vacuum, where nothing would happen until I consciously formed a new intention to get on with the day, in a way of my choosing. I can’t convey the strangeness of this feeling — it was like repeatedly discovering that I had misplaced my cane again, only to remember I can walk just fine. Me, forming an intention This new mode of living came with another alien sensation: that of having tons of time. The days were long and waiting for me to fill them, like so many empty moving trucks. There was nothing to do but do things. My attention span did improve noticeably. By the second day, both reading and meditation were far less effortful than they’ve been for the past few years, and I didn’t have to mentally “roll up my sleeves” to begin doing them. This was thrilling, and in hindsight should not be surprising –- my phone habits could only have trained my attention to branch constantly, seeking gratification on demand, and instantly-Googled answers to every question occurring to the mind. Life seemed quieter — calm and simple and local to the room I was in, like it does after a spa visit or meditation retreat. Even the experience of passive entertainment became simpler and less stressful. If I put on a movie, I would simply watch the movie until it was either over or it was clear I’d rather do something else. Then I would go and do something else, rather than drift away into my phone, where I would browse my WhatsApp chats, skip through people’s boring Instagram stories, or look up how old the actors were during filming. “One screen at a time, fellas” And thus I learned what was probably the most important lesson of the three days: without the internet, there was no way to collapse into the mushy, nihilistic state of semi-doing. There was just doing. I could always choose what to do, but not not to do.     Checking the Weather Throughout the three days I wrote on a clipboard things I later wanted to do or look up online. There were surprisingly few instances when I found I needed certain information right away. A few times while running errands I flipped on mobile data to look up an address, then flipped it off, email unchecked. My mom and I had discussed vague plans to go for a walk, and I wanted to check the forecast before proposing a time and place. I quickly realized I didn’t have access to basic weather information. I was proud of my elegant, old-fashioned solution. I called and asked my mom if she had seen the forecast. She had, and she told me what it was. Normally, by that time of day (noonish), I would have checked the weather 17 times or so already, all but one of them a pointless reflexive tic. Early crowdsourcing method The Return of “The World Out There” The larger goal that inspired this experiment is to figure out how to “put the internet back into a box in the basement” — that is, make it a contained tool I use only intentionally and sparingly. I did achieve this to some degree, and with that came many nostalgic feelings of what my mind used to be like. The most important of these feelings was a distinct sense that there’s a whole world out there. Not out there on the internet, but out there over the horizon. Out there in this same world are jungle cats, opera houses, subterranean hideouts, breakdancing circles, riverside tire swings, and old women playing mahjong. There are desert caravans and bullet trains and foggy valleys and kangaroos, and I can one day visit some of these things or maybe just read about them. Reconnecting with this feeling, which I used to have all the time, made me realize what exactly has happened to the internet over the past decade. It used to function as a magic window onto our vast and colorful world, and that made it very exciting to look through. Today, my internet experience is so uniformly shaped by the big platforms that I have little sense that the world it’s showing me is vast or interesting at all. It shows me the same expected things every day – the same partisan opinions, the same jokes, the same forms of recreational scrolling and tapping, the same tone and feeling. To go on the internet today –- at least without very specific intentions — is to swallow another large bucket of the same tepid punch you have every day. Still out there This is not because we’ve looked out the magic window so much that we’ve seen everything by now, it’s because the big online platforms have a certain business model, and that model entails a very uniform experience for its users, characterized by familiar memes, a poisonous culture war, and approval points in the form of hearts and stars. The typical online experience has become a very small and limited thing –- something the world itself has never been. After spending only a few days away from this experience, the world feels more like a vast and colorful thing again. Everything that hints at the real world out there – what I read in books, what my friends tell me about their lives, what I see when I go places in my city – makes the world seem like it’s bursting with things I’ve never known or seen, which of course it is and always was.   Available offline What Happened When I Went Online Again The first morning I was allowed to use the internet again, I waited until late morning. I already knew I never wanted to go back to all-day reflexive internet use. My first stop was meant to be harmless enough – Wordle, the trendy guess-the-word game that takes about three minutes and only lets you play it once a day. I felt the usual excitement when those first green-boxed letters appeared, but there was also a background unease to the whole thing. It felt like my life, which had been so vital and interesting lately, was on pause until I got the puzzle. The simple loop of intending and doing I had enjoyed for the past three days was now on hold, if only for a few minutes, until I figured out a five-letter word with “Y” as its second letter. This delay felt not just unnecessary, but borderline nihilistic, and I was relieved to be done with it. In my notes I described it as a “mild purgatory” — like being stuck in human flypaper. Then I went on Instagram, and had a similar experience. There was some initial gratification at seeing what my friends had been up to. Smiling faces. Cakes. Dogs. Toddlers. Mostly good feelings. But I also had that feeling that I was again delaying the living of life until I completed a three-minute ritual with marginal rewards and no clear purpose. Pictures had to be scrolled through, liked, tapped. Political opinions had to be scowled at, affirmed, or ignored. Purgatorial ritual #3068 I did it quickly because I had many more rituals to complete afterward – Gmail, Twitter, fantasy hockey, WhatsApp banter, chess games in progress. Aside from a few necessary emails and messages from friends, none of them seemed to justify the short but complete stoppage of living and doing that they demanded. How I Want to Use the Internet In the days since the experiment, I’ve found myself thumbing through my internet rituals out of habit, and unfortunately the absurdity of them doesn’t hit me as strongly as on that first day back. I realize now that putting the internet back into a box in the basement in a lasting way will require explicit boundaries, and I will have to practice observing them. Here’s a list of proposed rules I typed out after that first disorienting day back. These are the boundaries I’d like to observe, given my own routines. If the idea of putting the internet back into a box appeals to you, you might want to try something similar, adjusting for your own habits. 1. By default, data and Wi-Fi on my devices is off, and the laptop is closed. Calls and texts are accepted. 2. By default, the phone is out of arm’s reach (in its wall-mounted holster in the kitchen, if possible) 3. By default, I don’t use the internet before 11am 4. By default, Saturday and Sunday are offline days 5. By default, one or two weekdays are offline days (which ones depend on the week) 6. By default, before turning on data or Wi-Fi, I list on a sticky note what I’m going to do online 7. By default, don’t choose an online activity when an offline one will do 8. By default, avoid taking on hobbies that require regular internet use 9. When you notice you’re ignoring these rules, close the laptop and put the phone away, and choose an offline activity to do for a while* The “by default” part is important. This list represents the way I’d like to use the internet most of the time. The rules won’t make sense in every situation, but I’d like to follow them unless I have a clear reason not to. The fate of Wi-Fi renunciates These rules won’t be right for everyone – I don’t tend to overdo text messages, for example, so I don’t feel a need to limit them – but they could be a starting point for your own boundaries if you want to make the internet a smaller thing in your life. Living Without a Ripcord Perhaps the fundamental lesson of this experiment was that the vast majority of my internet use simply serves to delay the rest of my life. It’s a way of momentarily escaping the responsibility of using my life intentionally — a ripcord I apparently want to pull constantly.   It was a relief to discover that living without this ripcord quickly came to feel very natural and familiar. It’s as though I used to live this way all the time. *** *This is where a Cupboard Sheet comes in handy. Photos by Stephane Yaich, David Cain, CardMapr, Meg Jerrard, Wesley Hilario, Qi Bin, and David Cain Need help focusing? The big productivity books are written by people who don&#39;t especially struggle with productivity. The rest of us find other ways. I shared mine in this post. </description>
      <pubDate>18 Feb 22 09:16 EST</pubDate>
      <guid>https://www.raptitude.com/2022/02/what-i-learned-during-my-three-days-offline/</guid>
    </item>
    <item>
      <title></title>
      <link>https://narratively.com/nick-brown-smelled-bull/</link>
      <description>&lt;a href=&#34;https://narratively.com/nick-brown-smelled-bull/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The first thing I can recall clearly was sitting in a hospital room in the dark.I knew something was wrong — that there was something wrong with me — and yet, I couldn’t tell exactly what. I realized the left side of my face was numb. Hanging on the wall in front of me was a television, but there was something wrong with it too. A ghostly copy was superimposed over the standard set; it was rotated at roughly a 15-degree angle and faded away into the burnt cream walls. Is the TV the problem, or is it me? My mother and a nurse wearing scrubs entered from the left, a disorienting place outside of my field of vision.“That’s our girl,” my mom said, approaching my bed. “How are you doing today?”Why was she so nonchalant? Why wasn’t she worried? Considering the haphazard inventory I had just taken, I probably should have demanded answers or cursed a bit. Raised some hell. Instead, I replied with an uncertain “… good,” slightly alarmed that she, too, possessed a ghostly, tilted imprint. When I was young, my mother always went on, at length, about the difficulties of raising my prone-to-tantrums, bang-his-head-on-the-concrete-when-angry older brother. Then, turning to me, she’d say, “But you, you’re so easy. And calm. And you never complain.” I guess that hadn’t changed. I wanted to ask her what was happening — and where I was. Instead, I swept my arm in front of me and, trying to find out what would happen next, said, “And now?”Before she answered, another character entered from the hallway, but this one I couldn’t place. Fairly young — my age, by the look of him — his youth was accentuated by a clean-shaven chin under full, feminine lips and a baseball cap perched precariously on his head, above his boyish face. He had the look of a perpetually surprised toddler, lips slightly parted in wonder and curiosity.“Now you have physical therapy,” he commented.The physical therapist, a blonde woman with chin-length hair, stepped in from stage right, clipboard in hand and a laminated badge dangling from a lanyard around her neck. When she entered, the nurse left, not wanting to crowd the room.The physical therapist pushed a rolling walker to the edge of my bed and beckoned me to rise. My initial movements were the stop-motion stutter of a crude animation. I reached for one of the walker’s handles. And missed. The double image layered on top of what I thought was the actual walker jutted out awkwardly in a direction that led me to believe it couldn’t be the real one — was I wrong? I tried again. Yeah, I was wrong.“Are you OK? Ready to stand?” the physical therapist asked.Planting my feet shoulder-width apart, clinging to my walker, I clambered to a standing position — I’m generous when I use that phrase. Between my shaking limbs, bent knees and outstretched arms, I must’ve looked more like a member of a seniors’ Pilates class than the 25-year-old woman I presumed myself to still be. Everything, including myself, felt familiar yet foreign, an already-read book revisited accidentally. An eerie sense of déjà vu — my own personal uncanny valley, so familiar but not the same.“OK, Brooke.” The physical therapist then addressed my mother and her companion. “We’ll be back in 45 minutes.”The therapist led me down a long hallway lined with other rooms and other patients. Every few feet, the therapist paused and waited for me to inch toward her, patiently watching with a fixed smile for the stop-motion hermit crab to scuttle closer.“Now just a little farther to the elevator,” the therapist said, pulling me back to the task at hand. I had just discovered I was having issues multitasking: Whenever I started thinking too much, I couldn’t walk.My god, I thought, I am exhausted and we’re not even where we’re going yet.When we finally reached the elevator, I stepped inside, at the therapist’s behest.“I feel like I know you,” my voice hissed out of my mouth like a barely audible stream of gas. A death rattle that made syllables and managed to form words.At first, I wasn’t sure she had heard whatever had escaped my throat. Her back, still facing me, seemed crystallized in position. Finally, she turned and looked at me for a long moment. When the elevator doors dinged close, she took a deep breath and sighed.“I’m Linda.”“My grandpa’s girlfriend has your name.”Linda’s mouth tightened, but her eyes softened.“I know. I’ve introduced myself to you nearly every day for the past two weeks.”Luckily, my memories started to stick after that disconcerting moment with the TV. Unluckily, weeks had already elapsed since I had been admitted to the hospital, some of which time I’d been comatose. I started receiving various stories about what had happened. Some true, some, I would eventually come to realize, fiction.One day, shortly after I’d started to remember Linda the therapist, the boy with the childlike face and childlike hat — I’ll call him Stanley here — slipped into the hospital bed with me. Alarmed, but oddly complacent, I said nothing, even as he leaned close to me and whispered into my ear, “I’ve been telling everyone that I’m your boyfriend.”“Yeah, OK.”Hadn’t this happened before? Him divulging he was my boyfriend … it felt familiar. How many times had this happened?“OK,” he parroted and turned to Naked and Afraid on the TV.“My face is numb.”“Yeah, you’ve been saying that.”“That screen is double.”“Yeah, you’ve been saying that too.”“What happened?”Stanley cocked his head to the side like a confused dog and considered my question — or at least, I figured he was considering it. Maybe he was worried about me. Maybe my well-being concerned him.“What do you remember?” he asked me.“You moved your stuff into my room.” I knew this had happened, even though I hadn’t realized it a moment before. But I remembered that detail and I knew I knew him. In what capacity? His claim to be my boyfriend didn’t feel right — it couldn’t have been romantic. Wasn’t I just doing him a favor?His already round, wide eyes widened further. He pursed his lips and diverted his gaze.“You allowed me to move into your apartment temporarily.” Stanley paused. “That’s the last thing you remember? And you don’t remember what you had been doing that day?”“What day?”Stanley let out a huff of air in exasperation. He shook his head in exaggerated impatience, rolling his eyes.“The day you and Cassie climbed a redwood near the trailer park and you fell 25 feet out of it.”According to my mother, in the early days of my hospitalization, every time Stanley entered my hospital room and announced himself to the doctors and nurses as my boyfriend, I threw out an arm in a warped imitation of Vanna White and exclaimed, “I guess I have a boyfriend now.” Cue Pat Sajak chortling good-naturedly.It came back to me early on, distinctly, that he had never wanted to be my boyfriend before this.But whenever I broached the subject, Stanley told me he hadn’t known what he wanted before, but uncertain of whether I would live or die, he became aware of how he felt. My skepticism remained even as my memory wavered.Yet, he showed up each day, and I began to believe him when he said his feelings had changed. Trapped in my bed and visited by therapists I only partially knew and family members I only vaguely recognized, it was nice to have someone else come see me and do word puzzles in bed with me, even if I didn’t always remember who he was right away.Other friends of mine who came to see me in the hospital were wary of Stanley, but his insistence on his right to be there and his role in my life stifled any objections that even my best friend, Sam, thought to make. My mother and I had always communicated infrequently about my romantic endeavors. Coping as best she could, she remained intoxicated most of the time I was in the hospital and didn’t question Stanley’s version of events. Later, she said I seemed like I wanted him there.When I was released from the hospital, I couldn’t walk without an arm crutch, and my memory was still far from intact. Santa Clara Medical Center insisted I leave in a wheelchair, and I was wheeled out to Stanley’s car. He said we’d decided together that he’d move to San Diego with me. With no memory of the original conversation, I believed him, but I felt overwhelmed.Following the seven-hour drive to North County San Diego, I told my mom I didn’t want to live with him. And although Stanley repeatedly hinted he should stay at my parents’ home, my mom put her foot down and said Stanley couldn’t live with us.So he got a recruiting job and a room nearby. On weekdays after getting off work, he’d walk through the side gate without announcing he was coming. On one particular day in late fall, two months after my hospital stay, he came into the backyard while I skimmed messages on Facebook that I’d received as an inpatient.I had been talking to our mutual friend, Cassie (I’ve changed her name here, as well as Stanley’s), from college. We’d been exchanging messages on Facebook, and while looking at our conversation, I saw an older message she’d sent me, while I was in the hospital, which I had no memory of.“Cassie messaged me while I was in Santa Clara,” I mentioned to Stanley, my eye still fixed on the screen. “I said you joked around, saying you hoped my memory stayed impaired, and she replied, ‘Is there something he doesn’t want you to remember?’”I laughed. Stanley didn’t.“Why do you think that’s funny?” he demanded, pulling the laptop toward him. He didn’t sit down. “Why would you tell her that?” He shoved the laptop away and placed his hands on either side of his head. “Why would you say that to her?”“Hey, relax,” I grunted while using both the table and chair to pull myself to a standing position. Once facing him, I added, “I don’t see what the problem is.”“You don’t — you don’t — ” Livid, Stanley couldn’t seem to express himself through his rage.Instead of walking away or going inside, I just stood and watched him stutter as his face flushed until he finally formulated words. And boy, what words they were.“What is wrong with you?” he started. “Here I am, doing everything I can to help you — sticking around when we thought you were going to die, staying when you were r*tarded, not leaving when we weren’t sure if you’d get better. And I’m here now even though — look at you.” He paused to wave a hand from my short hair to my bare feet.Incapable of speaking, I retreated through the sliding glass door into the kitchen. All of the words I wanted to say slithered through my mind, broken, disconnected. But nothing came from me.“And you might be like this forever! And instead of telling Cassie how supportive I’ve been, you say that to her? Why couldn’t you have told her how good I’ve been to you — trying to make you look like less of a mess, getting your hair cut, taking you to get your face waxed because it was disgusting.”As he spoke, he encroached on my space, stepping forward until his face was less than a few inches from mine. His hands still flapped in the air to either side; I think he may have wanted to grab me by the shoulders but refrained. It wasn’t until he vibrated each hand on the left and right side of my face that I realized I was shaking too.Stanley pulled his hands back, made a noise that sounded like a mixture of an exasperated moan and a frustrated yelp. Finally, he stomped out of my parents’ kitchen like a schoolboy suffering a tantrum. All I heard next was the gate slamming behind him.Later, he pretended we’d never had that interaction — I only brought it up once in the following days, and he insisted he didn’t know what I was referring to.More than two years before I woke up disoriented in the hospital, it was the beginning of my “junior” school year at the University of California, Santa Cruz (UCSC). All of the out-of-town transfer students over the age of 22 were corralled on the first floor of the transfer dorm. That dorm became a haven for all of us who had spent our post-high school years not attending college. But we had finally pulled together those community college units to gain admittance to a four-year school. And by God, we were celebrating.Cue the night after we all moved in: Everyone left their dorm doors propped open and flitted from room to room, taking a shot here, nabbing a plastic cup of our hallmate Cassie’s homemade wine there. Everyone except me. Stationed at the school-supplied prefab wooden desk underneath my bunk bed sans bottom bunk, I was drinking whiskey and playing music from a USB-connected speaker.“Anyone dislike Tom Waits?” I shouted in the general direction of the bodies amassed in my room. “All right, well, that’s what we’re gonna listen to now.”Among the gyrating bodies, a short guy in a blue baseball cap, brim pushed up jauntily, slid forward with an elbow pointing at me. He looked too young to be drinking.“I like Tom Waits,” he offered. “I’m Stanley.”“Let me guess,” I snapped, “you like Rain Dogs. That’s fine ’n all, but we’re going to listen to some real sad shit right now.”Later, Stanley would divulge his first impression of me: feet up on my desk, pugging whiskey straight from the bottle and ranting to him about Tom Waits. He thought I was a bitch. And I would tell him that I thought he was a disrespectful asshole. That didn’t stop him, after our initial meeting, from tapping on my dorm door every day, asking if I wanted to go walk in the woods or mountain biking. And it didn’t stop me from taking a swig of my ever-present whiskey and replying, “Sure.”We weren’t together, but we weren’t not together. Before we slept together, Stanley spent all of his time with me and stopped seeing all of the other women he had been involved with. By the end of that first semester, we had slept together multiple times, met each other’s family at Thanksgiving, and still not talked about what, exactly, we were doing. At the time, I didn’t think a conversation was necessary; I figured we had a gentleman’s agreement and were on the same page: exclusive but unserious.Although we lived on the same hallway, Cassie and I weren’t particularly close outside of the companionship provided by a common pastime: drinking. At the end of that year in the transfer dorm together, we all dispersed. Cassie moved into UC Santa Cruz’s on-campus trailer park — the one I’d fall out of a tree next to, a year later — and I found a room in an old Victorian on Mission, not far from Laurel Street and downtown.Part of me figured Stanley wouldn’t skulk around my door anymore, since we no longer lived a few feet away from each other. But sure enough, he ended up in a sublet off of Laurel Street and would rap on my window from the front porch, softening his big brown eyes when I pulled back the blinds to see who it could be.One day, Stanley, now sitting by that window at the computer chair and desk my sublet provided, broached a conversation we had never touched upon before, one I always avoided with everyone: acquaintances, bar patrons, friends — whatever Stanley was.“How did you lose your virginity? I remember when I lost mine … ”For the life of me, if you asked me how Stanley lost his virginity, I wouldn’t be able to tell you anything about it. I stopped listening after his initial question.“Are you OK?”Stanley’s genial curiosity caught me off guard.“Yeah, I was just … thinking.”“You don’t look OK.” He came over and sat next to me on the sublet’s twin bed. A wood frame painted white housed a run-of-the-mill mattress, neither soft nor hard. Stanley peered into my eyes incredulously, daring me to confirm what I could see him working out in his mind. So I did.“It, uh, wasn’t my choice.”“Do you remember his name?”And I said it for the first time in nearly 10 years. I don’t know how I wanted Stanley to react. I don’t know what I wanted him to do — maybe nod? Maybe ask if I wanted a drink? Oh, God, I wanted a drink. The previous night, I had polished off my bedside whiskey and hadn’t had the chance to walk to the liquor store before Stanley popped over. But I know I didn’t want him to do what he did.Immediately, he bounded to the computer and opened Facebook.“And this was in San Diego? OK, let me see.”And then he began clicking on profiles and muttering to himself, “No, too young. Couldn’t be this one. Hmm, new to the area — no. You don’t know his last name?” Stanley glanced over at me and then stopped touching the computer.At the time, I didn’t have the vocabulary, but now I can describe how I felt — confused, disoriented, overwhelmed. I heard the words, I understood them, but none of them stuck with me. It’s almost like tunnel vision, but the opposite seems to happen — everything expands and your field of vision contains too much and none of it makes sense. Your eyes water because everything feels overexposed and lacks detail.I didn’t notice him rejoin me on the bed or when he took my limp hand from my lap and held it. But I did hear him when he said, “I think people place too much weight on a person’s sexual history.”And then he kissed me gently and we had sex, on a mattress that could have been hard or soft or just fine. But it hadn’t been love — he felt sorry for me. He insisted, afterward, that he cared about me, but he didn’t want to be together, couldn’t be in a relationship. And I understood because, I felt, who would want to be with me?No one knew about this interaction, but I’m sure the leeway I gave Stanley despite the boundaries he crossed — because of his reaction to a truth I hated so much — looked like love.In the months after I left the hospital, my memory slowly but surely came back to me. I remembered all of this, about how I met Stanley and what our relationship was like before the accident. But I still had some questions. Some missing pieces — like how I could have let any of this happen.“Icouldn’t tell you before,” said Cassie. “Because I thought you were in love with him. How could I tell you what Stanley had done?”This conversation with Cassie took place before I fell out of the tree, and it came back to me as I gradually regained my memory. Nearly seven months after leaving the dorms, we were sitting at an outdoor table on the patio of UCSC’s Kresge Café, where we often met to talk about the likes of Amiri Baraka or Jean Toomer for our poetry class. It was well into our second year at UCSC, our “senior year,” that Cassie and I began hanging out consistently and (relatively) sober; Cassie had an elective slot open, and I suggested she take a poetry class with me.Cassie rubbed her left arm with her right hand but kept her eyes on mine.It happened on Memorial Day Weekend when we all still lived in the transfer dorms, she said. Only a little over half of a year before our meeting at the Kresge Café. Memorial Day had been a transfer dorm hallmate’s birthday and everyone had gone to Cowell’s Beach to celebrate — everyone except me. They left before I returned from — where had I been? I don’t know. Drunk somewhere. Like always.Cassie described a beach bonfire. But then she and Stanley had run into the woods to find firewood. She described Stanley slinging his arm around her neck, the same way he did to me. Cassie hadn’t found this strange, and I didn’t think she would — when he did this to me, I felt more like a “bro” than a romantic partner. It was when she fell down that things changed.She described them losing balance and toppling over a log. And then she told me Stanley started ripping down her pants and putting his mouth on her … I can’t go there again.“I told him to stop and he did.” Her voice trailed off as if, maybe, she should excuse him for the initial violation since he was so good at following instructions afterward.“I am … so fucking angry — ”“This is why I didn’t want to tell you,” Cassie whispered. “I didn’t want you to hate me.”“No, no, no, no, no.” The word tumbled out of my mouth and wouldn’t stop. “No, no, no.” Maybe if I said it enough, she’d know. “Not with you — you did nothing wrong — with him. With him. He’s a fucking monster.”And I hated myself. Because I had been awake, drunk but awake, when they returned. Everyone else clambered upstairs to continue the party, but Stanley pulled me into his room and into his bed. After what he had done.When Cassie told me all of this, Stanley had been studying abroad for months. Neither of us had heard from him in that time. I heard from other mutual friends he had a girlfriend of sorts.A month after Cassie’s revelation, Stanley commented on the UCSC trailer park’s public page, a community Cassie was a part of, and received a harrowing response from a friend of Cassie’s: We’d rather not have any sexual assaulters in our community, thanks.Which, of course, caused Stanley to call me — the first time in nine months we’d had any contact.“What is she saying about me?” he shrieked.“Not really sure who or what you’re talking about.”“Don’t play fucking dumb: Cassie. It was an accident. I stopped. What is she telling people?”I sighed and tried to keep an even tone. “Whatever happened, it obviously caused her more harm than you thought.”“You were raped,” Stanley responded. It sounded more like an accusation than a comment; it felt more like an accusation.I didn’t answer, and he continued. “You know what real assault is like. You need to tell her. Call her right now and make sure you tell her. You have to tell her what it’s really like — that, what was his name? That the construction worker came into your room and held you down and told you not to scream and forced his fucking — ”“Hey, hey, hey now.” I didn’t need the play-by-play. “I get it, I get it. Jesus.”And because it’s easier to shove your hurt onto someone else than addressing the bleeding parts inside yourself, I called Cassie and did the worst thing I’ve ever done in my life: I told her it could have been worse.“Cassie,” my voice cracked as I told her everything and then said, “What Stanley did was inappropriate, but he stopped.”I n the months following my coma, these memories returned to me in sporadic waves. I remembered, and then I convinced myself I must be misremembering, I must be wrong. Stanley would storm out whenever I brought up the past, only to return the following day like nothing had happened, which made things even more confusing.But I finally called Cassie toward the end of January 2016, five months after I had moved back to San Diego. I wish I could say I had mustered the courage a month earlier, as soon as I realized there was something Stanley didn’t want me to remember, but how could I possibly tell her I remembered, that it had come back to me, and Stanley was still here?“Cassie?” I asked quietly when a voice answered the phone. I stood in the backyard of my parents’ house, the only place I could be alone.“Brooke! It’s so good to talk to you. How have you been? What happened?”I told her everything: Santa Clara, Stanley, not knowing exactly what had happened.“I called Stanley as soon as the ambulance took you away,” Cassie said slowly, “I figured he would have contacted your family. The hospital had to find your parents’ information? Why didn’t Stanley call your parents?”A foreboding sensation crept into my gut and my skin became cold and clammy. It was overcast, typical January weather in San Diego, but far from cold.“That night,” she said, “we had made it to the top, at least 85 feet up, and you were really confident — we were joking around — and then all of a sudden you looked at me and told me, ‘I have to get down. Now.’ Then you sped down, and I think climbing to a lower branch before you fell is what saved your life.”“And,” I started and then stopped to moisten my mouth — it had gone dry — and eased myself down to sit on the concrete patio. “That’s all that happened?”“Well,” Cassie added, “I did think it was weird when I heard Stanley was still with you in San Diego. Before we climbed the tree that night, you were telling me how much you hated him. You had him buy a plane ticket back home in front of you to be sure he was really leaving. He had just moved all of his shit into your room after his lease ended, and you wanted him gone.”“Cassie,” I replied weakly.“Well, it’s good the two of you have worked things out. It was just, y’know, weird.”It was true; my misgivings hadn’t been unwarranted.Stanley and I had been involved, but it was long over, and — as usual — Stanley used me right when I thought I was rid of him. When he came back from studying abroad, he stayed with me for about a week and insisted I mediate a conversation between him and Cassie. (I did, and she said she wasn’t going to press charges.) He found his own place, but then when the spring quarter ended and his sublease was up, he moved all his shit into my room; I protested but he insisted. I kept telling him that he needed to just go home, but he continued to insist, over and over again, that he needed to stay to make sure “Cassie wasn’t going to do anything.”I still have no memory of the night I fell out of the tree, but Cassie told me I had made him buy a plane ticket in front of me to be sure that he would leave.After concluding our phone call, I remained seated on the ground outside. I felt stupid; I was stupid. Stanley had been convincing me he was doing me a favor, that I needed him. When really, he needed me. Still paranoid about what had happened with Cassie and his reputation, he had been using me to convince everyone he was a good person.Aweek after my call with Cassie, I was baking cookies. Remembering the recipe, the measurements, the order I needed to mix the ingredients, exercising my fine-motor skills to mix them — it was all good practice. It was all rehabilitating, my occupational therapist told me.Next to the kitchen sink, my mom swirled a glass of champagne and said, almost as if she were channeling it from another plane, “Three days into your coma, Stanley told me we should pull the plug on you.”Above the bowl of sugar and butter, my hands held a jar of peanut butter and an overlarge spoon, motionless. I stopped to look at her, closing one eye to combat the double vision the damage to my occipital lobe had caused.My mom averted her eyes as she added, “And he would sit forever and try to guess the code to your phone — he was desperate to get into it.” Then she shrugged. “But you seemed like you wanted him around …”“When I was in a coma?” I asked.My mom ignored this and said, “Stanley told me he knew you and knew what you’d want.”Even knowing this, knowing my life had been disposable to him, I was too weak of a person to make him leave. Stanley kept coming by my parents’ house every day, telling me I should stop focusing on rehabilitating my mind and should instead make my physical appearance more appealing. Often, he’d drop me off at walk-in waxing salons, instructing them to make my face smooth, “less disgusting.”“I just want to be able to think again,” I’d whisper after.“This is probably the best you’re going to get,” he’d reply. “You need to take better care of yourself. You have a lot of competition.”This obsession with outward aesthetics culminated in him taking me to Calaveras Mountain, a small mountain in east Carlsbad, and bidding me to run to the top.“My physical therapist said I shouldn’t do any strenuous exercise without her … my body still can’t regulate temperature.”Stanley shot me a look of disdain and hissed, “My stepdad is a physiatrist — I know what I’m talking about. I guess you don’t actually want to get better.”Halfway up Calaveras, my double vision split even further — something I didn’t think was possible — and I felt bile rising in my esophagus. Taking a knee, I put both hands onto the dirt-covered path and threw up.“My dad was never easy on me,” Stanley solemnly whispered, a bizarre explanation for his actions.We walked the rest of the way down.“I think I need to go,” Stanley finally said one day.“Do whatever you need to do,” I responded.We were sitting at a Thai restaurant in a strip mall. Across the way, I had briefly worked as a hostess in a restaurant when I was newly 18; they tore it down and built a Red Lobster in its place.“You’re not upset?” He searched my face. “Would you want to stay together? You’d miss me.”I wondered who he was trying to convince.“Yeah, we can stay together … even though you tried to kill me.”Stanley reeled back as if he had just been slapped. His feminine lips parted and his bottom jaw hung open, aghast.Stanley, enraged, knocked over his tea. It had been almost empty. The outrage felt performative; the spill theatrical. I was beginning to get a headache; I just wished someone would be honest with me — my mom, Stanley, anyone who had been there. Everyone wanted to protect themselves at my expense. I felt like a child every time the thought “But what about me?” sprang into my head.“I just meant if it got to that point — if you were going to be brain dead.” His hands flailed and his lips flapped as they always did when he tried to make a point. I’d finally settled on Beaker — he looked like Beaker from the Muppets. “If you were brain dead, your mom would just keep you forever in a back room drooling all over yourself! Look at you now — you don’t even have your own bed and they’ve been taking your disability money for months.”That was sort of true; once I had been established as disabled by Social Security, they started dispensing $775 a month to me, an amount based on my previous W-2s and work history. But I chose to give it to my parents — the insurance had covered the majority of the medical costs, but my mother had racked up hotel bills staying in San Jose. I handed the provided debit card for my disability benefits to my father and said, “For everything I’ve done.”As I explained this, Stanley’s mouth quivered in a dumbstruck “O.” But his horror and confusion only infuriated me; I had told him all of this before. He knew this — or should have. Did he ever listen to me?“And did you say that?” I shot back, restraining myself, but barely.“Say what?”“‘If it got to that point?’”“I didn’t need to. That’s obviously what I meant.”Stanley left the same week.He telephoned me in February 2017, more than a year later.By this time, I had finished my bachelor’s degree by taking my remaining classes at UC San Diego, and I’d started working seasonal shifts as a production assistant at an academic publishing company. I took the train to work by myself. An eye surgery had corrected my double vision, and I no longer needed to close one eye or wear a patch to see. On paper, I appeared to be a legitimate, functioning adult, and no one asked about my abnormal gait or inability to write by hand.Uncertain if I should answer Stanley’s phone call, I watched his name manifest on my cell phone screen and blink away when I didn’t touch it. A month later — I don’t know if curiosity gripped me or if I hoped for an explanation, or at least an apology — I called him back.“I was surprised to see you calling,” Stanley said by way of greeting. “I took mushrooms and went to a really dark place and called you because I knew you’d make me feel better. Do you think I’m OK?”“What do you mean?”“Cassie.”“For someone who didn’t do anything wrong, you certainly are acting like you did something wrong.”“Fuck, Brooke, I didn’t do anything!”“You ripped her pants down — ”“I DIDN’T RIP HER PANTS DOWN. I PULLED THEM DOWN.”“Did you unbutton them?”“What?”“Did you unbutton her pants?”“I don’t know. What the fuck does that matter?”“It does matter. It all matters. You’ve tortured me for over two years — do you realize that? Cassie told you two months before my accident that what you did was fucked up, but she wasn’t going to do anything punitive. And then — and then — you lied to my family and friends, saying you were my boyfriend to paint some sort of sympathetic narrative for some made-up situation you thought you were in — something that wasn’t real. But what happened to me was real. Everything — my whole life — my whole life. And my whole life meant nothing to you … you — ”“Wow,” Stanley interrupted in amazement. “Your speaking — your speech is really good. You could barely string together a sentence before. You — ”“You!” I roared back. “You stressed me out all of the time. You interrupted me. You yelled at me until I shook. I — ” My voice cracked. I felt — all at once — I felt pain. Regret. Shame. Remorse. “In the time you’ve been out of my life, I’ve made such improvements,” I continued in a near whisper, “… amazing improvements … if you had never been around … if you hadn’t forced your way into my recovery … ” I trailed off.“You can’t put that on me — I was going through something — ”“No.” It was resolute enough to make Stanley fall silent. “You went through nothing. You did something very wrong to Cassie. And me — you probably stunted the progress I could have made. I’ll never know. Goodbye, Stanley.”Cassie doesn’t hate me, but she should. At least that’s how I feel about it.We were able to see each other in person in 2017, then we talked on the phone in the summer of 2019. She’s doing well, despite everything, and understands the emotional manipulation Stanley employed to keep me under his thumb. She’s given me grace I’m not yet ready to give myself.I don’t know where Stanley is or what he’s chosen to do with his life. I hope he’s done some self-reflection, but I doubt he has. The hold rape culture has on us all makes it nearly impossible for genuine self-reflection to occur in these types of men.My physical deficits are still an everyday part of my life, but I’ve come to accept my disability. Ironically, the trauma of my accident, recovery, and new identity as a disabled person pales in comparison to the effects of Stanley’s destructive presence. I’m suspicious of all romantic partners and don’t trust the motives anyone purports to have. I’m distrustful and resentful. I go to therapy to discern which parts of my skepticism are warranted and which are pure paranoia. Even when I know, am painstakingly shown the truth, it doesn’t feel real or genuine.Despite this, I’ve developed a tenuous romantic relationship — maybe the word “situation” is more accurate — with an old friend who lives on the other side of the country. I think this is all I’m capable of, and right now, it’s all I want. Maybe that’ll change, but for now, I’m grateful for my cognitive capabilities, the drive to stay sober, and the lack of responsibility for someone else’s emotional stability — maintaining my own is quite enough.</description>
      <pubDate>07 Feb 22 16:03 EST</pubDate>
      <guid>https://narratively.com/nick-brown-smelled-bull/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.executeprogram.com/</link>
      <description>&lt;a href=&#34;https://www.executeprogram.com/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Learn it.Code it.Remember it.Execute Program is a learning platform built by developers for developers, because we thought there should be a better way to learn – and remember – programming languages and tools.I think it may be the future of programming education. Imagine effortlessly remembering everything you learn.Truly interactive lessons.You get clear, detailed explanations, then immediately test your knowledge by writing and running real code. Plus, follow-up reviews reinforce your new skills.Our courses are built for professional developers who are serious about improving their skills with programming languages and tools.I strongly recommend this site to junior AND senior developers.Comprehensive courses.One small code example at a time.Over 6 million code examples studied since 2018.How long do courses take? It depends. Our courses include dozens of lessons and hundreds of code examples. They&#39;re compact, designed so developers can finish three daily lessons in about 30 minutes.The hands-down best resource I&#39;ve found is @garybernhardt&#39;s course that uses structured learning, practice &amp; spaced repetition to really drive home understanding.Only a couple minutes a day and it&#39;s given me a more grounded understanding of some things I had only a surface understanding of before.I wish these kinds of lessons existed [for] all programming languages and concepts.Our courses are worth it.Just ask other developers.This was the best online course experience I&#39;ve ever had – and it worked seamlessly on mobile too.Completed three courses and now I automagically know things by heart, which helps approaching code with more confidence.I know I&#39;ve found something that works well for me when I&#39;m actually excited to get an email that my review is ready from @exec_prog.Whoever made these courses really understands how people learn!Execute Program has added rocket fuel to our learners&#39; progress over the last few months. We are big fans.Our Courses2230 code examples in 256 lessons.Course cardTypeScript BasicsStatic types and the TypeScript language from the ground up.25 lessons172 code examplesCourse cardEveryday TypeScriptTypeScript types for everyday application development.52 lessons420 code examplesCourse cardAdvanced TypeScriptComplex types used in reusable library and framework code.29 lessons278 code examplesCourse cardRegular ExpressionsBoundaries, repetition, character sets and classes, wildcards, and more.16 lessons242 code examplesCourse cardModern JavaScriptModern JavaScript features supported by all browsers.44 lessons467 code examplesCourse cardJavaScript ConcurrencyCallbacks, promises, async/await and event loops.33 lessons222 code examplesCourse cardSQLThe SQL language from the ground up.33 lessons213 code examplesCourse cardJavaScript ArraysSlice, filter, map, reduce, and other array methods.24 lessons216 code examples</description>
      <pubDate>24 Mar 20 12:18 EDT</pubDate>
      <guid>https://www.executeprogram.com/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.theinformation.com/articles/how-apple-is-working-from-home?utm_source=hackernews&amp;utm_medium=unlock</link>
      <description>&lt;a href=&#34;https://www.theinformation.com/articles/how-apple-is-working-from-home?utm_source=hackernews&amp;utm_medium=unlock&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Apple&#39;s headquarters in Cupertino, California. Photo by Bloomberg March 30, 2020 10:02 AM PDT Normally, Apple’s hardware teams meet in person at the company’s Cupertino, California, headquarters to review upcoming products, often bringing key components of their devices to show colleagues. But now that they are sidelined at home due to Covid-19, members of those teams are improvising new tactics for getting their work done. During video calls, they have resorted to tracing shapes in the air to describe components they’ve had to leave back in the office, said two employees. Because of travel restrictions, they’ve had to make decisions based on grainy photos of parts sent from Chinese factories, rather than doing so in person. As the tech industry braces for an economic downturn caused by the global pandemic, its biggest companies, which sit on billions of dollars of cash reserves, are perhaps best positioned. But Apple, one of the world’s most valuable companies, faces a unique set of challenges because of its secretive culture, focus on hardware and dependence on Chinese manufacturing, according to interviews The Information conducted in recent days with a dozen current and former employees, as well as others who work closely with the company. Join now to read the full story - or - Continue with Apple Already a subscriber? Log in here The Weekend Crypto Startups Joseph Gordon-Levitt on Becoming Travis Kalanick By Jon Steinberg · Feb. 26, 2022 8:00 AM PST Hi, welcome to your Weekend!The world has watched armed conflicts unfold through social media before—the civil war in Syria, to name one, produced indelible images that rocketed around the internet. But there’s never been anything like what’s transpired this week in Ukraine. For every devastating account of Ukrainian civilians fleeing the Russian invasion, for every chilling video of ... Ukrainian Official Asks Apple to Shut Russia App Store By Wayne Ma · Feb. 25, 2022 Zendesk Shareholders Vote Overwhelmingly Against Momentive Deal By Martin Peers · Feb. 25, 2022 The Associated Press Cancels Sale of Migrant Boat NFT By Aidan Ryan · Feb. 25, 2022 </description>
      <pubDate>30 Mar 20 17:53 EDT</pubDate>
      <guid>https://www.theinformation.com/articles/how-apple-is-working-from-home?utm_source=hackernews&amp;utm_medium=unlock</guid>
    </item>
    <item>
      <title></title>
      <link>http://danshipper.com/charlie-munger-on-how-to-build-a-2-trillion-startup</link>
      <description>&lt;a href=&#34;http://danshipper.com/charlie-munger-on-how-to-build-a-2-trillion-startup&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Charlie Munger On How To Build A $2 Trillion Startup Imagine it’s January of 1884 in Atlanta, Georgia. Glotz, an affluent fellow citizen, has invited you to participate in a peculiar competition: You and twenty others are invited to present a plan to start a business that will turn a $2 million investment into a business worth $2 trillion by the year 2034. Glotz will personally give $2 million to the person who presents the most compelling pitch in exchange for half of the equity in the new venture. There are only a few stipulations: The new venture must exclusively sell beverages (specifically non-alcohol or “soft” beverages) For reasons unknown Glotz has decided that company must be named Coca-Cola You have 15 minutes. What would you say in your pitch? That’s the question that billionaire Coca-Cola investor Charlie Munger posed to an audience at a talk in July of 1996. What followed over the following few minutes was an entrancing exhibition of multi-disciplinary wisdom and business acumen. Munger’s main point is that the most complex questions often have basic answers rooted in elementary academic wisdom (mathematics, psychology, etc.) He wants to show that applying some of these ideas regularly can help us to better explain business success, and make better decisions. To start his talk, Munger lays out five principles he will use in his pitch to Glotz: Decide big no-brainer questions first Use math to help explain the world Think through problems in reverse The best wisdom is elementary academic wisdom Big (lollapalooza) results only come from a large combination of factors Munger then dives in to solving the problem with his first principle: the big no-brainer questions that can be answered first. “Well, Glotz, the big “no-brainer” decisions that, to simplify our problem, should be made first are as follows: First, we are never going to create something worth $2 trillion by selling some generic beverage. Therefore, we must make your name, “Coca-Cola,” into a strong, legally protected trademark. Second, we can get to $2 trillion only by starting in Atlanta, then succeeding in the rest of the United States, then rapidly succeeding with our new beverage all over the world. This will require developing a product having universal appeal because it harnesses powerful elemental forces. And the right place to find such powerful elemental forces is in the subject matter of elementary academic courses.” Off the bat, it’s interesting to note how his prescription for growth largely mirrors the conventional startup wisdom espoused by Peter Thiel and others: grow quickly in a small market that you can dominate and then expand from there. In the case of software, that market is typically a small niche of consumers. In the case of Coca-Cola (especially in the 1800s) it’s a small concentration of consumers in a geographically circumscribed area. Next Munger moves on to his second and third principles: numerical fluency and thinking in reverse. “We will next use numerical fluency to ascertain what our target implies. We can guess reasonably that by 2034 there will be about eight billion beverage consumers in the world. On average, each of these consumers will be much more prosperous in real terms than the average consumer of 1884. Each consumer is composed mostly of water and must ingest about sixty-four ounces of water per day. This is eight, eight-ounce servings. Thus, if our new beverage, and other imitative beverages in our new market, can flavor and otherwise improve only twenty-five percent of ingested water worldwide, and we can occupy half of the new world market, we can sell 2.92 trillion eight-ounce servings in 2034. And if we can then net four cents per serving, we will earn $117 billion. This will be enough, if our business is still growing at a good rate, to make it easily worth $2 trillion.” Munger continues: “A big question, of course, is whether four cents per serving is a reasonable profit target for 2034. And the answer is yes if we can create a beverage with strong universal appeal. One hundred fifty years is a long time. The dollar, like the Roman drachma, will almost surely suffer monetary depreciation. Concurrently, real purchasing power of the average beverage consumer in the world will go way up. His proclivity to inexpensively improve his experience while ingesting water will go up considerably faster. Meanwhile, as technology improves, the cost of our simple product, in units of constant purchasing power, will go down. All four factors will work together in favor of our four-cent-per-serving profit target. Worldwide beverage-purchasing power in dollars will probably multiply by a factor of at least forty over 150 years. Thinking in reverse, this makes our profit-per-serving target, under 1884 conditions, a mere on fortieth of four cents or one tenth of a cent per serving. This is an easy-to-exceed target as we start out if our new product has universal appeal.” In this section, Munger demonstrates the value of the basic math involved in a TAM (total addressable market) analysis as part of formulating a thesis for a business. Then he goes on to look at the basic cost structure of the business and ensures that the back-of-the-envelope math makes sense for him to reach his ultimate goal. As part of this analysis he makes a lot of forward looking predictions with the benefit of hindsight (the depreciation of the dollar, the real purchasing power of the average consumer, worldwide beverage purchasing power etc.) but for now we can ignore those issues. Munger continues on to the meat of his talk: the subject of creating a product compelling enough to be consumed daily by millions of people. This is where he’s going to bring out his fourth and fifth principles: the value of academic wisdom, and the forces that must be brought together to produce “lollapalooza” effects. “We must next solve the problem of invention to create universal appeal. There are two intertwined challenges of large scale: First, over 150 years, we must cause a new-beverage market to assimilate about one-fourth of the world’s water ingestion. Second, we must so operate that half the new market is ours while all of our competitors combined are left to share the remaining half. These results are lollapalooza results. Accordingly we must attack our problem by causing every favorable factor we can think of to work for us. Plainly, only a powerful combination of many factors is likely to cause the lollapalooza consequences we desire. Fortunately, the solution to these intertwined problems turns out to be fairly easy if one has stayed awake in all the freshman [college] courses. Let us start by exploring the consequences of our simplifying “no-brainer” decision that we must rely on a strong trademark. This conclusion automatically leads to an understanding of the essence of our business in proper elementary academic terms. We can see from the introductory course in psychology that, in essence, we are going into the business of creating and maintaining conditioned reflexes. The “Coca-Cola” trade name and trade dress will act as the stimuli, and the purchase and ingestion of our beverage will be the desired responses. And how does one create and maintain conditioned reflexes? Well, the psychology text gives two answers: (1) by operant conditioning and (2) by classical conditioning, often called Pavlovian conditioning to honor the great Russian scientist. And, since we want a lollapalooza result, we must use both conditioning techniques – and all we can invent to enhance effects from each technique.” Let’s take some time to define a few of the things that Munger is talking about here so that it’s easy to follow the rest of the argument. Munger is mostly interested in the psychology of consumer decision-making: how can we influence consumers to buy a lot of a certain type of product? There are two that he’s going to talk about here: operant conditioning and classical conditioning. Classical conditioning is the method by which a strong innate response can become invoked by a neutral stimulus. The most famous demonstration of classical conditioning is the work the Russian physiologist Ivan Pavlov did with dogs: he conditioned dogs to salivate at the sound of a bell. To do this, every time Pavlov fed his dogs he would ring a bell. After repeating this procedure a few times, Pavlov found that he could ring his bell and the dogs would salivate without any food being present! (It’s historically questionable whether Pavlov actually used a bell, but we’ll leave it in for simplicity.) Getting back to the subject at hand, it’s probably clear why this concept is so powerful: it means that it’s possible to trigger an innate biological response with a stimulus of your choice. Like, for example, a logo. Now let’s briefly talk about operant conditioning. B. F. Skinner, the famed Harvard behaviorist describes operant conditioning in this way: “When a bit of behavior is followed by a certain kind of consequence, it is more likely to occur again, and a consequence having this effect is called a reinforcer.” In case this is confusing, Skinner elaborates with an example: “Food, for example, is a reinforcer to a hungry organism; anything the organism does that is followed by the receipt of food is more likely to be done again whenever the organism is hungry.” There is also a distinction between different types of reinforcers: some are negative and some are positive: “Some stimuli are called negative reinforcers; any response which reduces the intensity of such a stimulus – or ends it – is more likely to be emitted when the stimulus recurs. Thus, if a person escapes from a hot sun when he moves under cover, he is more likely to move under cover when the sun is again hot. The reduction in temperature reinforces the behavior it is ‘contingent upon’ – that is, the behavior it follows. Operant conditioning also occurs when a person simply avoids a hot sun – when, roughly speaking, he escapes from the threat of a hot sun.” Now that we have a little bit more background, let’s get back to Munger. Right now he’s trying to figure out how to use operant conditioning to increase the consumption of his product: “The operant conditioning part of our problem is easy to solve. We need only (1) maximize rewards of our beverage’s ingestion and (2) minimize possibilities that desired reflexes, once created by us, will be extinguished through operant conditioning by proprietors of competing products. For operant conditioning rewards, there are only a few categories we will find practical: (1) Food value in calories or other inputs; (2) Flavor, texture, and aroma acting as stimuli to consumption under neural preprogramming of man through Darwinian natutal selection; (3) Stimulus, as by sugar or caffeine; (4) Cooling effect when man is too hot or warming effect when man is too cool. Wanting a lollapalooza result, we will naturally include rewards in all the categories. To start out, it is easy to decide to design our beverage for consumption cold. There is much less opportunity, without ingesting beverage, to counteract excessive heat, compared with excessive cold. Moreover, with excessive heat, much liquid must be consumed, and the reverse is not true. It also easy to decide to include both sugar and caffeine. After all, tea, coffee, and lemonade are already widely consumed. And, it is also clear that we must be fanatic about determining, through trial and error, flavor and other characteristics that will maximize human pleasure while taking in the sugared water and caffeine we will provide. And, to counteract possibilities that desired operant-conditioned reflexes, once created by us, will be extinguished by operant-conditioning-employing competing products, there is also an obvious answer: We will make it a permanent obsession in our company that our beverage, as fast as practicable, will at all times be available everywhere throughout the world. After all, a competing product, if it is never tried, can’t act as a reward creating a conflicting habit. Every spouse knows that.” After talking through operant conditioning, Munger turns to classical conditioning: “We must next consider the Pavlovian [classical] conditioning we must also use. In Pavlovian conditioning, powerful effects come from mere association. The neural system of Pavlov’s dog causes it to salivate at the bell it can’t eat. And the brain of man yearns for the type of beverage held by the pretty woman he can’t have. And so, Glotz, we must use every sort of decent, honorable Pavlovian conditioning we can think of. For as long as we are in business, our beverage and its promotion must be associated in consumer minds with all other things consumers like or admire. Such extensive Pavlovian conditioning will cost a lot of money, particularly for advertising. We will spend big money as far ahead as we can imagine. But the money will be effectively spent. As we expand fast in our new beverage market, our competitors will face gross disadvantages of scale in buying advertising to create the Pavlovian conditioning they need. And this outcome, along with other volume-creates-power-effects, should help us gain and hold at least fifty percent of the new market everywhere. Indeed, provided buyers are scattered, our highest volumes will give us very extreme cost advantages in distribution. Moreover, Pavlovian effects from mere association will help us choose the flavor, texture, and color of our new beverage. Considering Pavlovian effects, we will have wisely chosen the exotic and expensive-sounding name “Coca-Cola,” instead of a pedestrian name like “Glotz’s sugared, caffeinated water.” For similar Pavlovian reasons, it will be wise to have our beverage look pretty much like wine, instead of sugared water. And so we will artificially color our beverage if it comes out clear. And we will carbonate our water, making our product seem like champagne, or some other expensive beverage, while also making its flavor better and imitation harder to arrange for competing products. And, because we are going to attach so many expensive psychological effects to our flavor, that flavor should be different from any other standard flavor so that we maximize difficulties for competitors and give no accidental same-flavor benefit to any existing product.” Having dealt with Pavlovian conditioning, Munger moves on to social proof: “What else, from the psychology textbook, can help our new business? Well, there is that powerful ‘monkey-see, monkey-do’ aspect of human nature that psychologists often call ‘social proof.’ Social proof, imitative consumption triggered by mere sight of consumption, will not only help induce trial of our beverage. It will also bolster perceived rewards from consumption. We will always take this powerful social-proof factor into account as we design advertising and sales promotion and as we forego present profit to enhance present and future consumption. More than with most other products, increased selling power will come from each increase in sales. We can now see, Glotz, that by combining (1) much Pavlovian conditioning, (2) powerful social-proof effects, and (3) wonderful-tasting, energy-giving, stimulating and desirably-cold beverage that causes much operant conditioning, we are going to get sales that speed up for a long time by reason of the huge mixture of factors we have chosen. Therefore, we are going to start something like an autocatalytic reaction in chemistry, precisely the sort of multi-factor-triggered lollapalooza effect we need. The logistics and the distribution strategy of our business will be simple. There are only two practical ways to sell our beverage: (1) as a syrup to fountains and restaurants, and (2) as a complete carbonated-water product in containers. Wanting lollapalooza results, we will naturally do it both ways. And, wanting huge Pavlovian and social-proof effects we will always spend on advertising and sales promotion, per serving, over 40 percent of the fountain price for syrup needed to make the serving. A few syrup-making plants can serve the world. However, to avoid needless shipping of mere space and water, we will need many bottling plants scattered over the world. We will maximize profits if (like early General Electric with light bulbs) we always set the first-sale price, either (1) for fountain syrup, or (2) for any container of our complete product. The best way to arrange this desirable profit-maximizing control is to make any independent bottler we need a subcontractor, not a vendee of syrup, and certainly not a vendee of syrup under a perpetual franchise specifying a syrup price frozen forever at its starting level. Being unable to get a patent or copyright on our super important flavor, we will work obsessively to keep our formula secret. We will make a big hoopla over our secrecy, which will enhance Pavlovian effects. Eventually food-chemical engineering will advance so that our flavor can be copied with near exactitude. But, by that time, we will be so far ahead, with such strong trademarks and complete, “always available” worldwide distribution, that good flavor copying won’t bar us from our objective. Moreover, the advances in food chemistry that help competitors will almost surely be accompanied by technological advances that will help us, including refrigeration, better transportation, and, for dieters, ability to insert a sugar taste without inserting sugar’s calories. Also, there will be related beverage opportunities we will seize. This brings us to a final reality check for our business plan. We will, once more, think in reverse like Jacobi. What must we avoid because we don’t want it? Four answers seem clear: First, we must avoid the protective, cloying, stop-consumption effects of aftertaste that are a standard part of physiology, developed through Darwinian evolution to enhance the replication of man’s genes by forcing a generally helpful moderation on the gene carrier. To serve our ends, on hot days a consumer must be able to drink container after container of our product with almost no impediment from aftertaste. We will find a wonderful no-aftertaste flavor by trial and error and will thereby solve this problem. Second, we must avoid ever losing even half of our powerful trademarked name. It will cost us mightily, for instance, if our sloppiness should ever allow sale of any other kind of “cola,” for instance, a “peppy cola.” If there is ever a “peppy cola,” we will be the proprietor of the brand. Third, with so much success coming, we must avoid bad effects from envy, given a prominent place in the Ten Commandments because envy is so much a part of human nature. The best way to avoid envy, recognized by Aristotle, is to plainly deserve the success we get. We will be fanatic about product quality, quality of product presentation, and reasonableness of prices, considering the harmless pleasure it will provide. Fourth, after our trademarked flavor dominates our new market, we must avoid making any huge and sudden change in our flavor. Even if a new flavor performs better in blind taste tests, changing to that new flavor would be a foolish thing to do. This follows because, under such conditions, our old flavor will be so entrenched in consumer preference by psychological effects that a big flavor change would do us little good. And it would do immense harm by triggering in consumers the standard deprival super-reaction syndrome that makes “take-aways” so hard to get in any type of negotiation and helps make most gamblers so irrational. Moreover, such a large flavor change would allow a competitor, by copying our old flavor, to take advantage of both (1) the hostile consumer super-reaction to deprival and (2) the huge love of our original flavor created by our previous work. Well, that is my solution to my own problem of turning $2 million into $2 trillion, even after paying out billions of dollars in dividends. I think it would have won with Glotz in 1884 and should convince you more than you expected at the outset. After all, the correct strategies are clear after being related to elementary academic ideas brought into play by the helpful notions.” During the rest of the piece, Munger discusses the parallels between his fictional business plan and Coca-Cola’s actual business (hint: they’re pretty much the same.) He then goes on to relate his story to the main purpose of the talk: our failure to use basic academic wisdom to make better decisions. He chalks this up, in part, to a failure of academia to produce a useful synthesis of topics like psychology and behavioral economics. He thinks that academic departments are too narrowly focused, and the research of academics to circumscribed. Is there bullshit to sniff out here? If you scan the shelves at an airport bookstore, you’re likely to find lots of books that, like Munger’s talk, purport to unveil some of the key attributes of successful companies. If you’ve ever been tempted to pick up a copy of Think Like Zuck: The Five Business Secrets of Facebook’s Improbably Brilliant CEO Mark Zuckerberg you know what I mean. Because this is a common trope in the business literature – and it’s one that we often swallow uncritically – I want to take a few paragraphs to instill in you a healthy dose of skepticism in any person that purports to unveil the hidden attributes of successful companies: First, anything that attempts to reduce the success of a business to a few key principles misses out on the obscene complexity that underlies the growth of any kind of organization. Second, it misses the fact that many (but not all) organizations are incentivized to hide the real story behind their growth in order to protect their image, their investors, their employees, or their (perceived or real) competitive advantages. Third, these attributes are subject to interpretation via the halo effect: they are seen as good ONLY because the company is successful. Many times you’ll see a CEO characterized as a visionary perfectionist when the stock is up, and an micro-managing egoist when the stock is down. Fourth, Munger’s talk is (knowingly) given with the benefit of hindsight. It’s easy to point to many of these things as sure signs of success – once the success has been achieved. Fifth, the attributes you see are subject to selection bias: people generally only write books or give talks about successful companies. Just because a successful company has attribute A, doesn’t mean that there aren’t a thousand other companies with attribute A that went to the graveyard. What we’re really looking for is evidence that a particular company attribute played a causational role in their success – rather than just merely being associated with that success. This is incredibly difficult, if not impossible, to do. In the case of Munger’s talk I’m going to err on the side of believing him for two reasons: He puts his money where his mouth is. His very long investment track record provides some demonstration that his framework works at picking companies. He doesn’t claim universal applicability. His goal isn’t to give you a foolproof way of predicting company success – it’s to give you a framework, based in basic ideas, to help you think about that success. So assuming we trust Munger, the next question we have to ask ourselves is: can we use what he’s saying? Is all of this useful? Clearly, if you’re running a big company this kind of framework could help you make better decisions. For example, had the executives at Coke who decided to introduce “New Coke” understood conditioning better they might have scrapped the plan before it became a disaster. Similarly, if you’re an institutional investor evaluating a large consumer business like Coke he provides you an interesting framework to think about. I think the argument can also be made that it provides a good framework for early-stage technology investors to think about – one that agrees with the startup-focused investors like Paul Graham, Peter Thiel, and others. (I think it’s always interesting when people who come from different backgrounds and work on different problems come to the similar conclusions about something, it usually means that there’s some truth to what they’re saying.) Applying Munger’s framework to technology investing If I had to summarize Munger’s advice in a few sentences (with the benefit of reading lots of other articles from him) it would be something like the following: In order to get large (lollapalooza) effects like rapid growth you need to harness lots of different types of elementary forces. The forces to focus on create what we call positive feedback loops: they’re self-reinforcing. A causes B, B causes more A, which causes more B. The forces of this type that Munger cites are psychological: operant conditioning and classical conditioning. All companies have to harness these kinds of psychological forces to grow. But there are other ones to look for as well: Economies of scale: the larger you are, the cheaper it is to produce your product, the more products you sell, the larger you get Network effects: the larger the network becomes, the more valuable the network is, the larger the network gets Word of mouth: the more of your product you sell, the more people talk about you, the more of your product you sell Big data effects: the more data a search engine has the, the better search results it can return, the more data it gets Incumbent advantages: the more large customers you have, the easier it gets to sign large customers, the more large customers you get Finding companies that harness these effects is important for technology investors because the venture investment model is predicated upon huge returns from companies that dominate large winner-take-all markets. And how do you get a winner-take all market? One company has to be able to build an ever-increasing advantage against its competitors. It needs to be able to achieve high enough velocity to escape the gravity of market competition. But how do you build this kind of ever-increasing lead? You effectively harness the self-reinforcing forces we’ve been discussing: conditioning, economies of scale, network effects, word of mouth, big data effects, and others. But what if you’re not an investor. What if you’re thinking of starting your own company? Can you use these ideas to come up with a startup? Is this generative? The first question to ask about his framework is this: is it generative? By this I mean, will it help me generate new ideas for businesses to start. The answer to this question is clearly: no. Thinking to yourself, “What companies can I start that will have self-reinforcing feedback loops?” doesn’t make your brain generate new ideas. As always, the best way to generate these ideas is through experience. Paul Graham writes, “The way to get startup ideas is not to try to think of startup ideas. It’s to look for problems, preferably problems you have yourself.” Ok, so Munger’s framework isn’t generative. The next question we have to ask ourselves is: is it diagnostic? In other words, assuming that Munger has identified some key causational elements of successful companies, how valuable is it for helping us diagnose companies at the idea stage? The answer to this question is: if you’re looking to build a billion dollar company, it’s at best a marginally helpful guide. And the fact that it’s not incredibly helpful isn’t really Munger’s fault. There’s just no framework of elements out there that would allow us to perfectly predict how well our ideas are going to turn out. There are lots of reasons for this. Here are a couple: Success is stochastic not deterministic Execution is more important than mere idea Your initial idea of what you’re building is often much different than what you actually build Because what you actually build is often much different than what you think you’re building at the beginning, exact analysis of this kind is difficult to do. At best, what it can help you with is a general “sniff” test: In general, does my vision for this product seem like it has the potential to harness some of these forces? If the answer is yes then it’s time to get on to the next step and build the damn thing. – Works Cited: Poor Charlie’s Almanack: The Wit and Wisdom of Charles T. Munger, Expanded Third Edition, Charlie Munger Beyond Freedom and Dignity, B.F. Skinner 19 Jan 2015, 4:45am | 2 comments </description>
      <pubDate>24 Mar 20 12:32 EDT</pubDate>
      <guid>http://danshipper.com/charlie-munger-on-how-to-build-a-2-trillion-startup</guid>
    </item>
    <item>
      <title></title>
      <link>https://eryb.space/2020/05/27/diving-into-go-by-building-a-cli-application.html</link>
      <description>&lt;a href=&#34;https://eryb.space/2020/05/27/diving-into-go-by-building-a-cli-application.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;403 Forbidden openresty </description>
      <pubDate>27 May 20 10:37 EDT</pubDate>
      <guid>https://eryb.space/2020/05/27/diving-into-go-by-building-a-cli-application.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.drmaciver.com/2016/10/some-things-that-might-help-you-write-better-software/</link>
      <description>&lt;a href=&#34;https://www.drmaciver.com/2016/10/some-things-that-might-help-you-write-better-software/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I’ve argued before that most software is broken because of economic reasons. Software which is broken because there is no incentive to ship good software is going to stay broken until we manage to change those incentives. Without that  there will be no budget for quality, and nothing you can do is going to fix it. But suppose you’re in the slightly happier place where you do have budget for quality? What then? What can you do to make sure you’re actually spending that budget effectively and getting the best software you could be getting out of it? I don’t have an easy answer to that, and I suspect none exists, but I’ve been doing this software thing for long enough now that I’ve picked up some things that seem to help quality without hurting (and ideally helping) productivity. I thought it would be worth writing them down. Many of them will be obvious or uncontroversial, but if you’re already doing all of them then your team is probably doing very well. This is all based somewhat on anecdote and conjecture, and it’s all coloured by my personal focuses and biases, so some of it is bound to be wrong. However I’m pretty sure it’s more right than wrong and that the net effect would be strongly positive. Without further ado, here is my advice. Attitude If you do not care about developing quality software you will not get quality software no matter what your tools and processes are designed to give you. This isn’t just about your developers either. If you do not reward the behaviour that is required to produce quality software, you will not get quality software. People can read their managers’ minds, and if you say you want quality software but reward people for pushing out barely functioning rubbish, people are smart enough to figure out you don’t really mean that. Estimated cost: Impossible to buy, but hopefully if you’re reading this article you’re already there. If you’re embedded in a larger context that isn’t, try creating little islands of good behaviour and see if you can bring other people around to your way of thinking. Estimated benefit: On it’s own, only low to moderate – intent doesn’t do much without the ability to act – but it is the necessary precursor to everything else. Controversy level: Probably most people agree with this, although equally it doesn’t feel like most people implement this. I imagine there are some people who think they can fix this problem if they just find the right process. Maybe they’re right, but I’ve never seen something even approaching such a process. Automated Testing Obviously I have quite a few thoughts about automated testing, so this section gets a lot of sub headings. Continuous Integration You need to be running automated tests in some sort of CI server that checks every ostensibly releasable piece of software and checks whether it passes the tests. If you’re not doing this, just stop reading this article and go set it up right now, because it’s fundamental. Add a test that just fires up your website and requests the home page (or some equivalent if you’re not writing a website). You’ve just taken the first and most important step on the road from throwing some crap over the wall and seeing if anyone on the other side complains about it landing on them to actual functional software development. Estimated cost: Hopefully at worst a couple days initial outlay to get this set up, then small to none ongoing. Estimated benefit: Look, just go do this already. It will give you a significant quality and productivity boost. Controversy level: It would be nice to think this was uncontroversial. It’s certainly well established best practice, but I’ve worked at companies that don’t do it (a while ago), and a friend basically had to ramrod through getting this implemented at the company they’d joined recently. Local Automated Testing You need to be able to run a specific test (and ideally the whole test suite) against your local changes. It doesn’t really matter if it runs actually on your local computer, but it does matter that it runs fast. Fast feedback loops while you work are incredibly important. In many ways the length of time to run a single test against my local changes is the biggest predictor of my productivity on a project. Ideally you need to  be able to select a coherent group of tests (all tests in this file, all tests with this tag) and run just those tests. Even better, you should be able to run just the subset of whatever tests you ask to run that failed last time. If you’re using Python, I recommend py.test as supporting these features. If you’re currently using unittest you can probably just start using it as an external runner without any changes to your code. Estimated cost: Depends on available tooling and project. For some projects it may be prohibitively difficult (e.g. if your project requires an entire hadoop cluster to run the code you’re working on), but for most it should be cheap to free. Estimated benefit: Similarly “look, just go do this already” if you can’t run a single test locally. More specific improvements will give you a modest improvement in productivity and maybe some improvement in quality if they make you more likely to write good tests, which they probably will. Controversy level: Not very. I’ve only worked at one company where running tests locally wasn’t a supported workflow, and I fixed that, but workflows which support my slightly obsessive focus on speed of running a single test are rarely as good as I’d like them to be. Regression Testing The only specific type of automated testing that I believe that absolutely everybody should be doing is regression testing: If you find a bug in production, write a test that detects that bug before you try to fix the bug. Ideally write two tests: One that is as general as possible, one that is as specific as possible. Call them an integration and a unit test if that’s your thing. This isn’t just a quality improvement, it’s a productivity improvement. Trying to fix bugs without a reproducible example of that bug is just going to waste your time, and writing a test is the best way to get a reproducible example. Estimated cost: Zero, assuming you have local testing set up. This is just what you should be doing when fixing bugs because it’s better than the other ways of fixing bugs – it will result in faster development and actually fixed bugs. Estimated benefit: Depends how much time you spend fixing bugs already, but it will make that process faster and will help ensure you don’t have to repeat the process. It will probably improve quality somewhat by virtue of preventing regressions and also ensuring that buggier areas of the code are better tested. Controversy level: In theory, not at all. In practice, I’ve found many to most developers need continual reminders that this is a thing you have to do. Code Coverage You should be tracking code coverage. Code coverage is how you know code is tested. Code being tested is how you know that it is maybe not completely broken. It’s OK to have untested code. A lot of code isn’t very important, or is difficult enough to test that it’s not worth the effort, or some combination of the two. But if you’re not tracking code coverage then you don’t know which parts of your code you have decided you are OK with being broken. People obsess a lot about code coverage as a percentage, and that’s understandable given that’s the easiest thing to get out of it, but in many ways it’s the least important part of code coverage. Even the percentage broken down by file is more interesting that, but really the annotated view of your code is the most important part because it tells you which parts of your system are not tested. My favourite way to use code coverage is to insist on 100% code coverage for anything that is not explicitly annotated as not requiring coverage, which makes it very visible in the code if something is untested. Ideally every pragma to skip coverage would also have a comment with it explaining why, but I’m not very good about that. As a transitional step to get there, I recommend using something like diff-cover or coveralls which let you set up a ratcheting rule in your build that prevents you from decreasing the amount of code coverage. Estimated cost: If your language has good tooling for coverage, maybe a couple hours to set up. Long-term, essentially free. Estimated benefit: On its own, small, but it can be a large part of shifting to a culture of good testing, which will have a modest to large effect. Controversy level: Surprisingly high. Of the companies I’ve worked at precisely zero have tracked code coverage (in one case there was a push for it but younger me argued against it. My opinions on testing have changed a lot over the years). Property-based Testing Property-based testing is very good at shifting the cost-benefit ratio of testing, because it somewhat reduces the effort to write what is effectively a larger number of tests and increases the number of defects those tests will find. I won’t write too much about this here because I have an entire separate site about this. Estimated cost: If you’re using a language with a good property based testing tool, about 15 minutes to install the package and write your first test. After that, free to negative. If you’re not, estimated cost to pay me to port Hypothesis to a new language is around £15-20k. Estimated benefit: You will find a lot more bugs. Whether this results in a quality improvement depends on whether you actually care about fixing those bugs. If you do, you’ll see a modest to large quality improvement. You should also see a small to modest productivity improvement if you’re spending a lot of time on testing already. Controversy level: Not very high, but niche enough that most people haven’t formed an opinion on it. Most people think property based testing is amazing when they encounter it. Some push back on test speed and non-determinism (both of which have partial to complete workarounds in Hypothesis at least) Manual Testing At a previous job I found a bug in almost every piece of code I reviewed. I used a shocking and complicated advanced technique to do this: I fired up the application with the code change and tried out the feature. Manual testing is very underrated. You don’t have to have a dedicated QA professional on your team to do it (though I suspect it helps a lot if you do), but new features should have a certain amount of exploratory manual testing done by someone who didn’t develop them – whether it’s another developer, a customer service person, whatever. This will find both actual bugs and also give you a better idea of its usability. And then if they do find bugs those bugs should turn into automated regression tests. Estimated cost: It involves people doing stuff on an ongoing basis, so it’s on the high side because people are expensive, but it doesn’t have to be that high to get a significant benefit to it. You can probably do quite well with half an hour of testing of a feature that took days to develop. This may also require infrastructure changes to make it easy to do that can have varying levels of cost and difficulty, but worst case scenario you can do it on the live system. Estimated benefit: You will almost certainly get a moderate quality improvement out of doing this. Controversy level: Having QA professionals seems to be entirely against the accepted best practice in startups. The rest, similar to regression testing: Doing a bit of manual testing seems to be one of those things where people say “Of course we do that” and then don’t do it. Version Control You need to be using a version control system with good branching and merging. This is one of the few pieces of advice where my recommendation requires making a really large change to your existing workflow. I hope that it’s relatively uncontroversial that you should be using version control (not everybody is!). Ideally you should be using good version control. I don’t really care if you use git, mercurial, fossil, darcs, whatever. We could get into a heated argument about which is better but it’s mostly narcissism of small differences at this point. But you should probably move off SVN if you’re still on it and you should definitely move off CVS if you’re still on it. If you’re using visual source safe you have my sympathies. The reason is simple: If you’re working on a team of more than one person, you need to be able to incorporate each other’s changes easily, and you need to be able to do that without trashing your own work. If you can’t, you’re going to end up wasting a lot of your time. Estimated cost: Too project dependent to say. Importer tools are pretty good, but the real obstacle is always going to be the ecosystem you’ve built around the tools. At best you’re going to have a bad few weeks or months while people get used to the new system. Estimated benefit: Moderate to large. Many classes of problems will just go away and you will end up with a much more productive team who find it much easier to collaborate. Controversy level: Basically uncontroversial. Not as widespread as you might imagine, but not controversial. Once git started becoming popular basically everywhere I’ve worked used it (with one exception for mercurial and one exception for Google’s interesting perforce ish based system). Monorepos Use a single repository for all your code. It’s tempting to split your projects into lots of small repos for libraries and services, but it’s almost always a bad idea. It significantly constrains your ability to refactor across the boundary and makes coordinating changes to different parts of the code much harder in almost every way, especially with most standard tooling. If you’re already doing this, this is easy. Just don’t change. If you’re not, just start by either creating or designating an existing repository as the monorepo and gradually move the contents of other repos into it as and when convenient. The only exception where you probably need to avoid this is specific projects you’re open sourcing, but even then it might be worth developing them in the monorepo with some sort of external repo. This point has proved controversial, so if you’re still unconvinced I have written a longer advocacy piece on why you should use a monorepo. Estimated costs: Too project dependent to say, but can be easily amortised over time. Estimated benefits: Every time you do something that would have required touching two repos at once, your life will be slightly easier because you are not paying coordination costs. Depends on how frequent that is, but experience suggests it’s at least a modest improvement. Controversy level: High. This piece of advice is extremely love/hate. I think most of the people who love it are the ones who have tried it at least once and most of the people who hate it are those who haven’t, but that might be my biases speaking. It’s been pretty popular where I’ve seen it implemented. Static Analysis I do not know what the right amount of static analysis is, but I’m essentially certain that it’s not none. I would not be surprised to learn that the right amount was quite high and includes a type system of some sort, but I don’t know (I also would not be surprised to discover that it was not). However even very dynamic languages admit some amount of static analysis and there are usually tools for it that are worth using. I largely don’t think of this as a quality thing though. It’s much more a productivity improvement.  Unless you are using a language that actively tries to sabotage you (e.g. C, JavaScript), or you have a really atypically good static analysis system that does much more work than the ones I’m used to (I’m actually not aware of any of these that aren’t for C and/or C++ except for languages with actual advanced type systems), static analysis is probably not going to catch bugs much more effectively than a similar level of good testing. But what it does do is catch those bugs sooner and localise them better. This significantly improves the feedback loop of development and stops you wasting time debugging silly mistakes. There are two places that static analysis is particularly useful: In your editor. I use syntastic because I started using vim a decade ago and haven’t figured out how to quit yet, but your favourite editor and/or IDE will likely have something similar (e.g. The Other Text Editor has flycheck). This is a really good way of integrating lightweight static analysis into your workflow without having to make any major changes. In CI. The ideal number of static analysis errors in your project is zero (this is true even when the static analysis system has false positives in my opinion, with the occasional judicious use of ‘ignore this line’ pragmas), but you can use the same tricks as with code coverage to ratchet them down to zero from wherever you’re starting. Most languages will have at least a basic linting tool you can use, and with compiled languages the compiler probably has warning flags you can turn on. Both are good sources of static analysis that shouldn’t require too much effort to get started with. Estimated cost: To use it in your editor, low (you can probably get it set up in 10 minutes). To use it in your CI, higher but still not substantial. However depending on the tool it may require some tuning to get it usable, which can take longer. Estimated benefit: Depends on the tool and the language, but I think you’ll get a modest productivity boost from incorporating static analysis and may get a modest to large quality boost depending on the language (in Python I don’t think you’ll get much of a quality benefit. In C I think you’ll get a huge one even with just compiler warnings). Controversy level: Varies entirely depending on level of static analysis. Things that you could reasonably describe as “linting” are low. Things that require something closer to a type system much higher. Tools with a high level of false positives also high. You can definitely find an uncontroversial but still useful level of static analysis. I’ve seen it at a moderate subset of the companies I worked for. Production Error Monitoring You should have some sort of system that logs all errors in production to something more interactive than a log file sitting on a server somewhere. If you’re running software locally on end users’ computers this may be a bit more involved and should require end user consent, but if you’re writing a web application we’re all used to being pervasively spied on in everything we do anyway so who cares? I’ve used and like Sentry for this. There are other options, but I don’t have a strong opinion about it. Estimated cost: Depends on setup, but getting started with sentry is easy and it doesn’t cost a particularly large amount per month (or you can host the open source version for the cost of a server). Estimated benefit: Much better visibility of how broken your software is in production is the best thing for making your software less broken in production. It will also speed up your debugging process a lot when you do have production errors to fix, so it’s probably a net win in productivity too if you spend much time debugging production errors (and you probably do). Controversy level: Low but surprisingly it’s not nearly as widely implemented as you might expect. Another thing that is becoming more common though I think. Assertions I am a big fan of widespread use of assertions, and of leaving them on in production code. The main reason for this is simple: The single biggest factor in ease of debugging is making sure that the point at which the error is reported is as close as possible to the point at which the error occurs. Assertions are a very good way to do this because they turn a failure of understanding into a runtime error: If your code is not behaving in a way you’d expect, that becomes an error immediately, and it is much easier to debug than finding the downstream thing that actually went wrong at some point later. It also has a huge benefit when doing property-based testing, because they greatly increase the scope of properties tested – problems that might not have been noticed by the explicit test become much more visible if they trigger an assertion failure. Input validation while technically not an assertion also has the same effect – a function which checks its arguments rather than silently doing the wrong thing when given a wrong argument will be significantly easier to debug. John Regehr has a good post on the care and feeding of assertions that I recommend reading further. Estimated cost: Low if you just start adding them in as you develop and edit code. Requires a bit of careful thinking about what the code is doing, but that’s no bad thing. Estimated benefit: Modest. This won’t be life changingly good, but I have frequently been grateful for a well placed assertion in my code preventing what would otherwise be a much more confusing bug. Controversy level: People don’t really seem to have an opinion on this one way or another, but it’s not a common habit at all. I’ve not seen it be widespread at any company I’ve worked for. Code Review I think all projects with &gt; 1 person on them should put all code changes through code review. Code review seems to be a fairly cost effective defect finding tool according to the literature. I previously believed this not to be the case, but I’ve done some reading and I changed my mind. But regardless of whether you find defects, it will ensure two  very important things: At least one other person understands this code. This is useful both for bus factor and because it ensures that you have written code that at least one other person can understand. At least one other person thinks that shipping this code is a good idea. This is good both for cross checking but also because it forces you to sit down and think about what you ship. This is quite important: Fast feedback loops are good for development, but slow feedback loops for shipping make you pause and think about it. Over time this will lead to a significantly more maintainable and well designed piece of software. Estimated cost: You need to get a code review system set up, which is a modest investment and may be trivial. I can’t really recommend anything in this space as the only things I’ve used for this are Github and proprietary internal systems. Once you’ve got that, the ongoing cost is actually quite high because it requires the intervention of an actual human being on each change. Estimated benefit: It’s hard to say. I have never been part of a code review process that I didn’t think was worth it, but I don’t have a good way of backing that up with measurements. It also depends a lot on the team – this is a good way of dealing with people with different levels of experience and conscientiousness. Controversy level: Fairly uncontroversial, though at least amongst small companies it used to be weird and unusual. At some point in my career it went from “nobody does this” to “everybody does this”. I think a combination of GitHub pull requests and acknowledgement that most of the cool big companies do it seems to have taken this from a niche opinion to widespread practice in a remarkably short number of years. Continuous Delivery Another part of localising things to when they went wrong is that ideally once something has passed code review it will ship as soon as possible. Ideally you would ship each change as its own separate release, but that isn’t always practical if you’re e.g. shipping client side software. This helps ensure that when something goes wrong you have a very good idea of what caused it because not that much changed. Another important part of this is that when a release goes out you should always be able to roll it back easily. This is essential if you want to make releasing low cost, which is in turn essential for having this sort of frequent release. A thing I have never worked with personally but have regarded with envy is staged roll out systems which first roll out to a small fraction of the customer base and then gradually ratchet up until it reaches 100%, rolling back automatically or semi-automatically if anything seems to have gone wrong in the process. Estimated cost: The transitional period from infrequent to frequent deliveries can be a bit rough – you’ll need to spend time automating manual steps, looking for inefficiencies, etc. Take baby steps and gradually try to improve your frequency over time and you can spread this out fairly easily though. Estimated benefit: A modest quality improvement, and quite a large improvement in debugging time if you currently end up with a lot of broken releases. The release process changes you have to make to make this work will probably also be a significant net time saver. Controversy level: I’m not sure. It hasn’t seemed that controversial where I’ve seen it implemented, but I think larger companies are more likely to hate it. Auto formatting and style checking Code review is great, but it has one massive failure mode. Consider Wadler’s law: In any language design, the total time spent discussing a feature in this list is proportional to two raised to the power of its position. 0. Semantics 1. Syntax 2. Lexical syntax 3. Lexical syntax of comments Basically the same thing will happen with code review. People will spend endless time arguing about style checking, layout, etc. This stuff matters a bit, but it doesn’t matter a lot, and the back and forth of code review is relatively expensive. Fortunately computers are good at handling it. Just use an auto-formatter plus a style checker. Enforce that these are applied (style checking is technically a subset of static analysis but it’s a really boring subset and there’s not that much overlap in tools). In Python land I currently use pyformat and isort for auto-formatting and flake8 for style checking. I would like to use something stronger for formatting – pyformat is quite light touch in terms of how much it formats your code. clang-format is extremely good and is just about the only thing I miss about writing C++. I look forward to yapf being as good, but don’t currently find it to be there yet (I need to rerun a variant on my bug finding mission I did for it last year at some point). gofmt is nearly the only thing about Go I am genuinely envious of. Ideally you would have your entire project be a fixed point of the code formatter. That’s what I do for Hypothesis. If you haven’t historically done that it can be a pain though. Many formatting tools can be applied based on only the edited subset of the code. If you’re lucky enough to have one of those, make that part of your build process and have it automatically enforced. Once you have this, you can now institute a rule that there should be no formatting discussion in code review because that’s the computer’s job. Here’s a great post from GDS about this technique and how it’s helped them. Estimated cost: Mostly tool dependent, but if you’re lucky it’s basically free. Also some social cost – some people really dislike using style checkers (and to a lesser degree auto-formatters) for reasons that don’t make much sense to me. I personally think the solution is for them to get over it, but it may not be worth the effort of fighting over it. Estimated benefit: From the increased consistency of your code, small but noticeable. The effect on code review is moderate to large, both in terms of time taken and quality of review. Controversy level: Surprisingly high. Some people really hate this advice. Even more people hate this advice if you’re not running a formatter that guarantees style conforming code (e.g. I’m not on Hypothesis because none of the Python code formatters can yet). I’ve only really seen this applied successfully at work once. Documentation in the Repository You should have a docs section in your repository with prose written about your code. It doesn’t go in a wiki. Wikis are the RCS of documentation. We’ve already established you should be using good version control and a monorepo, so why would you put your documentation in RCS? Ideally your docs should use something like sphinx so that they compile to a (possibly internally hosted) site you can just access. It’s hard to keep documentation up to date, I know, but it’s really worth it. At a bare minimum I think your documentation should include: Up to date instructions for how to get started developing with your code Detailed answers to questions you find yourselves answering a lot Detailed post-mortems of major incidents with your product For most projects they should also include a change log which is updated as part of each pull request/change list/whatever. It may also be worth using the documentation as a form of “internal blogging” where people write essays about things they’ve discovered about the problem domain, the tools you’re using or the local style of work. Estimated cost: Low initial setup. Writing documentation does take a fair bit of time though, so it’s not cheap. Estimated benefit: This has a huge robustness benefit, especially every time your team changes structure or someone needs to work on a new area of the code base. How much benefit you’ll derive varies depending on that, but it’s never none – if nothing else, everybody forgets things they don’t do often, but also the process of writing the documentation can hugely help the author’s understanding. Controversy level: Another case of “most people probably agree this is a good idea but don’t do it”. Unless you’ve got someone pushing for it a lot, documentation tends to be allowed to slide. I’ve never really seen this work at any of the company’s I’ve worked for. Plan to always have more capacity than work Nick Stenning made an excellent point on this recently: If your team is always working at full capacity then delays in responding to changes will sky rocket, even if they’re coming in at a rate you can handle. As well as that, it tends to mean that maintenance tasks that can greatly improve your productivity will never get done – almost every project has a back log of things that are really annoying the developers that they’d like to fix at some point and never get around to. Downtime is an opportunity to work on that. This doesn’t require some sort of formal 20% time arrangement, it just requires not trying to fit a quart into a pint pot. In particular, if you find you’ve scheduled more work than got done, that’s not a sign that you slightly over estimated the amount you could get done that’s a sign that you scheduled significantly too much work. Estimated Cost: Quite expensive. Even if you don’t formally have 20% time, you’re probably still going to want to spend about 20% of your engineering capacity this way. It may also require significant experimentation to get your planning process good enough to stop overestimating your capabilities. Estimated Benefit: You will be better able to respond to changes quickly and your team will almost certainly get more done than they were previously getting done in their 100% time. Controversy level: Fairly high. Almost everywhere I’ve worked the team has consistently planned more work than they have capacity for. Projects should be structured as collections of libraries Modularity is somewhat overrated, but it’s not very overrated, and the best way of getting it is to structure things as libraries. The best way to organise your project is not as a big pot of code, but as a large number of small libraries with explicit dependencies between them. This works really well, is easy to do, and helps keep things clean and easy to understand while providing push back against it all collapsing into a tangled mess. There are systems like bazel that are specifically designed around structuring your project this way. I don’t have very fond memories of its origin system, and I’ve not used the open source version yet, but it is a good way of enforcing a good build structure. Otherwise the best way to do this is probably just to create subdirectories and use your language’s standard packing tools (which probably include a development mode for local development. e.g. pip install -e if you’re using Python). Some people may be tempted to do this as microservices instead, which is a great way to get all the benefits of libraries alongside all the benefits of having an unreliable network and a complicated and fragile deployment system.  There are some good reasons to use microservices in some situations, but using them purely as a way to achieve modularity is just a bad idea. Estimated cost: Quite low. Just start small – factor out bits of code and start new code in its own library. Evolve it over time. Estimated benefit: Small to moderate productivity enhancement. Not likely to have a massive impact on quality, but it does make testing easier so it should have some. Controversy level: Fairly low. I’m not sure people have an opinion on this one way or the other. Ubiquitous working from home I’m actually not a huge fan of massively distributed teams, mostly because of time zones. It tends to make late or early meetings a regular feature of peoples’ lives. I could pretend to be altruistic and claim that I disapprove of that because it’s bad for people with kids, which it is, but I also just really hate having to do those myself. But the ability to work from home is absolutely essential to a productive work environment, for a number of reasons: Open plan offices are terrible. They are noisy distraction zones that make it impossible to get productive work done. Unfortunately, this battle is lost. For whatever reason the consensus is that it’s more cost effective to cram developers in at 50% efficiency than it is to pay for office space. This may even be true. But working from home generally solves this by giving people a better work environment that the company doesn’t have to pay for. Requiring physical presence is a great way for your work force to be constantly sick! People can and should take sick days, but if people cannot work from home then they will inevitably come in when they feel well enough to work but are nevertheless contagious. This will result in other people becoming ill, which will either result in them coming and spreading more disease or staying home and getting nothing done. Being able to work from home significantly reduces the incentive to come in while sick. Not having physical access to people will tend to improve your communication patterns to be lower interrupt and more documentation driven, which makes them work better for everyone both in the office and not. I do not know what the ideal fraction of work from home to work in the office is, but I’d bet money that if most people are not spending at least two days per week working from home then they would benefit from spending more. Also, things will tend to work better as the fraction increases: If you only have a few people working from home at any given point, the office culture will tend to exclude them. As you shift to it being more normal, work patterns will adapt to accommodate them better. Estimated cost: There may be some technical cost to set this up – e.g. you might need to start running a VPN – but otherwise fairly low. However there may be quite a lot of political and social push back on this one, so you’re going to need a fair bit of buy in to get it done. Estimated benefit: Depends on team size and current environment, but potentially very large productivity increase. Controversy level: Fairly low amongst developers, fairly high amongst the non-developers who you’ll probably need to get sign off on it. No Long Working Hours Working longer work weeks does not make for more productive employees, it just results in burning out, less effective work and spending more time in the office failing to get anything done. Don’t have work environments that encourage it. In fact, it’s better if you don’t have work environments that allow it, because it will tend to result in environments where it goes from optional to implicitly mandatory due to who gets rewarded. It’s that reading managers’ minds thing again. Estimated cost: Same as working from home: Low, but may require difficult to obtain buy in. Will probably also result in a transitional period of lower productivity while people are still burned out but less able to paper over it. Estimated benefit: High productivity benefits, high quality benefits. Exhausted people do worse work. Controversy level: High. Depending on who you’re talking to this is either obviously the correct thing to do or basically communism (there may also be some people who think it’s basically communism and that’s why they like it). Good Work Culture Or the “don’t work with jerks” rule. People need to be able to ask questions without being afraid. People need to be able to give and receive feedback without being condescending or worrying that the other side will blow up at them or belittle them. People need to be able to take risks and be seen to fail without being afraid of how much it will cost them. There are two major components to this: Everyone needs to be on board with it and work towards it. You don’t need everyone to be exquisitely polite with everyone else at all times – a certain amount of directness is usually quite beneficial – but you do need to be helpful, constructive and not make things personal. Some people are jerks and you should fire them if they don’t fix their ways. It is really hard to do the second one, and most people don’t manage, but it’s also really important. Try to help them fix their ways first, but be prepared to let them go if you can’t, because if you’ve got a high performing jerk on the team it might be that they’re only or in large part high performing because they’re making everyone else perform worse. Even if they really are that good, they’re probably not good enough to justify the reduced productivity from everyone else. Note: This includes not just developers but also everyone else in the company. Estimated cost: High. Changing culture is hard. Firing people is hard, especially if they’re people who as individual performers might look like your best employees. Estimated benefit: Depending on how bad things are currently, potentially extremely high. It will bring everyone’s productivity up and it will improve employee retention. Controversy level: Another “Not controversial but people don’t actually do it”. I’ve mostly seen the end result of jerks leaving under their own volition and everyone breathing a sigh of relief and experiencing a productivity boost. Good Skill Set Mixing You generally want to avoid both silos and low bus factors. In order to do that, it’s important to have both overlapping and complementary skills on your team: A good rule of thumb is that any task should have at least two people who can do it, and any two people should have a number of significant tasks where one would obviously be better suited to work on it than another. The former is much more important than the latter, but both are important. Having overlapping skills is important because it increases your resilience and capacity significantly: If someone is out sick or on holiday you may be at reduced capacity but there’s nothing you can’t do. It also means there is always a second perspective you can get on any problem you’re stuck with. Having complementary skills is important because that’s how you expand capabilities: Two people with overlapping skills are much better able to work together than two people with nothing in common, but two people with identical skills will not be much better working together than either of them individually. On the other hand two people working together who have different skill sets can cover the full range of either of their skills. This is a hard one to achieve, but it will tend to develop over time if you’re documenting things well and doing code review. It’s also important to bear in mind while hiring. Estimated cost: Hard to say because what you need to do to achieve it is so variable, but it will probably require you to hire more people than you otherwise would to get the redundant skill sets you need, so it’s not cheap. Estimated benefit: Huge improvement in both total team and individual productivity. Controversy level: Not exactly controversial, but tends not to happen in smaller companies due to people not seeing the benefit. Where it happens it tends to happen by accident. Hire me to come take a look at what might be wrong I do do software consulting after all. This isn’t what I normally consult on (I’m pretty focused on Hypothesis), but if you’d like the help I’d be happy to branch out, and after accidentally writing 5000 words on the subject I guess I clearly have a few things to say on the subject. Drop me an email if you’re interested. Estimated cost: My rates are very reasonable. Estimated benefit: You’re probably better placed to answer this one than I am, but if this document sounds reasonable to you but you’re struggling to get it implemented or want some other things to try, probably quite high. Controversy level: Not at all controversial. Everyone thinks this is a great idea and you should do it. Honest. </description>
      <pubDate>24 Mar 20 12:31 EDT</pubDate>
      <guid>https://www.drmaciver.com/2016/10/some-things-that-might-help-you-write-better-software/</guid>
    </item>
    <item>
      <title>How to Do Research With a Professor</title>
      <link>http://www.cs.jhu.edu/~jason/advice/how-to-work-with-a-professor.html</link>
      <description>&lt;a href=&#34;http://www.cs.jhu.edu/~jason/advice/how-to-work-with-a-professor.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; by Jason Eisner (2012) Summary This is a bit of advice for lucky students who get to do research with a professor. Take this opportunity seriously. Either you make it your top priority, or you don&#39;t do it at all. That&#39;s the message. Read the rest of the page if you want to know why and how. Why This Webpage? I&#39;d find it awkward to say these things directly to a nice undergrad or master&#39;s student I was starting to work with. It would feel like talking down to them, whereas I like my research collaborators—however junior—to talk with me comfortably as equals, have fun, and come up with half the ideas. Still, it&#39;s important to understand up front what the pressures are on faculty-student collaborations. So here are some things to bear in mind. How the Professor Sees It [If the professor is female/male, click here.] Your research advisor doesn&#39;t get much credit for working with junior students, and would find it easier and safer to work with senior students. It&#39;s just that someone gave him/her a chance once: that&#39;s how he/she ended up where he/she is today. He/She&#39;d like to pay that debt forward. But should it be paid forward to you? Choosing you represents a substantial commitment on your advisor&#39;s part, and a vote of confidence in you. Time Investment The hours that your advisor spends with you, one-on-one, are hours that he/she no longer has available for retaining the semblance of a plausible life (sleeping / eating / parenting / avoiding divorce) consulting at rates of hundreds of dollars per hour preparing for class working on research with other students (grad or undergrad) or by himself/herself staying current with the latest papers and techniques in the field discharging many administrative and reviewing responsibilities So he/she does expect that you&#39;ll pay him/her back, by working as hard as he/she did when he/she got his/her chance. Research Agenda Investment Your advisor is not only devoting time to you, but taking a risk. You are being entrusted with part of his/her research agenda. The goal is to make new discoveries and publish them on schedule. If you drop the ball, then your advisor and others in the lab will miss important publication deadlines, or will get scooped by researchers elsewhere, or will be unable to take the next step that was depending on you. So, don&#39;t start doing research with the idea that it&#39;s something &#34;extra&#34; that may or may not work out. This is not an advanced course that you can just drop or do poorly in. Unless your advisor agrees otherwise, you are a critical player in the mission—you have a responsibility not to let others down. Remember, someone is taking a chance on you. Opportunity Cost I heard once that your boyfriend or girlfriend will ask increasingly tough questions as your relationship ages: &#34;Am I getting something out of it?&#34; &#34;Am I getting back as much as I&#39;m putting in?&#34; &#34;Am I getting as much as I&#39;m worth?&#34; Your advisor may also ask these questions. At first, he/she&#39;ll be happy that he/she attracted a smart student to work on a problem that needed working on. But he/she may sour if he/she comes to feel that he/she&#39;s wasting his/her time on you, or would have been wiser to assign the project to someone else. What Do You Get Out Of It? You too are giving up time from your other activities (including classwork!) to do this. So what do you get out of it? Most important, you get research experience. This is exceptionally important if you are considering doing a Ph.D. The Ph.D. puts you on a track to focus on research for the next 5+ years and possibly for your whole life. Are you sure you want to get married to research? Maybe, but try dating research first before you commit. Ph.D. programs are looking for students who are already proven researchers. Grades are not so strongly correlated with research success. The most crucial part of your application is letters from one or more credible faculty who can attest—with lots of supporting detail—that you have the creativity, intelligence, enthusiasm, productivity, technical background, and interpersonal and intrapersonal skills to do a great Ph.D. with your future advisor. A good friend of mine in college was taken under the wing of a senior professor in a different department. She was a demanding taskmaster, and my friend ended up spending much more time working in her lab than he expected. But it changed his life. She insisted that he apply to grad school in her field, and she got him accepted to a top Ph.D. program. He became a professor and is now the chairman of a department at a highly respected school, where he enjoys doing research with his own undergraduates. Even if you are not considering a Ph.D., you will learn a great deal from working closely with a professor. Often you may be working with the world&#39;s leading expert on a particular topic—that&#39;s the main criterion for tenure here. (So our tenured faculty have passed this bar at some point, and most of our untenured faculty are successfully building a case that they will do so.) Students don&#39;t always realize how respected and innovative our faculty are within their own subfields, but that&#39;s why you chose to attend a highly-ranked research university. Your advisor may or may not be a great classroom teacher, but he/she has shown himself/herself to be extremely good at working with graduate students to produce papers that advance the field. What you&#39;ll learn from doing that is quite different from what you&#39;ll learn in the classroom. What You Can Do to Succeed Here&#39;s some basic advice targeted at new research students. There are also many webpages about how to be a &#34;good grad student,&#34; which should also be useful to undergrads doing research. Time Commitment Make plenty of room. In order to make research your first priority, you may need to reduce your courseload or extracurriculars. This is worth discussing with both your academic advisor and your research advisor. Find out what the deadlines are. For example, there may be a target for submitting a paper to a particular conference. When planning for deadlines, bear in mind that everything will take twice as long as you expect—or four times as long if you&#39;ve never done it before. Often a paper takes roughly a year of work for a grad student (if it includes experiments), although they may be working on other things during that year as well. Be honest. If you suspect that you may not have time to do justice to the project after all, don&#39;t string your advisor along. Take a deep breath, apologize, and explain the situation. Then your advisor can make an informed decision about whether to suspend the project, give it to someone else, get a grad student involved, etc. This is better than a slow burn of agitation on both sides. Time Management Prepare for meetings. Establish a fixed time for weekly meetings with your advisor (and perhaps with senior students). Bring results, questions, and an agenda to your weekly meeting. Make weekly progress. Set goalposts, and be sure you make real progress from week to week. Use your meeting time or email each week to make sure that you agree on what the goal for next week is. Take the initiative. Be somewhat self-directed—find readings, play around with code, do mini-experiments. But do keep your advisor posted by email. Writing Writing is a form of thinking, a form of memory, and a form of communication. You should keep well-organized notes of several kinds. It is often useful to date your entries in such files and to keep them under version control. &#34;Write the paper first.&#34; The evolving paper is a way of organizing and sharing your thoughts and hammering out details. New ideas (including future plans) can go into that document, or appendices to it. Experimental logbook. This is a file recording the questions you asked, the experiments you ran to answer them (including the command-line details needed to reproduce them perfectly), the results, and your analysis of the results. Notes on your reading, including reading you plan to do. This should be organized by paper and/or by topic, aimed at helping you quickly recover the important points. Planning. Keep some kind of to-do list and time planning system that helps you set and discharge goals and track your effectiveness (see the LifeHacker website for some options). Working With Others Again, be honest. Be very clear at all times about what you do and don&#39;t understand. Don&#39;t fake it. It&#39;s okay to say you&#39;re confused or don&#39;t know something; you need to ask questions to get unconfused. Also be clear about what you have and haven&#39;t done. Pick a topic of mutual interest that you can handle. This is a matter for careful discussion at the start of the relationship. Be explicit about what you need from your advisor. You can take some initiative in shaping the kind of advising relationship that will work best for you. Every advisor has a typical advising style, which is some compromise between his/her advising philosophy, his/her personality, your personality, and the realities of limited time. But if you need a different kind of guidance or a different way of organizing your relationship, ask for it. Most advisors will appreciate the initiative and can adapt to some extent. Know how to ask for help. If you feel you would benefit from closer guidance, say: &#34;Please tell me exactly what you want me to do by next Wednesday and I will have it done.&#34; If you get stuck technically, ask your advisor to help you get unstuck! He/She can write out a more detailed plan for you, give you things to read, ask a senior grad student to work with you, point you to software libraries, etc. Asking the right person can be 100 times faster than doing it yourself. Your value to the project lies in how much you get done—it doesn&#39;t matter whether you invented it all yourself. This is not homework and getting help is not cheating. Anything that is already known in the field is fair game to reuse (with citations). And people can also help you invent the new stuff, as long as you acknowledge their help appropriately (possibly with co-authorship). Getting them to help you is part of the research. Get right as much as you can. Before you hand off a piece of code or writing to someone else -- including another student, your advisor, or a reviewer -- you ought to catch all the problems you can catch by yourself. For a problem that you intend to fix later, include a note to this effect. This allows the other person to focus their limited time on spotting the problems that were beyond your own horizon. Be a team player. If there are other people on the project, find out what they&#39;re working on. Ask plenty of questions. Get a broader sense of the project beyond your own little corner. Help out where you can. Share what you do. Back up your work, comment your code, log your experiments, and be ready to hand off your code and notes at any time. The project may live on after you. It&#39;s not necessary to keep private files. The best plan is to keep everything valuable in a shared version control repository that you, your advisor, and any other collaborators can browse and edit at any time. (A README file in the repository can describe the layout and list any additional resources, e.g., the URLs of a wiki, a Google Doc, etc.) An issue tracker is also useful. Discuss with your advisor how to set up this kind of project infrastructure, e.g., on github. Avoid diffusion. As a matter of etiquette, try not to spread your work over many different local directories, repositories, email threads, chat logs, Google documents, etc. For example, when sending email, try to continue on an existing thread where appropriate, rather than starting a new one. Your advisor is juggling more email and projects than you, so will find it helpful to keep related things together. Keep track of what you&#39;ve done. You may want to keep some notes on your contributions. You can give these to your advisor when it is time for a letter of recommendation. But I Don&#39;t Have a Project Yet! Now that you&#39;ve read this page, you understand more about how to ask a professor about research opportunities. When to ask (not too early). Usually you&#39;ll need to have taken at least a 300- or 400-level course in the appropriate research area. If you don&#39;t know basic concepts and terms, then it is hard to even discuss the research problem. Don&#39;t expect the professor to teach you the basics in his/her office: that&#39;s what the course is for. Who to ask. If you are doing extremely well in an upper-level course, then talk to the professor about whether he/she knows of any research opportunities in that area. It helps if the professor already has a high opinion of you from good interactions in class and through office hours. (You did go to office hours just to chat about ideas, right?) Even if he/she doesn&#39;t have anything for you, he/she may be able to hook you up with a colleague. How to ask. Advice from Marie desJardins: &#34;Ask the professor about his/her research. Professors love to talk about their research. But don&#39;t just sit there and nod. Listen carefully to what he/she&#39;s saying, think about it, and respond.&#34; He/She is trying to get a conversation going to assess where you can contribute meaningfully. To help the professor decide where to start the conversation, be sure to show him/her your resume and your transcript. Also describe the kinds of problems you excel at. Special skills or a remarkable track record may give you a foot in the door. For example, although my main research area is NLP, occasionally I do have problems that don&#39;t require much NLP knowledge. Rather, I&#39;m looking for someone who can develop a particular theorem or algorithm, or build a solid piece of system software, or design a beautiful user interface. So in this case, I might consider working with a great student who hasn&#39;t taken my NLP course. How to ask early. If you&#39;re not ready to start research yet, it&#39;s certainly still okay to ask a professor (or a senior grad student) how you could prepare to do research in his/her area. This might involve taking courses or MOOCs, reading a textbook or papers, or building certain mathematical or programming skills. When to ask (not too late). Timing is important. Research may not fit neatly into a semester. So approach the professor at least a year before you graduate. This gives you a couple of semesters plus summer and intersession. Hopefully, that&#39;s enough time for the professor to find an appropriate role for you and for you to get up to speed, define the problem and approach, do some initial work, refine the ideas, do some more work, fail, think hard, try again, succeed, write and submit a conference paper, revise the paper after acceptance, and present the paper at the conference. It&#39;s very common for a research project to take over a year even for a grad student who is doing research full-time! I&#39;ll give the final word to Jorge Chan of PhD Comics: This page online: http://cs.jhu.edu/~jason/advice/how-to-work-with-a-professor.html </description>
      <pubDate>05 Apr 20 22:30 EDT</pubDate>
      <guid>http://www.cs.jhu.edu/~jason/advice/how-to-work-with-a-professor.html</guid>
    </item>
    <item>
      <title></title>
      <link>http://joschu.net/blog/opinionated-guide-ml-research.html</link>
      <description>&lt;a href=&#34;http://joschu.net/blog/opinionated-guide-ml-research.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; An Opinionated Guide to ML Research Posted on 2020/01/24 ← back to blog index I originally wrote this guide in back in December 2017 for the OpenAI Fellows program In this essay, I provide some advice to up-and-coming researchers in machine learning (ML), based on my experience doing research and advising others. The advice covers how to choose problems and organize your time. I also recommend the following prior essays on similar topics: You and Your Research by Richard Hamming Principles of Effective Research by Michael Nielsen My essay will cover similar ground, but it’s more tuned to the peculiar features of ML. The keys to success are working on the right problems, making continual progress on them, and achieving continual personal growth. This essay is comprised of three sections, each covering one of these topics. Exercise. Before continuing, it’s useful to spend a few minutes about which findings and achievements in ML have been most interesting and informative to you. Think about what makes each one stand out—whether it&#39;s a groundbreaking result that changed your perspective on some problem; or an algorithmic idea that&#39;s reusable; or a deep insight about some recurring questions. You should aspire to produce results, algorithms, and insights of this caliber. Choosing Problems Honing Your Taste Your ability to choose the right problems to work on is even more important than your raw technical skill. This taste in problems is something you’ll develop over time by watching which ideas prosper and which ones are forgotten. You’ll see which ones serve as building blocks for new ideas and results, and which ones are ignored because they are too complicated or too fragile, or because the incremental improvement is too small. You might be wondering if there’s a way to speed up the process of developing a good taste for what problems to work on. In fact, there are several good ways. Read a lot of papers, and assess them critically. If possible, discuss them with others who have a deeper knowledge of the subject. Work in a research group with other people working on similar topics. That way you can absorb their experiences as well as your own. Seek advice from experienced researchers on what to work on. There’s no shame in working on ideas suggested by other people. Ideas are cheap, and there are lots of them in the air. Your skill comes in when you decide which one to work on, and how well you execute on it. Spend time reflecting on what research is useful and fruitful. Think about questions like When is theory useful? When are empirical results transferable? What causes some ideas to get wide uptake, whereas others are forgotten? What are the trends in your field? Which lines of work will make the other ones obsolete? Items 1-3 relate to optimizing your environment and getting input from other researchers, whereas item 4 is something you do alone. As empirical evidence for the importance of 1-3, consider how the biggests bursts of impactful work tend to be tightly clustered in a small number of research groups and institutions. That’s not because these people are dramatically smarter than everyone else, it’s because they have a higher density of expertise and perspective, which puts them a little ahead of the rest of the community, and thus they dominate in generating new results. If you’re not fortunate enough to be in an environment with high density of relevant expertise, don’t despair. You’ll just have to work extra-hard to get ahead of the pack, and it’s extra-important to specialize and develop your own unique perspective. Idea-Driven vs Goal-Driven Research Roughly speaking, there are two different ways that you might go about deciding what to work on next. Idea-driven. Follow some sector of the literature. As you read a paper showing how to do X, you have an idea of how to do X even better. Then you embark on a project to test your idea. Goal-driven. Develop a vision of some new AI capabilities you’d like to achieve, and solve problems that bring you closer to that goal. (Below, I give a couple case studies from my own research, including the goal of using reinforcement learning for 3D humanoid locomotion.) In your experimentation, you test a variety of existing methods from the literature, and then you develop your own methods that improve on them. Of course, these two approaches are not mutually exclusive. Any given subfield ML is concerned with some goals (e.g., object detection). Any “idea-driven” project will represent progress towards the subfield’s goals, and thus in a sense, it’s an instance of goal-driven research. But here, I’ll take goal-driven research to mean that your goal is more specific than your whole subfield’s goal, and it’s more like make X work for the first time than make X work better. I personally recommend goal-driven research for most people, and I’ve mostly followed this strategy myself. One major downside of idea-driven research is that there’s a high risk of getting scooped or duplicating the work of others. Researchers around the world are reading the same literature, which leads them to similar ideas. To make breakthroughs with idea-driven research, you need to develop an exceptionally deep understanding of your subject, and a perspective that diverges from the rest of the community—some can do it, but it’s difficult. On the other hand, with goal-driven research, your goal will give you a perspective that’s differentiated from the rest of the community. It will lead you to ask questions that no one else is asking, enabling you to make larger leaps of progress. Goal driven research can also be much more motivating. You can wake up every morning and imagine achieving your goal—what the result would look like and how you would feel. That makes it easier to stick to a long-running research program with ups and downs. Goals also make it possible for a team of researchers to work together and attack different aspects of the problem, whereas idea-driven research is most effectively carried out by “teams” of 1-2 people. Case Study of Goal-Driven Research: My Work During Graduate School For the first half of my PhD, my goal was to enable robots to manipulate deformable objects—including surgical robots tying knots, and household robots folding clothes. While this goal was determined by my advisor, Pieter Abbeel, as the main goal for his lab, I developed my own opinion on how to achieve this goal—my approach was based on learning from human demonstrations, and I was going to start with the problem of getting the PR2 to tie knots in rope. Various unexpected subproblems arose, one of which was trajectory optimization, and my work on that subproblem ended up being the most influential product of the knot-tying project. For the second half of my PhD, I became interested in reinforcement learning. While there are many problem domains in which reinforcement learning can be applied, I decided to focus on robotic locomotion, since the goal was concrete and the end result was exciting to me. Specifically, my goal was to get a 3D robot to learn how to run from scratch using reinforcement learning. After some initial exploration, I decided to focus on policy gradient methods, since they seemed most amenable to understanding and mathematical analysis, and I could leverage my strength in optimization. During this period, I developed TRPO and GAE and eventually achieved the original goal of 3D humanoid locomotion. While I was working on locomotion and starting to get my first results with policy gradient methods, the DeepMind team presented the results using DQN on Atari. After this result, many people jumped on the bandwagon and tried to develop better versions of Q-learning and apply them to the Atari domain. However, I had already explored Q-learning and concluded that it wasn’t a good approach for the locomotion tasks I was working on, so I continued working on policy gradient methods, which led to TRPO, GAE, and later PPO—now my best known pieces of work. This example illustrates how choosing a different problem from the rest of the community can lead you to explore different ideas. Goal Driven Research: Restrict Yourself to General Solutions One pitfall of goal-driven research is taking your goal too literally. If you have a specific capability in mind, there’s probably some way to achieve it in an uninteresting way that doesn’t advance the field of machine learning. You should constrain your search to solutions that seem general and can be applied to other problems. For example, while working on robotic locomotion, I avoided incorporating domain information into the solution—the goal was to achieve locomotion in simulation, in a way that was general and could be applied to other problems. I did a bit of feature engineering and reward shaping in order to see the first signs of life, but I was careful to keep my changes simple and not let them affect the algorithm I was developing. Now that I am using videogames as a testbed, I make sure that my algorithmic ideas are not specific to this setting—that they equally well could be applied to robotics. Aim High, and Climb Incrementally Towards High Goals Sometimes, people who are both exceptionally smart and hard-working fail to do great research. In my view, the main reason for this failure is that they work on unimportant problems. When you embark on a research project, you should ask yourself: how large is the potential upside? Will this be a 10% improvement or a 10X improvement? I often see researchers take on projects that seem sensible but could only possibly yield a small improvement to some metric. Incremental work (those 10% improvements) are most useful in the context of a larger goal that you are trying to achieve. For example, the seminal paper on ImageNet classification using convolutional neural networks (Krizhevsky, Sutskever, &amp; Hinton, 2012) does not contain any radically new algorithmic components, rather, it stacks up a large number of small improvements to achieve an unprecedented result that was surprising to almost everyone at the time (though we take it for granted now). During your day-to-day work, you’ll make incremental improvements in performance and in understanding. But these small steps should be moving you towards a larger goal that represents a non-incremental advance. If you are working on incremental ideas, be aware that their usefulness depends on their complexity. A method that slightly improves on the baseline better be very simple, otherwise no one will bother using it—not even you. If it gives a 10% improvement, it better be 2 lines of code, whereas if it&#39;s a 50% improvement, it can add 10 lines of code, etc. (I’m just giving these numbers for illustration, the actual numbers will obviously depend on the domain.) Go back and look at the list of machine learning achievements you admire the most. Does your long-term research plan have the potential to reach the level of those achievements? If you can’t see a path to something that you’d be proud of, then you should revise your plan so it does have that potential. Making Continual Progress To develop new algorithms and insights in machine learning, you need to concentrate your efforts on a problem for a long period of time. This section is about developing effective habits for this long-term problem solving process, enabling you to continually build towards great results. Keep a Notebook, and Review It I strongly advise you to keep a notebook, where you record your daily ideas and experiments. I have done this through 5 years of grad school and 2 years at OpenAI, and I feel that it has been tremendously helpful. I create an entry for each day. In this entry, I write down what I’m doing, ideas I have, and experimental results (pasting in plots and tables). Every 1 or 2 weeks, I do a review, where I read all of my daily entries and I condense the information into a summary. Usually my review contains sections for experimental findings, insights (which might come from me, my colleagues, or things I read), code progress (what did I implement), and next steps / future work. After I do my week in review, I often look at the previous week to see if I followed up on everything I thought of that week. Also, while doing this review, I sometimes transfer information into other sources of notes. (For example, I keep a list of backburner ideas and projects, separate from my notebook.) What’s the value in keeping this notebook and doing the regular reviews? First, the notebook is a good place to write down ideas as soon as you have them, so you can revisit them later. Often, when I revisit my journal entries during the week in review, I’ll fill in a missing piece in a puzzle, which didn’t occur to me at the time. Second, the notebook helps you keep your experimental results in a unified place, so you can easily find the results later. It’s easy to forget about your conclusions, e.g., which hyperparameters made a difference, and you’ll want to revisit your old notebook entries. Third, the notebook lets you monitor your use of time. You might wonder “where did last week go?”, and the notebook will help you answer that question. You might be disappointed with your throughput and realize you need to work on your time management. You also might look back at several months and realize that you’ve been jumping around between ideas too much—that you have a few half-finished projects but you didn’t follow any of these threads long enough to yield a notable result. When to Switch Problems To solve a challenging problem, you need to spend a sufficient amount of time on it. But in empirical machine learning research, it’s hard to know if you’ve tried an idea hard enough. Sometimes the idea has the potential to work, but if you get one detail wrong, you’ll see no signs of life. But other ideas are simply doomed to fail no matter how hard you work on them. In my experience, switching problems too frequently (and giving up on promising ideas) is a more common failure mode than not switching enough. Often, while you’re engaged in the long slog towards getting your current idea to work, another promising idea will come along, and you’ll want to jump to that idea. If your idea is quick to try and the potential upside is large, then go ahead and do it. But more commonly, your initial results on the new idea will be disappointing, and it’ll take a more sustained effort to yield significant results. As a rule of thumb, when you look back at which projects you’ve been working on over a period of months, you should find that there have been lots of small dead ends, but the majority of your time has been directed towards projects that yielded a deliverable such as a paper or a blog post. If you look back at your time and see that a substantial fraction was spent on half-finished projects—which were not definite failures, but which you abandoned in favor of some newer idea—then you should make a stronger effort towards consistency and follow-through in the future. One strategy, which I haven’t tried personally but makes a lot of sense upon reflection, is to devote some fixed time budget to trying out new ideas that diverge from your main line of work. Say, spend one day per week on something totally different from your main project. This would constitute a kind of epsilon-greedy exploration, and it would also help to broaden your knowledge. Personal Development No matter how you allocate your time during your research journey, you are bound to learn a lot. Each project will present new challenges, and you can pick up the background material and skills as you go along. However, you can significantly improve your chances to do great work in the long term by regularly setting aside time for your personal development. Specifically, you should allocate some fraction of your time towards improving your general knowledge of ML as opposed to working on your current project. If you don’t allocate this time, then your knowledge is likely to plateau after you learn the basics that you need for your day-to-day work. It’s easy to settle into a comfort zone of methods you understand well—you may need to expend active effort to expand this zone. The main ways to build your knowledge of ML are to read textbooks, theses and papers; and to reimplement algorithms from these sources. Early on in your career, I recommend splitting your time about evenly between textbooks and papers. You should choose a small set of relevant textbooks and theses to gradually work through, and you should also reimplement the models and algorithms from your favorite papers. Most students of machine learning don’t spend time reading textbooks after they finish their school courses. I think this is a mistake, since textbooks are a much more dense way to absorb knowledge than papers. Each conference paper typically contains one main new idea, along with a background section that’s too concise to learn anything from. There’s a lot of overhead, since you typically need to spend more time understanding the notation and terminology than the idea itself. On the other hand, good textbooks collect decades of ideas and present them in the proper order with the same notation. Besides reading the introductory machine learning textbooks, read other books in your areas of interest. A couple of my favorites were Numerical Optimization by Nocedal &amp; Wright, and Elements of Information Theory by Cover &amp; Thomas. Besides textbooks, I recommend reading PhD theses of researchers whose work interests you. PhD theses in ML usually are ordered as follows: (1) introductory and background material, (2) several papers that were previously published at conferences (it’s said that you just have to “staple together” your papers to write your thesis), and (3) a conclusion and outlook. You’re likely to benefit most from parts (1) and (3), since they contain a unifying view of the past and future of the field, written by an expert. Recent theses are often the best place to find a literature review of an active field, but older theses also often contain valuable gems of insight. Textbooks and theses are good for building up your foundational knowledge, but you’ll also need to read a lot of papers to bring your knowledge up to the frontier. When you are just starting your research career, I recommend spending a lot of time reimplementing ideas from papers, and comparing your results to the published ones. First of all, this gives you a much deeper understanding of the topic than you’d get by passively reading. Second, you’ll gain experience running experiments, and you’ll get much quicker feedback by reimplementing existing work (where the desired level of performance is known) than by doing original research. Once you can easily reproduce the state-of-the-art, you’ll be ready to go beyond it. Besides reading seminal papers and reimplementing them, you should also keep track of the less exceptional papers being published in your field. Reading and skimming the incoming papers with a critical eye helps you notice the trends in your field (perhaps you notice that a lot of papers are using some new technique and getting good results—maybe you should investigate it). It also helps you build up your taste by observing the dependency graph of ideas—which ideas become widely used and open the door to other ideas. Go forth and do great research! </description>
      <pubDate>04 Apr 20 10:00 EDT</pubDate>
      <guid>http://joschu.net/blog/opinionated-guide-ml-research.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://python.hamel.dev/concurrency/</link>
      <description>&lt;a href=&#34;https://python.hamel.dev/concurrency/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; An exploration of threads, processes, and coroutines in Python, with interesting examples that illuminate the differences between each. Credit:1 Motivation As a data scientist who is spending more time on software engineering, I was recently forced to confront an ugly gap in my knowledge of Python: concurrency. To be honest, I never completely understood how the terms async, threads, pools and coroutines were different and how these mechanisms could work together. Every time I tried to learn about the subject, the examples were a bit too abstract for me, and I hard time internalizing how everything worked. This changed when a friend of mine2 recommended a live coding talk by David Beazley, an accomplished Python educator. Because of restrictions with this YouTube video, I couldn’t embed the video in this article, so you will have to open it in a different window. This talk is incredibly intimidating at first. Not only is it coded live from scratch, but it also jumps immediately into socket programming, something that I had never encountered as a data scientist. However, if you go through it slowly and understand all the components (as we do in this blog post) it turns out to be the best educational material on Python concurrency I have ever encountered. This blog post documents what I learned along the way so others can benefit, too. Prerequisites Before getting started, David sets up the following infrastructure that is used to demonstrate concurrency. A cpu-bound task: Fibonacci To demonstrate concurrency, it is useful to create a task that can saturate your CPU (such as mathematical operations) for a noticeable period of time. David uses a function that computes a Fibonacci number. 1 2 3 4 #fib.py def fib(n): if n &lt;= 2: return 1 else: return fib(n-1) + fib(n-2) This function takes much longer for large inputs versus smaller inputs3, which allows us to profile different workloads. A Simple Web Server A web server is one of the best ways to illustrate different types of concurrency. However, to really demonstrate how things work it is useful to use something that is sufficiently low level enough to see how all the pieces work. For this, David sets up a web server using socket programming. If you aren’t familiar with socket programming, I’ll explain the important bits below, but feel free to dive deeper with this tutorial later if you like. To begin with, David starts with the below code (I’ve highlighted the most interesting bits): 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # server-1.py from socket import * from fib import fib def fib_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR,1) sock.bind(address) sock.listen(5) while True: client,addr = sock.accept() # waits for a connection to be established print(&#34;Connection&#34;, addr) fib_handler(client) # passes the client to a handler which will listen for input data. def fib_handler(client): while True: req = client.recv(100) # waits for data that sent by the client. if not req: break result = fib(int(req)) resp = str(result).encode(&#39;ascii&#39;) + b&#39;\n&#39; client.send(resp) # sends data back to the client. print(&#34;Closed&#34;) fib_server((&#39;&#39;, 25000)) Here is an explanation of this code: Lines 6-9 are socket programming boilerplate. It’s ok to just take this for granted as a reasonable way to set up a socket server. This also matches the the tutorial I linked to above. Line 11 waits for an incoming connection from a client. Once a connection is made, the server can begin receiving data from a client. The code will stop execution on this line until a connection is made. Line 13: Once a connection is established, the client object is passed to a function which can handle data sent by the client. Line 17: waits for data to be sent by the client. The code will stop execution on this line until data is received from the client. Line 21: The server sends a response back to the client. The code could stop execution on this line if the send buffers are full, but unlikely in this toy example. Testing the non-concurrent code In the above example, the server will only be able to accept a connection from a single client, because the call to fib_handler will never return (because it will run in an infinite loop unless a kill signal is received). This means that sock.accept() can only be called once. You can test this out by first running the server: Then establish a client: You can type numbers in as David does in his video and verifies that fibonacci numbers are returned. However, if you try to connect with another client at the same time from a different terminal session: You will notice that the second client just hangs and doesn’t return anything from the server. This is because the server is only able to accept a single connection. Next, we explore how to tackle this issue. Threads We can solve this issue with threads. You can add threads to the handler so that more connections can be accepted with the following code highlighted in yellow: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from socket import * from fib import fib from threading import Thread def fib_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR,1) sock.bind(address) sock.listen(5) while True: client,addr = sock.accept() print(&#34;Connection&#34;, addr) Thread(target=fib_handler, args=(client,)).start() def fib_handler(client): while True: req = client.recv(100) if not req: break result = fib(int(req)) resp = str(result).encode(&#39;ascii&#39;) + b&#39;\n&#39; client.send(resp) print(&#34;Closed&#34;) fib_server((&#39;&#39;, 25000)) You can verify that this works by connecting two separate clients to the server by running the following command in two separate terminal windows: By executing the fib_handler in a thread, the main while loop in fib_server will continue, allowing sock.accept() to receive additional clients. If you haven’t encountered threads before this tutorial provides a good introduction to the topic. Thread performance &amp; the GIL When code stops execution and waits for an external event to occur (like a connection to be made, or data to be sent), this is often referred to as blocking. One important utility of threads is that it allows blocking tasks to release control of the CPU when the CPU is not being used. However, the Python interpreter can only run on one thread at a time due to the Global Interpreter Lock. Because Python can only run a single thread at any given time, any CPU-bound work in threads must take turn running one after the other. Therefore, you have to think carefully about what kind of tasks you execute in threads with Python. If you try to execute CPU bound tasks, these tasks will slow each other down. David demonstrates this with the below script that sends requests to our threaded server: 1 2 3 4 5 6 7 8 9 10 11 12 13 #perf1.py from socket import * import time sock = socket(AF_INET, SOCK_STREAM) sock.connect((&#39;localhost&#39;, 25000)) while True: start = time.time() sock.send(b&#39;30&#39;) resp = sock.recv(100) end = time.time() print(end-start) If you run several instances of this script (after starting the server first): You will see the execution times for each script linearly increase as you increase the number of these scripts running in parallel. For this particular task, adding threads does not make anything faster. But why? This is because the fibonacci task is CPU bound so threads will compete with each other for resources. Python threads work by interleaving the execution of different tasks on your CPU.4 Only one thread runs at a time, and have the ability to take turns executing in small bits until all threads are done. The details of how thread processing is interleaved is carried out by the GIL and your operating system, so you need not worry about this detail (with one exception mentioned below). Interleaving a bunch of CPU bound tasks will not speed up the total runtime of those tasks. However, if your tasks involve lots of non-CPU time, such as waiting for network connections, or disk I/O, threading tasks may result in a considerable speedup. A canonical way of simulating a non-cpu bound task in python is to use the built-in function time.sleep(). To check my understanding about threads and performance, I ran the below experiment5 and changed time.sleep(2) to fib(20) and back again: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import logging import threading import time import fib def thread_function(name): logging.info(&#34;Thread %s: starting&#34;, name) time.sleep(2) ## Change this line of code to fib(20) logging.info(&#34;Thread %s: finishing&#34;, name) if __name__ == &#34;__main__&#34;: format = &#34;%(asctime)s: %(message)s&#34; start = time.time() logging.basicConfig(format=format, level=logging.INFO, datefmt=&#34;%H:%M:%S&#34;) threads = list() for index in range(3): logging.info(&#34;Main : create and start thread %d.&#34;, index) x = threading.Thread(target=thread_function, args=(index,)) threads.append(x) x.start() for index, thread in enumerate(threads): logging.info(&#34;Main : before joining thread %d.&#34;, index) thread.join() logging.info(&#34;Main : thread %d done&#34;, index) end = time.time() print(f&#39;total time: {end-start}&#39;) As expected, increasing the number of threads while running time.sleep(2) did not increase the program’s overall execution time (the program runs in roughly 2 seconds). On the other hand, replacing time.sleep(2) with fib(20) causes this program’s running time to increase as more threads are added. This is because fib(20) is a cpu bound task so interleaving the task doesn’t really help much. You should try running the same thing to see for yourself. You will often hear that Python is not good at parallelism and that you can only run on one CPU core at a time. This is likely referring to the aforementioned issues with threads and the GIL. Because you are limited to one thread, this means that thread-based tasks can only use one CPU core at a time (a single thread cannot run across multiple CPUs). Outside of Python, threads are a popular choice for parallelizing CPU-bound tasks because you are able to run a separate thread per CPU core simultaneously. However, with Python you must look for other ways to accomplish parallelism for cpu-bound tasks. Another interesting but less known aspect that David discusses is the relation between the following two types of tasks: things that take much longer to compute on the CPU, like fib(30), demonstrated with perf1.py. things that compute relatively fast on the CPU, like fib(1), demonstrated with perf2.py. The Python GIL will prioritize the first type of task at the expense of the second if they are made to compete for resources in threads. You can optionally follow along with a demonstration of this here. This is interesting because this is the opposite of how typical operating systems prioritize threads (by favoring shorter running tasks) and is something unique to the implementation of the Python GIL. More importantly, this behavior has a very practical consequence: if you are running a web-server where most tasks are fairly quick, an expensive cpu-bound task can grind everything to a halt. Threads are not just about making things faster It is tempting to think of Python threads as a tool to make things run faster, but that’s not the only use case. Recall that the socket server used threads to allow multiple connections at once without any speedup. David illustrates another way to use threads with his code used to measure the runtime of short-running tasks: perf2.py: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # perf2.py # requests/sec of fast requests from threading import Thread from socket import * import time sock = socket(AF_INET, SOCK_STREAM) sock.connect((&#39;localhost&#39;, 25000)) n = 0 def monitor(): global n while True: time.sleep(1) print(n, &#39;reqs/sec&#39;) n = 0 Thread(target=monitor).start() while True: sock.send(b&#39;1&#39;) resp =sock.recv(100) n += 1 In this case, David uses a single thread with a blocking call to sleep(1) to make sure that monitor only prints once per second, while allowing the rest of the program to send requests hundreds of times per second. In other words, this is a clever use of threads and blocking that allow part of a program to run at a desired time interval while allowing the rest of the program to run as usual. 6 These different angles of looking at threads allowed me to understand threads more holistically. Threads are not only about making certain things run faster or run in parallel, but also allows you to control how your program is executed. How threads work A thread is always contained in a process, and each process contains one or more threads. Threads in the same process can share memory which means they can easily communicate and write to common data structures. Threads are useful in the following two scenarios: When there are lots of non-cpu bound tasks (disk I/O, network calls, etc.). Outside of Python, if you want to parallelize a CPU bound task by splitting up the task across individual threads running on separate CPU cores. A process can span across multiple CPU cores, however a single thread can only utilize one CPU core. Generally speaking, only one thread can run cpu-bound tasks on a single core at any given time. If multiple threads are sharing a CPU core, your operating system will interleave these threads. There are some exceptions to this rule. For example single CPU cores are able to run multiple threads concurrently by using things like SMT/hyper-threading or compute over data in parallel using SIMD, which is popular in scientific computing libraries. On the other hand, processes offer isolation which is helpful when you have different users or different programs that should not be sharing information. Since we cannot run more than a single thread at a time in Python, a common workaround is to spawn several Python processes. This is discussed more below. Chapter 2 of This book discusses what processes and threads are in greater detail from an operating system perspective. Processes For CPU Bound Tasks One way to solve the problem with the GIL and cpu-bound tasks competing for resources is to use processes instead of threads. Processes are different from threads in the following respects: Python threads share a memory space, whereas each process has a separate memory space. This is an important consideration if you need to share variables or data between tasks. Processes have significant overhead compared to threads because data and program state has to be replicated across each process. Unlike Python threads, processes are not constrained to run on a single CPU, so you can execute cpu-bound tasks in parallel on different cores. David uses python processes in his server example by using a process pool.7 The relevant lines of code are highlighted below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # server-3.py # Fib microservice from socket import * from fib import fib from threading import Thread from concurrent.futures import ProcessPoolExecutor as Pool pool = Pool(4) def fib_server(address): sock = socket(AF_INET, SOCK_STREAM) sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1) sock.bind(address) sock.listen(5) while True: client, addr = sock.accept() print(&#34;Connection&#34;, addr) Thread(target=fib_handler, args=(client,), daemon=True).start() def fib_handler(client): while True: req = client.recv(100) if not req: break n = int(req) future = pool.submit(fib, n) result = future.result() resp = str(result).encode(&#39;ascii&#39;) + b&#39;\n&#39; client.send(resp) print(&#34;Closed&#34;) fib_server((&#39;&#39;,25000)) If you then start this version of the server with: python server-3.py And run the profiler perf2.py, we can make the following observations: The requests/sec are lower than the thread based version, because there is more overhead required to execute tasks in a pool. However, if you also run perf1.py it will not materially interfere with the first task (from perf2.py), as this will not compete for resources on the same CPU. The above example involves a CPU-bound task (computing the fibonacci number). However, if we simulated a non CPU-bound task instead such as time.sleep(), using processes instead of threads would actually be detrimental to overall performance. A concrete example of this is provided in the section below. This is a realistic example that allow you to gain more intuition about how threads and processes work. This tutorial contains more examples of Python processes and threads. A note for data scientists: processes vs. threads I’ve found many data scientists (formerly including myself) blindly apply processes and completely ignore threads. I understand why - processes are a kind of least common denominator where you can achieve some kind of parallelism regardless of if your task is CPU bound or not. However, I’ve found that this approach is very suboptimal and prevents full utilization of compute sources. Some examples to clarify where threads or processes might be more appropriate: If you are downloading lots of files from the internet, consider using threads. This is because most of your time is spent on network I/O, not on the CPU. For example, this article demonstrates a 50% speedup when using threads compared to processes for downloading files. If you are transforming or cleaning a large dataset, this work is mostly CPU bound so using processes makes sense. The only part of this that isn’t CPU-bound is reading and writing the data to disk. If you just want to load a bunch of files into memory or write a bunch of files to disk, without really doing any transformations, consider using threads as the work is mostly disk I/O and not CPU bound. Keep in mind that threads can be more memory efficient than processes because of differences in the way they work. So using lots of processes when you don’t need them can lead to memory bloat. Most importantly, try avoid having to think about processes and threads where you can and use scientific computing libraries like numpy and write vectorized operations wherever you can. It is always worth being aware of the concurrency tools available in the library or framework you are using (especially numerical computing and other data science libraries) and consider using them when appropriate. Asynchronous programming Recall that Python can only operate on one thread at a time, and the operating system automatically decides when to interrupt each thread to allow the threads to take turns running. This is called pre-emptive multitasking since the operating system, not you, determine when your thread makes the switch. When you don’t care about how tasks are interleaved, threads are great because you don’t have to worry about how they are scheduled. However, there is third type of concurrency paradigm in Python that allows you to control how this switching occurs: Asynchronous Programming. This is also called cooperative multitasking which means each task must announce when it wants to switch. One way to achieve cooperative multitasking is to create a coroutine. One way to create coroutines in Python is by using the yield statement. David provides some intuition on how you can achieve multi-tasking with yield in the following code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from collections import deque def countdown(n): while n &gt; 0: yield n n -=1 tasks = deque() tasks.extend([countdown(10), countdown(5), countdown(20)]) def run(): while tasks: task = tasks.popleft() try: x=next(task) print(x) tasks.append(task) except StopIteration: print(&#34;Task&#34;) When you run this code, you can see from the output the three countdown tasks are being interleaved: 1 2 3 4 5 6 7 8 9 10 11 12 &gt; run() 10 5 20 9 4 19 8 3 18 ... This clever use of yield allows you to pause execution of a task and move onto a different task kind of like threading, except you, not the operating system are controlling how compute is interleaved. This is the key intuition for understanding the rest of the talk, which goes on to to push this example further. One of the most popular ways to accomplish async programming is by using the various utilities in the built-in asyncio module, which uses similar yield statements under the hood. I didn’t end up diving deeply into the asyncio module or this particular flavor of programming as my goal was to understand the concept so that I wouldn’t be lost when encountering this in the wild. Conclusion There is no silver bullet with regards to choosing the correct type of concurrency in Python. You have to consider how much of your task is CPU bound vs non-CPU bound (and if it is feasible to break up the task appropriately) to determine whether tweaking your code will make a material difference. Most importantly, I recommend only reaching for these tools when you need them rather than trying to prematurely optimize your code. Always start with the simplest code, without any concurrency, and build incrementally from there. If you do add concurrency, make sure you can justify it through a measurable difference in performance or functionality. I’ve sometimes found that my code was slow in places I didn’t expect and that concurrency wasn’t the tool I needed at all! Profiling your code is beyond the scope of this blog post, however I hope this post demystified the confusing jungle of terminology of python concurrency so that you can more quickly navigate these topics in the future. Other Notes Not all programs that run in Python using threads are limited to a single CPU. It is possible to escape the constraints of the GIL by carefully writing C code that has a Python interface. This is what popular scientific computing libraries such as NumPy and SciPy do to achieve parallelism. In David’s code, deque from the collections module was introduced, which is a very handy data structure not only for async programming but also for threads because they are thread-safe, which means that you don’t have to worry about race conditions. Similarly, the queue module provides other types of thread-safe queues. Furthermore, one of my favorite python libraries, fastcore, contains a module called parallel which makes using threads and processes easy for many use cases. Terminology The following is terminology associated with Python concurrency that is often confused that we didn’t touch on in this blog post: Concurrency: this means creating programs do more than one thing at a time. It does not mean parallelization. If two parts of a program take turns executing until they are both complete this is concurrency even if they don’t run any faster than if run separately. Multiplexing: this means sharing resources. Mutex: (stands for Mutual Exclusion) this is used to enforce exclusive access to a resource across threads to avoid race conditions. Resources &amp; Thanks GitHub repo that contains David’s code. The PyCon youtube video for this talk. David’s page including links to courses. I recently took David’s Advanced Python class and it was excellent. I read the first two chapters of this book on operating systems to research how processes and threads worked. However, I’ve been told8 that this free book and this book are good choices as well. Thanks to Jeremy Howard, Dan Becker, and Zach Mueller for reviewing this post. Photo is from Luan Gjokaj on UnSplash. ↩︎ That friend is Jeremy Howard. He kept recommending the talk to me anytime the topic of concurrency came up. I eventually caved and decided to really focus on this talk. ↩︎ This fibonacci algorithm runs in Ω(1.6n) time. source ↩︎ Python threads are idiosyncratic because of the Global Interpreter Lock (GIL), which prevent multiple threads from executing code Python code at once. It is important not to confuse the behavior of Python threads with threads generally. ↩︎ Code is originally from this tutorial on threads. ↩︎ If the monitor task took any meaningful CPU time then the rest of the program would not run as “usual” because it might be competing for resources. But that is not the case here. ↩︎ One of the most popular ways of using process pools is with the built-in multiprocessing module. ↩︎ Alternate operating systems book recommendations are from Kenneth Chiu from this and this tweet in response to this blog post. ↩︎ </description>
      <pubDate>17 Feb 21 11:05 EST</pubDate>
      <guid>https://python.hamel.dev/concurrency/</guid>
    </item>
    <item>
      <title>Most software companies ignore user behavior</title>
      <link>https://www.reifyworks.com/writing/2019-08-26-a-most-software-companies</link>
      <description>&lt;a href=&#34;https://www.reifyworks.com/writing/2019-08-26-a-most-software-companies&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; One of the most important skills to build as a marketer is follow through. In marketing that means taking every opportunity to gather data in order to understand your users and improve your process. Understanding your users doesn’t end when they purchase your product. One deficiency we’ve consistently seen inside companies small and large alike is that they don’t have ready access to information about what users are doing with their product. Marketers are incentivized to instrument the hell of out lead generation and a section of the funnel, sales religiously updates CRMs to track prospects and potential revenue expansion opportunities, but noone is tying it all together by looking to information that you own about what users are doing with your product. The reason why this is so important to get right early is that it will deeply inform decisions that you’ll need to make later on … if you’re around that long. Decisions about pricing and packaging, feature development, marketing spending, hiring, strategic investment of all sorts, etc., are more intelligently made when they’re backed by data that is enriched by how users are actually using your product. There are myriad technical approaches to gathering the data necessary for this method, but an ideal setup should contain the following, regardless of how it is implemented: A “User table” which rolls up all useful usage information into one easy to query data source. You likely have some version of this powering either a homegrown “admin dashboard” or feeding into one or more external systems. A time-bounded data source which is amenable to more complex queries, including historical queries, and can be used to generate reports. Amazon Redshift is a popular solution for this component. Here’s an example row: ID name score feature1 feature2 plan monthly seats last_seen user_since 1 Reify 87 1 0 small 99 3 019-08-29 019-01-02 Maintaining and curating this date over time will allow you to answer crucial questions about your trial users, customes, ex-customers, and so on. Recording and maintaining this data is so simple that it’s becoming table stakes for good marketers – don’t get left behind. Bonus Implementation Tips Track feature usage in your user table with columns denoting whether or not a user has or has not used this feature Associate plan and payment information with your user table rows so that you can easily segment your user data by important revenue based facets Create an “engagement metric” number which takes core features of your product into account and rolls them up into a 0-100 score that can be tracked over time. Use the data in your users table to power all kinds of interesting customer communication. Want to communicate with users who have or haven’t used a specific feature and ask them why? Now you can. Want to segment trial users into those who haven’t used that one killer feature and those who have? Go for it. </description>
      <pubDate>24 Mar 20 12:27 EDT</pubDate>
      <guid>https://www.reifyworks.com/writing/2019-08-26-a-most-software-companies</guid>
    </item>
    <item>
      <title></title>
      <link>http://jkff.info/articles/ire/</link>
      <description>&lt;a href=&#34;http://jkff.info/articles/ire/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Incremental Regular Expressions¶ By Eugene Kirpichov &lt;ekirpichov@gmail.com&gt; Code available at github: http://github.com/jkff/ire This article was originally published in the Russian functional programming journal http://fprog.ru/ and this translation was intended to be published in Peter Seibel’s http://codequarterly.com/ journal, but as Peter eventually decided not to proceed with the journal, I am publishing it on my personal space. I would like to thank Peter for his multitude of extremely valuable comments that have helped me greatly improve the clarity and flow of the article. Introduction¶ The problem of regular expression matching is a well-known one, with a multitude of solutions. The corresponding algorithms are very beautiful, motivating many curious programmers to write their own regular expression engines for fun. The approaches to this problem are quite different in their area of application. Here are some questions that the developer of an engine has to answer. Supported operators: capturing groups, backreferences, execution of arbitrary code, greedy matching? The more features are supported, the harder it is to implement the engine efficiently; most of the features rule out whole classes of matching algorithms. Number and kind of regular expressions: How many regexes shall we test for? How big shall they be? Shall they be small expressions for data validation, or are we talking about a full-fledged lexer with dozens of tokens specified by their regular expressions? Pattern of usage: How many times is matching against a single expression performed? Is it OK to spend a lot of time compiling it but match very quickly afterwards? Size of change of matched text: How big is the matched text and how often does it change? Let us list some of the existing approaches and engines. awk, grep, re2 use the so called “automata-theoretic” approach, which allows them to guarantee linear matching time, however some features (for example, backreferences) cannot be implemented at all within their approach, and others (such as capturing groups) are quite hard to implement efficiently. Also, with this approach it is difficult to control memory overhead while retaining high efficiency — even re2 at times suffers from exponentially large memory consumption, though it is designed to avoid such situations. Besides, this approach allows for certain curious uses, for example, re2 uses automata theory to compute the minimal and maximal possible strings matching the regular expression, thus making Google Code Search possible by greatly reducing the amount of code to be actually matched against the expression when processing a query. Modified automata-theoretic approach, “tagged” automata: libtre, regex-tdfa — linear matching time is also guaranteed; it is possible to do approximate search; Semiring-based approach: weighted-regexp — also linear matching time and constant memory consumption; a very simple, beautiful and efficient implementation; Recursive descent: most of the other engines (Perl and PCRE-compatible engines, Java, irregexp etc.)—the whole range of features, but “pathological” cases are possible where matching time sharply increases. In a fantastic blog post Dan Piponi [1] outlined yet another approach using monoids and finger trees. This approach only works for “true” regular expressions (i.e. we only have character classes, braces and the operators +, *, ?, |) but it allows to perform the matching incrementally: after small changes of the input string we can recompute the result very quickly without scanning the whole string. These changes include concatenating two strings and cutting a string in two pieces. Obviously these two operations form a basis for many others (insertion into the middle, appending or prepending a character etc). In this article we further develop Piponi’s approach, which employs a number of beautiful algorithmic techniques from the world of functional programming, to build a Java library for incremental regular expression matching. This is also an interesting investigation into how the functional approach blends together with an imperative language. There were several reasons to choose Java: Increase the probability that this library will be indeed used and will not remain a purely academical toy because of difficulties with its integration with existing projects; Facilitate understanding of the presented ideas for the (currently) quite broad community of imperative programmers; Show that studying the functional ideas and approaches is fruitful also for programming in non-functional languages. Where could such an approach to regular expression matching be useful? The ability to quickly recompute the result after changes of the input string would be useful in text editors for incremental highlighting of arbitrary syntax, with tokens specified by their regular expressions (vim, for instance, allows rather flexible definition of syntax rules, but does not always do the highlighting and re-highlighting correctly, because it uses a rather naive approach to highlighting). Unfortunately, the engine developed in this article has performance characteristics that do not allow it to serve this purpose — however, not all hope is lost: some possible improvements will be outlined. One could imagine a problem in bioinformatics where one assembles a DNA sequence, say, using a genetic algorithm, from some “basis” sequences using glue and scissors (concatenations and cuts), and optimizes a function depending on the presence and position of particular patterns in the sequence. Problem statement¶ The problem that we’re going to solve is incremental matching of strings against regular expressions. There are several ambiguities to resolve here, however. Let us outline the major features of our engine: they are basically the same as those in Dan Piponi’s article. The regular expression (or set of regular expressions) is fixed: once we fix the expressions, we obtain a way to do incremental matching of strings against them. “Incremental” means “match result is efficiently maintained under certain operations”, where the operations include concatenating two strings and splitting a string in part around an index. Obviously, these two operations form a basis for all kinds of rearrangements, such as inserting or deleting a word from the middle of a string, or appending characters to its back or front, etc. There is one more ambiguity to resolve: whether to make the “incremental” interface pure or impure in the mathematical sense: whether concatenation and splitting modify their arguments or create new results. We choose the pure option (concatenation and splitting are mathematical functions), because, as it is the case with nearly any kind of data structures having both a “pure” and “impure” implementation, it turns out dramatically easier to reason about mathematically and declaratively, and also dramatically easier to implement, debug and test. We shall elaborate more on the importance of this design decision in the section ‘The importance of purity’ section. The automata-theoretic approach¶ The most well-known approach to regular expression matching is based on finite automata and is studied in most university courses on compiler construction. You can throuroughly familiarize yourself with it, for example, in the article by Russ Cox. Let us provide just a quick refresher of the most basic concepts of how finite automata are used for matching text. Incrementalization will follow in a surprisingly simple fashion. Constructing the automaton¶ Given a regular expression, one uses, for example, the “Thompson’s construction” (due Ken Thompson), and builds a finite automaton where one state is declared “initial”, one is declared “final” (though in theory nothing prevents having more than one initial or final state, and we shall use this opportunity in our algorithm), and some states are connected to each other by edges. An edge from \(A\) to \(B\) with subscript \(c\) means: “If state \(A\) is active and the character \(c\) is input, then state \(B\) becomes active instead of \(A\)”. There are also ε (epsilon)-edges: if \(A\) is connected to \(B\) by an ε-edge, then, upon activation of \(A\), \(B\) is also immediately activated. Epsilon edges are needed, for example, when constructing an automaton for the “?” (“optional”) construction: given an automaton for R, you can insert an epsilon transition from the initial to the final state, giving an automaton for “R?”. If no edge from state \(A\) fits the input, then it is simply deactivated. The automaton is called “non-deterministic” because, given a state and a character, you can’t determine which edge to follow, because there can be several of them. Instead, you follow all of them at once and therefore, at any given moment, several states may be active. The matching process¶ To perform matching, the initial states are activated and each character of the sequence is fed as input in turn. If in the end at least one “final” state is active, then the matching is declared successful. Example. The regular expression “.*a(b*a|bc+)a”. An automaton for this expression is shown on the picture below (somewhat simplified with respect to the Thompson’s construction, by removing several redundant nodes and epsilon edges), and here is the sequence of its active states upon feeding it with the string “aabcca”. A non-deterministic automaton for the regular expression “.*a(b*a|bc+)a” String seen so far Active states   \(s_1\) a \(s_1, s_2, s_3, s_4\) a a \(s_1, s_2, s_3, s_4, s_5, s_8\) aa b \(s_1, s_3, s_6\) aab c \(s_1, s_6, s_7, s_8\) aabc c \(s_1, s_6, s_7, s_8\) aabcc a \(s_1, s_2, s_3, s_4, s_9\) Since in the end the active set contains a final state, \(s_9\), the matching is declared successful. Shrinking the automaton¶ To speed up matching, one sometimes eliminates ε-edges and performs determinization of the automaton (each state can have at most 1 out-going edge with a given subscript) [2]. As a result, a completely different automaton is obtained, which, however, corresponds to the same regular expression. In a deterministic automaton, at any given moment, exactly one state is active, which allows for a more efficient implementation of the matching procedure. However we won’t use determinization because as a result, the automaton may blow up exponentially [3], which, as we’ll later see, is completely unacceptable in our case. We shall use only the first part, namely elimination of ε-edges: it can only decrease the size of the automaton (compare the NFA above and below). The algorithm is very simple: take “shortcuts” through ε-edges, i.e. whenever one node is reachable from another through a chain of ε-edges, copy edges from the second node to the first. Then remove all ε-edges (since they’re now unnecessary) and nodes that became redundant (unreachable) as a result of this. The non-deterministic automaton corresponding to the expression “.*a(b*a|bc+)a”, after removing ε-edges Thus we have outlined the approach to testing whether a string matches a regular expression using finite automata. Finding match positions and capturing groups is more difficult, and we direct the reader to Russ Cox’ article for a more thourough treatment — we do not present the traditional approach here ourselves, because we shall use a different one. Approach to incremental matching¶ Now let us gradually arrive to the basic ideas of incremental matching. First, let us depict this automaton in a slightly different way. Every possible input character has a “transition function”: “What will be the next states of the automaton after feeding this character to the input, if its current state is \(S\)?” It can also be seen that the notion of such a “transition function” makes sense not only for individual characters, but for whole strings. Strings have transition functions, too! Given a string’s transition function, however long this string might be, we can emulate feeding it to the automaton’s input without even looking at its characters. A string’s transition function is computed from the transition functions of its characters (which is also shown on the picture below); in the same way, given transition functions for two strings, we can compute the transition function of their concatenation. Transition functions of a non-deterministic automaton and their composition Note that the transition function of a concatenation of two strings is the composition of their transition functions, in a slighty unusual sense. If we were speaking of deterministic automata, then transition functions would be regular mathematical functions from \(S\) to \(S\), where \(S\) is the automaton’s set of states. Given two strings \(p\) and \(q\) with corresponding transition functions \(f\) and \(g\), feeding \(pq\) to the automaton will take \(s\) to \(f(s)\) and then to \(g(f(s))\), which means that the transition function of \(pq\) is \(g \circ f\). However, we’re using NFAs, and transition functions of characters and strings take states to sets of states. Note that we do not say that the inputs of these functions are also sets of states: it suffices to define them only at individual “singleton” states: applying a transition function to a set of states means applying it to each of these states individually and taking a union of the results (imagine that a simulation of the automaton occurs simultaneously for all these states). So suppose again that the transition functions of \(p\) and \(q\) are \(f\) and \(g\). Then if the automaton’s initial state is \(s\) (some individual state), then \(pq\) will take the automaton first to \(f(s)\) (a set of states) and then to \(\bigcup_{r \leftarrow f(s)}{g(r)}\). This is the definition of composition for transition functions of non-deterministic automata: \((f \circ g)(s) = \bigcup_{r \leftarrow f(s)}{g(r)}\). This definition is curious because it has several interpretations. The interpretation given just now; The graphical interpretation as connectivity in a two-layer graph, as on the picture above; Multiplication of boolean matrices: if we represent the transition function \(f\) as an \(N x N\) boolean matrix (where \(N\) is the number of states in the automaton) with \(1\) in cell \(s,t\) if \(t \in f(s)\). Then we may rephrase the definition \((f \circ g)(s) = \bigcup_{r \leftarrow f(s)}{g(r)}\) as follows: \((f \circ g)(s,t) = \bigvee_{r \leftarrow f(s)}{t \in g(r)} = \bigvee_{r \leftarrow 1..N}{(s,r) \in f \wedge (r,t) \in g}\) . Note the extreme similarity with matrix multiplication: \((AB)[i,j] = \sum_{k \leftarrow 1..N}{A[i,k]*B[k,j]}\): only summation is replaced with logical “or” (\(\vee\)) and multiplication is replaced with logical “and” (\(\wedge\)). This interpretation is of course not new; it is a well-known fact shown in most textbooks on graph theory that connectivity in graphs may be computed using multiplication of boolean matrices corresponding to their incidence matrices. However, it opens some opportunities for optimization by employing well-known algorithms for fast matrix multiplication. Making use of transition functions¶ Let us now consider how this knowledge of transition function multiplication helps us implement incremental matching. We shall start with a simpler problem: match testing, i.e. answering the question “Does the string \(s\) match regular expression \(R\)?” This is by definition equivalent to the question “Does the transition function of \(s\) take \(R\)‘s automaton to a final state?”. So, if we maintain transition functions of strings under the incremental operations (concatenations and splits), we’ll be able to also maintain the answer to this question. Handling concatenations is simple: given that we’ve learnt to rapidly compute transition functions of string concatenations, we’ve essentially learnt to rapidly recompute results of “yes/no”-style match tests after concatenations. Therefore, if we carry with each string its transition function (a device for rapidly performing a match test), then when concatenating two strings, we’d compose their functions, yielding again a device for rapidly testing their concatenation for a match. Reducing splits to concatenation¶ Handling splits is harder: it is not obvious how to get the transition function of a part of a string, knowing only that of the whole string. However, this problem of splitting is, curiously, reduced to the problem of concatenation: if we represent a string as an hierarchical concatenation (a tree) of several smaller parts (chunks), then parts of this string will be concatenations of some of these chunks. More precisely, a part of the string equals the concatenation of all complete subtrees fully enclosed within that part, with incomplete chunks giving birth to new tiny but complete subtrees — see picture below. Representing a part of a string as the concatenation of its smaller parts If a split goes through the middle of a chunk, we’ll still have to recompute the transition function for the resulting “sub-chunks” character by character, but most of the chunks or bigger parts of the hierarchy will remain intact and we won’t have to recompute their transition functions, thus saving most of the computation. All that remains is to choose a good way of representing a string as many chunks, so that concatenation and splitting are efficient, and memory overhead is not too high. This is what we consider in the next section. Note again that this still does not allow finding positions of matches — only whether the string matches the expression or not. This is perhaps the most interesting algorithmic problem in this article, and we shall address it later when more background is given. Putting it together¶ Now, before going to the technical and most interesting parts, let us recap on the basic idea of incremental matching. We represent strings as trees of chunks (small “atomic” strings). With each string (actually with each node in a tree representing a string) we carry its transition function with respect to the regular expression of interest. To perform a match test, we simply take the transition function, apply it to the automaton’s initial state and check whether we hit a final state. We don’t even look at the string per se. When concatenating two strings, we multiply their transition functions. When splitting a string into parts, we reduce that to concatenation of some of its nodes — remember that we keep the transition functions for all the nodes. An example of such a datastructure is illustrated on the picture below. Example representation with cached transition functions for the string \(bcabbccabcccab\) Ropes: Strings with fast concatenation¶ This data structure, which is a tree of small arrays, is called a “rope”, and ropes are frequently used for representing strings where efficient concatenation and splitting are needed while preserving reasonably low memory overhead (e.g. having a balanced tree of individual characters is not an option because it blows up memory usage). Ropes are a long-known datastructure and there are many varieties of them, usually differing in the kind of balanced trees they use. One of these varieties is described in the article Ropes: An alternative to strings by Hans-J. Boehm, Russ Atkinson, and Michael Plass, but we’ll use a different one, to be shown later in the article. Maintaining the transition functions of rope nodes at concatenations and splits is simple for every variety of ropes: exactly as we assemble new nodes from old nodes during rebalancing, we assemble the transition functions of new nodes from transition functions of old nodes (by composing them). Even the definition of rebalancing operations doesn’t have to be modified, just the constructor for composite nodes (nodes with children) has to multiply the children’s transition functions to obtain one for the parent, and the constructor for chunks has to assemble the transition function from transition functions of characters. Generalizing ropes¶ Note that there is not much special about transition functions that allows us to maintain them under splits and concatenations. The only reason why we could do so is because we can compute the transition function for a concatenation of two strings from their transition functions. So, essentially ropes allow us to maintain absolutely any kind of “additive” characteristic (and we’re of course not restricted to speaking about strings, i.e. lists of characters — for example, lists of numbers are just as fine). There is just one restriction: in order for the additivity to make any sense, it must not be dependent on the order in which we add up the items to obtain the whole; this property is called associativity: since for concatenation holds \(a(bc) = (ab)c\), the additive characteristic \(f\) must obey \(f(a(bc)) = f((ab)c)\), that is, if the additivity is expressed as \(f(ab) = g(f(a), f(b))\), then \(g\) must obey \(g(g(x,y),z) = g(x,g(y,z))\). Here are some examples of additive (associative) characteristics: The sum of a list of numbers The maximum and minimum number The sum of squares of a list of numbers The sum and size of a list of numbers (allowing to maintain the average, e.g. for answering “range average” queries) The number of times a given character occurs in the string (for example, the newline character) There are many more examples, you can find them near the end of the article, in the section “Monoids”. Below is an example of a rope of numbers maintaining the minimum and maximum — the combining operation here is \(g((m_1,M_1), (m_2,M_2)) = (min(m_1,m_2), max(M_1,M_2))\) A rope of numbers with “cached” minimums and maximums. Monotone split operation¶ In addition to splitting at a position, one may implement one more very important and beautiful operation on ropes: splitting on a monotone predicate. We shall need this operation when we get from “testing for a match” to “locating matches”, but we first provide an abstract setting, because it will be a lot easier to understand how locating matches can be done using this abstract algorithm, than to go in the opposite direction (recognize its beauty inside the full complicated algorithm for locating matches). Suppose \(f\) is a predicate on strings. Suppose that \(f\) is such that a string may only gain (but not lose) the property \(f\) when symbols are appended to it on the right, i.e., \(\forall s_1, f(s_1) \Rightarrow \forall s_2, f(s_1 + s_2)\). In this case let us call \(f\) monotone. Then obviously each string \(s\) satisfying the property \(f\) has a minimal prefix satisfying \(f\). Let us illustrate this notion and the algorithm for its efficient computation on a rope: Splitting a rope of numbers by the monotone predicate “Sum of squares exceeds 140” This picture shows a rope of numbers, storing in each node the sum of squares of numbers within this node, and it also shows how the rope is split to find the minimal prefix whose sum of squares is greater than 140. The algorithm resembles a “hot and cold” game; the figure shows sums of squares of prefixes before and after various edges and (when scanning a leaf chunk) individual numbers; those that do not yet satisfy the condition are marked with “cold” blue color, and those that do are marked with “hot” red. This picture also shows that when scanning through a leaf chunk, we have to recompute and add up the squares of numbers, i.e. the information stored in nodes of the rope is not sufficient. On this picture an edge is marked red if the predicate is true for the prefix which ends where this edge ends. To find the split point, we have to descend from the root of the rope to a leaf chunk, doing steps downward or to the right, at each level scanning edges left-to-right until we find a red edge (this is similar to a binary search procedure), and finally split the leaf chunk, this time using regular linear search. Since we only move downward or to the right, at any moment the edges that have been considered cover together an ever-growing prefix of the original rope, and each new scanned edge appends the rope covered by this edge to this prefix. If the predicate is not true before scanning an edge but becomes true after scanning it, this means that it becomes true somewhere inside the part of the rope covered by the destination node of this edge, and we have to descend downward into this node in order to find the split point. In order to be constantly aware of whether the predicate is true, we should be able to quickly compute \(f(ps)\), given \(f(p)\) and \(f(s)\) for any two ropes \(p\) and \(s\), since when moving downward or to the right, we increase the “current prefix” (\(p\)) with sub-ropes covered with each scanned edge (\(s\)), and when we get to the leaf chunks, during linear search we increase \(p\) with single-element sub-ropes corresponding to elements of the chunk. Now note that match testing also sometimes fits this pattern: Given transition functions for \(p\) and \(s\), we can quickly compute the transition function of \(ps\), and given that transition function, we know the answer to the match test. Some regular expressions are monotone, i.e. if a string matches the expression, then appending characters on the right won’t make it lose this property. One class of such regular expressions is expressions of the form .*R.* for any R, because they correspond to the question “Is there a match of R somewhere in the string?”, which obviously is monotone. So, we can use this “monotone split” procedure to find the smallest prefix of the string containing a match of our regex. Splitting a rope for the string acabaabacabccaaba on the monotone predicate “matches .*bcc.* ” This is key to finding match positions. Finding match positions¶ Suppose we have to find a match of \(R\) in the string. This problem can partly be reformulated as testing the string against the expression “\(.*R.*\)”, but this only tells us the answer to the question “is there a match of \(R\) somewhere in the string?”, but not to “where is the match?” Two key ideas will help us find the match positions. As said above, the answer to the first question (the presence of a match) is “monotone”. That is, starting from some prefix of the string, the answer will be positive for all subsequent prefixes. The first occurrence of \(R*\) ends exactly where the first such prefix ends. It is known [4] that, given a regular expression \(R\) or its corresponding automaton \(A\), one can build a regular expression \(R′\) and the automaton \(A′\) which recognize reverses of the strings recognized by \(R\) and \(A\), simply by reversing all sequences inside \(R\) and, correspondingly, all arrows in \(A\). For example, the expression \(a+(b|c*d)x*\) recognizes the string \(abbdxx\), and the expression \(x*(b|dc*)a+\) recognizes \(xxdbba\). Therefore, we can find the beginning of the match by launching the reversed (“backward”) automaton (automaton for the reversed expression) backward over the string, starting from where the match ends. So, we use the “split on monotone predicate” operation to find the end of the first match, and use it again, but this time in backward direction and with the backward automaton, to find its beginning. If the expression is such that strings matching it are usually small, we can just run the backward automaton character by character; if not, we can also have the nodes of the rope store transition functions not only for the “forward” automaton, but also transition functions of reversed parts of the string with respect to the “backward” automaton. It’s easy to see that all rope operations change trivially — instead of composing one pair of transition functions, we compose two: \((f_1,b_1) \circ (f_2,b_2) = (f_2 \circ f_1, b_1 \circ b_2)\) — note that the order of composition for backward transition functions is reversed because for strings if \(a=bc\), then \(reverse(a)=reverse(c)reverse(b)\). There is actually a number of complications here, related to possible overlaps of occurrences of different items from the given system of regular expressions (or even self-overlaps), but the main idea is the same: split to find the end of the match, split backward to find the beginning. The curious reader is directed to the source code. Implementation¶ Let us put together the presented algorithms and overview the structure of the whole program. Graphically this structure is shown on the pictures below. This kind of diagrams is called “concept maps”, drawn with IHMC CmapTools software. Program structure overview¶ Program structure overview The user specifies several regular expressions as strings, which through compilation (parsing and converting to a finite automaton) get transformed to an object of type PatternSet. Such an object is capable of “indexing” regular strings, yielding objects of type IndexedString. They, in turn, are capable of efficiently searching for all matches of the patterns in themselves, and they can also efficiently be concatenated or split (on a monotone predicate). These are of course the very ropes maintaining transition functions. Ropes and additive measures¶ Program structure: ropes and additive measures “Indexed strings” are implemented with ropes (Rope). The program implements only ropes over characters, because further generalization was not necessary within the scope of our problem and the Java type system would cause the generic implementation to have a lot of syntactic garbage. A rope is a string that knows its “measure” of type M. The measure is computed as the sum of measures of individual characters (Function&lt;Character,M&gt;) under an arbitrary additive measure (Reducer). Ropes are implemented with a special kind of balanced trees that will be described later in the article. During rebalancing operations measures of new nodes are summed from measures of old nodes using this additive operation. For regular expression matching, the additive operation composition of transition functions for the expression’s automaton (more precisely, for two automata: forward and reverse). Finite automata¶ Program structure: finite automata Automata are implemented in a way that facilitates representing and computing their “transition functions”. An automaton can tell its transition function for any given character, and the transition function is a function from the automaton’s state type to the same type. A value of the state type is a “black box” that only tells what patterns are “terminated” by this state: if after scanning a string, the automaton’s state terminates certain patterns, then there are occurrences of these patterns ending at the end of this string. Transition functions are represented not as arbitrary functions but as a special kind of objects with efficient composition [5]. These objects are placed into the ropes used as “indexed strings”. Relationship between DFAs and NFAs¶ Program structure: the connection between deterministic and non-deterministic automata The notion of an automaton is used in the program in two ways: for deterministic and non-deterministic automata. The state type of a deterministic automaton [6] is the set of integer numbers from \(0\) to some \(N\); correspondingly, transition functions are functions from \(0 … N\) to \(0 … N\). They are implemented as one-dimensional int[] arrays, and their composition is computed as easily as c[i] = b[a[i]]. In the case of non-deterministic automata, values of the state type are subsets of some “basis” state set, and a transition function is determined by the way in which it transforms each individual basis state to several other basis state, i.e. it is specified by a transformation of the form int → int[]. Composition of such functions is expressed as \(c[i] = \bigcup_{j \leftarrow a[i]} b[j]\) (thread the second function through all the outputs of the first function). For the sake of efficiency, this transformation is implemented by a boolean matrix with every element represented as one bit in a single array [7]. Multiple regular expressions¶ The aforementioned datastructure allows to match a string against a single regular expression. It is natural to demand a more practically useful generalization: matching against multiple expressions (there exist also other approaches to this problem, see for example the article “Compact DFA structure for multiple regular expressions matching” by Lin, Tang et al.). The result of such a match is a set of facts of the form “The portion \(i..j\) matched expression \(k\) ”. The problem of matching against multiple regular expressions is solved quite easily: we build an automaton for their union, \(R_1|R_2|...\), but we distinguish between final states for different expressions, i.e. a state of the resulting automaton is not simply “final” or “non-final”, but it has an associated bitset: for which of the expressions it is final. This change only slightly influences other parts of the program, such as automata minimization or finding match positions. Examples and benchmarks¶ Let us show an example of usage of the library.: PatternSet pat = RegexCompiler.compile(&#34;007&#34;,&#34;008&#34;) IndexedString s1 = pat.match(&#34;as00haklsdjhfla00&#34;); IndexedString s2 = pat.match(&#34;7jhd7dsh008dsfa&#34;); System.out.println(s1.append(s2).getMatches()); The program prints: This means that occurrences of the first and second pattern were found correspondingly in positions 15–17 and 25–27. This code uses the following aspects of the API: RegexCompiler.compile — compile several regular expressions to an automaton recognizing any of them. PatternSet.match — index a “regular” string, preparing it to searching for the given patterns. IndexedString.append — compute the concatenation of two strings indexed by the same pattern set. IndexedString.getMatches — find matches of the pattern set in an indexed string. Now let us discuss the library’s performance. This discussion won’t be a simple one, because the performance is influenced by a large number of factors. Size of the automaton, which depends approximately linearly on the number and size of individual regular expressions: it linearly influences both the performance of all operations and the memory consumption (larger is worse). The program uses an algorithm for minimization of non-deterministic automata described in the article “On NFA reductions” by Ilie and Navarro. (however, the implementation is extremely inefficient, but this doesn’t influence matching performance because minimization is only done in RegexCompiler.compile), but it usually shrinks the automaton just by several dozen percents. Size of the leaf chunks in ropes linearly influences search performance (larger is slower), has almost no influence at all on concatenation performance, and linearly influences memory consumption (the larger the chunks, the fewer the memory overhead). Features of the particular regular expression influence the automaton’s “shape”, which in turn influences the speed of operations on it (“hairy” expressions lead to dense boolean matrices for transition functions, which are slower to multiply in the current implementation). Number of matches linearly influences the search time in the current implementation (larger is worse), but there is room for optimization here. It is also necessary to balance the share of time devoted to indexing the string, doing concatenations/splits and searching. Indexing is done quite slowly, and one needs a large number of concatenations/splits and search to overweight it and to get an advantage over a “traditional” regular expression engine. Test setup¶ In light of the above, let us consider just a single test and analyze its performance. Take the set of regular expressions from the regex-dna problem from the Language Shooutout and consider performance of matching operations compared to the standard regular expression engine bundled with Java (java.util.regex.Pattern), varying length of the input DNA string (but keeping constant the total number of matches) and size of the leaf chunks: 8, 16, 32, … 512 characters. We do not measure splitting performance separately, because splitting is used during search, and we do not measure concatenation performance because it is so fast (allocate a few objects and compose a few transition functions) that it is difficult to imagine a scenario where it would be the bottleneck. Here is the pattern set: [cgt]gggtaaa|tttaccc[acg] a[act]ggtaaa|tttacc[agt]t ag[act]gtaaa|tttac[agt]ct agg[act]taaa|ttta[agt]cct aggg[acg]aaa|ttt[cgt]ccct agggt[cgt]aa|tt[acg]accct agggta[cgt]a|t[acg]taccct agggtaa[cgt]|[acg]ttaccct Let us generate the input as a random sequence of the characters “ a, g, c, t ” of length \(50000 N\) (\(N\) will vary from 1 to 10) where any two consequent characters are distinct (therefore the aforementioned patterns can’t occur there), choose 100 random positions in the sequence and insert there occurrences of strings randomly chosen from the set of \(8 × 2 × 3 = 48\) strings defined by the given pattern set (8 patterns, each with 2 alternatives, each alternative matching 3 different strings). The program will compute the occurrence count of each pattern. Benchmark results and interpretation¶ Results of the benchmark are shown on the pictures below. The performance characteristics of each of the two programs (our engine and the standard Java engine) are shown in the terms that are most appropriate for them: for our engine it is the indexing speed (in characters per second, because indexing speed is proportional to the number of characters) and search speed (in occurrences per second, because search speed is proportional to the number of occurrences). For the Java engine a more appropriate characteristic is “characters processed per second”; it is displayed on the same graph with our engine’s “indexing speed’, though this comparison is somewhat flawed. On graphs in the left part of the picture, different curves correspond to different base sizes of chunks in the rope datastructure, and the bold curve corresponds to the Java engine. The question “When is our engine better than the Java engine?” is best answered by the top left graph, which shows the dependency of search speed on the string length. It can be seen that the Java engine’s search time is proportional to the length of the string, and our engine’s time is proportional to the number of occurrences. With small base chunk sizes (4–32 characters) our engine is much faster for large strings. On graphs in the right part of the picture, different curves correspond to different lengths of the input string. They are displayed to show how the base chunk size influences search and indexing speed. It can be seen that with increase of this chunk size indexing speed increases rapidly (but with a limit) and search speed decreases just as rapidly. We can conclude that for large strings with a small number of occurrences our engine is more efficient, especially if tuned for a small base chunk size. However, in this case there is a sharp increase in memory consumption: memory consumption per leaf chunk does not depend on the chunk size, but there are 128 times more of 4-character chunks in a string then there are 512-character chunks, therefore the memory consumption is also 128 times larger. Performance benchmarks Memory overhead Almost all the time is spent composing transition functions of characters (which is done through boolean matrix multiplication) for recomputing transition functions of leaf chunks during splits, and almost all the memory overhead is devoted to storing these boolean matrices. We haven’t considered how performance depends on the complexity of regular expressions and on the number of occurrences. A comprehensive treatment of performance questions would take up too much space; curious readers are encouraged to play with the library themselves. What’s next?¶ The current implementation has a number of drawbacks. It is not yet clear which of these can be fixed and which can’t, but in any case, they are interesting algorithmic problems worth thinking about. The questions of match semantics, such as “greediness” etc. are not considered in the program at all. It is unclear which of the popular solutions (POSIX, Perl, …) are efficiently implementable within the automata-theoretic approach. Capturing groups are not supported. The article Regular Expression Matching in the Wild describes a way to support them with automata, but the proposed solution does not fit well with our approach. Multiplication of boolean matrices (used for computing the composition of transition functions) uses a well-optimized but rather naive algorithm; perhaps in some scenarios other algorithms would be faster (for example, ones using sparse matrices). During matching using splits, a lot of unneeded work is done: for example, during the backward split (which is used to find the beginning of a match) there’s no need to compute the transition function for the two resulting strings. Fixing this problem would increase performance by a couple dozen percents. Matching time is proportional to the number of occurrences, the leaf chunk size and the automaton’s size. This makes the program nearly useless as an efficient incremental lexer, because in lexing problems the number of occurrences is very large. One of the ways to fix this problem is to modify the “split by monotone predicate” algorithm to split not into two, but into many parts, for example, on the “edges” of a monotone integer-valued function. Conclusion¶ So, we’ve built a library that does incremental matching of strings against a set of regular expressions using balanced trees and monoids. The library is very efficient in the case of long strings, few expressions, few occurrences, frequent incremental recomputation and a lot of free memory, and is quite inefficient in other cases. It’s hard to say, for which of the cases it is at all possible to make it efficient: for example, whether it is possible to create a high-performance incremental lexer with it, and whether we can at all name this experiment an algorithmic success. The author hopes at least that the presented techniques will inspire the algorithmically inclined readers for new research and will prove of use to them in other problems. In any case, we can say that this development is an interesting and successful experience of blending the functional and imperative approaches. Let us list the used techniques, ideas and traditions from functional programming, and discuss how well they fit with the imperative nature of the target language (Java). The main datastructure, the rope, is pure (immutable). This decision went very well with the Java language and dramatically simplified development and debugging, despite the absence of language features such as algebraic datatypes and pattern matching. Nearly all of the library’s API is pure (doesn’t have side effects). However, mutable state and side effects are abandoned only on an architectural level, but the implementation has quite a few usages of mutable state, both for the sake of performance (multiplication of boolean matrices) and, paradoxically, readability (building an automaton from a regular expression). See the source code for, correspondingly, PowerIntTable and RegexCompiler for details. All in all, this means that the purely functional approach to programming fits well with imperative languages and doesn’t prevent us from using mutable state in the cases where it brings more use than harm. Contrary to the common myth “functional programming is inefficient and leads to excessive memory consumption”, the only performance bottleneck is in the imperative algorithm of transition function multiplication, and memory is used for storing these transition functions for rope nodes as bitmasks. Apparently there is no connection between the overheads and the pure nature of the algorithms [8]. The core of the program is manipulation of higher-order functions: ropes are parameterized by monoids, and their most important operation, splitting, is parameterized by a predicate. Since Java does not have a compact syntax for function definition (such as lambda expressions) and type inference, usage of these entities causes quite a lot of syntactic garbage (especially types in declarations). However, though their usage is extremely important for the program as a whole, it is concentrated in a rather small region of the code, isolated from the library’s end users. However, if the Java type system were a bit more powerful and a bit less verbose, it would be possible to generalize the library, without loss of performance, to searching not just strings but arbitrary sequences. The author would like to thank Dmitry Demeschchuk, Julia Astakhova, Alexey Ott and other reviewers of the original Russian version of this article for their feedback. The project is published on GitHub at http://github.com/jkff/ire. Appendix 1: Implementation of ropes¶ It has already been said that, when representing a string by a balanced tree, in order to keep memory usage reasonable, one should associate each leaf of the tree not with one character but with a chunk. Therefore, the datastructure suggested in Dan’s post (finger trees, described, for example, by Heinrich Apfelmus and in the original paper) is not a good fit: it assumes one node per element of the sequence (string). We should choose one of the kinds of balanced trees satisfying our requirements. Let us list the requirements. It should be possible for the nodes to store the sum of their subtree with respect to an arbitrary additive measure; It should be cheap to update this sum during rebalancing operations (and there should be few of them); The tree’s height should be logarithmic in the number of elements; Concatenation and splitting operations should be efficient. One of the simplest (in terms of implementation) but nevertheless quite efficient balanced trees are trees of constant height, for example “2–3-trees” and “B-trees” (which are frequently used for DBMS indices). In such trees, the length of the path from root to each leaf is the same, therefore (since each non-leaf node has at least 2 children) the height is logarithmic. Usually they are used for a quite different class of problems, namely that of representing sets and searching them, but they are also a perfect fit for representing sequences (strings). The basic idea is that a node is allowed to have \(K\) to \(2K-1\) children (for some \(K\)) and most operations, such as insertion, splitting and concatenation, preserve this property; and when they don’t, a rebalancing occurs: either an overflown node is split into two, or two underflown nodes are merged into one. We shall use a variation on this theme: 2–3 trees with chunks in leaves, where the chunk size may vary from \(N\) to \(2N-1\), and all data is stored in leaves, not in nodes [9]. The next picture illustrates the implementations of all operations on such trees. Essentially two operations suffice: splitting and concatenation, all others can be expressed through them. When digesting the pictures, it is important to remember that we’re dealing with trees of constant height. Note also that the chunk size invariant may be broken, but only in the case where there are less than \(N\) elements total: in this case the tree is represented by a single underflown chunk. Rope operations Let us explain these pictures briefly in the order in which they appear, top to bottom, left to right: Concatenating a rope of height \(h+1\) and a rope of heigh \(h\) has two cases: when the first one is a 2-node and a 3-node respectively. Concatenating a 2-node of height \(h+1\), which is a 2-node with children of height \(h\) and another rope of height \(h\) simply makes a 3-node. Concatenating a 3-node of height \(h+1\) with a node of height \(h\) is the only case where a rope’s height increases: we put those 4 \(h\)-high nodes into a 2-node of 2-nodes. Concatenating a rope (2-node or 3-node) of height \(h+1\) and a rope with height smaller than \(h\) is reduced, through the associativity of concatenation, to recursively concatenating the last \(h\)-node of the first rope with the second rope, and concatenating the remainder of the first rope with the result. Concatenating two ropes of equal height is the simplest case: they make a 2-node. Concatenating two chunks is the only slightly non-trivial case: if their total size is smaller than the maximum chunk size (i.e. \(2N\)), then we simply concatenate the arrays and form a new chunk. If it’s bigger than \(2N-1\) (though it can’t be bigger than \(4N-2\)), then half of this number is between \(N\) and \(2N-1\), which allows us to perfectly make a 2-node of halves of their concatenation. Splitting a 2-node or 3-node is done by summing its 2 or 3 children until the predicate of the sum becomes true, and then descending into the child that caused this, because the split point must be somewhere inside it. Then we assemble the splitting result of the original node from its other children and parts of the split child. An example is drawn for the case where already the sum of the first child satisfies the predicate. Splitting chunks is done in a most straightforward linear fashion. One of the most important aspects of this datastructure is its “purity”: operations on it do not change an existing instance but form a new one instead, i.e. they are functions in the mathematical sense. We have already mentioned the importance of the decision to make the incremental interface “pure”, but now it is time to elaborate. The importance of purity¶ There are many advantages to using a pure approach to algorithms and datastructures, which have manifested themselves during the implementation of this program, particularly in the implementation of ropes. Exceptional ease of implementation. Essentially we can take the diagrams drawn on the picture above and translate them mechanically to code. Lack of mutability causes the code to be a lot simpler, and its correctness (or lack thereof) becomes more obvious, because the code doesn’t have the time dimension anymore, and in order to understand how it works, one does not need to mentally trace a sequence of intermediate steps [10]: the code is just an enumeration of various cases where for each branch it is declared that “such and such input yields such and such output”. And indeed, to the author’s surprise, after the first successful compilation only 1 or 2 silly mistakes were fixed before the code passed all the tests. Ease of debugging. During debugging one often wants to look at the values of some expressions “in advance” in order to understand whether it is necessary to step into them, or their result is correct and the error is somewhere later in the code [11] When these expressions are “pure” (i.e., don’t have side effects), such an approach is possible. If side effects are present, then evaluating the expression in the debugger will change the program’s internal state and further debugging will be pointless. Complete thread safety. It is well known that most standard mutable datastructures do not allow concurrent reading and modification, and one must synchronize access to them in a multi-threaded program. However, it is often desirable to provide non-blocking read access, even if not the most current state of the datastructure will be read. There exist tricks allowing to do that for mutable datastructure (see, for example, the implementation of the ConcurrentHashMap or the ConcurrentSkipListMap classes in the Java standard library), but for immutable datastructures no tricks are necessary, because every instance can be safely read without worrying about it being concurrently modified: it cannot be modified at all. High performance and low memory consumption in certain scenarios. There exist situations where it is useful to preserve the “original” version of a datastructure after applying an operation to it (for example, to preserve access to two ropes after computing their concatenation). Most importantly, these situations arise in backtracking enumeration algorithms and genetic algorithms (for example, when it is possible to combine two genomes in several ways, when one wants to keep both the genomes and the result of their crossover). Of course, one might just copy the original datastructure, but that might be very inefficient, especially if the structure is large. On the contrary, for pure datastructures there’s no need to copy, and we get a performance advantage. Also, as shown on the picture above, many operations on ropes allocate minuscule (constant or logarithmic) amounts of extra memory. The picture below shows the object graph for two ropes and their concatenation. It can be seen that most of the memory is used in a shared fashion, but each object is nevertheless accessible independently. Sharing of memory after rope concatenation. To even better explain how rope concatenation and splitting work, and why they are so easy to implement correctly, let us simply show the code. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43private static &lt;M&gt; Rope&lt;M&gt; append(Rope&lt;M&gt; left, Rope&lt;M&gt; right) { int blockSize = left.factory.getBlockSize(); Reducer&lt;M&gt; reducer = left.factory.getReducer(); M sum = reducer.compose(left.sum, right.sum); if (left.h == right.h) { // Case &#34;Two non-leaves of equal height&#34; if (left.h &gt; 0) return new Rope&lt;M&gt;(left, right, sum); // Case &#34;Two leaves, both large enough to be children of a 2-node&#34; if (!left.isUnderflownBlock() &amp;&amp; !right.isUnderflownBlock()) return new Rope&lt;M&gt;(left, right, sum); // Case &#34;Two leaf chunks, rebalancing needed&#34; String bigBlock = left.block + right.block; if (bigBlock.length() &lt;= 2 * blockSize - 1) return new Rope&lt;M&gt;(left.factory, bigBlock, sum); return new Rope&lt;M&gt;( new Rope&lt;M&gt;(left.factory, bigBlock.substring(0, blockSize)), new Rope&lt;M&gt;(left.factory, bigBlock.substring(blockSize, bigBlock.length())), sum); } else if (left.h == right.h + 1) { if (left.c == null) // 2-node of h + h -&gt; 3-node return new Rope&lt;M&gt;(left.a, left.b, right, sum); else // 3-node of h + h -&gt; 2-node of 2-nodes return new Rope&lt;M&gt;( new Rope&lt;M&gt;(left.a, left.b, reducer.compose(left.a.sum, left.b.sum)), new Rope&lt;M&gt;(left.c, right, reducer.compose(left.c.sum, right.sum)), sum); } else if (right.h == left.h + 1) { // Symmetrical } else if (left.h &gt; right.h + 1) { // Break the larger tree into nodes, regroup using associativity if (left.c == null) return left.a.append(left.b.append(right)); else return (left.a.append(left.b)).append(left.c.append(right)); } else { // right.h &gt; left.h + 1 // Symmetrical } } And then the splitting code, with a slightly curious interface. This function splits a rope into two, given a monotone function on the string represented by the rope. Monotonicity is exploited by requiring to provide functions that compute \(f(a+b)\) given \(f(a)\) and \(b\), where \(b\) is either a string (represented by a rope) or a single character (for finding the rising edge within a leaf-level rope chunk). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50public &lt;S&gt; Pair&lt;Rope&lt;M&gt;, Rope&lt;M&gt;&gt; splitAfterRise( S seed, Function2&lt;S, Rope&lt;M&gt;, S&gt; addChunk, Function2&lt;S, Character, S&gt; addChar, Predicate&lt;S&gt; toBool) { if (block != null) { // Simple linear search inside the chunk S s = seed; for (int i = 0; i &lt; block.length(); ++i) { if (toBool.isTrueFor(s)) return Pair.of( new Rope&lt;M&gt;(this.factory, block.substring(0, i)), new Rope&lt;M&gt;(this.factory, block.substring(i, block.length()))); s = addChar.applyTo(s, block.charAt(i)); } if (toBool.isTrueFor(s)) return Pair.of(this, new Rope&lt;M&gt;(this.factory, &#34;&#34;)); return null; } else { // Start from seed if (toBool.isTrueFor(seed)) return Pair.of(new Rope&lt;M&gt;(this.factory, &#34;&#34;), this); S afterA = addChunk.applyTo(seed, a); // If adding node A made the condition true, descend into A if (toBool.isTrueFor(afterA)) { // Split A and assemble result from b, c and parts of a Pair&lt;Rope&lt;M&gt;, Rope&lt;M&gt;&gt; sa = a.splitAfterRise(seed, addChunk, addChar, toBool); return (c == null) ? Pair.of(sa.first, sa.second.append(b)) : Pair.of(sa.first, sa.second.append(b).append(c)); } // Same for B S afterB = addChunk.applyTo(afterA, b); if (toBool.isTrueFor(afterB)) { Pair&lt;Rope&lt;M&gt;, Rope&lt;M&gt;&gt; sb = b.splitAfterRise(afterA, addChunk, addChar, toBool); return (c == null) ? Pair.of(a.append(sb.first), sb.second) : Pair.of(a.append(sb.first), sb.second.append(c)); } // Same for C, if this is a 3-node if (c == null) return null; S afterC = addChunk.applyTo(afterB, c); if (toBool.isTrueFor(afterC)) { Pair&lt;Rope&lt;M&gt;, Rope&lt;M&gt;&gt; sc = c.splitAfterRise(afterB, addChunk, addChar, toBool); return Pair.of(a.append(b).append(sc.first), sc.second); } return null; } } Appendix 2: Monoids¶ Remember how, given a finite automaton, we can associate every string with a “transition function” with respect to this automaton, and when concatenating two strings their transition functions are composed (let us denote the composition of \(f_1\) and \(f_2\) as \(f_1 \circ f_2\). Composition of transition functions (similarly to concatenation of strings) has a few simple and useful properties: For any transition functions \(f\), \(g\) and \(h\) holds \(f \circ (g \circ h) = (f \circ g) \circ h\). This property of the “\(\circ\) ” operator is called “associativity”. There exists a special transition function \(u\) that maps every state of the automaton to itself. It is called the “unit” of the “ \(\circ\) ” operator because, just as \(1 ⋅ x = x ⋅ 1 = x\) holds for the multiplication operator, for \(\circ\) holds \(u \circ f = f \circ u = f\). These two properties allow us to say that transition functions of a finite automaton form a monoid. More precisely, it is said that the set \(M\), the operation \(⊗\) and the element \(u ∈ M\) (called the “unit” of this operation) form a monoid if the aforementioned two properties hold. Since the notion of a monoid is so simple and general, it is unsurprising that upon a close look at the “casual” objects in programming one may see dozens of monoids. Some of them are listed in the table below. Some applications of monoids to programming are also listed in Dan Piponi’s article Monoids and their uses. Monoids The set \(M\) Operation ⊗ Unit \(u\) Comment Numbers + 0 Natural, integer, real, complex, quaternions… Numbers × 1   Integers LCM 1   Polynomials LCM 1   Numbers, strings… MIN, MAX Maximal and minimal element   Booleans AND TRUE   Booleans OR FALSE   Matrices + 0 Over numbers (+, ×), over numbers (+, MIN), over booleans (OR, AND), … Sets Union Empty set   Sets Intersection Complete set Restricted to subsets of the “complete” set Lists, strings… Concatenation Empty sequence   Dictionaries Union Empty dictionary “Conflicts” are resolved in another monoid: \((dic_1 ⊗ dic_2)[key] = dic_1[key] ⊕ dic_2[key]\) Functions of type A → B \((f ⊗ g)(a)=f(a) ⊕ g(a)\) \(e(a) = e_B\) \((B,⊕,e_B)\) is a monoid Permutations Multiplication Identity permutation   Functions Composition Identity function   Tuples \((x,y)\) where \(x ∈ X, y ∈ Y\) \((x_1, y_1) ⊗ (x_2, y_2) = (x_1 ⊕_X x_2, y_1 ⊕_Y y_2)\) \((u_X, u_Y)\) If \((X, ⊕_X, u_X)\) and \((Y, ⊕_Y, u_Y)\) form monoids … … …   [1]Dan Piponi is a specialist on computer graphics, having participated in the creation of all three “Matrices”, “Star Trek” and some other movies. [2]There exist several algorithms for determinization, described, for example, in the article “An \(O(n log n)\) algorithm for minimizing the states in a finite automaton” by Hopcroft or see the Brzozowski’s algorithm. [3]For example, any deterministic automaton for an expression of the form (0|(01*)(01*)(01*)… 0)* will have size \(O(2^n)\), where \(n\) is the number of repetitions of (01*). [4]This idea has been taken from the article Regular Expression Matching in the Wild by Russ Cox and is used in his re2 engine. [5]There’s a similar situation in graphics programming: coordinate transformations are also represented not with arbitrary functions but with matrices of numbers that can be efficiently multiplied (composed). [6]Actually we don’t need deterministic automata in the final program; they were only used during testing and encouraged creating the automaton abstraction, of which these two are particular cases. [7]Curiously, composition of such transformations is then also captured by multiplication of such boolean matrices. [8]In the early stages of development there was a problem where computing the transition function for a chunk of \(N\) characters would require \(N\) intermediate matrices, but this problem was easily solved with a small API change without sacrificing its purity. [9]A similar datastructure is used for a similar purpose in the “Data.Sequence” module in the Haskell standard library. [10]It is instructive to look, for comparison, at some implementation of rebalancing in mutable red-black trees. [11]However, it’s not a secret to anyone that Real Programmers don’t use debuggers: unfortunately, they won’t be able to appreciate this particular advantage. </description>
      <pubDate>22 Apr 20 19:37 EDT</pubDate>
      <guid>http://jkff.info/articles/ire/</guid>
    </item>
    <item>
      <title></title>
      <link>http://morrick.me/archives/8840</link>
      <description>&lt;a href=&#34;http://morrick.me/archives/8840&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Every time I gather observations and thoughts for a piece on the iPad, I feel I keep returning to the same old insights I’ve had for years. I knew Apple would complicate the iPad’s user interface this way. That many people are happy with it doesn’t mean it’s inherently a good idea.  Anyway. The other day, Apple introduced new iPad Pros, and an updated MacBook Air line-up. Most notably on the iPad hardware front, along with improving whatever feature was improvable, Apple has presented a new accessory — the Magic Keyboard. It has a trackpad. And on the software front, the upcoming iPadOS 13.4 will offer full mouse and trackpad support.  Trackpad support was of course well received by iPad fans and all the people using the iPad as a main (or sole) computing device for work and leisure. Some praised the innovation of the new cursor, which Apple in their marketing describe as being The biggest thing to happen to the cursor since point and click. (Let me pause and eyeroll for a moment here). It’s an interesting take and a good execution. It’s also the least Apple could do on such a device — devising a cursor that is more context-aware and responsive than the one you find in a traditional computer is frankly more consequential than innovative. As is consequential the fact that now the iPad supports mouse/trackpad input. Some of the comments I saw floating around mentioned how Apple has finally given in to the pressing requests from the iPad community, from people who wanted a more ‘Surface-like’ approach for the iPad, so as to make it a more suitable device for productivity. While that may also be true, what I think is that Apple has actually given in to adding mouse/trackpad support to the iPad because they were essentially out of options. And because for them it is a convenient problem solver. It’s Mr Wolf in Pulp Fiction: the one you call when you need a professional to clean up your mess. And the iPad’s user interface still looks a bit messy. You may be accustomed to it. You may be so proficient at moving inside of it that you even love it. I’m not here to criticise your preferences or the iPad as a device. You wanted a ‘faster horse’ — enjoy your faster horse[1]. I’m simply speaking from a conceptual standpoint. And from that standpoint, what I see is that the iPad’s user interface is a patchwork. Features, gestures, combinations of gestures, user interface layers, all stitched together over the years.  Steve Jobs was quoted as saying: “People think focus means saying yes to the thing you’ve got to focus on. But that’s not what it means at all. It means saying no to the hundred other good ideas that there are. You have to pick carefully. I’m actually as proud of the things we haven’t done as the things I have done. Innovation is saying ‘no’ to 1,000 things.” By contrast, it appears the iPad is increasingly saying yes to everything. Those who have no problems with the poor discoverability of several gestures or features still see the iPad as a flexible device that adapts to the needs of its users. They say, “If you feel that the multitasking interface is opaque, it’s okay. You’re not accustomed to it, and you probably don’t need it. The iPad keeps being intuitive for those who only use it at a basic level.” From a visual standpoint, there might be very little difference between a feature that is not visible and a feature that is out of the way. Conceptually, this is a big deal instead. A feature that is not visible and your only way to find it is by reading about it somewhere, or seeing a video tutorial, is something undiscoverable and poorly executed. A feature that is out of the way, but you get hints of its existence by the system, is an indication of at least a modicum of design-oriented thinking behind it. If the iPad’s user interface were truly well thought-out, the more so-called ‘pro’ features would be more discoverable. I wouldn’t get feedback messages from regular folks telling me, I didn’t know I could do this on my iPad, with some even adding that they discovered some gesture or feature while erroneously performing a known one. The more layers of interaction you give to the device, the trickier things get. If the solution to a previously undiscoverable feature is to make the feature (more) discoverable through the use of a different input source, you may have found a way out of the dead end you got stuck in, but it’s not good design, strictly speaking. (I remember an exchange between a woman and an electronics shop’s employee: after buying a Windows laptop she returned to the shop to complain about the poor trackpad performance, and the employee told her to “just use a mouse”. Why not make a better trackpad, instead?)  The comparison with Microsoft’s Surface The iPad getting proper mouse input support, and the new Magic Keyboard for the iPad featuring a regular trackpad, have naturally invited people and reviewers to draw comparisons between the iPad and the Surface. But I don’t see it as Apple ‘catching up’ with Microsoft. I see it more as Apple bringing their racing car to a different kind of championship. Microsoft’s Surface may have its flaws. Its user interface may have its inconsistencies and limitations, but it doesn’t bear the signs of the iPad’s long-standing identity crisis. The Surface and the iPad have different origin stories, and those are reflected in the way you approach and use these devices. The Surface wasn’t really born as a pure tablet with a tailored mobile operating system on it. The concept Microsoft wanted to contribute was of an ultracompact laptop first, with tablet functionalities added on as a convenient alternative to perform quick tasks as needed, without burdening the user with a device fixed in its laptop configuration and behaving like a laptop all the time.  Still, all the devices in the different Surface product lines are essentially laptops (of different weights and capabilities) that can work as, or transform into, tablets when the need arises. Even the first generation of Surface devices back in 2012–2013 were hardly ever seen in the wild without their keyboard, despite it being ‘optional’. They’re very much touchscreen computers with a tablet mode, with productivity as their main purpose. Technically, their Apple counterpart would be something more akin to a ModBook than an iPad. Their operating system, in a way or another, has always been some version of Windows with additional touch- and tablet-friendly features enabled, to make the Surface a more versatile device.  The Surface knows what it is. And Surface users know what to expect from it, in terms of functionality and interface. The user interface could be improved here and there, but it’s not ambiguous. The levels of interaction comfort aren’t either. There is a distinctive best/good/okay comfort range as you go from operating a Surface like a Windows laptop, to using it as a tablet with pen input, to using it with touch input with just your fingers. But that feels fine because that’s the experience the Surface is supposed to provide.  What Microsoft has strived to do over the past eight or so years has been to improve the Surface experience within that model, within that paradigm, and I’d say they’ve been rather successful at that. The next step is represented by devices like the Neo and the Duo, that introduce the new dual screen idea in form and function. The aim is, again, to improve productivity by creating a literal dual space to multitask and facilitate interoperation between apps and tasks, if and when needed.  The iPad, on the other hand, has had a more varied history, and has been more of a chameleon — with regard to both purpose and interface. It was born as a separate device with unique characteristics to fill the perceived void between a laptop and a smartphone. In 2010, when introducing the iPad, Steve Jobs said, In order to really create a new category of devices, those devices are going to have to be far better at doing some key tasks. They’re gonna have to be far better at doing some really important things: better than the laptop, better than the smartphone. And in its first iterations, the iPad was exactly that; its identity pretty clear — ‘a big iPhone’ that could be just as easy to use as an iPhone, but better at doing certain things due to its bigger display. And better than a laptop because certain basic tasks and operations were simply more intuitive to carry out thanks to the multi-touch interface. That really killed all the remaining netbooks still in use at the time, and many non-tech-savvy people were happy to use a small laptop-sized device that was much less intimidating to use than a traditional computer. All thanks to its user interface and its very operating system, that was not Mac OS X slapped on a touch-based device, but something that felt much more integrated and suitable for such a device. The learning curve was also low because people already knew iOS thanks to the iPhone’s success. Then, unfortunately, Steve Jobs passed away. I can see your eyes rolling from here, but bear with me. Although I’ve never denied my utter preference for Jobs’s leadership over Cook’s, I’m not trying to argue that the iPad would necessarily have had a better development and trajectory under Jobs, but it’s undeniable that the iPad is perhaps the device that has suffered the most from Jobs’s absence. Under his tenure, Apple released the first-generation iPad and the iPad 2. The iPad 2 was a first real improvement over the iPad 1: it was thinner, more powerful, and it had cameras. The iPads that came out afterwards, between 2012 and 2015, were essentially the same thing as the iPad 2, with obvious improvements in the hardware, and some improvements in the software. Conceptually, very little moved forward. The iPad Air 2, produced between 2014 and 2016, for all intents and purposes was just like the first iPad, just faster, better, and with more capable apps. As for its conceptual evolution, as for changing the computing experience altogether, however, the iPad felt like a device stuck in stagnant waters. And it still felt pretty much like a device that didn’t know what it wanted to become. It was created as a consumption device first, with the ability to serve as an artistic tool for creation and to do the occasional productivity task if you tried really hard, with the right apps, and jumping through the right hoops. Styluses and external keyboards have always been usable on it, but the iPad has always been a ‘touch-first’ device, meant to be used like a tablet, not like an ultraportable laptop. I can’t speak for Jobs here, but I’m pretty sure he would have said something like, If you need to use the iPad as a laptop replacement, maybe it’s better if you just used a real laptop. But then an increasing number of people, especially tech nerds, started to demand from Apple something more akin to Microsoft’s Surface in features and functionality. And Apple, from 2015–2016 onwards, started to oblige, little by little. And so they have been repurposing the iPad as it goes along without really jettisoning anything. The process has been utterly additive. Employing the famous Jobs’s analogy of trucks and cars, I’d say that from its origins as a sports car, the iPad has progressively become a sports car that can be retrofitted with a trailer, off-road tyres, a 4WD transmission, and so forth.  Some look at the latest iPad Pro, at the full support for mouse input in iPadOS 13.4, at the new Magic Keyboard with trackpad, as a winning combination of tools that make the iPad a truly versatile device. And maybe it is so from a practical standpoint. Again, conceptually, I look at ten years of the iPad and I see its trajectory as going from being a ‘jack of some trades, master of some’ to being a ‘jack of all trades, still master of some, but not all’.  The story and evolution of Microsoft’s Surface are perhaps simpler and less ambitious, but over the years have proceeded with a much clearer process, iterations, and intentions. Apple now probably aims for the iPad to be a sort of blank-slate device, so technically capable that it can do anything you want it to do. But all this retrofitting to make it also behave like a compact laptop has been — still is — a painful process to behold. I keep feeling the iPad could have been so much more in so many different, countercurrent ways, and all it has done in ten years is to become something more conventional. Where the iPad is truly at the forefront today is hardware (industrial design + manufacturing + tech specs). But idea, concept, purpose? Not anymore. Others are trying to match the iPad in hardware, Apple is borrowing ideas and purposes from others. If there’s combined progress in all this, it’s inertial. Again, I can’t be sure, I don’t have the ability to see alternate timelines, but I truly wonder what was Jobs’s ultimate idea for the iPad. What direction he wanted to point it. I’m not saying that things would have been better if Steve Jobs were still among us. But I’m sure we would have felt a stronger sense of direction for the iPad. A clearer vision, even if more polarising, perhaps.  What I felt back in 2010–2011 was that Jobs’s plan could have been to gradually evolve the iPad into a unique computing device, using the tablet format and the multi-touch interface to effectively revolutionise what it meant to be productive using something that is not a traditional computer; to end up with a device that could go beyond the old and established paradigms and metaphors of traditional desktop computing. If he had wanted the iPad to progressively become a Surface-like device, he would have probably sherlocked the aforementioned ModBook and create a touch MacBook with Mac OS X. Maybe this is the root of my general feeling of disappointment in the iPad — that Apple didn’t make enough efforts to come up with a transformative UI that could revolutionise how people can be productive on a tablet, without having to resort to traditional paradigms and input devices. Without reinventing the computing wheel for so many tasks just so they can be easily carried out on an iPad, even when it would make much more sense to just use a laptop. Yes, maybe my expectations have always been high on this front. But not unreasoningly so. Is it really too much to ask of a tablet today, after seeing how innovative certain parts of the Apple Newton’s user interface could be more than 20 years ago? For some, having an iPad acquire more Surface-like capabilities may be a success, a much awaited move that will solve so many things. For me this move, that brings the iPad even closer to a Mac laptop in functionality, in turn makes the iPad even less compelling.  The big picture Judging by previous feedback I received after publishing other articles on the iPad and ranting about my disappointment, a lot of people think I’m still clinging to the past, to the Mac and traditional computers, that I’m averse to change, that I’m ‘old’ and not flexible enough to adapt to this bright future of computing spearheaded by this incredibly awesome and innovative device that is the iPad.  Others mistake my criticism for the iPad at the conceptual level for criticism aimed at the device itself. Nothing could be further from the truth. I do think the iPad is an impressive device. I don’t deny it’s an engineering feat. I absolutely think you can do all kinds of serious work on it. And I’m happy for all those who are able to make the most of it. (Yes, whenever the iPad vs Mac debate rages on Twitter, I have indeed indulged in some sarcasm. But come on, who doesn’t on Twitter?) However, as someone who for several years has cultivated a deep interest for the history of computing and the user interface, I simply can’t look at the iPad (or the Surface, for that matter) and see real progress. Again, I’m not talking about computing power and features. The iPad Pro today is so much more powerful than a supercomputer from the 1970s. I’m talking conceptually. The ideas that drove the computer scientists at RAND corporation to create the RAND tablet in the mid-1960s were more advanced in scope than the ideas behind any tablet available today. And in certain respects more daring, as that tablet was meant to be operated without any keyboard whatsoever. It had an amazing handwriting recognition for the time, and all input came via its stylus. And some of the capabilities of Sketchpad, the groundbreaking program written by Ivan Sutherland in 1963, are still hard to beat in intuitiveness and execution, almost sixty years later. So when I see a tablet device in 2020 become more usable thanks to it finally supporting mouse input of all things, and not because of some other advancement in touch technology, input method, user interaction or user interface design, forgive me if I feel underwhelmed and a bit disheartened. What we do with our devices today is something people like Alan Kay envisaged in the 1960s and 1970s. So no, I’m not clinging to the past or averse to change. I see where we are today and I’m baffled we haven’t advanced further. Or rather, the hardware has. But concepts, paradigms and metaphors are still the ones that have been circulating for more than sixty years. Today I see future-looking hardware marred by backward-looking software, interfaces, and interactions. In a sense, everyone’s clinging to the past, in a way or another. Then why do I still choose the Mac over the iPad? Until I see real progress on those fronts I mentioned above, why should I waste time, money, and energies to be able to do on an iPad the same things I can already do with ease, experience and efficiency on a Mac? I would gladly undergo the re-learning process if that meant mastering a new device or interface concept that would bring significant benefits over ‘the old ways’ in terms of interaction, productivity, fulfilment, and so forth — or even something new in a meaningful way, something that was not possible before. But for now I keep seeing ‘the old ways’ re-emerge here and there behind the external layer of coolness of the iPad. I can’t be averse to change when I don’t even really perceive change in the first place. </description>
      <pubDate>30 Mar 20 11:38 EDT</pubDate>
      <guid>http://morrick.me/archives/8840</guid>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/pdf/1904.01596.pdf</link>
      <description>&lt;a href=&#34;https://arxiv.org/pdf/1904.01596.pdf&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;%PDF-1.5 %� 113 0 obj &lt;&lt; /Filter /FlateDecode /Length 4177 &gt;&gt; stream xڥZے��}߯�K*TՊK��ON6v��ם�U���CA�3��%����&gt;}�Ej8�Jm� l��4��O7����h�իH~�r��͗I�RQXD�Z��V�^ei�*mV7��/��kokw�כX���(�ֿ���z�a��#.���+h���w�*�Ҍ����ϾG��H%ׯ�����&amp;�tP58�&#34;���gs��̫?����wὍ2a����f�ޘ��e� n����5������ú8��*�h��n����C��3�����fn���#v����$��M�P�}?�K\�d�׶��U�p�F�Fߕ����u�&lt;�+�~�������o���%%s߶�r�̯� �YZ��V^����:��N���f��X�*�ߣ�V��e��c�uv�^�8�N���rQc��v�!h����$e�@�m��X�3Fa�6��C5H�?V�7`$�;��/��?�i�R��#�ut�������!�݁�Ɗ���/�n��^����W�\�oϿݮu�������3������i#�a��G��#����v[���Rl ��;u��{�h�=׷;�����b/��Y�l�������z��#C#��tw���බG�շ�l����2�$ ��%�X��&lt;4�ƁPZ�5�a��c٣ua}�[�gb�Fœ���HՉ��r @�s��rB���*8��^^��n����t������Z�Kz������j�ߏU[C��p��������$��x��*�����qV��㈛a��U�&amp;ۣ���sG[�B���,�Z�=���\yj�5ph^�f2���~&#39;�2��[����b@��k�͖� �M`�`�������=I��� v��*�3�O�c����dh�(�Y�Qp_ �b����r�H�v�q���`)x&#39;4�|�4��FB�yj��zZ�t2�`a�C�n�߭���+�Xx�X����t�U�F��&#39;O���[�/Ȍ��[��+����|���q�kY��6�gԋ�մ��� �T5^����Q��ୄ����YF��`k���a�&amp;��&lt;+�7���vX����$׉��AK��b&lt;����Cub��Oj1���j8�{vyQP��N�v��x���ѨH�����yrCgA��k4�,�[i%MJۀV��;� a��qш��`V�g%���g� ݹr��X �v� ����D8nxO��4Y)��I�����:��&lt;������(xk���&#34;W��&amp;CSu�����!B��w����\8.���4V&lt;v ��l��V���&#39;6��y�s,�uz͛(�x&lt;����$@P���P�K��� �J�$]ʹ&lt;19����,� g&#34;Ԙ����!r$b-����E_��I�O��`�Q�v�H�����˹x���:���&#34;���Ѐ��Bd��.�Qd����I�/qDd�8�g���=��H��o�&amp;� �XG� �jQ, ��6y�Fq6_��5�� �o+$���x���^V��lƄz�p˺--�&#34;O){&#39;��8-��������$K�Y��29���&amp;SkX4��$��ȩ`_����&#34;�X�{�4b�Aв�X�,*�K����L�p�P���T��ܲxKg��f�����;� �]�&lt;���� �g�&#39;�Y5�O;���w�J��H��m������&amp;��N,�</description>
      <pubDate>12 Jun 20 12:03 EDT</pubDate>
      <guid>https://arxiv.org/pdf/1904.01596.pdf</guid>
    </item>
    <item>
      <title></title>
      <link>http://www.jerrydallal.com/LHSP/LHSP.htm</link>
      <description>&lt;a href=&#34;http://www.jerrydallal.com/LHSP/LHSP.htm&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;The Little Handbook of Statistical Practice Gerard E. Dallal, Ph.D Chief, Biostatistics Unit Jean Mayer USDA Human Nutrition Research Center on Aging at Tufts University 711 Washington Street Boston, MA 02111 Gerard.Dallal@tufts.edu ** ** ** IMPORTANT ANNOUNCEMENT!!! ** ** ** *Permissions* How to cite these pages Introductory remarks and an advisory Is statistics hard? An all-important foundation! Cause &amp; Effect Study design Intention-To-Treat Analysis Meta Analysis Random Samples / Randomization Randomization plans Units of analysis Creating data files The basics Look At the Data! Logarithms Summary Statistics Location and Spread Correlation Coefficients Probability The Normal Distribution Outliers The Behavior of Sample Means (or Why Confidence Intervals Always Seem to be Based On the Normal Distribution) Confidence Intervals and Tests of Signficance Confidence Intervals Probability &amp; Statistics / A Boy &amp; His Dog CIs for Logarithmically Transformed Data LARGE SAMPLE Formulas for Confidence Intervals Involving Population Means Other Intervals Paired Data / Paired Analyses What does pairing really do? The Ubiquitous Sample Mean! What Student Did What Student Really Did Significance Tests Prologue Significance Tests/Hypothesis Testing Significance Tests Simplified Student&#39;s t Test for Independent Samples P values Why P=0.05? A Valuable Lesson A very nice take on it Exposures Outcomes Oh, what the heck! One-Sided Tests Contingency Tables Proportions Odds Paired Counts Sample Size Calculations Some Underlying Theory &amp; Some Practical Advice An Underappreciated Consequence of Sample Size Calculations As They Are Usually Performed Controlled Trials Surveys Group Randomized, Multi-level, and Hierarchical Studies Nonparametric Statistics Simple Linear Regression Introduction to Simple Linear Regression How to Read the Output From Simple Linear Regression Analyses Correlation and Regression Frank Anscombe&#39;s Regression Examples Transformations In Linear Regression Which fit is better? The Regression Effect / The Regression Fallacy Comparing Two Measurement Devices: Part I Comparing Two Measurement Devices: Part II Linear models: Nomenclature Multiple Linear Regression Introduction to Regression Models Student&#39;s t Test for Independent Samples Is A Special Case of Simple Linear Regression Introduction to Multiple Linear Regression The Most Important Lesson You&#39;ll Ever Learn About Multiple Linear Regression Analyses How to Read the Output From Multiple Linear Regression Analyses The Meaning of Regression Coefficents What Does Multiple Regression Look Like? What Does Multiple Regression Look Like? (Part 2) Why Is a Regression Line Straight? Partial Correlation Coefficients Which Predictors Are More Important? The Extra Sum of Squares Principle Simplifying A Multiple Regression Equation Using the Bootstrap to Simplify a Multiple Regression Equation The Real Problem! Which variables go into a multiple regression equation? The Mechanics of Categorical Variables With More Than Two Categories Interactions In Multiple Regression Models Regression Diagnostics Collinearity Centering Other Regression Diagnostics Analysis of Variance Single Factor ANOVA How to Read the Output From One-Way Analysis of Variance Multiple Comparisons Labeling Similar Means After Performing an Analysis of Variance A web page that creates the labels Adjusting Results for Other Variables Adjusted Means, a.k.a. Least Squares Means Adjusted Means: Adjusting For Numerical Variables Adjusted Means: Adjusting For Categorical Variables Which Variables Should We Adjust For? Multi-Factor Analysis of Variance The Model For Two-Factor Analysis of Variance Pooling Effects Fixed and Random Factors Randomized (Complete) Block Designs Repeated measures analysis of variance Repeated measures analysis of variance: Part I Repeated measures analysis of variance: Part II Why SAS&#39;s PROC MIXED Can Seem So Confusing Pre-test/Post-test Experiments Serial Measurements Crossover Studies Logistic Regression Poisson Regression Degrees of Freedom A good case can be made that the best set of articles about statistical practice written for the practitioner is the series of Statistics Notes appearing in the British Medical Journal. There have been many attempts at online statistics instruction. HyperStat is one of the better ones, not only for the content but also for the additional links. Counter resetAugust 8, 2007 [back to home page] </description>
      <pubDate>24 Mar 20 22:14 EDT</pubDate>
      <guid>http://www.jerrydallal.com/LHSP/LHSP.htm</guid>
    </item>
    <item>
      <title>Work 2.0 - the interruptible programmer</title>
      <link>https://www.stevestreeting.com/2010/09/04/work-2-0/</link>
      <description>&lt;a href=&#34;https://www.stevestreeting.com/2010/09/04/work-2-0/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I’m 37, and I’ve been a (professional) developer for 16 years. You would have thought that in that time, I’d have figured out an effective work style which delivered the desired outcomes (code cut, products shipped etc) without causing detrimental knock-on effects - but, sadly, you’d be wrong. I think the style in which I practiced my craft for the first 15 years of my career was much the same as every other enthusiastic developer: you put a ton of hours in. 12-16+ hour days, evening and weekend coding marathons, pizza in the keyboard, crunch times, 3am debugging sessions where you just can’t go to bed because you can feel the source of that bug just beyond your fingertips, dammit, desperate last-minute sprints to deadlines where you manage to slot that last piece in, Jack Bauer-like, just before the world goes to hell. If you’re in the demographic I’m talking about, you’re nodding sagely, and probably grinning a little too, reminiscing on past trials and glories. This sort of crazy dedication is respected in our circles, and is pretty much expected of any developer who has claimed to earn their stripes. But, it turns out this kind of thing is not good for your health - who knew? Those of you who know me or keep up with my blog know that I’ve been dragged kicking and screaming away from my old ways, because of back issues that I initially ignored, then tried to cope with using token accommodations, and finally succumbed to in a big way. Being self-employed, this was a major problem. Crawling out of the pit I dug for myself took a long time and a lot of frustration - I read quite a few productivity books on the subject to try to find answers on how to keep working, and in the end found that the answers you mould for yourself tend to be the best ones. I’d like to share one of the things I learned along the way. But I’m ‘In The Zone’!! So, I want to talk about the biggest problem I encountered: concentration periods. I can’t sit at a desk for longer than about an hour at a time now; if I don’t get up and walk around, do some gentle stretching etc, at least this often, I’ll pay for it badly once I do move, and probably over the next few days too. I also can’t realistically work more than a standard 8 hour day without pain any more. The problem with this was that, as a programmer, the style which I developed over 15+ years involved getting gradually ‘Into The Zone’ and coding for very long periods at a time, uninterrupted. This is a common theme among coders, who like to shut themselves away for hours at a time, wear headphones to avoid distractions, have ‘quiet times’ and so on - and it’s also why we tend to react really badly when interrupted. Programming requires concentration, and concentration seems to run on a valve system - it takes time to warm up, and once it’s going, you don’t want to turn it off because starting it up again is a major hassle. I thought there was no way around this, and had begun to resign myself to just being less productive because of it. However, over the last 6  months in particular, I’ve discovered that, far from being an intractable problem, this ‘slow warm up, long uninterrupted focus time’ approach is to a large degree a learned behaviour, and it’s possible to re-train yourself to cope with things differently. It’s a little like when people learn to adopt polyphasic sleep patterns - it’s not that you can’t do it, it’s just that when you’ve become accustomed to doing things a certain way, changing that is initially very, very hard. But it’s not impossible, given the right amount of motivation and time to adjust. So, my goal was to acclimatise myself to many shorter work chunks during the day instead of a few very large ones, while still maintaining productivity. The key to this was to learn how to get back ‘In The Zone’ in the shortest time possible - much like the way polyphasic sleepers train themselves to achieve REM sleep more quickly. I’m mostly there now, or at least way better at it than I was, so, what techniques did I use to make this transition? 1. Embrace interruptions This is less of a technique and more of a deliberate psychological adjustment which cuts across all the practical approaches I’ll cover next. Instead of being the typical coder who avoids interruptions at all costs, you need to accept them, and learn to manage them better. It’s hard - you have to try to set aside years of resisting interruptions and initially, until you adjust, you’ll feel like you can’t get enough done. Many people will probably want to give up, unless there’s something specific motivating them to push through it - for me, daily pain was a great motivator. My main message here is that the transition is just a phase, and that it is possible to be an interruptable programmer who still gets things done. But you have to learn not to fight against it, hence why this is the first point. 2. Maintain context outside of your head at all times Much of the problem with interruptions is that of losing context. When you’re in that Zone, you’re juggling a whole bunch of context in your head, adjusting it on the fly, and maintaining and tweaking connections between issues constantly. Interruptions make you drop all that, and it takes time to pick it all up again. My answer to this was to externalise as much as possible, on as many levels as possible: Maintain a running commentary on your current task I am my very own chronicler. I write notes on what I’m doing all the time, whether it’s adding a comment line to a ticket, committing frequently and writing detailed commit notes (you do use a DVCS to make light commits more practical, right? ;)) scribbling a drawing on (ordered) pieces of paper. This really isn’t that onerous, and in fact externalising your thoughts can often help you clarify them. Basically the guide is that roughly every 30 minutes, I should have generated some new piece of context which is stored somewhere other than my head. If I haven’t, then that’s context I’d have more trouble re-building mentally if I’m interrupted. It doesn’t take much time to do, and it has other benefits too such as recording your thought &amp; decision process. Ruthlessly ignore tangental issues You might have noticed that in the last bullet, I used the words ‘current task’, singular. Not ‘tasks’. There is no such thing as having more than one ‘current task’ - there is only the one task you’re actually working on, and distractions. We probably all use bug trackers / ticket systems to track bugs and feature requests, but when you’re working on a ticket, it’s very common to spot a new bug, or identify an opportunity for improvement, or think of a cool new feature. How many of us go ahead and deal with that right away, because it’s in the area we’re already in, or it’s ‘trivial’, or it’s a cool idea that you want to try right now?  I know I did - but I don’t any more; any tangental issues not related to what I’m currently doing get dumped into the ticket system and immediately forgotten until I’m done with the current task, regardless of their size, relevance or priority. It sounds simple and obvious, and this might even be official procedure in your organisation, but I challenge most coders to say that they actually do this all the time. The benefit is that even the tiniest of distractions add an extra level of context that you have to maintain, which is then harder to pick up again after an interruption. For this to work, you need a ticket system which is fast, lightweight, and doesn’t require you to be anal about how much detail you put in initially. You need to be in &amp; out of there in 30 seconds so you can offload that thought without getting distracted - you can flesh it out later. Always know what you’re doing next This is one from GTD (‘Next actions’), but it’s a good one. When you come back from a break or interruption, you should spend no time at all figuring out what you need to be doing next. Your ticket system will help you here, and so will the running commentary that hopefully you’ve been keeping on your active task. If you’ve been forced to switch gears or projects, so long as you’ve maintained this external context universally, you should have no issue knowing what the next actions on each item are. The important thing is to have one next action on each project. If you have several, you’ll have to spend time choosing between them, and that’s wasted time (see the next section on prioritisation). At any one time, you should not only have just one current task, but one unambiguous next action on that task. Half the problem of working effectively is knowing what you’re doing next. Prioritise Negatively I mentioned next actions in the previous section, but how do you decide what comes next? A lot of time can be frittered away agonising over priorities, and I used to struggle with it; I would plan on the assumption that I wanted to do everything on the list, and I just needed to figure out which I needed to do first. I discovered that I could cut the amount of time I spent on planning, and also get better, less ambiguous priorities by inverting the decision making process - to assume a baseline that I wouldn’t do any of the tasks, and assessing the negative outcomes of not doing each one. So instead of ‘which of feature A or B is more important to have?’, it became ‘Let’s assume we ship without feature A and B. What are the issues caused by omitting them in each case?’. It might appear to be a subtle difference, but having to justify inclusion entirely, rather than trying to establish a relative ordering assuming they all get done eventually, tends to tease out more frank evaluations in my experience. Recognise the benefits of breaks Much of the above is about limiting the negative aspects of taking breaks, but the fact is, that they have many work-related benefits too. I’m willing to bet that all coders have stayed late at work, or late into the night, trying to fix a problem, only to find that they fix it within 15 minutes the next day, or think of the answer in some unlikely place like the shower. The reason for this is very simple - extended periods of concentration seem productive, and can be on operational / sequential thinking, but for anything else such as creative thinking or problem solving, it’s very often exactly the opposite. Not only do tired minds think less clearly, but often the answer to a problem lies not in more extensive thinking down the current path which you’ve been exploring in vain for the last few hours, but in looking at the problem from a completely different perspective. Long periods of concentration tend to ‘lock in’ current trains of thought, making inspiration and strokes of genius all too rare. Creativity always happens when you’re not trying, and it’s an often under-appreciated but vital element of the programming toolbox. Interrupting that train of thought can actually be a very good thing indeed. There’s more I could talk about, but that’s quite enough for now I think. I hope someone finds this interesting or useful 😀 </description>
      <pubDate>14 May 20 00:05 EDT</pubDate>
      <guid>https://www.stevestreeting.com/2010/09/04/work-2-0/</guid>
    </item>
    <item>
      <title></title>
      <link>https://github.com/pretzelhammer/rust-blog/blob/master/posts/learning-rust-in-2020.md</link>
      <description>&lt;a href=&#34;https://github.com/pretzelhammer/rust-blog/blob/master/posts/learning-rust-in-2020.md&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Learning Rust in 2020 09 May 2020 · #rust · #programming · #exercises Table of Contents Intro TL;DR Practical Rust Resource Reviews HackerRank Project Euler LeetCode Codewars Advent of Code Rustlings Exercism Conclusion Discuss Notifications Further Reading Intro When I started learning Rust I made the mistake of following the advice to read The Book first. While it&#39;s a great resource, it&#39;s pretty overwhelming for a beginner to get told &#34;If you&#39;d like to learn this programming language the best way to start is to read this 20 chapter book!&#34; Most people give up before they even get started when they get advice like this. Nobody ever told someone to read a 20 chapter book just to get started with Javascript or Python. Rust&#39;s learning curve is no joke but you gotta give the people what they want, and they want to program, not read about programming. Programming is fun and reading about programming is not as fun. The first 10% of this article is gonna be me giving you advice on how to learn Rust in 2020 following a practical hands-on coding approach. This is the good part of the article. You can safely exit after this part (I&#39;ll tell you when). The remaining 90% of this article is me ranting about how most online coding challenge sites have poor support for Rust. TL;DR If you&#39;re a total Rust newbie and want to learn as much as possible in just one day you should read fasterthanlime&#39;s excellent A half-hour to learn Rust and then checkout the awesome Rustlings repo and complete the exercises. If you&#39;re a Rust beginner you should get started on Exercism&#39;s Rust Track. If you get stuck you should ask your friends Google and StackOverflow for help. I recommend taking the time to get comfortable reading and navigating the Rust Standard Library Docs which is amazing and has simple practical examples for how to use everything inside of it. Rust by Example is also a really good high-level reference that you can use to quickly learn Rust syntax and features. If you want to gain a deeper understanding of a certain Rust concept only then do I recommend finding the appropriate chapter in The Book to read. The best part of completing an exercise on Exercism is that you get access to all the solutions by other members which you can sort by most-starred to see particularly idiomatic or clever solutions. This is a great way to learn! At this point you&#39;re probably an advanced beginner and can find your own path. If you need more guidance and would like to continue working on small simple programs I recommend doing the exercises from the Advent of Code 2018 Calendar. The reason why I specifically recommended the 2018 calendar is because once you&#39;re finished with an exercise you can compare your solution to BurntSushi&#39;s Advent of Code 2018 Rust solutions. BurntSushi writes really clean, readable, idiomatic Rust code. Reading the code of an experienced Rustacean will teach you as much as the exercises themselves. Exit now, the good part of the article is over. Practical Rust Resource Reviews Alternative title: Reviews of Free Online Resources a Rust Beginner can use to Practice Writing Small Simple Rust Programs Most of these resources weren&#39;t specifically created for the purpose of teaching Rust, however they can all be used to learn and practice Rust and many of them explicitly support Rust submissions and provide Rust-specific versions of problems. The resources are ordered from worst to best. HackerRank Rust is a supported language on HackerRank except you aren&#39;t allowed to submit Rust solutions to most of the problems on their site. I tried to upload my solution directly and they refused it: This is really strange because I was able to browse Rust solutions for the problem above submitted by other HackerRank users, so it&#39;s possible to submit a Rust solution somehow. I tried Googling this issue but Google didn&#39;t return any useful results. There&#39;s no way for me to evaluate HackerRank other than to tell you not to waste your time with it like I did. Project Euler When I first started to learn programming back in 2012 I commonly heard &#34;If you wanna get up to speed quickly in a new programming language solve some Project Euler problems with it!&#34; which was okay advice at the time since there were not many other alternatives but in my opinion Project Euler has very little to do with programming. Project Euler problems are more math problems than they are programming problems. Their challenge lies almost entirely in the mathematical reasoning required to reach the solution as the programming required is usually trivial. I would not recommend solving Project Euler problems as a way to learn Rust unless you&#39;re very mathematically inclined and have some nostalgia for the site. LeetCode Rust is a supported language on LeetCode. For every problem on LeetCode you get a solution template which usually contains a single unimplemented function which you then have to implement and submit in order to solve the problem. For more involved problems the solution template might include a struct and an impl block with several unimplemented methods. Unfortunately, these solution templates are not created by humans, they are automatically generated, which results in a lot of really awkward and unidiomatic Rust code. Examples: LeetCode generated Rust Idiomatic Rust tree problems represent links as Option&lt;Rc&lt;RefCell&lt;Node&gt;&gt;&gt; Option&lt;Rc&lt;RefCell&lt;Node&gt;&gt;&gt; is overkill for tree links and Option&lt;Box&lt;Node&gt;&gt; works just as well and is much easier to work with methods which obviously mutate self still borrow it immutably, e.g. fn insert(&amp;self, val: i32) methods that mutate self need to borrow it mutably, e.g. fn insert(&amp;mut self, val: i32) signed 32-bit integers are used for all numbers, even if the problem is undefined for negative integers, e.g. fn nth_fib(n: i32) -&gt; i32 problems which are undefined for negative integers should use unsigned integers, e.g. fn nth_fib(n: u32) -&gt; u32 functions always take ownership of their arguments, even if it&#39;s unnecessary, e.g. fn sum(nums: Vec&lt;i32&gt;) -&gt; i32 if you don&#39;t need ownership then borrow fn sum(nums: &amp;[i32]) -&gt; i32 functions sometimes ignore basic error cases, e.g. for fn get_max(nums: Vec&lt;i32&gt;) -&gt; i32 what i32 should be returned if nums is empty? if a result might be undefined the return type should be wrapped in an Option, e.g. fn get_max(nums: &amp;[i32]) -&gt; Option&lt;i32&gt; Other LeetCode issues, specific to Rust: LeetCode doesn&#39;t allow you to pull in 3rd-party dependencies in solutions. Normally I think this is okay for most languages but Rust in particular has a pretty slim standard library which doesn&#39;t even include regex support so a lot of the more complex string parsing problems on LeetCode are pointlessly difficult to solve in Rust but have otherwise trivial solutions in other languages which have regex support in their standard libraries. None of the problems in the concurrency category accept solutions in Rust. What? Fearless concurrency is one of Rust&#39;s major selling points! After solving a problem you can go to the problem&#39;s comments section to see other user&#39;s solutions (as many users like to publish their solutions there) but because Rust isn&#39;t very popular on LeetCode sometimes you won&#39;t find any Rust solutions ;( General LeetCode issues: LeetCode has a surprising amount of very low quality problems. Problems can be liked and disliked by users but problems are never removed even if they hit very high dislike ratios. I&#39;ve seen lots of problems with 100+ votes and 80%+ dislike ratios and I don&#39;t understand why they are kept on the site. Problem difficulty ratings are kinda off. Problems are rated as Easy, Medium, or Hard but there are many Easy problems with lower solve rates than many Hard problems. Not all problems accept solutions in all languages, and you can&#39;t filter problems by which languages they accept. None of the graph problems on LeetCode accept Rust solutions, for example. LeetCode blocks &#34;premium&#34; problems behind a steep monthly paywall but doesn&#39;t offer any kind of premium free-trial so there&#39;s no telling if the quality is actually any better than the free problems. Things LeetCode does right: Solutions to problems are tested against a suite of secret unit tests, but if you fail a particular test case they show you the failed case. All of the generated Rust code at least follows rustfmt conventions. Codewars Codewars is a misleading name. There&#39;s no war going on at Codewars. There&#39;s no time limit to solve problems and your solutions aren&#39;t judged on their speed of execution or memory usage. You aren&#39;t in competition with anyone else. This isn&#39;t a bad thing, just worth pointing out. Rust is a supported language on Codewars. For every problem on Codewars you get a solution template which usually contains a single unimplemented function which you then have to implement and submit in order to solve the problem. These solution templates are created by humans, including humans who aren&#39;t familiar with Rust, so you occasionally get some awkward and unidiomatic Rust. Examples: Codewars&#39; Rust Problems Idiomatic Rust sometimes don&#39;t follow rustfmt conventions, e.g. fn makeUppercase(s:&amp;str)-&gt;String always follows rustfmt conventions, e.g. fn make_uppercase(s: &amp;str) -&gt; String sometimes takes signed integer arguments for problems that aren&#39;t defined for negative integers, e.g. fn nth_fib(n: i32) -&gt; i32 if a problem isn&#39;t defined for negative integers use unsigned integer arguments, e.g. fn nth_fib(n: u32) -&gt; u32 sometimes a problem asks you to return -1 for the null case, e.g. fn get_index(needle: i32, haystack: &amp;[i32]) -&gt; i32 if a result can be null the return type should be wrapped in an Option, e.g. fn get_index(needle: i32, haystack: &amp;[i32]) -&gt; Option&lt;usize&gt; sometimes don&#39;t take advantage of deref coercion, e.g. fn do_stuff(s: &amp;String, list: &amp;Vec&lt;i32&gt;) takes advantage of deref coercion, e.g. fn do_stuff(s: &amp;str, list: &amp;[i32]) All of the issues above only happen sometimes since there are Rustaceans of various skill-levels on Codewars translating problems to Rust. This is a huge step up from LeetCode where all of the generated Rust problem code is consistently unidiomatic. However, the Rust community on Codewars as a whole might lean towards the inexperienced side since I&#39;ve seen some highly upvoted &#34;idiomatic&#34; solutions that were also a bit on the awkward side. Examples: Codewars&#39; highest upvoted Rust solutions Idiomatic Rust sometimes use an explicit return at the end of a function block, e.g. return result; blocks are evaluated as expressions and implicitly return their last item, an explicit return at the end of a function block is unnecessary, e.g. result often use compact formatting to make the solution look more concise should follow rustfmt conventions sometimes make unnecessary allocations, e.g. str_slice.to_string().chars() if you don&#39;t need to allocate then don&#39;t, e.g. str_slice.chars() often try to solve the problem using nothing but iterators at the cost of everything else iterators are expressive and idiomatic, but if you have to chain 15 of them in a row and there are multiple levels of nested iterators in-between then perhaps you should consider refactoring to use some helper functions, intermediate variables, and maybe even a for-loop Again, the issues above only happen sometimes. An experienced Rustacean can spot them easily but there are a lot of Rust newbies on these sites who have no clue they are learning anti-patterns. Other Codewars issues, specific to Rust: Rust doesn&#39;t seem that popular on Codewars, the site has 9000 exercises but only 300 of them have been translated to Rust ;( Other general Codewars issues: Your solution is tested against a suite of secret unit tests, if you fail one of the secret unit tests you aren&#39;t shown the failed test case. This is especially annoying if the test case tests for an edge case that wasn&#39;t clearly communicated in the problem description. Things Codewars does right: There&#39;s a small whitelist of 3rd-party dependencies you can use to help solve problems with Rust. This whitelist includes: rand, chrono, regex, serde, itertools, and lazy_static which helps round out Rust&#39;s standard library and puts it more on par with other languages. You can filter problems by language. Submitting a solution to a problem also automatically publishes the solution. You can view and upvote other members&#39; solutions. You can sort solutions by most upvotes to see particularly concise and clever solutions, which sometimes will also be very idiomatic (but sometimes not, as explained above). Problem difficulty grading is pretty good! Instead of grading problems as Easy, Medium, or Hard like LeetCode, Codewars chooses to grade problems from easiest to hardest as: 8 kyu, 7 kyu, 6 kyu, 5 kyu, 4 kyu, 3 kyu, 2 kyu, 1 kyu. I completed 60 problems in the 8 kyu - 4 kyu range and every level felt a little more difficult than the last, which aligned with my expectations. Advent of Code Advent of Code is totally language-agnostic. This would seem like a minus at first but seeing how horribly HackerRank, LeetCode, and Codewars handle their support for Rust on their sites it&#39;s actually a plus. Advent of Code also gets placed above the previously mentioned sites because AoC&#39;s exercises are really interesting, diverse, and high quality in my opinion. General AoC issues: After you finish an exercise there&#39;s no way to see other people&#39;s Rust solutions unless you search from them on Google, and even after you find some there&#39;s no telling how good or idiomatic they are. To solve the above issue I recommend going through the 2018 Calendar problems and comparing your solutions to BurntSushi&#39;s AoC 2018 Rust solutions. BurntSushi writes really clean, readable, idiomatic Rust code. If you want to go through the 2019 Calendar then I recommend comparing your solutions to bcmyers&#39; AoC 2019 Rust solutions. The reason I specifically suggest bcmyers&#39; is because he made a youtube playlist of him coding up the solutions and he does a great job of explaining his thought process and why he&#39;s doing what he&#39;s doing while he&#39;s coding. Things AoC got right: High quality, interesting, curated exercises that are tied together with a narrative. Language agnostic, so while it doesn&#39;t teach you any Rust patterns it at least doesn&#39;t teach you any Rust anti-patterns either. Rustlings Rustlings is sooo good. All Rustlings exercises are hand-crafted for Rust with love and it&#39;s a wonderful breath of fresh air. Finally, a set of exercises that really teach you idiomatic Rust! If you&#39;re a total Rust newbie you should absolutely checkout Rustlings and get started on the exercises. I highly recommend reading fasterthanlime&#39;s A half-hour to learn Rust first as it&#39;ll get you up to speed on a lot of Rust syntax and concepts super quickly. I have only 1 tiny Rustlings criticism: there are some sudden difficulty spikes in the &#34;error-handling&#34; and &#34;conversions&#34; exercises that I could see some users getting overwhelmed by. I assume most probably make it through, or at least I hope. I also have 1 tiny non-criticism: it&#39;s too short. This is a non-criticism because it&#39;s one of Rustlings design goals to be a quick and gentle introduction to Rust but it&#39;s so good that of course I wish it was somehow longer. Exercism Exercism has a Rust track, which is a collection of exercises roughly ordered by subject and difficulty. The Rust track shares a lot of exercises in common with other tracks, but all of the exercises were translated to Rust by experienced Rustaceans and don&#39;t suffer from any of the awkward unidiomatic Rust issues that are common on LeetCode and Codewars. There are about a dozen Rust-specific problems that require you to implement a standard library trait, or write a macro, or write a parallel solution using multiple threads, or write unsafe Rust code. These exercises are by far the highlights of the track and I wish there were more of them. Exercism is second only to Rustlings as a resource for learning Rust. The only reason I placed it above Rustlings is Rustlings can be completed in an evening and Exercism&#39;s Rust track will take at least a month to complete so it just has a lot more content. Exercism issues, specific to the Rust track: &#34;Mentored mode&#34; is useless, as most of the Rust mentors on the site are inactive, and the students heavily outnumber them, so it&#39;s much better to go through a track in &#34;practice mode&#34;. There are 92 exercises but a good chunk of them don&#39;t really teach you anything new so they kinda feel like busywork. They could probably cut ~20 exercises from the track to make it feel a lot tighter. Things Exercism does right: All problems are translated to Rust or written for Rust by experienced Rustaceans. There are problems which specifically teach Rust&#39;s idioms, design patterns, and unique features. Problem difficulties are fairly graded, easy problems are easy, medium problems are medium, hard problems are hard. You can include whatever 3rd-party dependencies that you want in your solutions. All unit tests are public, if you&#39;re failing a test you know exactly why. After you submit a solution you can browse other user&#39;s solutions, and you can sort solutions by which received the most stars. Conclusion Same as the TL;DR :) Discuss Discuss this article on learnrust subreddit official Rust users forum Twitter rust subreddit Hackernews Github Notifications Get notified when the next article get published by Following pretzelhammer on Twitter or Subscribing to this repo&#39;s release RSS feed or Watching this repo&#39;s releases (click Watch -&gt; click Custom -&gt; select Releases -&gt; click Apply) Further Reading Common Rust Lifetime Misconceptions Sizedness in Rust Tour of Rust&#39;s Standard Library Traits RESTful API in Sync &amp; Async Rust Learn Assembly with Entirely Too Many Brainfuck Compilers </description>
      <pubDate>12 May 20 19:14 EDT</pubDate>
      <guid>https://github.com/pretzelhammer/rust-blog/blob/master/posts/learning-rust-in-2020.md</guid>
    </item>
    <item>
      <title>Maybe We Can Have Nice Things</title>
      <link>https://noncombatant.org/2021/02/16/maybe-we-can-have-nice-things/</link>
      <description>&lt;a href=&#34;https://noncombatant.org/2021/02/16/maybe-we-can-have-nice-things/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Noncombatant 😚 About 🤓 Other Writing 🧐 Bandcamp 🎵 GitHub 💻 16 February 2021 18 Feb: See below for some nice updates! Programming languages advance by introducing new constraints. A key reason we don’t use assembly language for everything is that the lack of constraints make it too hard to use for everyday programming. Before goto was considered harmful, people wrote machine code that jumped all over the place, and programmers had to maintain a mental model of the complete machine state and the full implications of each jump — a recipe for bugs. Then, structured programming was introduced: structured languages still compiled down to gotos (or arbitrary jumps), but the programmer could think in terms of more limited jumps: if, switch/case, call, return, for. These constrained jumps are much easier to understand; for example, when you’re reading code, you can know that return doesn’t return just anywhere. It returns only to the caller, as identified by a pointer on the stack. Later, language designers added additional constrained jumps like throw/catch, and virtual function calls. (throw is a little bit too goto-y for my taste, since you can’t tell locally where the relevant catch block is. But that’s a story for another time.) A key innovation of C++ was to introduce RAII, which essentially ‘piggybacks’ on the value of the stack and enriches it with a lot more power. (The additional complexity is usually manageable, and worth it.) It allows you extend the automatic memory management that the stack provides, initializing and cleaning up complex resources instead of just primitive values like integers and floats. You can automatically close open files, release dynamic storage, and so on. And it’s deterministic. But there was still the problem of the heap: a free-fire zone with no constraints, riddled with memory leaks (heap resources allocated but never released) and use-after-free bugs (heap resources re-used even after having been released). A key innovation of Rust has been to statically constrain the lifetimes of heap resources, enabling us to more completely solve the worst remaining memory unsafety problem. (Previous solutions to the heap lifetime problem were dynamic, not static, and hence expensive in space and time — as well as being non-deterministic. These limitations reduce the applicability of dynamically-managed languages to applications and environments where these costs are affordable.) And, of course, taming object lifetimes greatly eases the problem of safe, efficient concurrency. Concurrency is the key to improving performance in modern systems. Beyond memory safety, Rust makes more use of typefulness than I typically see in other mainstream languages in its niche. For example, Rust’s rich enums and pattern matching make it easier to write state machines, the new type idiom makes it easier to get additional type safety (and improves the interface-as-documentation factor), and so on. You can work to get similar benefits in other languages, but Rust’s syntactic mechanisms and idiomatic usage create affordances for these easier patterns. Another freeing constraint Rust has introduced has been to systematize and automate dependency management: the Cargo package management system. Good dependency management is a monstrously hard problem. Any dependency management system, including manual or ad hoc management, poses a variety of problems: Version conflicts: Inevitably, the latest version of the foo package has changed its API in such a way that it can’t interoperate with the bar package. Now resolve these conflicts all up and down the dependency tree. Fun! Supply-chain security: When you pull in a dependency, you must trust the transitive closure of all the authors who have committed to that dependency sub-tree. This includes not just the code and the programs you can build with it, but build scripts themselves. Micro- and even pico-dependencies: The tendency to multiply the previous 2 problems by creating extremely tiny, single-use packages. Proliferation: The tendency to multiply the previous 3 problems by creating several packages that serve the same basic need, leading to a situation in which effort solving the same problem is fragmented — including documentation, training, code reviewer/auditor hours, and so on. (I call this the “Occam’s Razor now has 5 blades” problem.) The NPM ecosystem provides the clearest modern illustration of these problems. (See page 11 of Github’s report on security, for example.) However, for all of NPM’s problems, at least it is a package management system at all! It’s easy to pick on NPM (or predecessors like CPAN, or CTAN, or...), but even at its worst it’s a huge improvement over manually managing dependencies (such as by manually vendoring them into your source tree, or just telling the user to install such-and-such libraries before attempting to compile). Life is better with NPM, and with Rust’s Cargo, Go’s go get, and so on. Even when they aren’t perfect yet, they provide a framework for improvement, by constraining where dependencies come from and how we maintain them. But a lot of work is still necessary. As an example of a Nice Thing Indeed, Cargo has this add-on package called supply-chain, which will show you all the packages a given package depends on. It will also estimate how many individual publishers author those dependencies. Here is what happens when you run supply-chain on itself: ~/src/rust/cargo-supply-chain % cargo supply-chain publishers The following crates will be ignored because they come from a local directory: - cargo-supply-chain The `crates.io` cache was not found or it is invalid. Run `cargo supply-chain update` to generate it. Fetching publisher info from crates.io This will take roughly 2 seconds per crate due to API rate limits Fetching data for &#34;adler&#34; (0/79) [77 items, including some surprising ones, elided...] Fetching data for &#34;xattr&#34; (78/79) The following individuals can publish updates for your dependencies: 1. alexcrichton via crates: flate2, wasm-bindgen-backend, wasi, bitflags, proc-macro2, wasm-bindgen-macro, wasm-bindgen, openssl-probe, unicode-xid, wasm-bindgen-macro-support, filetime, semver, tar, unicode-normalization, libc, js-sys, bumpalo, log, wasm-bindgen-shared, cfg-if, cc, web-sys [55 authors elided...] 57. zesterer via crates: spin Note: there may be outstanding publisher invitations. crates.io provides no way to list them. Invitations are also impossible to revoke, and they never expire. See https://github.com/rust-lang/crates.io/issues/2868 for more info. All members of the following teams can publish updates for your dependencies: 1. &#34;github:rustwasm:core&#34; (https://github.com/rustwasm) via crates: web-sys, js-sys, wasm-bindgen-macro, wasm-bindgen-macro-support, wasm-bindgen-backend, wasm-bindgen, wasm-bindgen-shared 2. &#34;github:servo:cargo-publish&#34; (https://github.com/servo) via crates: core-foundation-sys, percent-encoding, form_urlencoded, unicode-bidi, core-foundation, idna, url 3. &#34;github:servo:rust-url&#34; (https://github.com/servo) via crates: percent-encoding, form_urlencoded, idna, url 4. &#34;github:rust-bus:maintainers&#34; (https://github.com/rust-bus) via crates: security-framework-sys, security-framework, tinyvec 5. &#34;github:rust-lang-nursery:libs&#34; (https://github.com/rust-lang-nursery) via crates: bitflags, log, lazy_static 6. &#34;github:serde-rs:owners&#34; (https://github.com/serde-rs) via crates: serde_derive, serde, serde_json 7. &#34;github:rust-lang:libs&#34; (https://github.com/rust-lang) via crates: libc, cfg-if 8. &#34;github:rust-lang-nursery:log-owners&#34; (https://github.com/rust-lang-nursery) via crates: log 9. &#34;github:rust-random:maintainers&#34; (https://github.com/rust-random) via crates: getrandom Github teams are black boxes. It&#39;s impossible to get the member list without explicit permission. ~/src/rust/cargo-supply-chain % cargo supply-chain update Note: this will download large amounts of data (approximately 250Mb). On a slow network this will take a while. Now, that’s a lot of dependencies by a lot of publishers whom I don’t know. (Although it’s not automated, if you dig around you’ll find that many of those authors are well-established members of the Rust development team, so trusting them is an easier sell.) Another bummer is that, when I built supply-chain, my default $CFLAGS broke the build (Update 18 Feb: with an almost certainly spurious and not security-relevant warning, -Wunused-macros). (My flags are quite persnickety: -Weverything -Werror -std=c11. Very little code builds with these flags. 😇) Apparently, some of supply-chain’s own dependencies depend on C code. Alas. But that’s OK! Cargo provides a framework for working on these problems. Over time, I’d like to see things move along these lines: Replace C/C++ dependencies with Rust, and reduce the use of unsafe. This has been happening, and will continue to, over time. (See the Safety Dance project, which is a focused on reducing the use of unsafe.) Coalesce the most common dependencies into a (semi-)official ‘extended std’, so that they can appear as a single dependency with a single publishing team. This is controversial in some communities, but I think it would go a long way toward reducing the problems. Obviate some of the micro-dependencies by folding them into larger, more general packages including the language itself, std, and ‘extended std’ (where and if appropriate). This is also sometimes controversial, but again I think it would help. Perhaps supply-chain, check, clippy, or a new tool could provide some indication of a package’s reputation or something like what the Perl community jokingly calls kwalitee: Not quality, but overall ‘smells’. Good test coverage? Is the package version greater than 1.0? Actively maintained? Frequently used? Maintained by the same people for a long time? A low proportion of lines of code in unsafe blocks, or in C/C++/assembly? Some of these things can be more or less automatically determined, and tooling could flag packages that stand out. Fun update 18 Feb: Such a thing exists, and is called crev. Awesome! Another good thing about Rust is its friendly community. Not all systems programming communities are as welcoming as Rust’s is. Rust, and some other communities, have taken proactive steps to maintain a healthy community. I think it’s fair to say the Rust community is doing relatively well, especially in the systems programming niche. Like all language communities, whether of natural languages or artificial languages, the community and the body of literature and the oral tradition are what matter. In its niche, Rust looks like the option with the most momentum around a more positive, healthier community. The community and the language are probably not perfect — nothing is, if perfect is even a thing — but Rust looks like the community most open to solving its problems, and most capable of solving systems programming problems. Thanks to Adrian Taylor for reminding me to mention typefulness, concurrency, and Safety Dance. Thanks to Sergey Davidoff, supply-chain maintainer, for pointing me at crev and noting that Safety Dance is more about reducing unsafe than C. </description>
      <pubDate>19 Feb 21 09:09 EST</pubDate>
      <guid>https://noncombatant.org/2021/02/16/maybe-we-can-have-nice-things/</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.regehr.org/archives/1687</link>
      <description>&lt;a href=&#34;https://blog.regehr.org/archives/1687&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Fuzzing is sort of a superpower for locating vulnerabilities and other software defects, but it is often used to find problems baked deeply into already-deployed code. Fuzzing should be done earlier, and moreover developers should spend some effort making their code more amenable to being fuzzed. This post is a non-comprehensive, non-orthogonal list of ways that you can write code that fuzzes better. Throughout, I’ll use “fuzzer” to refer to basically any kind of randomized test-case generator, whether mutation-based (afl, libFuzzer, etc.) or generative (jsfunfuzz, Csmith, etc.). Not all advice will apply to every situation, but a lot of it is sound software engineering advice in general. I’ve bold-faced a few points that I think are particularly important. Invest in Oracles A test oracle decides whether a test case triggered a bug or not. By default, the only oracle available to a fuzzer like afl is provided by the OS’s page protection mechanism. In other words, it detects only crashes. We can do much better than this. Assertions and their compiler-inserted friends — sanitizer checks — are another excellent kind of oracle. You should fuzz using as many of these checks as possible. Beyond these easy oracles, many more possibilities exist, such as: function-inverse pairs: does a parse-print loop, compress-decompress loop, encrypt-decrypt loop, or similar, work as expected? differential: do two different implementations, or modes of the same implementation, show the same behavior? metamorphic: does the system show the same behavior when a test case is modified in a semantics-preserving way, such as adding a layer of parentheses to an expression? resource: does the system consume a reasonable amount of time, memory, etc. when processing an input? domain specific: for example, is a lossily-compressed image sufficiently visually similar to its uncompressed version? Strong oracles are worth their weight in gold, since they tend to find application-level logic errors rather than the lower-level bugs that are typically caught by looking for things like array bounds violations. I wrote a bit more about this topic a few years ago. Finally, a twitter user suggested “If you’re testing a parser, poke at the object it returns, don’t just check if it parses.” This is good advice. Interpose on I/O and State Stateless code is easier to fuzz. Beyond that, you will want APIs for taking control of state and for interposing on I/O. For example, if your program asks the OS for the number of cores, the current date, or the amount of disk space remaining, you should provide a documented method for setting these values. It’s not that we necessarily want to randomly change the number of cores, but rather that we might want to fuzz our code when set to single-core mode and then separately fuzz it in 128-core mode. Important special cases of taking control of state and I/O include making it easy to reset the state (to support persistent-mode fuzzing) and avoiding hidden inputs that lead to non-deterministic execution. We want as much determinism as possible while fuzzing our code. Avoid or Control Fuzzer Blockers Fuzzer blockers are things that make fuzzing gratuitously difficult. The canonical fuzzer blocker is a checksum included somewhere in the input: the random changes made to the input by a mutation-based fuzzer will tend to cause the checksum to not validate, resulting in very poor code coverage. There are basically two solutions. First, turn off checksum validation in builds intended for fuzzing. Second, have the fuzzer generate inputs with valid checksums. A generation-based fuzzer will have this built in; with a mutation-based fuzzer we would write a little utility to patch up the test case with a valid checksum after it is generated and before it is passed to the program being fuzzed. afl has support for this. Beyond checksums, hard-to-satisfy validity properties over the input can be a serious problem. For example, if you are fuzzing a compiler for a strongly typed programming language, blind mutation of compiler inputs may not result in valid compiler inputs very often. I like to think of validity constraints as being either soft (invalid inputs waste time, but are otherwise harmless) or hard (the system behaves arbitrarily when processing an invalid input, so they must be avoided for fuzzing to work at all). When we fuzz a C++ compiler to look for wrong code bugs, we face a hard validity constraint because compiler inputs that have UB will look like wrong code bugs. There is no simple, general-purpose solution to this kind of problem, but rather a family of techniques for explicitly taking validity properties into account. The most obvious solution — but often not the right one — is to write a new generational fuzzer. The problem is that if you do this, you cannot take advantage of modern coverage-driven fuzzing techniques, which are amazing. To fit into a coverage-driven fuzzing framework you have a couple of options. First, write a custom mutator that respects your validity constraints. Second, structure-aware fuzzing, which basically means taking the mutated data from the fuzzer and translating it into something like what the program being fuzzed expects to see. There’s a lot of research left to be done in making coverage-driven fuzzers work well in the presence of validity constraints without requiring a lot of manual effort. There are some significant subtleties here, maybe I’ll go into them another time. Putting something like a SAT solver into the fuzzer is not, generally speaking, the answer here, first because some validity constraints like checksums are specifically difficult for solvers, and second because some validity constraints (such as UB-freedom in a C++ program) are implicit and cannot be inferred, even in principle, by looking at the system being fuzzed. A lot of code in a typical system cannot be fuzzed effectively by feeding input to public APIs because access is blocked by other code in the system. For example, if you use a custom memory allocator or hash table implementation, then fuzzing at the application level probably does not result in especially effective fuzzing of the allocator or hash table. These kinds of APIs should be exposed to direct fuzzing. There is a strong synergy between unit testing and fuzzing: if one of these is possible and desirable, then the other one probably is too. You typically want to do both. Sanitizers and fuzzers often require tweaks or even significant changes to the build process. To make this easier, keep the build process as clean and simple as possible. Make it easy to switch out the compiler and modify the compiler options. Depend on specific tools (and versions of tools) as little as possible. Routinely build and test your code with multiple compilers. Document special build system requirements. Finally, some fuzzer blockers are sort of silly and easy to avoid. If your code leaks memory or terminates its process with a deep call stack, it will be painful to test using a persistent-mode fuzzer, so don’t do these things. Avoid handling SIGSEGV or, if you really must do this, have a way to disable the handler for fuzzer builds. If your code is not compatible with ASan or UBSan, then these extremely useful oracles are harder to use. In particular, if your code uses a custom memory allocator you should consider turning it off for fuzzer builds, or else adapting it to work with ASan, or else you’ll miss important bugs. Unblock Coverage-Driven Fuzzers Because coverage-driven fuzzers refocus their effort to try to hit uncovered branches, they can be blocked in certain specific ways. For example, if a coverage-driven fuzzer is presented with too many uncoverable branches, it can spend so much time on them that it becomes less likely to hit more interesting branches elsewhere in the program. For example, one time I compared afl’s coverage on a program compiled with and without UBSan, and found that (in whatever time limit I used) it covered quite a lot less of the sanitized program, compared to the unsanitized build. On the other hand, we definitely want our fuzzer to look for sanitizer failures. My advice is to fuzz both sanitized and unsanitized builds of your program. I don’t know how to budget fuzzing resources for these different activities and don’t know of any principled work on that problem. It may not matter that much, since fuzzing is all about overkill. Sometimes your program will call branchy, already-heavily-fuzzed code early in its execution. For example, you might decompress or decrypt input before processing it. This is likely to distract the coverage-driven fuzzer, causing it to spend a lot of time trying to fuzz the crypto or compression library. If you don’t want to do this, provide a way to disable crypto or compression during fuzzing. Any interpreter in your program is likely to make life difficult for a coverage-driven fuzzer, since the relevant program paths are now encoded in the data being interpreted, which is generally opaque to the fuzzer. If you want maximum mileage out of coverage-driven fuzzers, you may want to try to avoid writing interpreters, or at least keep them extremely simple. An obvious way to deal with embedded interpreters — which someone must have thought of and tried, but I don’t have any pointers — would be to have an API for teaching the fuzzer how to see coverage of the language being interpreted. Enable High Fuzzing Throughput Fuzzing is most effective when throughput is very high; this seems particularly the case for feedback-driven fuzzers that may take a while to learn how to hit difficult coverage targets. An easy throughput hack is to make it possible to disable slow code (detailed logging, for example) when it is not germane to the fuzzing task. Similarly, interposing on I/O can help us avoid speed hacks such as running the fuzzer in a ramdisk. “But I Want Fuzzing My Code to be Harder, Not Easier” I don’t have a lot of sympathy for this point of view. Instead of aiming for security through obscurity, we would do better to: fuzz early and thoroughly, eliminating fuzzable defects before releasing code into the wild write code in a programming language with as strong of a type system as we are willing to tolerate — this will statically eliminate classes of bad program behaviors, for example by stopping us from putting the wrong kind of thing into a hashmap aggressively use assertions and sanitizers to get dynamic checking for properties that the type system can’t enforce statically Anti-fuzzing techniques are a thing, but I don’t think it represents a useful kind of progress towards better software. Conclusion Randomized testing is incredibly powerful and there’s no point avoiding it: if you don’t fuzz your code, someone else will. This piece has described some ways for you, the software developer, to make fuzzing work better. Of course there are plenty of other aspects, such as choosing a good corpus and writing a good fuzzer driver, that are not covered here. Acknowledgments: Pascal Cuoq and Alex Groce provided feedback on a draft of this piece, and it also benefited from suggestions I received on Twitter. You can read the conversation here; it contains some suggestions and nuances that I did not manage to capture. </description>
      <pubDate>24 Mar 20 12:24 EDT</pubDate>
      <guid>https://blog.regehr.org/archives/1687</guid>
    </item>
    <item>
      <title>Advice on Research and Writing</title>
      <link>https://www.cs.cmu.edu/~mleone/how-to.html</link>
      <description>&lt;a href=&#34;https://www.cs.cmu.edu/~mleone/how-to.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A collection of advice about how to do research and how to communicate effectively (primarily for computer scientists). Writing and Publishing How to Organize your Thesis, by John W. Chinneck. Advice to Authors of Extended Abstracts, by William Pugh. Hints on good mathematical writing, by David Goss A primer on mathematical writing, by Steven L. Kleiman How To Have Your Abstract Rejected, by van Leunen and Lipton. An Evaluation of the Ninth SOSP Submissions, or, How (and How Not) to Write a Good Systems Paper by Roy Levin and David D. Redell How to Get a Paper Accepted at OOPSLA, by Alan Snyder. Includes comments from an OOPSLA program committee. Advice for 1996 POPL submissions Research Skills Graduate Study in the Computer and Mathematical Sciences: A Survival Manual, by Dianne O&#39;Leary How to be a Good Graduate Student/Advisor, by Marie desJardins A Letter to Research Students, by Duane A. Bailey How to do Research in the MIT AI Lab, ed. David Chapman The IUCS Graduate Student Survival Guide. Includes Survival Skills for Graduate Women and The Assistant Professor&#39;s Guide to the Galaxy. Speaking How to Give a Good Research Talk, by Simon Peyton Jones et al. How to Present a Paper in Theoretical Computer Science, by Ian Parberry. Career Development Networking on the Network by Phil Agre Computer Science Faculty and Research Positions The Young Scientists&#39; Network CRA Committee on the Status of Women in Research Includes a Graduate School Information Kit for Women in Computer Science and Engineering and the Distributed Mentor Project for female undergraduates. ACM SIGMOD academic careers information Includes transcripts from the Workshop on Academic Careers for Women. Related Topics and Resources Information resources for graduate students by Jennifer Myers. A Guide for New Referees in Theoretical Computer Science, by Ian Parberry. On Being A Scientist: Responsible Conduct In Research, from the National Academy of Sciences Papers on women in computer science. Includes Why Are There So Few Female Computer Scientists?, by Ellen Spertus. Study, Research, and Writing Skills web page from the American Communication Association. Information for current and prospective graduate students by Timothy Finin. A Guide for Applying to Graduate Schools by Piroz Mohseni. Ivan Sutherland, &#34;Technology and Courage,&#34; in CMU Computer Science: A 25th Anniversary Commemorative, ed. Richard F. Rashid. ACM Press, 1991. Alan Jay Smith, &#34;The task of the referee,&#34; IEEE Computer, April 1990, pp. 65-71. Dissertation Advice Dissertation News, published by The Association for Support of Graduate Students. Resources for dissertation research (gopher) How to cope with &#34;burnout&#34;, by Andreas Gehmeyr. How to Complete and Survive a Doctoral Dissertation, by David Sternberg. St. Martin&#39;s Press, New York. ISBN 0-312-39606-6 How to Get Control of Your Time and Your Life, by Alan Lakein. Signet Books. ISBN 0-451-16772-4 Procrastination: Why you do it, what to do about it, by Jane Burka and Lenora Yuen. Addison-Wesley. ISBN 0-201-55089-X Humor How to Have a Bad Career in Research/Academia by David Patterson Burnout Prevention and Recovery at MIT A Dictionary of Useful Research Phrases mleone@cs.cmu.edu </description>
      <pubDate>03 Apr 20 18:08 EDT</pubDate>
      <guid>https://www.cs.cmu.edu/~mleone/how-to.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://ericlippert.com/2020/03/27/new-grad-vs-senior-dev/</link>
      <description>&lt;a href=&#34;https://ericlippert.com/2020/03/27/new-grad-vs-senior-dev/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A student who I used to tutor in CS occasionally sent me a meme yesterday which showed “NEW GRAD vs SENIOR DEVELOPER”; the new grad is all caps yelling NO! YOU CAN’T JUST USE BRUTE FORCE HERE! WE NEED TO USE SEGMENT TREES TO GET UPDATE TIME COMPLEXITY DOWN TO O(LOG N)! BREAK THE DATA INTO CHUNKS AT LEAST! OH THE INEFFICIENCY!!! and the senior developer responds Ha ha, nested for loops go brrrrrrrrrrrr… OK, that’s silly and juvenile, but… oh no, I feel a flashback coming on. … … … It is 1994 and I am a second-year CS student at my first internship at Microsoft on the Visual Basic compiler team, reading the source code for InStr for the first time. InStr is the function in Visual Basic that takes two strings, call them source and query, and tells you the index at which query first appears as a substring of source, and the implementation is naive-brute-force. I am shocked to learn this! Shocked, I tell you! Let me digress slightly here and say what the naive brute force algorithm is for this problem. Aside: To keep it simple we’ll ignore all the difficulties inherent in this problem entailed by the fact that VB was the first Microsoft product where one version worked everywhere in the world on every version of Windows no matter how Windows was localized; systems that used Chinese DBCS character encodings ran the same VB binary as systems that used European code pages, and we had to support all these encodings plus Unicode UTF-16. As you might imagine, the string code was a bit of a mess. (And cleaning it up in VBScript was one of my first jobs as an FTE in 1996!) Today for simplicity we’ll just assume we have a flat, zero-terminated array of chars, one character per char as was originally intended. The extremely naive algorithm for finding a string in another goes something like this pseudo-C algorithm: bool starts(char *source, char *query) { int i = 0; while (query[i] != &#39;\0&#39;) { if (source[i] != query[i]) return false; i = i + 1; } return true; } int find(char *source, char *query) { int i = 0; while (source[i] != &#39;\0&#39;) { if (starts(source + i, query)) return i; i = i + 1; } return -1; } The attentive reader will note that this is the aforementioned nested for loop; I’ve just extracted the nested loop into its own helper method. The extremely attentive reader will have already noticed that I wrote a few bugs into the algorithm above; what are they? Of course there are many nano-optimizations one can perform on this algorithm if you know a few C tips and tricks; again, we’ll ignore those. It’s the algorithmic complexity I’m interested in here. The action of the algorithm is straightforward. If we want to know if query “banana” is inside source “apple banana orange” then we ask: does “apple banana orange” start with “banana”? No. does “pple banana orange” start with “banana”? No. does “ple banana orange” start with “banana”? No. … does “banana orange” start with “banana”? Yes! We’re done. It might not be clear why the naive algorithm is bad. The key is to think about what the worst case is. The worst case would have to be one where there is no match, because that means we have to check the most possible substrings. Of the no-match cases, what are the worst ones? The ones where starts does the most work to return false.  For example, suppose source is “aaaaaaaaaaaaaaaaaaaa” — twenty characters — and query is “aaaab”. What does the naive algorithm do? Does “aaaaaaaaaaaaaaaaaaaa” start with “aaaab”? No, but it takes five comparisons to determine that. Does “aaaaaaaaaaaaaaaaaaa” start with “aaaab”? No, but it takes five comparisons to determine that. … and so on. In the majority of attempts it takes us the maximum number of comparisons to determine that the source substring does not start with the query. The naive algorithm’s worst case is O(n*m) where n is the length of source and m is the length of the query. There are a lot of obvious ways to make minor improvements to the extremely naive version above, and in fact the implementation in VB was slightly better. The implementation in VB was basically this: char* skipto(char *source, char c) { char *result = source; while (*result != &#39;\0&#39; &amp;&amp; *result != c) result = result + 1; return result; } int find(char *source, char *query) { char *current = skipto(source, query[0]); while (*current != &#39;\0;) { if (starts(current, query)) return current - source; current = skipto(current + 1, query[0]); } return -1; } (WOW, EVEN MORE BUGS! Can you spot them? It’s maybe easier this time.) This is more complicated but not actually better algorithmically; all we’ve done is moved the initial check in starts that checks for equality of the first letters into its own helper method. In fact, what the heck, this code looks worse. It does more work and is more complicated. What’s going on here? We’ll come back to this. As I said, I was a second year CS student and (no surprise) a bit of a keener; I had read ahead and knew that there were string finding algorithms that are considerably better than O(n*m). The basic strategy of these better algorithms is to do some preprocessing of the strings to look for interesting features that allow you to “skip over” regions of the source string that you know cannot possibly contain the query string. This is a heavily-studied problem because, first off, obviously it is a “foundational” problem; finding substrings is useful in many other algorithms, and second, because we genuinely do have extremely difficult problems to solve in this space. “Find this DNA fragment inside this genome”, for example, involves strings that may be billions of characters long with lots of partial matches. I’m not going to go into the various different algorithms that are available to solve this problem and their many pros and cons; you can read about them on Wikipedia if you’re interested. Anyways, where was I, oh yes, CS student summer intern vs Senior Developer. I read this code and was outraged that it was not the most asymptotically efficient possible code, so I got a meeting with Tim Paterson, who had written much of the string library and had the office next to me. Let me repeat that for those youngsters in the audience here, TIM FREAKIN’ PATERSON. Tim “QDOS” Paterson, who one fine day wrote an operating system, sold it to BillG, and that became MS-DOS, the most popular operating system in the world. As I’ve mentioned before, Tim was very intimidating to young me and did not always suffer foolish questions gladly, but it turned out that in this case he was very patient with all-caps THIS IS INEFFICIENT Eric. More patient than I likely deserved. As Tim explained to me, first off, the reason why VB does this seemingly bizarre “find the first character match, then check if query is a prefix of source” logic is because the skipto method is not written in the naive fashion that I showed here. The skipto method is a single x86 machine instruction. (REPNE SCASB, maybe? My x86 machine code knowledge was never very good. It was something in the REP family at least.) It is blazingly fast. It harnesses the power of purpose-built hardware to solve the problem of “where’s that first character at?” That explains that; it genuinely is a big perf win to let the hardware do the heavy lifting here. But what about the asymptotic problem? Well, as Tim patiently explained to me, guess what? Most VB developers are NOT asking if “aaaab” can be found in “aaaaaaa…”. The vast majority of VB developers are asking is “London” anywhere in this address, or similar problems where the strings are normal human-language strings without a lot of repetitions, and both the source and query strings are short.  Like, very short. Less than 100 characters short. Fits into a cache line short. Think about it this way; most source strings that VB developers are searching have any given character in them maybe 2% of the time, and so for whatever the start character is of the query string, the skipto step is going to find those 2% of possible matches very quickly. And then the starts step is the vast majority of the time going to very quickly identify false matches. In practice the naive brute force algorithm is almost always O(n + m).  Moreover, Tim explained to me, any solution that involves allocating a table, preprocessing strings, and so on, is going to take longer to do all that stuff than the blazingly-fast-99.9999%-of-the-time brute force algorithm takes to just give you the answer. The additional complexity is simply not worth it in scenarios that are relevant to VB developers. VB developers are developing line-of-business solutions, and their line of business is not typically genomics; if it is, they have special-purpose libraries for those problems; they’re not using InStr. … … … And we’re back in 2020. I hope you enjoyed that trip down memory lane. It turns out that yes, fresh grads and keener interns do complain to senior developers about asymptotic efficiency, and senior developers do say “but nested for loops go brrrrrrr” — yes, they go brrrrrr extremely quickly much of the time, and senior developers know that! And now I am the senior developer, and I try to be patient with the fresh grads as my mentors were patient with me. UPDATE: Welcome, Hacker News readers. I always know when I’m linked from Hacker News because of the huge but short-lived spike in traffic. The original author of the meme that inspired this post has weighed in. Thanks for inspiring this trip back to a simpler time! </description>
      <pubDate>28 Mar 20 15:11 EDT</pubDate>
      <guid>https://ericlippert.com/2020/03/27/new-grad-vs-senior-dev/</guid>
    </item>
    <item>
      <title></title>
      <link>https://cstw.osu.edu/writing-resources/dissertation-and-thesis-support</link>
      <description>&lt;a href=&#34;https://cstw.osu.edu/writing-resources/dissertation-and-thesis-support&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Ohio State nav bar The Ohio State University Help BuckeyeLink Map Find People Webmail Search Ohio State Who We AreFAQs, Facts, &amp; MythsFaculty, Staff, &amp; TAsUpcoming EventsPublicationsHistoryLocation and ContactWhat We DoWriting CenterWriting Across the CurriculumWriting Associates ProgramColumbus Global AcademyResearchTips and ToolsMake an AppointmentCancel a Writing Center AppointmentSchedule a Writing Center Classroom VisitWCOnline SupportWork for UsWriting Center PositionsWAC PositionsAlumni and FriendsDonate Search Search Home Page not found Page not found The requested page could not be found. If you have a disability and experience difficulty accessing this site, please contact us for assistance via email at asc-accessibility@osu.edu. Privacy Statement LOGIN © 2022. The Ohio State University Designed and built by ASCTech Web Services </description>
      <pubDate>25 Jun 20 07:52 EDT</pubDate>
      <guid>https://cstw.osu.edu/writing-resources/dissertation-and-thesis-support</guid>
    </item>
    <item>
      <title></title>
      <link>http://danshipper.com/nothing-happens-until-the-sale-is-made</link>
      <description>&lt;a href=&#34;http://danshipper.com/nothing-happens-until-the-sale-is-made&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; How to get your first 10 customers Preface: the assumption for this essay is that you’re building a B2B app, and you have something built but you’re having trouble getting people to pay for it There are three problems with getting your first few customers: You (probably) don’t know how to sell things You don’t know who you’re selling to You don’t even really know what you’re selling Nobody tells you how to answers these questions, and so most people go out to get initial traction in a haphazard way: They have a vague idea in mind for who wants their product They’ve already built the product, so they put together a landing page which which, like, totally speaks to the core value proposition They write some combination of any of the following: A few half-baked ads A few forum posts A few comments on relevant blogs A few blog posts A few cold emails to journalists (because, dude, we would BLOW UP if we could just get ‘Crunched) They send these out into the wild, and (no surprise!), get very few responses They conclude that the product must suck and that nobody wants it, because Mark Zuckerberg did exactly the same thing to launch Facebook at Harvard and look at how that worked out for him If you try to get initial traction this way, it’s very difficult to untangle why it didn’t work: Were the ads targeted to the wrong keyword? Was the copy not compelling enough? Was the sample size too small? Or does no one want what I’m selling? When they fail to get initial traction, most people conclude that the product is the problem. That no one wants what they’re selling. They never consider that the way they’re selling might be COMPLETELY wrong (either in the way the product is being pitched, who it’s being pitched to, or some combination of the two.) I think most of us have been lulled into this sense that the second you post your new product to a listserve you should automatically get sucked into a 4 minute montage scene featuring dark ominous 9 Inch Nails background music only to be spat out at the end with enough money to buy that estate on the Amalfi Coast. And when that doesn’t happen, our first response is always to blame the product. Formally this is called the actor observer bias which tells us that we tend to blame things that don’t go right in our lives to circumstances beyond our control: “No one responded to my emails, so the product must suck. Nobody wants it, otherwise it would already be on TechCrunch.” This is wrong. Here’s the truth: You have learned nothing from spending $200 on Adwords, or writing a few comments, or sending cold emails to journalists. Let me repeat: You have learned nothing. You get a big Zero. You have no actionable information. Your product could suck. But it could still also be completely your fault. Or it could be completely random that you didn’t get any responses. Maybe the journalists were having a bad day, or the three people who clicked on your ads were just bots. The point is: buying a few ads, or sending a few emails, or writing a few blog posts is not enough to conclude anything. Untangling why you’re not making sales seems like an almost insurmountable problem, especially when you realize that at the beginning you don’t even really know what you’re selling. The problem with startups is that you have to figure out WHAT you’re selling AS you’re selling it. It’s like having a big black bag with a product inside it, and you have to run around selling it to people you see on the street. And worse, you’re not allowed to look into the bag to know what it is you’re selling. You can put your hands into it and feel around, but that’s the extent of it. Ok, so how do you deal with this? How do you start to figure out what you’re selling, who you’re selling to, and how to sell? How do you get those first few customers? Find the value The most important question to answer when you’re starting out is this: Where’s the value of my product? Who is it valuable for? And why is it valuable for them? Everything else flows from that. And it turns out that the best way to figure out these basic things out is BY selling your product. Remember that Reid Hoffman quote about startups being like jumping off a cliff and building a plane on the way down? This is why: You need to figure out what you’re doing as you’re doing it. You need to figure out how to sell and who you’re selling to and what you’re selling while you’re selling it. And if you took VC money you have to do that in a very, very short time period. At the beginning most people do things the opposite way: they assume that the product provides real value, they assume they know what that value is, and they combine these assumptions into a sales pitch. Then they run around and talk to a few people, get lukewarm responses, and conclude that the product sucks. Don’t lie to yourself There are some times in life when you’re not going to get into trouble lying to yourself. If your friend ditches you for dinner, you can always tell yourself: “Well I didn’t like him anyway.” If you get a bad score on a math test, you can say: “The teacher sucks, it’s not my fault.” We’re taught that this is an okay way to deal with the difficult situations that life throws at us. That these white lies are just harmless rationalizations that keep us sane. There are at least two industries where you can’t do this: Entrepreneurship and rocket science If a SpaceX engineer lies to himself, the rocket blows up on the launch pad. If the entrepreneur lies to herself, her company will fail before it even gets started. Telling yourself: I know what this product is, I know who wants this, and I know how to sell it, when you really don’t is only going to result in YOUR failure. Admitting to yourself that you need to find out what you don’t know is the first step in the right direction. It’s important to recognize that if you’re just starting out you have no idea where the value of your product lies. And you should make it your goal, not to ram your vision down other people’s throats, but to honestly find out whether your product is valuable, and if it is, who it’s valuable for. Start with people first, not “hits” Most people start out by posting on forums or buying ads or writing blog posts because sitting behind a computer is a lot less scary than actually talking to people. But in order to answer the questions that you need to answer at the beginning you’re going to need to talk to people, either on the phone or face to face. And you’re going to need to do that a lot. This advice has been reiterated so many times that by now it’s pretty banal. But most blog posts say “talk to customers” and leave it at that. Let’s talk more specifically about how to do it. Identifying prospects Before you can go out and talk to people, you have to develop an idea of who you’re going to be talking to. Here’s a quick process for doing this: (Note this advice is primarily targeted at B2B companies, but I believe, can be successfully applied to B2C companies as well) Develop a hypothesis about the target customer Find companies that fit the bill Think about: who at the company is going to want this? Get their email and get in touch Rinse and repeat As an example of step one, our first hypothesis about potential customers for Firefly was that people who would find it useful would probably have installed a live chat program like Olark or SnapEngage on their site. So we started contacting their customers. How to find someone at a company Once you have a company in mind that you think will be relevant, now it’s time to actually find someone to talk to. The lowest hanging fruit is reaching out to their catchall email like sales@ or team@. I wouldn’t recommend doing this, most of that stuff gets ignored. There are really three options for doing this: Look through LinkedIn Look at something like Hoovers (if it’s a big company) Look on their management team page How to find anyone’s email with just their name Once you find someone at a company, the next step is to actually get their email address. Most people don’t list their emails publicly so this requires some digging. What I usually like to do is take their name e.g. Bob Smith and use an email address validator tool (like this one) and do a little guess and check. So if I was trying to find Bob Smith who works at XYZ company I would go to the email validator and try different combinations: bob.smith@xyzcompany.com bsmith@xyzcompany.com bob@xyzcompany.com And very often you’ll be able to guess their email after a few tries. Be careful though, some companies have catchall accounts that make any combination valid – so you’ll have to test at least one or two to verify that you’re getting a real result. There are also other ways to do this: Go to their Press page (many company list a press contact, and you can guess their company email format from the press contact’s email) Use something like Jigsaw.com Call their corporate number and ask to be transferred If you’re a student take advantage of it Being a student has certain advantages, and one of them is that fact that people will be more willing to talk to you, and more willing to help you out. Here’s an example cold email that you might send to someone that you want to talk to: Subject: Hello from Philly! Hi there, I’m a student and I’m working on a project. I know you work in Industry X and I’d love to get some advice. For reasons XYZ I feel like you would be really relevant for the problems I’m thinking about. Are you free to chat on X date next week? By the way we both went to UPenn. Go Quakers! This is useful because it’s a low pressure way to start talking to people. You don’t even really have to be a student to do this – but in general, asking for advice in the beginning will be much lower stress than doing “cold sales emails.” It’s also more useful when you don’t know anything. Be honest If you use the tactic above, be VERY careful to be honest about it. If you ask someone for advice and then go in and try to sell them they’re going to be angry. And rightfully so. If you ask someone for advice, mean it. They might buy from you eventually, but remember that you’re still in the process of learning about your product, learning about your market, and learning how to sell. The sales will come eventually if you do enough outreach. Don’t rush it. Shut up and listen to them When you’re on a sales call it’s very important to shut up and listen. Most people have a tendency to drone on and on about their product. Don’t do this. If your call is successful, you’ll have talked way less than your prospect. (That’s advice Nat Turner gave me and it’s golden.) Here are some questions you might want to ask: What’s your organizational focus in the near term? What are your priorities? How frequently do you have X problem? What’s your process like for buying software? Who would be involved in the process? How long does it normally take? What other software are you currently using? What do you do day-to-day? What kind of tasks do you do repeatedly? Keep the chain going The biggest mistake you can make is letting someone who’s interested get off the phone without clear next steps. You want to know where you’re going before they hang up because people are busy, and unless they’re committed to something they’re unlikely to remember to do anything. Feel free to ask: “So what are the next steps?” To be clear, you only have to do the outbound stuff at the beginning I know that after I publish this article there’s going to be someone in the comments complaining about scale. “Doing this doesn’t scale,” they will say. “It doesn’t apply to a high growth business.” To be clear, I’m not advocating doing this forever (unless your business model demands it). But I do think it’s vastly more effective than just spending a couple of hundred bucks on Adwords. Nothing happens until the sale is made If there’s one thing to take away from this post (practical tips aside) it’s this: You should admit to yourself that you (probably) don’t know anything about your business. That you (likely) don’t know how to sell, who you’re selling to, or what you’re selling. That your product is inside of a black bag that you can’t look into. And that you should begin to deal with that uncertainty by selling your product anyway. Something special happens every time you make a sale: a little piece of the product becomes visible. The product is still wrapped in its black bag, but when you make a sale you get to take a peak at it, only for a second. And you have to pay attention to it, and write it down, and think about it, otherwise you’ll miss it completely. The more sales you make, the more you’ll understand your product. And finally after about a year you’ll begin to know what it looks like. You’ll know every nook and cranny, every blemish and every beautiful curve. And you’ll know all of this even though you’ve never taken it out of its black bag. And you won’t find out any of this by haphazardly sending emails to journalists, or posting on forums: Nothing happens until the sale is made. The sale is where you start to find the value, it’s where you start to learn how to sell, and it’s where you start to figure out what you’re selling. ___ If you liked this post sign up to receive future posts by email here. Or follow me on Twitter. 17 Jul 2013, 6:49pm | 80 comments </description>
      <pubDate>24 Mar 20 12:32 EDT</pubDate>
      <guid>http://danshipper.com/nothing-happens-until-the-sale-is-made</guid>
    </item>
    <item>
      <title></title>
      <link>https://martinfowler.com/bliki/TwoHardThings.html</link>
      <description>&lt;a href=&#34;https://martinfowler.com/bliki/TwoHardThings.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; There are only two hard things in Computer Science: cache invalidation and naming things. -- Phil Karlton Long a favorite saying of mine, one for which I couldn&#39;t find a satisfactory URL. Like many good phrases, it&#39;s had a host of riffs on it. A couple of them I feel are worth adding to the page There are 2 hard problems in computer science: cache invalidation, naming things, and off-by-1 errors. -- Leon Bambrick There are only two hard problems in distributed systems: 2. Exactly-once delivery 1. Guaranteed order of messages 2. Exactly-once delivery -- Mathias Verraes There are so many variations on the “there are only two hard problems in computer programming...” joke that I’m starting to suspect that programming isn’t actually very easy. -- Nat Pryce Revisions 2009-07-14: original post 2010-12-21: added off-by-one variation (unattributed) 2015-08-14: added distributed tweet 2017-03-30: added proper tweet for off-by-one and mention of Tim Bray&#39;s source 2017-12-22: added the Phillip Scott Bowden tweet 2021-05-24: added Nat Pryce tweet Acknowledgements Leon Bambrick let me know about better sources. </description>
      <pubDate>29 May 20 00:26 EDT</pubDate>
      <guid>https://martinfowler.com/bliki/TwoHardThings.html</guid>
    </item>
    <item>
      <title>Tips for Founders Sales: Lessons From Starting Two B2B Startups</title>
      <link>http://philstrazzulla.com/2020/04/06/tips-for-founders-sales-lessons-from-starting-two-b2b-startups/</link>
      <description>&lt;a href=&#34;http://philstrazzulla.com/2020/04/06/tips-for-founders-sales-lessons-from-starting-two-b2b-startups/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Thus far I’ve founded two bootstrapped B2B startups, and led sales on both.  One is off to the races, profitable, and growing.  We even hired a general manager so that I can free myself up to work on other projects.  The other is slightly more nascent, and just barely at ramen profitability. It’s really hard to get started with founder led selling.  I’m a (slight) introvert, and had basically no sales experience before starting my first business.  While I have a business mind, and an MBA in addition to my programming skills, it was still very challenging for me to get started. I used to view sales as this dark art that I could never master.  I’m not “salesy.”  I’m much more of a steak than sizzle person.  I’m too honest.  I don’t look, talk or act like the various stereotypes of a sales person.  And so, I thought it was basically unattainable for me to be successful with sales for the first year of my first business. I’m proud to say that through a lot of struggle and learning, I’ve actually become a decent sales person.  For whatever I lacked in initial extroversion and unblended confidence, I make up for in understanding of strategy and product.  I’m even fairly confident I could hit quota for any post product/market fit b2b SaaS startup out there. What I’ve learned about b2b founder sales It’s been nearly five years now since I started the first business.  As a result, I get introduced to other founders every month or two who are starting to sell their products and want advice.  Coming out of these conversations, I find myself repeating the same themes. So, in no particular order, here’s my advice when starting to do B2B sales at your startup: Find a sales mentor who’s done pre-product/market fit selling before.  It’s essential that someone has done the selling at the earliest stages of a company’s lifecycle.  Even someone who led sales at a Series A company won’t have the proper mindset or experience to help you through this.  Ideally, it’s another founder who’s been through it, and actually done the selling vs the strategy behind sales.  Another bonus is if they’ve sold to the exact customer persona you’re trying to reach.Network with account executives who sell into a similar persona.  Ask them to walk you through their entire sales process, from initial outreach to demo.  Give them your sales pitch, and listen to their feedback.  You’ll get good practice demo’ing, and some advice.  However, you should realize that most sales reps selling a post product/market fit product will have very little understanding of why someone buys their product, that’s really up to you to figure out.Record your first 50 demos and listen to them each within 24 hours of the pitch.  You will start to make small adjustments in messaging, in how long you answer questions, etc.  Be your own coach and try to look objectively at your pitch.Ask for demos for software you are thinking about buying.  Think about what the sales reps do well, and what they don’t do well.  Mainly do this because you will realize 90% of sales reps are pretty mediocre. They don’t show up on time.  They ramble. They don’t do any research.  They are too aggressive.  You can be 10x better than they are as a sales person, even if you’ve never sold before.  And, you’ll have to be to get started without a brand, and a product that is probably half complete.Ask sales people you admire what books and blogs they read.  My recommendations: FirstRound Review’s articles, this book on founding sales, and the Challenger Sale are good places to start.  There is also a hilarious instagram account you will start empathizing with.Spend as much time in person with your prospects as possible.  That means demos, as well as conferences, dinners, coffee, whatever you can.  This will allow you to build trust, and learn a lot faster about your customer than doing calls or even video calls. Working out of one of their offices side by side is a great way to hear how they talk, what they care about, etc. This is great for product development, and even better for sales.Sales calls will probably become the most important way you will get feedback on your product in the next 6-18 months.  Keep track of the themes you hear, and start to think about how you can build those into your offering / start charging for them. Record the closed/lost reasons for no-sale in a structured way so you can see what % fell out of the funnel due to pricing, competitors, etc.Sales can be a grind.  I used to get nervous before calls, and found that creating a routine pre-demo really helped – jumping jacks, review the script, and believe that the product I’m offering will help the person on the other end of the phone.  You also need to let go of any ego or expectations of being treated like a human being.  Most people view sales people as a nuisance.  You will get let down a lot by your prospects every single day, but that makes the wins so much sweeter.  Plus, it’s a thing that happens to everyone, not just you.It’s going to take you a few months to make your first sales (assuming your product is &gt;$1k/yr).  Don’t get discouraged.  Don’t think “we need to change the pitch/outreach/etc.”  If you’ve been thoughtful about your process from the get-go, just keep building your pipeline.Celebrate the wins.  I’m so bad at this and have some sort of Catholic guilt about it.  When someone says “yes” – celebrate.  When someone signs the contract – celebrate.  When someone goes live – celebrate.  High five your co-founder.  Get a beer after work.   Tell your significant other.  Enjoy the moment and pat yourself on your back. There are a million nuances to sales.  My first business was straight B2B SaaS where we were selling HR a product to help with their recruiting.  Getting headspace was tough.  Getting budget was tough.  Getting them to think about their job in a new way was tough. In my new business, I’m selling to marketing.  It’s a completely different buyer that has more budget and is more likely to experiment with new products.  I also have an advantage in that I’m putting reviews of their software online, which means they care a lot more than if I was selling them a tool they can ignore.  This allows me to cut through the noise more effectively.  Of course, it comes with many other challenges, and some I haven’t even run into yet. I hope you enjoy your journey to becoming an A+ sales person, which is a very attainable goal for any founder.  My journey has helped me build win new business, think deeper about product, and kickstarted my personal branding efforts.  Good luck, and feel free to connect if I can be helpful in your journey! </description>
      <pubDate>19 May 20 10:14 EDT</pubDate>
      <guid>http://philstrazzulla.com/2020/04/06/tips-for-founders-sales-lessons-from-starting-two-b2b-startups/</guid>
    </item>
    <item>
      <title></title>
      <link>https://martinfowler.com/bliki/PostIntelliJ.html</link>
      <description>&lt;a href=&#34;https://martinfowler.com/bliki/PostIntelliJ.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I get a lot people offering me free copies of their new software development tool. Sadly I don&#39;t have time to look at them - and frankly I&#39;m usually underwhelmed. Very rarely do I get enthusiastic about a tool. In late 2000 I was keeping more of an eye on tools - particularly those involving refactoring in Java. In those days there were no decent refactoring tools in Java. The Smalltalk Refactoring Browser had proved that it could be done, but I was waiting for someone to cross the Refactoring Rubicon. It was under this guise that I first tried out IntelliJ IDEA by (what&#39;s now called JetBrains). I was impressed by its refactoring support - one the first tools to do it in Java. But what really got my attention was all the other things it did. The essence of its capability was that it creates a parse tree in memory while you&#39;re editing - and uses that parse tree to help you. Suddenly static typing isn&#39;t just something the compiler spanks you with - now the editor can give you type correct method completion. At times its ability to guess what I wanted to type was creepy. I&#39;ve become addicted to ALT-ENTER whenever something looks wrong. The biggest endorsement of IntelliJ came from Thoughtworks developers. If anyone suggested a standard IDE for Thoughtworks projects we needed tear-gas to control the riots. There were JBuilder zealots, textpad zealots, slickedit zealots - don&#39;t even get me started on the emacs zealots. Within six months nearly everyone was using IntelliJ. Voluntarily and eagerly. Even Simon Harris caved in. I was known for my annoying habit of stating how Smalltalk&#39;s IDE was better than anything I&#39;d seen. No longer. For me IntelliJ was first leap forwards in IDEs since Smalltalk. Power and usability that suddenly made everything else second rate. We had entered the post-IntelliJ era IntelliJ isn&#39;t the only IDE in this new world. Eclipse followed pretty fast and has many of the features that made us love IntelliJ so much. But I complement Eclipse by calling it a post-IntelliJ IDE. And although Eclipse is really good, I still see a strong preference towards IntelliJ around Thoughtworks. (Visual Studio is still stuck in the pre-IntelliJ world. Fortunately JetBrains have ReSharper that takes it into the new age.) I don&#39;t know how long IntelliJ and ReSharper will continue to be favorites. The tools business is brutal and developers are fickle. But whatever happens in the future, I&#39;ll always see IntelliJ as the inflection point of IDEs, a milestone tool. </description>
      <pubDate>25 Feb 21 18:10 EST</pubDate>
      <guid>https://martinfowler.com/bliki/PostIntelliJ.html</guid>
    </item>
    <item>
      <title>This 23-Year-Old Built and Sold His Startup While in School - Here’s How He Did It</title>
      <link>https://firstround.com/review/This-23-Year-Old-Built-and-Sold-His-Startup-While-In-School-Heres-What-Made-the-Difference/</link>
      <description>&lt;a href=&#34;https://firstround.com/review/This-23-Year-Old-Built-and-Sold-His-Startup-While-In-School-Heres-What-Made-the-Difference/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;From the moment Dan Shipper stepped foot on the University of Pennsylvania’s campus, he knew he wanted to learn how to build a real software business with paying customers and steady revenue. He passed with flying colors last month when he both graduated from school and sold his company Firefly (the first company backed by Dorm Room Fund) to Pegasystems for multiple millions.Shipper’s success didn’t take anyone by surprise. Targeted early as a technology wunderkind, he was receiving multiple job offers by his sophomore year. He chose to stay in school, he says, because he wasn’t done learning. Now, having seen his first company through an exit, he has a degree and perspective on what makes a real difference for young companies and entrepreneurs.In this exclusive First Round Review interview, Shipper shares the three tactics that moved the needle the most for him and Firefly, and how beginning founders can get a head start on success.Give Yourself Enough Time to Fail (or Succeed)In its first 10 months, Firefly brought in $11,000 in revenue — total. While Shipper and his co-founder Justin Meltzer (pictured above, right) were sure the company solved a concrete problem — allowing two people to collaboratively browse the same webpage without any special software — sales weren’t promising. It would have been a valid decision to throw in the towel at that point, he says.“Because you have very limited information, it&#39;s easy to grab onto the data you have and spin a story about it,” Shipper says. “Like when you have a conversation with someone who really loves your product, you walk away thinking about how this is going to be the biggest thing ever. On the flipside, you see negative information, and you hit the lowest low, wondering why you’re even trying. Even though events like that make you feel very strong emotions, in both cases, the actual prospects for your company haven’t changed very much.”The human brain is not well equipped to handle uncertainty, so you anchor to whatever evidence seems solid. And if you happen to have the track record Firefly did 10 months in, you’d be hard pressed to forge on. But according to Shipper, “Knowing this is really helpful. Once you know where your brain is going to go, you can begin to see a way around things.”The other piece is to focus on what you’re learning — not just the numbers themselves. “Instead of looking at who said what or how many customers are signing up, think about all the data you’re gathering,” he says. “So maybe you have this goal of getting 1,000 new customers a month, instead of looking at the past data thinking it’s impossible, think about what those bad months told you about customers, how you might do better, and how (or whether) what you’re building actually fits in to people’s lives and jobs.”For Firefly, Shipper and Meltzer made a big push to learn everything they could about the customer support industry — which they decided was the primary audience for their product. The key was to look at the last 10 months, and determine whether they needed to shift course based on the results. In their case, they stayed on the same path while making only small modifications to their approach. This allowed them to close big deals that paid off in the long run.“When you shift gears toward learning, you start thinking about whether what you’re doing really fits into the lives of the people who are buying your product,” he says. “It’s a good tool to help you manage your psychology because learning about your customers is much more immediate than something like monthly recurring revenue, which typically lags far behind.”A lot of entrepreneurs cite relentlessness as a good quality to have. You want to be relentless about finding product market fit. You want to be relentless about building your product. There’s this idea that to succeed you can’t let up — you have to be driving hard toward your goal at all times. While Shipper agrees that this energy can be productive, it’s easy to be relentless about the wrong things.“Being relentless is not enough,” he says. “It’s so easy to get trapped in these operant conditioning cycles where you have one good experience at a networking event, so you think you should go to every single one that’s similar even if that doesn’t directly serve the overall goals of your company.”“You have to pair relentlessness with something else, and I think the best thing is being scrappy.”As a tiny, bootstrapped company, Firefly had to make very deliberate decisions about what it could afford in terms of time and money. It had no choice but to be scrappy in its approach, which Shipper says helped him and the team execute against the right goals. “We set our sights on finding customers and trying to sell the product, and we didn’t think about anything else for a while,” he says. “The best way to know if someone is going to buy a product is to actually try to sell it to them.”Acting scrappy also makes you humble, and humility is sometimes the best sales tactic early on.“A lot of entrepreneurs — especially young entrepreneurs — enter markets without really knowing that much about them. When we started, we didn’t have a great sense of the customer service industry, but we were in a situation where we had to. You have to invest the time and energy in information gathering before you’re every going to have a good sales strategy.” Rather than try to gather information and then implement a sales strategy, in the early days Shipper and Meltzer used sales calls to actively collect data. “A lot of what we were doing was unsuccessfully pitching a lot of prospective customers and then humbly examining what had gone wrong so that we could do better next time.”“Going out there and selling is the best way to get the information you need.”A lot of startups don’t give themselves enough time and runway to fail, so they never do find that coveted product-market fit. As Shipper has observed, this outcome usually coincides with the belief that successful companies should see consistent, promising growth. But in his experience, startups go through long plateaus punctuated by a step change in growth. You have to wait for the step change.“Induction doesn’t work with early stage startups. You can’t look at what’s happened in the past and project that you’ll be doing exactly the same amount of business in the future, or even along the same trajectory,” says Shipper. “Surface-level pattern recognition works very for many things, but it tends not to work well for startups. Simplistic pattern recognition says something like, ‘This company only made $1,000 in its first three months, therefore the team should pack up and go home.’ You have to understand why they only made $1,000 during those months to be able to say whether it’s because their business isn’t viable or for another reason. The psychology of running an early-stage company is difficult to manage partly because the emotional part of our brains uses surface-level pattern recognition to judge how we’re doing.”In evolutionary biology, there’s a theory called punctuated equilibrium, the idea that species evolve almost imperceptibly for thousands if not millions of years, and then suddenly massive changes occur when there is a global catastrophe or the environment shifts radically. Suddenly, the rate of evolutionary change spikes.“Especially when you’re at the very early stages, startups behave a lot like this. One day you’re at the beginning level, and then you jump to the next, either when you get funding or land a huge customer or a big news story comes out that directly relates to your product. Something happens and you&#39;re dealing with a different situation right away. The thing is, it’s usually very hard to predict when or if that will happen to you, so you have to keep working. You can’t predict the future from the past. For example, take a look at DuckDuckGo’s daily search queries graph, which is publicly available on their site. DDG was founded in 2008 as a search engine that doesn&#39;t track users&#39; activity. The NSA scandal hit around July of 2013. Since that time, they’ve pretty much tripled the number of searches they were seeing per day.”In the case of Firefly, Shipper and Meltzer had a bunch of game-changing deals in the hopper for quite some time, with no clear sense of whether any would close. “If you’re trying to sell to larger organizations, they have a lot of priorities, a lot of people involved. You basically have to wait for the stars to align to get a green light on your product. Deals require very, very long leads,&#34; Shipper says.“We’d have a full pipeline, but then one company would say they were going to push the deal back a quarter, others would just drop off, sometimes without us knowing why. In most cases, it was clear that larger businesses were hesitant to work with a startup, so they’d want to see how long they could keep us around, and how consistent we’d be in our service. The only thing to do was stick around and try to get people aligned around using this new technology.” Eventually Firefly’s pipeline started to produce.This isn’t to say that every startup should hang on to the very last straw. You simply don’t want to quit too early. “The best thing to do is nail down why things are not going well. Is it really clear that no one wants to buy what you’re selling because they don’t have the problems you’re trying to solve? Or is there evidence that your product could be useful eventually with enough feedback and traction?” You have to be brutally honest when you answer these questions, Shipper says. A lot of times things could improve if you only gave yourself more time and tried more things.Dan Shipper (L) and Justin Meltzer (R)Sell from the Very BeginningTraditionally, companies build a product they feel good about and then try to sell it. Firefly started selling before its product was fully baked, and it turned out to be one of the best decisions they made. “We did a lot of outbound sales, cold calls, cold emails, and it was so much better from a learning perspective than using something like AdWords to sell the product,” Shipper says. “So many people say that to test an idea you should just throw up a landing page and buy some AdWords and see who you attract. Unless you have a lot of experience with AdWords that’s a bad idea. There’s this gigantic learning curve to doing online marketing right, so most likely you’re just going to buy ads that no one ever clicks on, or you’ll get people coming to your page who immediately leave when they see nothing real, and then they’ll never get converted. You could have the wrong keywords, bad ad copy, who knows?”The Firefly team had a very different approach in mind — one that ended up paying off in big ways.“We had the beginnings of the technology, and we knew which market would probably find it the most helpful — that was it. Then we started figuring out how to get it out there.”Shipper and Meltzer started pitching customers who already had live customer service chat widgets that would pop up on their sites to help current visitors. They figured that being able to share a browser screen would be even better for companies to provide a higher level of service. They made a big list of those companies and emailed them to see if they were interested in what Firefly could do.“The other thing we did was look for people within companies who had customer support or customer service roles and made appointments to talk to them. The tendency in these conversations is to do 100% of the talking, because you want to tell them everything your product can do — but you actually want the reverse. You want them to spend all the time talking. What do they not understand? Where do they keep getting stuck as you walk them through the demo? What do they ask the most questions about? That’s how you use your product as a catalyst to get information about how it should work.”Even though Firefly was still a work in progress at the time, Shipper and Meltzer were able to convert some of these early customers. They made it clear to the people they were talking to that they were specifically trying to improve their lives and jobs, and made them feel like they had a stake in the result.“You figure out what your product is only after selling it.”Of course the tactic of selling from the beginning only works if you’re prepared to immediately funnel the feedback you get into product development. Shipper and Meltzer made it a high priority to capture all of the data they gleaned from email replies and conversations to enter it into a feedback loop that yielded real, noticeable changes in how Firefly operated as software and a company.When asked how they managed to evolve both their product and sales process at the same time with a tiny team, Shipper says that it was critical for everyone involved to both understand the business and be able to ship code. This might sound like trying to build your own herd of unicorns, but it vastly accelerated their ability to land customers and deliver something to them on time. “Not only could everyone involved understand what was going on, but it was just easier to keep everyone in the know and on the same schedule. We’d all focus on sales during business hours and then switch to programming at night.” At an early stage, this is feasible.Notably, Shipper says, not all of the feedback they got through selling was about improving the product. A lot of what they learned improved their approach to sales too. “We had this initial idea of going to companies that used live chat customer service systems, but in doing so we realized how small most of them were. It occurred to us that we could sign up all of them individually and probably still not make that much. It was only after talking to them that we decided to go straight to the chat companies themselves that sell to the smaller businesses. They were bigger, making more money, looking for a competitive edge.” In their research, the Firefly team had discovered that the customer service chat sector was deeply fragmented, with upwards of 60 companies offering nearly identical products. In this environment, Firefly’s co-browsing capabilities would be a huge differentiator.This was the break the company needed to reach a much broader audience. It also led them down the path toward their current API model, allowing anyone to include their code on their platform for whatever purpose they wanted — not strictly customer support. “You can build any kind of collaborative app with our software,&#34; Shipper says. &#34;Financial advisors can co-browse with their clients on an online portfolio, for example, without having to use straight screen sharing. We got there by seeing how we could sell to one company that would sell to many others.”Stay Small and Scrappy for as Long as Possible“There are a lot of good things about not needing or even wanting to be a huge business immediately,” Shipper says. “You have time to really learn your industry and your customers, and how your product should change. It gives you time to concentrate on building the skills you’ll need to be successful instead of having your head in the clouds removed from the business on the ground.”While small and scrappy can sound a lot like cash-strapped and vulnerable, Shipper argues that holding onto both can actually give you more control over your business. When you’re a small fish in a big pond, you’re immune to a lot of the problems larger and even mid-stage startups face (security issues, HR challenges, external pressure from a large pool of investors). “You end up having more flexibility to work on what you want on your own terms — and you can always reserve the option to go raise money or try to do something bigger.”Shipper knows this on a personal level, having fielded some outrageous offers to drop out of school and join other companies. He steered Firefly the same way he steered his personal life — keeping things simple until he had all the information in hand and a clear idea of what he wanted.“It’s hard to turn down a lot of money, but I kept asking myself and the team this big question: If we did bring in all this money, what did we actually need it for? Money wasn’t really our bottleneck. Our bottleneck was figuring out a really good marketing strategy, how we could efficiently close customers, stuff like that,&#34; he says. &#34;I think a lot of people get stuck in this mindset that if they have more employees then they’ll have more man hours and will be able to do more. They forget that more employees means more time spent hiring and more baggage. Not to mention all the time you spend raising money to pay these people.”Getting blinded by early millions and promises of rapid growth can actually make it harder to go fast, Shipper says. “If you’re at this early stage of discovery, where you’re doing your research and hammering out your strategy, you want a small team that doesn’t have the pressure that comes with taking a check. Once I understood that, I didn’t think about the money.”One of the biggest problems early-stage startups that do take venture funding face is that they&#39;ve sold a vision they can’t deliver on fast enough. “I think there’s a point for every startup where you have to decide if you want to go for it, and you think you can be huge, or whether you want to grow more slowly on your own. A lot of people think this point is the day you start building your product. My opinion is that it comes far down the line when you really know a lot more about your business.”“Funding can make you do things that you never would if you didn&#39;t feel like you had to.”“The prevailing wisdom is that you should raise more money than you think you need, and I agree that’s probably true,” Shipper says. “It’s just that it should only happen when you’re confident in the fact that more money will allow you to grow much faster — when you’ve already found your trajectory and you just need to accelerate it. This isn’t always the case.” Until you feel this confidence, you should be building with as little money as possible. “Honestly, most products that get huge amounts of early funding could be built for something like $50,000,&#34; he says.Staving off large rounds can also give you more time to think about the type of backers and advisors you want to work with eventually. For Shipper, it gave him the space to cultivate relationships with people over email or through occasional dinners. He had the benefit of seeing who would maintain a longstanding interest in Firefly. “You want to look for people who will take the time themselves to really understand what’s going on not just in your business but in your industry. You want people who are in it for the long-term, and who are more interested in helping you succeed than in looking good.”Incidentally, emphasizing this stay-small strategy also helped Firefly attract the right type of employees. “When you’re not raising any money and you’re building this thing on your own, you don’t oversell to people. I think this type of honesty resonates. Sure, there might be a chance that we’ll get big, but we’re not going to tell you that just so we can hire you or get you to work harder. You have to be here because you want to be, and you like the idea of being part of something self-sustaining, not just a big exit.”All of this combined made Firefly an attractive acquisition target, Shipper says. By selling from the beginning and not being deterred by initial failure, their product gained so much momentum that customer support organizations were starting to demand co-browsing functionality.“More importantly, I think we gave ourselves a lot of options to choose what we wanted,” he says. “You can’t sell a company unless there’s a buyer first, and I think we found the right kind of buyer by running the company the way we wanted to for so long.”</description>
      <pubDate>24 Mar 20 12:32 EDT</pubDate>
      <guid>https://firstround.com/review/This-23-Year-Old-Built-and-Sold-His-Startup-While-In-School-Heres-What-Made-the-Difference/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.cs.cornell.edu/~cristian/Politeness.html</link>
      <description>&lt;a href=&#34;https://www.cs.cornell.edu/~cristian/Politeness.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A computational approach to politeness with application to social factors Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, Christopher Potts Proceedings of ACL, 2013. Nominated for the Best Paper Award PDF Talk slides Fun: Check how polite your requests are using our Politeness Web App Data and Code: ConvoKit   (legacy code: Stanford Politeness API, legacy data:  Stanford Politeness Corpus) Related research:                                          Conversational Behavior                                     Anti-Social Computing Teaser:                                              Politeness and status: successful and failed candidates before and after elections.                                                                                         Editors that will eventually succeed (diamond marker) are significantly more polite than those that will fail (circle markers). Following the elections, successful editors become less polite while unsuccessful editors become more polite. ABSTRACT:                                     We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains.  We use our framework to study the relationship between politeness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite.  We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom.  Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community. BibTeX ENTRY:                                     @InProceedings{Danescu-Niculescu-Mizil+al:13b,  author={Cristian Danescu-Niculescu-Mizil and Moritz Sudhof and Dan Jurafsky   and Jure Leskovec and Christopher Potts},  title={A computational approach to politeness with application to social factors},  booktitle={Proceedings of ACL},  year={2013}} </description>
      <pubDate>12 Jun 20 12:02 EDT</pubDate>
      <guid>https://www.cs.cornell.edu/~cristian/Politeness.html</guid>
    </item>
    <item>
      <title>Building a Literal Library of Building Blocks</title>
      <link>https://sulami.github.io/posts/building-a-literal-library-of-building-blocks/</link>
      <description>&lt;a href=&#34;https://sulami.github.io/posts/building-a-literal-library-of-building-blocks/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Posted on 2019-02-02 This post I know, insert the obligatory &#34;I haven&#39;t posted in a while&#34; bit here. is heavily inspired by a remark Zach Tellman made on the defn podcast, where he says: Having been a professional programmer for a decade, I have a decade&#39;s worth of experience at writing stuff from scratch, not a decade&#39;s worth of tools in my toolbox. And that seems like a not optimal set of circumstances. [Quote at 57:45] I have listened to this some time around Christmas, and this quote has kept me thinking over the past couple of months. What Zach is talking about is a project he is working on which would allow you to capture explorative programming in the branching fashion in which it happens. His example revolves around using a shell to perform some work, like extracting some specific values from a file. You should really go and listen to the episode, he has a lot of very great insights. He explains how we work out these sequences of commands that to accomplish our goal, but never generalise them, but instead throw them away, just to write them from scratch the next time we encounter a similar problem. This rings true for all kinds of programming, not just shell scripting, though shell scripts are especially susceptible to this. Like Zach, I believe this to be a suboptimal situation. Especially being a functional programmer, I believe in small, abstract building blocks, composition, and code reuse, rather than overly specific, bespoke solutions that have to be written from scratch every time. I am someone who tinkers a lot, and there is a lot of code I never commit anywhere. As a matter of fact, I have a habit of creating throw-away files or whole projects in /tmp just to play with something for anywhere between five minutes and a weekend. At the same time I also have a repository on my Github literally called playground, which contains all kinds of small things that I did not want to go through the hassle of creating a Github repository for. Interesting aside: while creating a local repository has so little friction that I do it all the time, only a fraction of them ever touch Github&#39;s servers, as creating a repository through the web interface incurs so much friction. This repository has allowed me to cannibalise some snippets of codes I used in the past, but it is not what I would call a comprehensive library of generalised solutions to problems I repeatedly face. And that has been hugely helpful already, for example I have written about path-finding using the A* algorithm before, so I had a working implementation ready when I needed it for another project. Having a library, in the worldly sense of the word, of useful, generalised snippets of code would institutionalise the knowledge of them. You would not have to remember how to invert a binary tree, because if you have ever played with binary trees you would already have an implementation handy, and it would be tried and tested, and performance-optimised. Practical Implementations Having arrived at the decision of generalising and collecting useful snippets of code somewhere, we are now facing the question of where somewhere actually is, and how we distribute the snippets in a way that allows us to easily use them. The simplest solution would be to maintain one or several collections of useful snippets, and just copy-pasting them into the code you are writing. While this is fast and simple, it does not facilitate innovation flowing in either direction. Updates to the generalised versions are not included in downstream products using them, and vice versa. The result would likely be a duplication of similar, but subtly different solutions to all kinds of problems, scattered over various projects. Bugs that have long been fixed in one of them might still be present in others. The alternative solution is packaging your snippets, and using them as a library. Most of the practical implementation will depend on the programming language you are using, and what kind of projects you are usually working on. Zach Tellman himself has a Clojure library called Potemkin, which is a collection of &#34;some ideas which are almost good&#34;, and which he uses as a dependency for most of his other libraries. While this incurs some overhead, namely the packaging of the library, it does come with a lot of advantages. Other people can benefit from your library. Depending on the scale of the overhead involved with building a library, splitting snippets by topic into &#34;actual&#34; libraries might make sense. It does require more abstraction, and more documentation, but that is not a bad thing. For a simple library with a handful of data structures or functions, writing a quick readme and some docstrings takes less than an hour. There is still room for a default, catch-all library that is just for personal use and contains miscellaneous snippets without any particular topic, and it can be where new snippets end up first. If a section of it grows large enough, it can be extracted into its own library. The bottom line here is, if you write something that solves a problem, keep it somewhere, ideally where you can find it again. Even if it is not generalised or documented, it might come in handy in the future. </description>
      <pubDate>01 Apr 20 12:03 EDT</pubDate>
      <guid>https://sulami.github.io/posts/building-a-literal-library-of-building-blocks/</guid>
    </item>
    <item>
      <title>The Art of Reading More Effectively and Efficiently</title>
      <link>https://aliabdaal.com/read-more-effectively/</link>
      <description>&lt;a href=&#34;https://aliabdaal.com/read-more-effectively/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; It might seem odd to have a blog post devoted entirely to reading. After all, if you’re reading this, chances are you can read. But reading effectively and efficiently is its own skill - one that we’re never really taught how to do.Throughout our academic life, we’re programmed to believe that effective reading is measured by speed and breadth. The more we can read, the smarter we look. And the more broadly we can read, the more intelligent we seem.In fact, I’ve fallen prey to this myself, making a clickbait video called How I Read 100 Books a Year. Full disclosure: I don’t actually. It’s closer to 50. But that makes for a less clickable video (sorry, not sorry).Because of this obsession we have with reading more, we miss out on a lot of valuable insights. Wisdom from across the ages, the lessons mastered by people who&#39;ve overcome extraordinary challenges, and the chance to gain knowledge that challenges our beliefs. All because we&#39;re never taught the ultimate meta-skill: the art of reading.Reading more effectively and efficiently means developing a watertight process to capture ideas, analyse arguments, and ask the right questions. It means identifying the right books to read, understanding the different reading goals, and using evidence-based techniques to increase reading productivity.In many ways, improving the way we read is the number one skill that can change our lives for the better.The Importance of Effective &amp; Efficient Reading“A person who won’t read has no advantage over one who can’t read&#34; - Mark TwainBooks have had an enormous impact on my own life. They’ve acted as a personal mentor, and as a vehicle for compounding knowledge.🤓 Books have been my Personal MentorsIf someone asked me to name the most influential people in shaping my life (outside of my immediate family), I wouldn&#39;t find it too hard to identify a group of people who&#39;ve transformed my thinking through their incredible actions, ideas, and journeys. But the number one influence in my life wouldn&#39;t be people at all. It would be books.By reading lots of books (and by trying to read effectively), I&#39;ve managed to accumulate decades worth of knowledge and experience from the world&#39;s most incredible minds, with minimal personal effort. I&#39;ve learned from mistakes without having to fail, I&#39;ve learned from successes without having to take huge risks, and I&#39;ve travelled thousands of miles without leaving the comfort of my bed in Cambridge.Reading is the mentor without the cost, the pain, and the discomfort. I honestly wouldn&#39;t have started 6med, my YouTube channel, or decided to write a book without the encouragement, motivation, inspiration, and boundless insights offered by my paper friends. Seriously. My only regret is that I didn&#39;t learn to read properly sooner.3 Books That Changed My Life🧠 Books help us Compound Knowledge&#34;Compound interest is the 8th Wonder of the World&#34; - Albert EinsteinJust as money accumulates exponentially, so too does personal knowledge as it snowballs and branches out over time. In other words, the more we read and the better our reading processes are, the more our ideas, beliefs, and opinions begin to develop at an ever-increasing rate.Not only does our brain begin effortlessly creating connections between seemingly disparate pieces of information, but cohesive and creative solutions to some of our most puzzling and perplexing problems gradually emerge. It&#39;s a personal superpower that all of us have the opportunity to discover.“To develop a complete mind: Study the science of art; Study the art of science. Learn how to see. Realise that everything connects to everything else” - Da VinciThe Reading ObjectiveIncreasing our ability to read more effectively, as a means to unlock our own personal potential, begins by deciding on a reading goal. After all, we’re probably going to have a different objective and experience reading Paradise Lost compared to our favourite Harry Potter book.Many brilliant authors talk about books as having a rather loose objective of success, happiness, and personal fulfilment. Roald Dahl, for instance, said that &#34;if you are going to get anywhere in life, you have to read a lot of books&#34;. And J. K. Rowling once said that &#34;something very magical can happen when you read a good book&#34;.But I’d agree, these opinions are abstract, subjective, and largely unhelpful in guiding the way in which we should read. Instead, it&#39;s easier and more useful for our purposes to segment reading objectives into three distinct categories, as identified by Mortimer Adler in How to Read a Book.🤪 Category 1: Reading to EntertainIn this category, we read books purely for enjoyment. It’s how we spend the majority of our time as readers. There are no rules and there&#39;s no need to think too deeply or critically about what we’re reading. The goal is simple: we can relax and immerse ourselves in the story.There’s nothing inherently wrong with reading to entertain ourselves.It&#39;s a healthy way to escape from everyday stress and, if you&#39;re anything like me, a perfect way to finish a productive day of work. In particular, I enjoy reading (or listening to) fantasy novels (with the Brandon Sanderson books being a personal favourite). I even created a video on My Favourite Fantasy Books, which you can check out if you&#39;re interested.🗞 Category 2: Reading to InformIn this second category, we read books to learn specific facts or information about something. These books are typically easy to navigate and simple in their layout and structure. This lets us consume them effortlessly and jump around to relevant sections without losing the gist of what&#39;s being said. The goal is to learn without judgement.For example, we&#39;d read the newspaper, a tourist guide, or the Guinness World Records, all to inform. Although we may find aspects of each of them entertaining, we primarily read these things to develop a factual picture of current affairs, a particular location, or some other snippet of knowledge.Again, for most of us, reading to inform isn&#39;t too problematic.📖 Category 3: Reading to UnderstandIt&#39;s the final category of reading - reading to understand - that most of us (including me) tend to struggle with. It therefore deserves most of our attention when it comes to improving the efficiency and effectiveness of our reading.The problem is that out of the three reading categories, reading to understand requires the greatest cognitive effort. It forces us to challenge our preconceptions, critically analyse the status quo, and directly confront ideas that we may not be immediately comfortable with. This is hard. It can be uncomfortable. But it’s the only way for us to level-up our thinking and personal growth.Ultimately, this is a skill that few of us have mastered. But it&#39;s at the very heart of meaningful productivity and improving the way we read. Therefore, we need a method that takes us from reading at an elementary level (like when we’re reading to entertain and inform) to reading at an analytical or syntopical level.Let&#39;s dive into how we can do this.The Four Levels of ReadingWhile the three categories of reading help guide our reading goal, the four cumulative levels of reading help guide our reading style. These levels were again devised by Mortimer Adler and operate to help us understand a book at a far deeper level than what most of us are used to. As we move up the levels we&#39;ll not only find ourselves more capable of grasping the author&#39;s perspectives and forge deeper insights, but we&#39;ll have a process that works with every single book we decide to read.This is great stuff.👶 Level 1: Elementary ReadingThe first level of reading is the style of reading that everyone knows how to do, as it&#39;s what we&#39;re taught in school. As an elementary reader we can easily understand the words on the page, follow the plot, and have a solid grasp of what the book is trying to say.However, even at this elementary level, it&#39;s easy to screw it up by trying to read too quickly.As you know, I&#39;m all about increasing productivity, but trying to improve reading speed before understanding the fundamentals of effective reading is only going to hinder our capacity to learn new information.My advice - we should try and first improve our reading level. Then, once we&#39;ve mastered the art of reading analytically, we can worry about reading faster (and we&#39;ll talk more about this later).&#34;Every book should be read no more slowly than it deserves, and no more quickly than you can read it with satisfaction and comprehension” - Adler🔎 Level 2: Inspectional ReadingThis second level of reading requires marginally more skill than at the elementary reading level. As an inspectional reader we&#39;re tasked with unearthing the overall framework of the book and mapping out the general picture the author is trying to paint. The idea is that we&#39;re making some preliminary calculations about the book&#39;s content and worth before delving into it properly.There are two aspects to inspectional reading: systematic skimming and superficial reading.Systematic SkimmingWith systematic skimming our aim is to decide whether or not this is a book we actually want to spend the time reading. I like to ask myself &#34;is this one of the greats that I&#39;d happily spend the next few hours of my life looking at?&#34;. If the answer is anything less than &#34;hell yes!&#34; then I won&#39;t bother reading it.To help me answer this question, I first look at the title, the blurb, and the contents page to determine what the book is about and understand its high-level structure. I then flip through the book concentrating on each chapter&#39;s introduction, conclusion, and any sub-headings that interest me. In other words, I do a surface level examination of the book before writing a couple of sentences that neatly summarises everything.Another way to systematically skim a book is by reading a book summary. My favourite way of doing this is with the service Shortform. If a book’s available on Shortform (they’re always adding new titles), I’ll read through the summary first, and if I think it’s interesting, I’ll buy the book on Kindle and read it properly.Superficial ReadingThe objective of superficial reading is to quickly read the book without stopping to reflect or analyse. Speed reading isn&#39;t a problem here as we&#39;re still not reading to understand, but seeing if it is interesting enough to continue onwards to level 3. This process shouldn&#39;t take too long. And if the book doesn&#39;t instantly grab us, we can just put it down. There are plenty of better books we could be reading.A quick note - Sometimes, a book might be really good, but we’ve just encountered it at the wrong time. That’s okay. We don’t have to read it if we don’t find it interesting right now. We can always come back to it later.🎓 Level 3: Analytical ReadingThe third level of reading is about examining the book in-depth. I’ll be honest - I’m not great at this level of reading, but I’m always trying to improve.Analytical reading involves discovering the book&#39;s central meaning, evaluating the author&#39;s arguments, and developing a thorough understanding of the book.&#34;Some books are to be tasted, others to be swallowed, and some few to be chewed and digested&#34; - Francis BaconIn particular, this level requires us to actively read the book and &#34;the more active the reading the better&#34; (Adler). I&#39;m a big fan of just making the book my own by highlighting text, linking arguments, and synthesising information through marginalia.“Marking a book is literally an experience of your differences or agreements with the author. It is the highest respect you can pay him” - Edgar Allen PoeHowever, perhaps the most critical component of active reading is continually questioning what we&#39;re being told. Specifically, there are three core questions that we should be asking when reading a book analytically:The Holistic Stage: What is the book about as a whole?We largely uncover the answer to this question during the systematic skimming and superficial reading within level 2. The main difference is that, in the holistic stage of level 3, we&#39;re tasked with identifying the questions the author is asking and trying to solve. Put another way, what was it the author was trying to answer by writing this book?Furthermore, our written summary of the book is going to be more comprehensive than a couple of sentences. Think about how the structure and ideas flow in general, helping to guide us to the given conclusion.The Specific Stage: What is the book saying in detail and how is it being said?While reading the book, we need to ensure we&#39;ve fully understood the author&#39;s approach and be comfortable with interpreting their thinking. We should take the time to identify the special keywords that the author has chosen, verify our understanding of them, and try to appreciate their perspective.In each chapter, the author will also make certain claims and propositions, which we should restate in our own words and decide whether or not their argument is strong. We should carefully evaluate how these claims and propositions are connected, and check to see they flow logically from one point to the next.The Veracity Stage: Is the book true, whether in whole or in part?In the veracity stage, our task is to constructively analyse. To show where the author has been uninformed, misinformed, illogical or incomplete in their arguments, clearly explaining what the shortcomings are and how the author&#39;s reasoning could be improved. If we can&#39;t do that then our criticism is unlikely to be constructive or valid.“The person who says he knows what he thinks but cannot express it usually does not know what he thinks” - Adler📚 Level 4: Syntopical ReadingThe final level of reading is about our understanding of a subject more generally. Whereas analytical reading focuses on our comprehension of a specific book, syntopical reading helps shape our opinion and increase our overall fluency of the wider topic through understanding how different books relate to one another. This may sound a little abstract, but bear with me.&#34;The benefits [of syntopical reading] are so great that it is well worth the trouble of learning how to do it&#34; - AdlerThe first step is to begin by deciding the subject we want to tackle (eg: productivity or habit-formation). We can then draw up a bibliography of books on the topic, and select just a handful of them that we believe to be most relevant.Having compiled the list of books, we can begin reading syntopically. This means reading each of the books analytically and building mental connections between each of them. I try to define common subject keywords in my own words, identify and answer the most pressing questions that the books collectively address, and make an informed decision about the strengths of each author&#39;s argument.&#34;Creativity is just connecting things&#34; - Steve JobsThrough syntopical reading we&#39;re connecting the best ideas on a subject, which acts as a powerful catalyst giving rise to creative solutions and real insight. It&#39;s truly game-changing (when we actually do it).How to Read MoreOnly once we&#39;ve mastered how to read effectively, by working up the four levels, should we think about reading efficiently.Reading more exposes us to more opinions, helps us build connections between different ideas, and entrenches our existing knowledge. Think of effective reading as a well constructed rocket, and efficient reading as a necessary upgrade to its performance. It just takes things up a notch.There are three steps to reading more:❤️ Step 1 - Love to ReadThe first step of reading more is having the willingness to read more. And falling in love with the act of reading itself.If you don’t already love reading, Naval offers some valuable advice:Read what you love until you love to readIn other words, don’t just pick up the classics because “that’s what clever people do”. Find the books written on topics that fascinate you and by those people you admire most. Just as we can fall in love with exercise by finding the sports we enjoy, we can fall in love with reading by finding the books we enjoy. The ‘fun factor’ is essential to productive reading.Similarly, if you begin reading a book and you aren’t enjoying it, then there’s no obligation to continue. Just stop. We don&#39;t need to finish a book just because we started it. Otherwise we&#39;d be committing to a classic sunk cost fallacy, investing further time into a book simply because we&#39;ve spent valuable time on it already.The truth is most books won&#39;t deserve our attention. So find the books you love and discard the rest.📱 Step 2 - Make it Easy to Access BooksMake it as easy as possible to pick up a book and read it.If there&#39;s one piece of advice I&#39;d recommend here, it would be listening to audiobooks - particularly when reading to entertain rather than to understand. Personally, I&#39;ve found audiobooks to be pretty awesome when working out or driving in my car, as this allows me to incorporate &#39;reading&#39; into my life where I&#39;d previously have been &#34;too busy&#34;. There&#39;s literally no excuse for not reading.However, when reading to understand and analyse, we&#39;re going to want to actually carry the book around with us. Even a Kindle would suffice. That way we no longer have to waste time in queues, on public transport, or even when going to the toilet. Throughout the day you&#39;ll find numerous opportunities to spend 5 or 10 minutes reading. So keep a book nearby. You don&#39;t know when the next great reading opportunity will arise.Other than that, try minimising distractions. One study in 2009 found that we&#39;re exposed to approximately 100,000 words each day from all the media and information we consume. By comparison, that&#39;s about the same length as one-quarter of War and Peace by Leo Tolstoy. It&#39;s a constant stream of disturbances that impede our ability to think and limit our desire to read well.If you&#39;re struggling with this, try creating a comfortable reading space, downloading a productivity app, or just being more mindful with your note taking. Anything that increases your focus.👀 Step 3 - Work on Improving your Reading TechniquesThe final, and least important aspect of effective and efficient reading, is technique. This is typically where most articles on reading begin but I&#39;ve realised that this stuff is pointless unless everything else is in order.If there&#39;s one reading technique that&#39;s going to help the most, it&#39;s improving our consistency. Consistency really is king. Just as the key to YouTube growth is posting 1-3 new videos every week, the key to reading growth is picking up our book for a small chunk of time every day. Although James Clear recommends reading 20 pages at the start of our day, I think it&#39;s easier to set a time target of 20 minutes. And if we have an average reading speed of 250 words per minute, we’ll roughly get through a book every week by doing this. Small actions really do lead to big results (see my video review of Atomic Habits for more on this).Making a public commitment to reading is also incredibly powerful. In one study, it was found that the simple act of betting on a horse to win, and publicly committing to it, elevated the punter&#39;s confidence in their chosen horse&#39;s chance of winning. Taking a stand placed pressure on them to behave consistently with that commitment. Similarly, by using a site like Goodreads (please follow me lol), we can remain accountable and place reasonable pressure on ourself to stay true to our reading goals.The final reading technique is speed reading. However, this comes with a word of warning: only speed read books that you don&#39;t want to understand. Why is that? Well, when reading at speed we&#39;re not going to have the time to think about what is being said or develop the insights necessary for true comprehension. As Woody Allen humorously observed: “I took a speed-reading course and read War and Peace in twenty minutes. It involves Russia.”Despite this, reducing subvocalisation, reading the middle, and using a pointer are all valid speed reading techniques that may be appropriate with inspectional reading or when reading to inform.“In the case of good books, the point is not to see how many of them you can get through, but rather how many can get through to you&#34; - AdlerConclusionReading is hard work, but if we want to rapidly build our knowledge and have high-impact ideas then it&#39;s a crucial skill to learn. No true genius has got to where they were without understanding the importance of effective and efficient reading. It really is the number one factor that will change your life forever. I promise.If you&#39;re ready to commit to levelling up, why not check out the Notion template I use to take notes when reading. It incorporates the advice in this article and gives you a simple framework to capture your thoughts as you read.You&#39;ll never look at books the same way ever again. </description>
      <pubDate>26 Feb 21 09:01 EST</pubDate>
      <guid>https://aliabdaal.com/read-more-effectively/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.aaronkharris.com/asking-questions</link>
      <description>&lt;a href=&#34;https://www.aaronkharris.com/asking-questions&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;404This page could not be found</description>
      <pubDate>30 Mar 20 17:57 EDT</pubDate>
      <guid>https://www.aaronkharris.com/asking-questions</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf</link>
      <description>&lt;a href=&#34;https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;%PDF-1.4 %�쏢 5 0 obj &lt;&gt; stream x��YY�$��~�_яUȕ�}&lt;��ly�E¦����f��03=��=���;����X�aYB��lu�����ed�j#�O����������Bnn.�\(���.�6/�0 n���z��^_�D�Q�l��B��������Q������шd�W�+�?܏J$���n����x7ܕ�C�p3N�+�`� ���a�#�:-�R×��&amp;z#�p[g�q���|���J�x��U;�����F�����Œ����C}HR�a�Z�����������8��Ɂ����m��٨��!�~�6�a�\�:O��;����XS l������K�2��yг9�����_ۿ��i�`�sL����&amp;��s)n�W�����t�`S���H��$����p�oV�D�������k�a��t�s���2���t�P���#��Lh��8���S��[��uC� �F(�R�J����.�����%�d����yj;�{���|�dh�&lt;�[�qh��q{�4 �s?o��)�9�\��A�</description>
      <pubDate>12 Jun 20 12:17 EDT</pubDate>
      <guid>https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf</guid>
    </item>
    <item>
      <title>A Survival Guide to a PhD</title>
      <link>https://karpathy.github.io/2016/09/07/phd/</link>
      <description>&lt;a href=&#34;https://karpathy.github.io/2016/09/07/phd/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This guide is patterned after my “Doing well in your courses”, a post I wrote a long time ago on some of the tips/tricks I’ve developed during my undergrad. I’ve received nice comments about that guide, so in the same spirit, now that my PhD has come to an end I wanted to compile a similar retrospective document in hopes that it might be helpful to some. Unlike the undergraduate guide, this one was much more difficult to write because there is significantly more variation in how one can traverse the PhD experience. Therefore, many things are likely contentious and a good fraction will be specific to what I’m familiar with (Computer Science / Machine Learning / Computer Vision research). But disclaimers are boring, lets get to it! Preliminaries First, should you want to get a PhD? I was in a fortunate position of knowing since young age that I really wanted a PhD. Unfortunately it wasn’t for any very well-thought-through considerations: First, I really liked school and learning things and I wanted to learn as much as possible, and second, I really wanted to be like Gordon Freeman from the game Half-Life (who has a PhD from MIT in theoretical physics). I loved that game. But what if you’re more sensible in making your life’s decisions? Should you want to do a PhD? There’s a very nice Quora thread and in the summary of considerations that follows I’ll borrow/restate several from Justin/Ben/others there. I’ll assume that the second option you are considering is joining a medium-large company (which is likely most common). Ask yourself if you find the following properties appealing: Freedom. A PhD will offer you a lot of freedom in the topics you wish to pursue and learn about. You’re in charge. Of course, you’ll have an adviser who will impose some constraints but in general you’ll have much more freedom than you might find elsewhere. Ownership. The research you produce will be yours as an individual. Your accomplishments will have your name attached to them. In contrast, it is much more common to “blend in” inside a larger company. A common feeling here is becoming a “cog in a wheel”. Exclusivity. There are very few people who make it to the top PhD programs. You’d be joining a group of a few hundred distinguished individuals in contrast to a few tens of thousands (?) that will join some company. Status. Regardless of whether it should be or not, working towards and eventually getting a PhD degree is culturally revered and recognized as an impressive achievement. You also get to be a Doctor; that’s awesome. Personal freedom. As a PhD student you’re your own boss. Want to sleep in today? Sure. Want to skip a day and go on a vacation? Sure. All that matters is your final output and no one will force you to clock in from 9am to 5pm. Of course, some advisers might be more or less flexible about it and some companies might be as well, but it’s a true first order statement. Maximizing future choice. Joining a PhD program doesn’t close any doors or eliminate future employment/lifestyle options. You can go one way (PhD -&gt; anywhere else) but not the other (anywhere else -&gt; PhD -&gt; academia/research; it is statistically less likely). Additionally (although this might be quite specific to applied ML), you’re strictly more hirable as a PhD graduate or even as a PhD dropout and many companies might be willing to put you in a more interesting position or with a higher starting salary. More generally, maximizing choice for the future you is a good heuristic to follow. Maximizing variance. You’re young and there’s really no need to rush. Once you graduate from a PhD you can spend the next ~50 years of your life in some company. Opt for more variance in your experiences. Personal growth. PhD is an intense experience of rapid growth (you learn a lot) and personal self-discovery (you’ll become a master of managing your own psychology). PhD programs (especially if you can make it into a good one) also offer a high density of exceptionally bright people who will become your best friends forever. Expertise. PhD is probably your only opportunity in life to really drill deep into a topic and become a recognized leading expert in the world at something. You’re exploring the edge of our knowledge as a species, without the burden of lesser distractions or constraints. There’s something beautiful about that and if you disagree, it could be a sign that PhD is not for you. The disclaimer. I wanted to also add a few words on some of the potential downsides and failure modes. The PhD is a very specific kind of experience that deserves a large disclaimer. You will inevitably find yourself working very hard (especially before paper deadlines). You need to be okay with the suffering and have enough mental stamina and determination to deal with the pressure. At some points you will lose track of what day of the week it is and go on a diet of leftover food from the microkitchens. You’ll sit exhausted and alone in the lab on a beautiful, sunny Saturday scrolling through Facebook pictures of your friends having fun on exotic trips, paid for by their 5-10x larger salaries. You will have to throw away 3 months of your work while somehow keeping your mental health intact. You’ll struggle with the realization that months of your work were spent on a paper with a few citations while your friends do exciting startups with TechCrunch articles or push products to millions of people. You’ll experience identity crises during which you’ll question your life decisions and wonder what you’re doing with some of the best years of your life. As a result, you should be quite certain that you can thrive in an unstructured environment in the pursuit research and discovery for science. If you’re unsure you should lean slightly negative by default. Ideally you should consider getting a taste of research as an undergraduate on a summer research program before before you decide to commit. In fact, one of the primary reasons that research experience is so desirable during the PhD hiring process is not the research itself, but the fact that the student is more likely to know what they’re getting themselves into. I should clarify explicitly that this post is not about convincing anyone to do a PhD, I’ve merely tried to enumerate some of the common considerations above. The majority of this post focuses on some tips/tricks for navigating the experience once if you decide to go for it (which we’ll see shortly, below). Lastly, as a random thought I heard it said that you should only do a PhD if you want to go into academia. In light of all of the above I’d argue that a PhD has strong intrinsic value - it’s an end by itself, not just a means to some end (e.g. academic job). Getting into a PhD program: references, references, references. Great, you’ve decided to go for it. Now how do you get into a good PhD program? The first order approximation is quite simple - by far most important component are strong reference letters. The ideal scenario is that a well-known professor writes you a letter along the lines of: “Blah is in top 5 of students I’ve ever worked with. She takes initiative, comes up with her own ideas, and gets them to work.” The worst letter is along the lines of: “Blah took my class. She did well.” A research publication under your belt from a summer research program is a very strong bonus, but not absolutely required provided you have strong letters. In particular note: grades are quite irrelevant but you generally don’t want them to be too low. This was not obvious to me as an undergrad and I spent a lot of energy on getting good grades. This time should have instead been directed towards research (or at the very least personal projects), as much and as early as possible, and if possible under supervision of multiple people (you’ll need 3+ letters!). As a last point, what won’t help you too much is pestering your potential advisers out of the blue. They are often incredibly busy people and if you try to approach them too aggressively in an effort to impress them somehow in conferences or over email this may agitate them. Picking the school. Once you get into some PhD programs, how do you pick the school? It’s easy, join Stanford! Just kidding. More seriously, your dream school should 1) be a top school (not because it looks good on your resume/CV but because of feedback loops; top schools attract other top people, many of whom you will get to know and work with) 2) have a few potential advisers you would want to work with. I really do mean the “few” part - this is very important and provides a safety cushion for you if things don’t work out with your top choice for any one of hundreds of reasons - things in many cases outside of your control, e.g. your dream professor leaves, moves, or spontaneously disappears, and 3) be in a good environment physically. I don’t think new admits appreciate this enough: you will spend 5+ years of your really good years living near the school campus. Trust me, this is a long time and your life will consist of much more than just research. Adviser Student adviser relationship. The adviser is an extremely important person who will exercise a lot of influence over your PhD experience. It’s important to understand the nature of the relationship: the adviser-student relationship is a symbiosis; you have your own goals and want something out of your PhD, but they also have their own goals, constraints and they’re building their own career. Therefore, it is very helpful to understand your adviser’s incentive structures: how the tenure process works, how they are evaluated, how they get funding, how they fund you, what department politics they might be embedded in, how they win awards, how academia in general works and specifically how they gain recognition and respect of their colleagues. This alone will help you avoid or mitigate a large fraction of student-adviser friction points and allow you to plan appropriately. I also don’t want to make the relationship sound too much like a business transaction. The advisor-student relationship, more often that not, ends up developing into a lasting one, predicated on much more than just career advancement. Pre-vs-post tenure. Every adviser is different so it’s helpful to understand the axes of variations and their repercussions on your PhD experience. As one rule of thumb (and keep in mind there are many exceptions), it’s important to keep track of whether a potential adviser is pre-tenure or post-tenure. The younger faculty members will usually be around more (they are working hard to get tenure) and will usually be more low-level, have stronger opinions on what you should be working on, they’ll do math with you, pitch concrete ideas, or even look at (or contribute to) your code. This is a much more hands-on and possibly intense experience because the adviser will need a strong publication record to get tenure and they are incentivised to push you to work just as hard. In contrast, more senior faculty members may have larger labs and tend to have many other commitments (e.g. committees, talks, travel) other than research, which means that they can only afford to stay on a higher level of abstraction both in the area of their research and in the level of supervision for their students. To caricature, it’s a difference between “you’re missing a second term in that equation” and “you may want to read up more in this area, talk to this or that person, and sell your work this or that way”. In the latter case, the low-level advice can still come from the senior PhD students in the lab or the postdocs. Axes of variation. There are many other axes to be aware of. Some advisers are fluffy and some prefer to keep your relationship very professional. Some will try to exercise a lot of influence on the details of your work and some are much more hands off. Some will have a focus on specific models and their applications to various tasks while some will focus on tasks and more indifference towards any particular modeling approach. In terms of more managerial properties, some will meet you every week (or day!) multiple times and some you won’t see for months. Some advisers answer emails right away and some don’t answer email for a week (or ever, haha). Some advisers make demands about your work schedule (e.g. you better work long hours or weekends) and some won’t. Some advisers generously support their students with equipment and some think laptops or old computers are mostly fine. Some advisers will fund you to go to a conferences even if you don’t have a paper there and some won’t. Some advisers are entrepreneurial or applied and some lean more towards theoretical work. Some will let you do summer internships and some will consider internships just a distraction. Finding an adviser. So how do you pick an adviser? The first stop, of course, is to talk to them in person. The student-adviser relationship is sometimes referred to as a marriage and you should make sure that there is a good fit. Of course, first you want to make sure that you can talk with them and that you get along personally, but it’s also important to get an idea of what area of “professor space” they occupy with respect to the aforementioned axes, and especially whether there is an intellectual resonance between the two of you in terms of the problems you are interested in. This can be just as important as their management style. Collecting references. You should also collect references on your potential adviser. One good strategy is to talk to their students. If you want to get actual information this shouldn’t be done in a very formal way or setting but in a relaxed environment or mood (e.g. a party). In many cases the students might still avoid saying bad things about the adviser if asked in a general manner, but they will usually answer truthfully when you ask specific questions, e.g. “how often do you meet?”, or “how hands on are they?”. Another strategy is to look at where their previous students ended up (you can usually find this on the website under an alumni section), which of course also statistically informs your own eventual outcome. Impressing an adviser. The adviser-student matching process is sometimes compared to a marriage - you pick them but they also pick you. The ideal student from their perspective is someone with interest and passion, someone who doesn’t need too much hand-holding, and someone who takes initiative - who shows up a week later having done not just what the adviser suggested, but who went beyond it; improved on it in unexpected ways. Consider the entire lab. Another important point to realize is that you’ll be seeing your adviser maybe once a week but you’ll be seeing most of their students every single day in the lab and they will go on to become your closest friends. In most cases you will also end up collaborating with some of the senior PhD students or postdocs and they will play a role very similar to that of your adviser. The postdocs, in particular, are professors-in-training and they will likely be eager to work with you as they are trying to gain advising experience they can point to for their academic job search. Therefore, you want to make sure the entire group has people you can get along with, people you respect and who you can work with closely on research projects. Research topics t-SNE visualization of a small subset of human knowledge (from paperscape). Each circle is an arxiv paper and size indicates the number of citations. So you’ve entered a PhD program and found an adviser. Now what do you work on? An exercise in the outer loop. First note the nature of the experience. A PhD is simultaneously a fun and frustrating experience because you’re constantly operating on a meta problem level. You’re not just solving problems - that’s merely the simple inner loop. You spend most of your time on the outer loop, figuring out what problems are worth solving and what problems are ripe for solving. You’re constantly imagining yourself solving hypothetical problems and asking yourself where that puts you, what it could unlock, or if anyone cares. If you’re like me this can sometimes drive you a little crazy because you’re spending long hours working on things and you’re not even sure if they are the correct things to work on or if a solution exists. Developing taste. When it comes to choosing problems you’ll hear academics talk about a mystical sense of “taste”. It’s a real thing. When you pitch a potential problem to your adviser you’ll either see their face contort, their eyes rolling, and their attention drift, or you’ll sense the excitement in their eyes as they contemplate the uncharted territory ripe for exploration. In that split second a lot happens: an evaluation of the problem’s importance, difficulty, its sexiness, its historical context (and possibly also its fit to their active grants). In other words, your adviser is likely to be a master of the outer loop and will have a highly developed sense of taste for problems. During your PhD you’ll get to acquire this sense yourself. In particular, I think I had a terrible taste coming in to the PhD. I can see this from the notes I took in my early PhD years. A lot of the problems I was excited about at the time were in retrospect poorly conceived, intractable, or irrelevant. I’d like to think I refined the sense by the end through practice and apprenticeship. Let me now try to serialize a few thoughts on what goes into this sense of taste, and what makes a problem interesting to work on. A fertile ground. First, recognize that during your PhD you will dive deeply into one area and your papers will very likely chain on top of each other to create a body of work (which becomes your thesis). Therefore, you should always be thinking several steps ahead when choosing a problem. It’s impossible to predict how things will unfold but you can often get a sense of how much room there could be for additional work. Plays to your adviser’s interests and strengths. You will want to operate in the realm of your adviser’s interest. Some advisers may allow you to work on slightly tangential areas but you would not be taking full advantage of their knowledge and you are making them less likely to want to help you with your project or promote your work. For instance, (and this goes to my previous point of understanding your adviser’s job) every adviser has a “default talk” slide deck on their research that they give all the time and if your work can add new exciting cutting edge work slides to this deck then you’ll find them much more invested, helpful and involved in your research. Additionally, their talks will promote and publicize your work. Be ambitious: the sublinear scaling of hardness. People have a strange bug built into psychology: a 10x more important or impactful problem intuitively feels 10x harder (or 10x less likely) to achieve. This is a fallacy - in my experience a 10x more important problem is at most 2-3x harder to achieve. In fact, in some cases a 10x harder problem may be easier to achieve. How is this? It’s because thinking 10x forces you out of the box, to confront the real limitations of an approach, to think from first principles, to change the strategy completely, to innovate. If you aspire to improve something by 10% and work hard then you will. But if you aspire to improve it by 100% you are still quite likely to, but you will do it very differently. Ambitious but with an attack. At this point it’s also important to point out that there are plenty of important problems that don’t make great projects. I recommend reading You and Your Research by Richard Hamming, where this point is expanded on: If you do not work on an important problem, it’s unlikely you’ll do important work. It’s perfectly obvious. Great scientists have thought through, in a careful way, a number of important problems in their field, and they keep an eye on wondering how to attack them. Let me warn you, `important problem’ must be phrased carefully. The three outstanding problems in physics, in a certain sense, were never worked on while I was at Bell Labs. By important I mean guaranteed a Nobel Prize and any sum of money you want to mention. We didn’t work on (1) time travel, (2) teleportation, and (3) antigravity. They are not important problems because we do not have an attack. It’s not the consequence that makes a problem important, it is that you have a reasonable attack. That is what makes a problem important. The person who did X. Ultimately, the goal of a PhD is to not only develop a deep expertise in a field but to also make your mark upon it. To steer it, shape it. The ideal scenario is that by the end of the PhD you own some part of an important area, preferably one that is also easy and fast to describe. You want people to say things like “she’s the person who did X”. If you can fill in a blank there you’ll be successful. Valuable skills. Recognize that during your PhD you will become an expert at the area of your choosing (as fun aside, note that [5 years]x[260 working days]x[8 hours per day] is 10,400 hours; if you believe Gladwell then a PhD is exactly the amount of time to become an expert). So imagine yourself 5 years later being a world expert in this area (the 10,000 hours will ensure that regardless of the academic impact of your work). Are these skills exciting or potentially valuable to your future endeavors? Negative examples. There are also some problems or types of papers that you ideally want to avoid. For instance, you’ll sometimes hear academics talk about “incremental work” (this is the worst adjective possible in academia). Incremental work is a paper that enhances something existing by making it more complex and gets 2% extra on some benchmark. The amusing thing about these papers is that they have a reasonably high chance of getting accepted (a reviewer can’t point to anything to kill them; they are also sometimes referred to as “cockroach papers”), so if you have a string of these papers accepted you can feel as though you’re being very productive, but in fact these papers won’t go on to be highly cited and you won’t go on to have a lot of impact on the field. Similarly, finding projects should ideally not include thoughts along the lines of “there’s this next logical step in the air that no one has done yet, let me do it”, or “this should be an easy poster”. Case study: my thesis. To make some of this discussion more concrete I wanted to use the example of how my own PhD unfolded. First, fun fact: my entire thesis is based on work I did in the last 1.5 years of my PhD. i.e. it took me quite a long time to wiggle around in the metaproblem space and find a problem that I felt very excited to work on (the other ~2 years I mostly meandered on 3D things (e.g. Kinect Fusion, 3D meshes, point cloud features) and video things). Then at one point in my 3rd year I randomly stopped by Richard Socher’s office on some Saturday at 2am. We had a chat about interesting problems and I realized that some of his work on images and language was in fact getting at something very interesting (of course, the area at the intersection of images and language goes back quite a lot further than Richard as well). I couldn’t quite see all the papers that would follow but it seemed heuristically very promising: it was highly fertile (a lot of unsolved problems, a lot of interesting possibilities on grounding descriptions to images), I felt that it was very cool and important, it was easy to explain, it seemed to be at the boundary of possible (Deep Learning has just started to work), the datasets had just started to become available (Flickr8K had just come out), it fit nicely into Fei-Fei’s interests and even if I were not successful I’d at least get lots of practice with optimizing interesting deep nets that I could reapply elsewhere. I had a strong feeling of a tsunami of checkmarks as everything clicked in place in my mind. I pitched this to Fei-Fei (my adviser) as an area to dive into the next day and, with relief, she enthusiastically approved, encouraged me, and would later go on to steer me within the space (e.g. Fei-Fei insisted that I do image to sentence generation while I was mostly content with ranking.). I’m happy with how things evolved from there. In short, I meandered around for 2 years stuck around the outer loop, finding something to dive into. Once it clicked for me what that was based on several heuristics, I dug in. Resistance. I’d like to also mention that your adviser is by no means infallible. I’ve witnessed and heard of many instances in which, in retrospect, the adviser made the wrong call. If you feel this way during your phd you should have the courage to sometimes ignore your adviser. Academia generally celebrates independent thinking but the response of your specific adviser can vary depending on circumstances. I’m aware of multiple cases where the bet worked out very well and I’ve also personally experienced cases where it did not. For instance, I disagreed strongly with some advice Andrew Ng gave me in my very first year. I ended up working on a problem he wasn’t very excited about and, surprise, he turned out to be very right and I wasted a few months. Win some lose some :) Don’t play the game. Finally, I’d like to challenge you to think of a PhD as more than just a sequence of papers. You’re not a paper writer. You’re a member of a research community and your goal is to push the field forward. Papers are one common way of doing that but I would encourage you to look beyond the established academic game. Think for yourself and from first principles. Do things others don’t do but should. Step off the treadmill that has been put before you. I tried to do some of this myself throughout my PhD. This blog is an example - it allows me communicate things that wouldn’t ordinarily go into papers. The ImageNet human reference experiments are an example - I felt strongly that it was important for the field to know the ballpark human accuracy on ILSVRC so I took a few weeks off and evaluated it. The academic search tools (e.g. arxiv-sanity) are an example - I felt continuously frustrated by the inefficiency of finding papers in the literature and I released and maintain the site in hopes that it can be useful to others. Teaching CS231n twice is an example - I put much more effort into it than is rationally advisable for a PhD student who should be doing research, but I felt that the field was held back if people couldn’t efficiently learn about the topic and enter. A lot of my PhD endeavors have likely come at a cost in standard academic metrics (e.g. h-index, or number of publications in top venues) but I did them anyway, I would do it the same way again, and here I am encouraging others to as well. To add a pitch of salt and wash down the ideology a bit, based on several past discussions with my friends and colleagues I know that this view is contentious and that many would disagree. Writing papers Writing good papers is an essential survival skill of an academic (kind of like making fire for a caveman). In particular, it is very important to realize that papers are a specific thing: they look a certain way, they flow a certain way, they have a certain structure, language, and statistics that the other academics expect. It’s usually a painful exercise for me to look through some of my early PhD paper drafts because they are quite terrible. There is a lot to learn here. Review papers. If you’re trying to learn to write better papers it can feel like a sensible strategy to look at many good papers and try to distill patterns. This turns out to not be the best strategy; it’s analogous to only receiving positive examples for a binary classification problem. What you really want is to also have exposure to a large number of bad papers and one way to get this is by reviewing papers. Most good conferences have an acceptance rate of about 25% so most papers you’ll review are bad, which will allow you to build a powerful binary classifier. You’ll read through a bad paper and realize how unclear it is, or how it doesn’t define it’s variables, how vague and abstract its intro is, or how it dives in to the details too quickly, and you’ll learn to avoid the same pitfalls in your own papers. Another related valuable experience is to attend (or form) journal clubs - you’ll see experienced researchers critique papers and get an impression for how your own papers will be analyzed by others. Get the gestalt right. I remember being impressed with Fei-Fei (my adviser) once during a reviewing session. I had a stack of 4 papers I had reviewed over the last several hours and she picked them up, flipped through each one for 10 seconds, and said one of them was good and the other three bad. Indeed, I was accepting the one and rejecting the other three, but something that took me several hours took her seconds. Fei-Fei was relying on the gestalt of the papers as a powerful heuristic. Your papers, as you become a more senior researcher take on a characteristic look. An introduction of ~1 page. A ~1 page related work section with a good density of citations - not too sparse but not too crowded. A well-designed pull figure (on page 1 or 2) and system figure (on page 3) that were not made in MS Paint. A technical section with some math symbols somewhere, results tables with lots of numbers and some of them bold, one additional cute analysis experiment, and the paper has exactly 8 pages (the page limit) and not a single line less. You’ll have to learn how to endow your papers with the same gestalt because many researchers rely on it as a cognitive shortcut when they judge your work. Identify the core contribution. Before you start writing anything it’s important to identify the single core contribution that your paper makes to the field. I would especially highlight the word single. A paper is not a random collection of some experiments you ran that you report on. The paper sells a single thing that was not obvious or present before. You have to argue that the thing is important, that it hasn’t been done before, and then you support its merit experimentally in controlled experiments. The entire paper is organized around this core contribution with surgical precision. In particular it doesn’t have any additional fluff and it doesn’t try to pack anything else on a side. As a concrete example, I made a mistake in one of my earlier papers on video classification where I tried to pack in two contributions: 1) a set of architectural layouts for video convnets and an unrelated 2) multi-resolution architecture which gave small improvements. I added it because I reasoned first that maybe someone could find it interesting and follow up on it later and second because I thought that contributions in a paper are additive: two contributions are better than one. Unfortunately, this is false and very wrong. The second contribution was minor/dubious and it diluted the paper, it was distracting, and no one cared. I’ve made a similar mistake again in my CVPR 2014 paper which presented two separate models: a ranking model and a generation model. Several good in-retrospect arguments could be made that I should have submitted two separate papers; the reason it was one is more historical than rational. The structure. Once you’ve identified your core contribution there is a default recipe for writing a paper about it. The upper level structure is by default Intro, Related Work, Model, Experiments, Conclusions. When I write my intro I find that it helps to put down a coherent top-level narrative in latex comments and then fill in the text below. I like to organize each of my paragraphs around a single concrete point stated on the first sentence that is then supported in the rest of the paragraph. This structure makes it easy for a reader to skim the paper. A good flow of ideas is then along the lines of 1) X (+define X if not obvious) is an important problem 2) The core challenges are this and that. 2) Previous work on X has addressed these with Y, but the problems with this are Z. 3) In this work we do W (?). 4) This has the following appealing properties and our experiments show this and that. You can play with this structure a bit but these core points should be clearly made. Note again that the paper is surgically organized around your exact contribution. For example, when you list the challenges you want to list exactly the things that you address later; you don’t go meandering about unrelated things to what you have done (you can speculate a bit more later in conclusion). It is important to keep a sensible structure throughout your paper, not just in the intro. For example, when you explain the model each section should: 1) explain clearly what is being done in the section, 2) explain what the core challenges are 3) explain what a baseline approach is or what others have done before 4) motivate and explain what you do 5) describe it. Break the structure. You should also feel free (and you’re encouraged to!) play with these formulas to some extent and add some spice to your papers. For example, see this amusing paper from Razavian et al. in 2014 that structures the introduction as a dialog between a student and the professor. It’s clever and I like it. As another example, a lot of papers from Alyosha Efros have a playful tone and make great case studies in writing fun papers. As only one of many examples, see this paper he wrote with Antonio Torralba: Unbiased look at dataset bias. Another possibility I’ve seen work well is to include an FAQ section, possibly in the appendix. Common mistake: the laundry list. One very common mistake to avoid is the “laundry list”, which looks as follows: “Here is the problem. Okay now to solve this problem first we do X, then we do Y, then we do Z, and now we do W, and here is what we get”. You should try very hard to avoid this structure. Each point should be justified, motivated, explained. Why do you do X or Y? What are the alternatives? What have others done? It’s okay to say things like this is common (add citation if possible). Your paper is not a report, an enumeration of what you’ve done, or some kind of a translation of your chronological notes and experiments into latex. It is a highly processed and very focused discussion of a problem, your approach and its context. It is supposed to teach your colleagues something and you have to justify your steps, not just describe what you did. The language. Over time you’ll develop a vocabulary of good words and bad words to use when writing papers. Speaking about machine learning or computer vision papers specifically as concrete examples, in your papers you never “study” or “investigate” (there are boring, passive, bad words); instead you “develop” or even better you “propose”. And you don’t present a “system” or, shudder, a “pipeline”; instead, you develop a “model”. You don’t learn “features”, you learn “representations”. And god forbid, you never “combine”, “modify” or “expand”. These are incremental, gross terms that will certainly get your paper rejected :). An internal deadlines 2 weeks prior. Not many labs do this, but luckily Fei-Fei is quite adamant about an internal deadline 2 weeks before the due date in which you must submit at least a 5-page draft with all the final experiments (even if not with final numbers) that goes through an internal review process identical to the external one (with the same review forms filled out, etc). I found this practice to be extremely useful because forcing yourself to lay out the full paper almost always reveals some number of critical experiments you must run for the paper to flow and for its argument flow to be coherent, consistent and convincing. Another great resource on this topic is Tips for Writing Technical Papers from Jennifer Widom. Writing code A lot of your time will of course be taken up with the execution of your ideas, which likely involves a lot of coding. I won’t dwell on this too much because it’s not uniquely academic, but I would like to bring up a few points. Release your code. It’s a somewhat surprising fact but you can get away with publishing papers and not releasing your code. You will also feel a lot of incentive to not release your code: it can be a lot of work (research code can look like spaghetti since you iterate very quickly, you have to clean up a lot), it can be intimidating to think that others might judge you on your at most decent coding abilities, it is painful to maintain code and answer questions from other people about it (forever), and you might also be concerned that people could spot bugs that invalidate your results. However, it is precisely for some of these reasons that you should commit to releasing your code: it will force you to adopt better coding habits due to fear of public shaming (which will end up saving you time!), it will force you to learn better engineering practices, it will force you to be more thorough with your code (e.g. writing unit tests to make bugs much less likely), it will make others much more likely to follow up on your work (and hence lead to more citations of your papers) and of course it will be much more useful to everyone as a record of exactly what was done for posterity. When you do release your code I recommend taking advantage of docker containers; this will reduce the amount of headaches people email you about when they can’t get all the dependencies (and their precise versions) installed. Think of the future you. Make sure to document all your code very well for yourself. I guarantee you that you will come back to your code base a few months later (e.g. to do a few more experiments for the camera ready version of the paper), and you will feel completely lost in it. I got into the habit of creating very thorough readme.txt files in all my repos (for my personal use) as notes to future self on how the code works, how to run it, etc. Giving talks So, you published a paper and it’s an oral! Now you get to give a few minute talk to a large audience of people - what should it look like? The goal of a talk. First, that there’s a common misconception that the goal of your talk is to tell your audience about what you did in your paper. This is incorrect, and should only be a second or third degree design criterion. The goal of your talk is to 1) get the audience really excited about the problem you worked on (they must appreciate it or they will not care about your solution otherwise!) 2) teach the audience something (ideally while giving them a taste of your insight/solution; don’t be afraid to spend time on other’s related work), and 3) entertain (they will start checking their Facebook otherwise). Ideally, by the end of the talk the people in your audience are thinking some mixture of “wow, I’m working in the wrong area”, “I have to read this paper”, and “This person has an impressive understanding of the whole area”. A few do’s: There are several properties that make talks better. For instance, Do: Lots of pictures. People Love pictures. Videos and animations should be used more sparingly because they distract. Do: make the talk actionable - talk about something someone can do after your talk. Do: give a live demo if possible, it can make your talk more memorable. Do: develop a broader intellectual arch that your work is part of. Do: develop it into a story (people love stories). Do: cite, cite, cite - a lot! It takes very little slide space to pay credit to your colleagues. It pleases them and always reflects well on you because it shows that you’re humble about your own contribution, and aware that it builds on a lot of what has come before and what is happening in parallel. You can even cite related work published at the same conference and briefly advertise it. Do: practice the talk! First for yourself in isolation and later to your lab/friends. This almost always reveals very insightful flaws in your narrative and flow. Don’t: texttexttext. Don’t crowd your slides with text. There should be very few or no bullet points - speakers sometimes try to use these as a crutch to remind themselves what they should be talking about but the slides are not for you they are for the audience. These should be in your speaker notes. On the topic of crowding the slides, also avoid complex diagrams as much as you can - your audience has a fixed bit bandwidth and I guarantee that your own very familiar and “simple” diagram is not as simple or interpretable to someone seeing it for the first time. Careful with: result tables: Don’t include dense tables of results showing that your method works better. You got a paper, I’m sure your results were decent. I always find these parts boring and unnecessary unless the numbers show something interesting (other than your method works better), or of course unless there is a large gap that you’re very proud of. If you do include results or graphs build them up slowly with transitions, don’t post them all at once and spend 3 minutes on one slide. Pitfall: the thin band between bored/confused. It’s actually quite tricky to design talks where a good portion of your audience learns something. A common failure case (as an audience member) is to see talks where I’m painfully bored during the first half and completely confused during the second half, learning nothing by the end. This can occur in talks that have a very general (too general) overview followed by a technical (too technical) second portion. Try to identify when your talk is in danger of having this property. Pitfall: running out of time. Many speakers spend too much time on the early intro parts (that can often be somewhat boring) and then frantically speed through all the last few slides that contain the most interesting results, analysis or demos. Don’t be that person. Pitfall: formulaic talks. I might be a special case but I’m always a fan of non-formulaic talks that challenge conventions. For instance, I despise the outline slide. It makes the talk so boring, it’s like saying: “This movie is about a ring of power. In the first chapter we’ll see a hobbit come into possession of the ring. In the second we’ll see him travel to Mordor. In the third he’ll cast the ring into Mount Doom and destroy it. I will start with chapter 1” - Come on! I use outline slides for much longer talks to keep the audience anchored if they zone out (at 30min+ they inevitably will a few times), but it should be used sparingly. Observe and learn. Ultimately, the best way to become better at giving talks (as it is with writing papers too) is to make conscious effort to pay attention to what great (and not so great) speakers do and build a binary classifier in your mind. Don’t just enjoy talks; analyze them, break them down, learn from them. Additionally, pay close attention to the audience and their reactions. Sometimes a speaker will put up a complex table with many numbers and you will notice half of the audience immediately look down on their phone and open Facebook. Build an internal classifier of the events that cause this to happen and avoid them in your talks. Attending conferences On the subject of conferences: Go. It’s very important that you go to conferences, especially the 1-2 top conferences in your area. If your adviser lacks funds and does not want to pay for your travel expenses (e.g. if you don’t have a paper) then you should be willing to pay for yourself (usually about $2000 for travel, accommodation, registration and food). This is important because you want to become part of the academic community and get a chance to meet more people in the area and gossip about research topics. Science might have this image of a few brilliant lone wolfs working in isolation, but the truth is that research is predominantly a highly social endeavor - you stand on the shoulders of many people, you’re working on problems in parallel with other people, and it is these people that you’re also writing papers to. Additionally, it’s unfortunate but each field has knowledge that doesn’t get serialized into papers but is instead spread across a shared understanding of the community; things such as what are the next important topics to work on, what papers are most interesting, what is the inside scoop on papers, how they developed historically, what methods work (not just on paper, in reality), etcetc. It is very valuable (and fun!) to become part of the community and get direct access to the hivemind - to learn from it first, and to hopefully influence it later. Talks: choose by speaker. One conference trick I’ve developed is that if you’re choosing which talks to attend it can be better to look at the speakers instead of the topics. Some people give better talks than others (it’s a skill, and you’ll discover these people in time) and in my experience I find that it often pays off to see them speak even if it is on a topic that isn’t exactly connected to your area of research. The real action is in the hallways. The speed of innovation (especially in Machine Learning) now works at timescales much faster than conferences so most of the relevant papers you’ll see at the conference are in fact old news. Therefore, conferences are primarily a social event. Instead of attending a talk I encourage you to view the hallway as one of the main events that doesn’t appear on the schedule. It can also be valuable to stroll the poster session and discover some interesting papers and ideas that you may have missed. It is said that there are three stages to a PhD. In the first stage you look at a related paper’s reference section and you haven’t read most of the papers. In the second stage you recognize all the papers. In the third stage you’ve shared a beer with all the first authors of all the papers. Closing thoughts I can’t find the quote anymore but I heard Sam Altman of YC say that there are no shortcuts or cheats when it comes to building a startup. You can’t expect to win in the long run by somehow gaming the system or putting up false appearances. I think that the same applies in academia. Ultimately you’re trying to do good research and push the field forward and if you try to game any of the proxy metrics you won’t be successful in the long run. This is especially so because academia is in fact surprisingly small and highly interconnected, so anything shady you try to do to pad your academic resume (e.g. self-citing a lot, publishing the same idea multiple times with small remixes, resubmitting the same rejected paper over and over again with no changes, conveniently trying to leave out some baselines etc.) will eventually catch up with you and you will not be successful. So at the end of the day it’s quite simple. Do good work, communicate it properly, people will notice and good things will happen. Have a fun ride! </description>
      <pubDate>12 Jun 20 12:17 EDT</pubDate>
      <guid>https://karpathy.github.io/2016/09/07/phd/</guid>
    </item>
    <item>
      <title></title>
      <link>https://sandimetz.com/blog/2017/6/1/the-half-life-of-code</link>
      <description>&lt;a href=&#34;https://sandimetz.com/blog/2017/6/1/the-half-life-of-code&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;This post originally appeared in my Chainline Newsletter. I&#39;ve been thinking about the half-life of code. In his Software that Fits in Your Head talk, Dan North defines the half-life of software as (I&#39;m paraphrasing) &#34;the amount of time required for half of an application&#39;s code to change so much that it becomes unrecognizable.&#34; In that talk he tells the story of working on a high quality, non-trivial application whose code&#39;s half-life was six weeks. Yup. Six. Weeks. I saw that talk nearly a year ago and have been distracted by its implications ever since. The upsides of a short code half-life are significant. Imagine how much better your life would be if your application&#39;s code always reflected the most accurate, up-to-date understanding of the problem at hand. Think about how much costs would go down if you never had to navigate dead code. Consider the value of having of an application that is free of speculative additions that were thrown in to support features that will never arrive. I want these things, and am intrigued by the thought that taking half-life into consideration might help me achieve them. Code is read many more times than it is written. Writing code costs something, but over time the cost of reading is often higher. Anyone who ever looks at a piece of code has to invest brain-power into figuring out what it does. Dead code and speculative cruft make it hard to decipher the intention of code. It follows that you can reduce overall costs by optimizing code for reading rather than writing. The easiest code for subsequent readers to understand would be frugal, i.e. it would be simple, correct, and without embellishment. I suspect, however, that I&#39;m preaching to the choir. I suspect that you already believe everything stated in the prior paragraph. It&#39;s not that you disagree that it would be a good thing to have frugal code, it&#39;s that the applications that you work on are so far removed from this ideal that you despair of ever reaching it. In my experience, most applications are a mess. Successful business rely on long-lived applications that endure a constant barrage of new requirements. Changes are commonly made under urgent time pressure, which drives applications towards disorder. As entropy increases, it becomes harder and harder to add features except by way of one more hack. The accumulated mess leads to hacks, hacks lead to more hacks, and then you&#39;re in a loop. Velocity gradually slows, and everyone comes to hate the application, their job, and their life. If it makes you feel any better, there&#39;s a way in which having a big mess is a sign of success. The reason your competitors don&#39;t have messes is that they went out of business. You won, and your prize is an application that betrays the ravages of time. And, as if that&#39;s not alarming enough, I fear that the coding culture that led to your current success may be dooming you to future failure. If your existing application impedes change, nothing good will come of doing more of what you&#39;ve done. If this is the state you&#39;re in, it&#39;s time to change how you write code. Here&#39;s where the concept of half-life matters. You can lower your costs by reducing the half-life of your least stable code. The parts of your applications that change the most also cost the most. These dynamic parts are often fundamental to your business. They are the result of lengthy, evolutionary development, and suffer from never ending churn. You depend on them the most, yet they are hard to understand, and only getting worse. Vast other swaths of your application are likely equally unpleasant, but relatively stable. This stability essentially makes them free. It&#39;s not ugly code that costs money--it&#39;s change. Ugly code just exacerbates costs. Time is in appallingly short supply. The good news, however, is that you&#39;re not obligated to fix things that aren&#39;t costing you money. The most efficient way to improve long-lived applications is to focus on the code that churns--this is where your efforts most pay off. For maximum effect, commit to crafting solutions that are both frugal and easily replaceable. The last item above is key. Code that isn&#39;t easy to replace doesn&#39;t get replaced, instead it gets expanded. Its conditionals get bigger. The number of class names it knows about grows larger. These sorts of expansions tightly couple the code you&#39;re changing to other parts of your application. This coupling makes it difficult to swap in alternative implementations, which it turn leads to a long half-life for the code. Unstable code that has a long half-life inevitably accumulates cruft. This complicates the code, and programmers hesitate to neaten what they don&#39;t understand. The trick to maintaining frugality over the course of many changes is to insist on code that&#39;s easily replaceable. Achieving replaceable code necessitates developing a culture that values polymorphic objects and loosely-coupled code. You have a bargain with other programmers about how you will write code. Your current application is this bargain made manifest. If you&#39;re finding that the original pact has outlived its usefulness, the first step to improving your life is to start talking to one another about how you wish you were writing code. Dan&#39;s talk, and the idea of the half-life of code, can spur this discussion. Start today. :-) Thanks for reading, Sandi News: 99 Bottles of OOP in JS, PHP, and Ruby! The 2nd Edition of 99 Bottles of OOP has been released! The 2nd Edition contains 3 new chapters and is about 50% longer than the 1st. Also, because 99 Bottles of OOP is about object-oriented design in general rather than any specific language, this time around we created separate books that are technically identical, but use different programming languages for the examples. 99 Bottles of OOP is currently available in Ruby, JavaScript, and PHP versions, and beer and milk beverages. It&#39;s delivered in epub, kepub, mobi and pdf formats. This results in six different books and (3x2x4) 24 possible downloads; all unique, yet still the same. One purchase gives you rights to download any or all. </description>
      <pubDate>28 Feb 21 15:56 EST</pubDate>
      <guid>https://sandimetz.com/blog/2017/6/1/the-half-life-of-code</guid>
    </item>
    <item>
      <title></title>
      <link>https://ferd.ca/clever-functional-design.html</link>
      <description>&lt;a href=&#34;https://ferd.ca/clever-functional-design.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 2019/08/24 Clever Functional Design One of the best bits of software design I recall participating in was something done a few years ago at Heroku. It&#39;s been long enough since then that I feel comfortable boasting about it here. It&#39;s one small piece of data-centred functional design, where thinking a bit harder about the problem at hand greatly simplified what would have been a more straightforward and obvious implementation choice in a mutable language. The Feature Heroku&#39;s routing stack had one very interesting feature it provided to all users: a router log. The router log contains fields such as the overall request time, the time it took to send the request, the time it took to send the body, heroku-specific error codes and so on. They&#39;re broader categories of interesting time span. At the same time, Heroku engineers had internal logs for all the requests for which users had public logs. These internal logs contained more detailed information that more often made sense to display during specific debugging activities. They included information such as time spent parsing headers from the client, time to first response packet (calculating what would essentially be the time after which the whole request was sent, but before which the back-end application would respond), POSIX statuses detected by the proxy on socket issues, and so on. During more intricate debugging, other values could be required from the logs, but adding them would need code changes. This information was maintained in what was essentially a monolithic proxy that contained both the business logic (what logs to create, which servers to route to, and so on) and the proxying logic (how to shuttle HTTP from point A to point B). At some point during my Heroku days, we rewrote the entire routing stack to clearly divide the business concerns (routing and features for Heroku apps) and the proxying logic. The idea was to clean up deeply intertwined code, clarify and properly specify the proxying behaviour, and allow to reason about and change what we offered to customers without having to know all the innards of HTTP proxying logic to do so. This divide was successful, and eventually allowed us to open source the proxying logic: since it was no longer business related and was commodity infrastructure, vegur became public. This division came with a few challenges though, and one of them was with the logs: how were we to take a Heroku feature, such as the router logs we had, with their own specific needs that could change according to business requirements, and bake them into a generic proxy library, where all the interesting measurements and samplings were to take place? The Straightforward Design The approach we took in the original router was to just take all the samples as we needed them, mostly as spans. You essentially just intersperse the logging and observational needs with the actual business end of the code, and do what you must. One straightforward way to do this is with timestamps and might look something like this: T1 = stamp(), do_something(), T2 = stamp(), report_duration(&#34;some label&#34;, T2-T1) You take a timestamp, do the thing, take a second timestamp, and report the difference as the duration of the operation. Eventually you get tired of doing this, and you might wrap them up in a helper that takes a closure (or wraps some object in OO design) and hides the reporting: with_timestamp(&#34;some label&#34;, fun() -&gt; do_something() end) Both approaches work fine. The latter offers slightly more encapsulation, and also prevents having overlapping timestamps where two measurements intersect. The logic is always set at the call-site, and things can be a bit tricky with error handling, but that&#39;s generally how you do it. The same kind of approach is still rather broadly used in distributed tracing with the addition of a context, which lets you define some lineage or nesting of operations: %% approach 1 Ctx = new_span(&#34;some label&#34;), T1 = stamp(), NewCtx = do_something(Ctx), T2 = stamp(), close_span(NewCtx) %% approach 2 with_span(&#34;some_label&#34;, fun(Ctx) -&gt; do_something(Ctx) end) Of course if you&#39;ve got mutability going on and some global scope available, you&#39;ll cheat a bit and hide the span context within the program&#39;s state differently. In any case, the old approach was based on these kinds of mechanism. When time came to split them up into their business and general parts, the tools used for logging needed to be decoupled as well. The general and straightforward approach to that is to do it through dependency injection. Our new approaches might now look something like this: f(Logger) -&gt; T1 = stamp(), do_something(Logger), T2 = stamp(), Logger(&#34;some label&#34;, T2-T1) Kind of similarly to passing the context or a span in the distributed tracing approach, you now parametrize each contract of dependent functions to take some contextual cues that explain how to do things. It would have become a bit cumbersome to do it through all involved components of a proxying library, but it would have been possible to do it, and even more easily with tool or IDE support. This, however, was the road we decided not to take. The Weakness of the Straightforward Approach The problem with the dependency injection approach, aside from its cumbersomeness, is that it did not sufficiently decouple what was a business concern from what was a generic library. Sure, we would hide and abstract away the tools chosen—which logging or tracing library would be used—but in no way would it really de-couple the design. It would be tricky, for example, to properly track the concerns of &#34;public user logs&#34; and &#34;internal engineer logs&#34;. The biggest design issue was something we uncovered by simply asking ourselves this question: if some other project were to use this library, would their reporting need to change every time Heroku decided to log new metrics? Sure, the implementation could be independent. But the straightforward design only de-coupled the technical dependencies and which code was used. It did not get rid of the logical coupling that still existed between Heroku&#39;s business logic and the proxy&#39;s need to just shuttle HTTP. If we went with that approach, there was still a very deep dependency between both code bases. Heroku would unsurprisingly rely on the proxy, but it felt weird that the proxy&#39;s instrumentation would have to be defined by the Heroku product requirements. Another slightly less important issue came from implementation details. I mention it not because it was a huge blocker to logical decoupling, but because this implementation detail ended up providing the key to the nicer design. The Vegur proxy had been written to use passive TCP sockets used in a blocking mode, because those were faster back in the day (implementation changes and optimizations within the BEAM VM have since then made this unnecessary). This, and other earlier design choices, made it so the proxy itself had 3 major moving parts: an HTTP server parsing module, which would listen to incoming requests from the public Internet and parse them an HTTP client parsing module, which would forward the traffic to a customer&#39;s back-end and listen to the responses an inner loop that would use both the client and server bits and would handle the data transfer across both of them as a bridge. This meant that some concerns we had in terms of metrics would sometimes reside all within one bit of code (i.e. the time it takes to parse headers is self-contained to any of the components), but sometimes it would cross boundaries. For example, knowing the time it took to parse the request body required taking measurements in the HTTP server, but which could be intertwined with operations taking place both in the inner loop and the HTTP client. It had no clear structural hierarchy. Worse, some debugging scenarios required taking some measurements that started in the HTTP server, and finished in the HTTP client. That made it particularly difficult to localize and isolate concerns well, and the overall requirements of Heroku&#39;s reporting risked having a huge impact on the structure of the proxy. Dependency injection would not be enough to fix this, we needed to think about the problem differently. A Functional Design Even in today&#39;s modern distributed tracing ecosystem, the design of most tracing libraries is deeply centered on the concept of a span: a pre-defined point A to point B period of time, which is written and instrumented as such in the code. The big Eureka! moment for our approach in Vegur was in realizing that what when debugging, what we care about is picking arbitrary points on a timeline. Spans let you represent things a bit like this: |------------------- Request --------------------| |--- Header Parsing ---|-- Body parsing --| ... |-- Cookie --| | ... | Those are contiguous subdivisions of the whole timeline. What we wanted, instead, was a flat timeline on which we could pick arbitrary intervals: request start end of request | | | start header parsing | | | first packet sent ... | | | | | | |-x--x--x------x---x----x------------------x-----x---...| | | | | | | | ... start body parsing | | end cookie end body parsing start cookie All these things happen in a continuum. The divisions of what we wanted to report were not structural to this timeline, they were views or selections of various points of interests, and a measurement between them. The &#34;first packet sent&#34; event is something that could be useful to multiple metrics: time between the first packet received and the first packet sent (&#34;how long we took to process the headers&#34; time between header parsing being done and sending the first packet (&#34;how long we took to make a routing decision&#34;) time between the first packet sent and the last header packet sent (&#34;time to send the request headers&#34;) time between the first packet sent and the last request packet sent (&#34;time to send the full request&#34;) and so on. Being able to report on all of these was context-dependent for the consumer, meaning that&#39;s usually a business concern. But the proxying library itself only cared about specific arbitrary points we thought could be useful to its technical users. That distinction and approach as a timeline was the pivotal point we needed in the design. What the proxying library needed to do was not provide all the metrics and spans Heroku expected. What it needed to provide was the list of all important points on a timeline, whether they represented a singular event, or a duration. It would then be up to the consumer to report things however they wanted, whenever they wanted. The flat timeline was particularly interesting for this because it is easily representable as an immutable data structure. If all you have is a bunch of local monotonic timestamps, all you need to do is maintain a local data structure that maintains sequences of labelled points in time: [{Label1, T1}, {Label2, T2}, ...] Since timestamps are generally sortable locally—you need some fancy cheats to make it work in a distributed setting—then all the local timelines between the HTTP client, server, and inner loop modules could be maintained independently, but merged reliably: just sort by timestamp. |-x----------------x---------------------------------...| |--x--x------x--------x------------------| |—---------------------------x---...| | | | v v v |-x--x--x------x---x----x------------------x----x----...| This would let us write one generic well-defined data structure, use it wherever and whenever we needed it, and just merge them near the end of each timeline. No need to coordinate context-passing around, just a fetch and a merge once per item. Then, the business end of code in Heroku&#39;s router could ask for that timeline once the request was ready to be logged, get one well-known data structure, and do as many selections for as many debugging reports as it required. If you wanted to send 15 logs out of there, it did not matter to the proxy library. Just analyze the timeline, generate what you need, and that&#39;s it. Interestingly, since the final data structure could be represented easily in the base types of the language, Heroku&#39;s router was able to create its own compatible timeline that itself could be cast and merged with the proxy&#39;s timeline, without having them actually share the implementation (which would also have been fine). This would later let us augment the proxying logs with all the routing and business decisions for all kinds of debugging purposes (how much time would we spend queued to find an available back-end to route to?). This turned into app-specific routing flags that could allow to do deeper introspection of routing logic for specific applications, at nearly no overhead in code. Lesson Learned The approach itself here is mildly interesting. It has some intriguing implications in the context of designing implementations of distributed tracing libraries. The implementation is so straightforward in Vegur that I didn&#39;t spend the time to describe it here. The true bigger lesson here is in systems design. It relates to functional, immutable, and declarative approaches to structuring communication flows. The straightforward answer to our decoupling issue was to respond to the technical concern: just make it so the dependency does not know about the libraries used by its parent. I think it would have strictly speaking solved the most blocking problem in getting the code to build. This would have been easy to do, and the only thing that made it annoying was the fact we were using a functional programming language with no mutable data structures nor global shared context. But satisfying the compiler is not enough to make for good design. This approach would have made it hard to get maintainable code given implementation details, and did not remove any logical coupling. Rather than dismissing this challenge as &#34;a bad fit for functional programming&#34;, it is what led to a better solution: re-think the data structure, gain better insights in the distinction between how the data is produced and how the data is consumed. Take that separation, and make it explicit. Build around it. Turn it into a data contract. This, in turn, lets you more transparently change either ends. You might need to add new measurement points in the producer-side when the consumer needs it, but the properly declared abstraction makes it so the other consumers will not be effected by the change. The end result was a purely functional data structure that was mergeable, testable, and in line with functional design, but that&#39;s just a technical aspect of the result: it was the structural constraint of already being in an immutable context that prompted the cleaner design. Most challenges of working in a functional, declarative, or immutable language are not necessarily due to the language itself. They come from being thrown in a context where easier shortcuts are not as practical as we are used for them to be, and having to re-think both our problem and our solutions. </description>
      <pubDate>24 Mar 20 12:22 EDT</pubDate>
      <guid>https://ferd.ca/clever-functional-design.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://gravitational.com/blog/how-to-ssh-properly/</link>
      <description>&lt;a href=&#34;https://gravitational.com/blog/how-to-ssh-properly/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; How to SSH Properly This blog post has been updated as of 01/25/2021. SSH Best Practices There’s no denying that SSH is the de facto tool for *nix server administration. It’s far from perfect, but it was designed with security in mind and there’s been a huge amount of tooling written over the years to make it easier to use. In addition, many popular products and just about every server deployment system integrates with SSH somehow. It is universally supported across pretty much all architectures and distributions, from Raspberry Pis all the way up to massive supercomputer clusters. SSH is a powerful tool which often grants a lot of access to anyone using it to log into a server. In this post, I’m going to talk about a few different ways that you can easily improve the security of your SSH model without needing to deploy a new application or make any huge changes to user experience. In essence, this blog post is a collection of industry best practices to SSH security, and it’s written with OpenSSH users in mind. SSH certificates Most people can agree that using public key authentication for SSH is generally better than using passwords. Nobody ever types in a private key, so it can’t be keylogged or observed over your shoulder. SSH keys have their own issues, however. The next level up from SSH keys is SSH certificates. OpenSSH has supported the use of certificates since OpenSSH 5.4 which was released back in 2010. With SSH certificates, you generate a certificate authority (CA) and then use this to issue and cryptographically sign certificates which can authenticate users to hosts, or hosts to users. You can generate a keypair using the ssh-keygen command, like this: $ ssh-keygen -t rsa -b 4096 -f host_ca -C host_ca Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in host_ca. Your public key has been saved in host_ca.pub. The key fingerprint is: SHA256:tltbnMalWg+skhm+VlGLd2xHiVPozyuOPl34WypdEO0 host_ca The key&#39;s randomart image is: +---[RSA 4096]----+ | +o.| | .+..o| | o.o.+ | | o o.= E| | S o o=o | | ....+ = +.| | ..=. %.o.o| | *o Oo=.+.| | .oo=ooo+..| +----[SHA256]-----+ $ ls -l total 8 -rw-------. 1 gus gus 3381 Mar 19 14:30 host_ca -rw-r--r--. 1 gus gus 737 Mar 19 14:30 host_ca.pub The host_ca file is the host CA’s private key and should be protected. Don’t give it out to anyone, don’t copy it anywhere, and make sure that as few people have access to it as possible. Ideally, it should live on a machine which doesn’t allow direct access and all certificates should be issued by an automated process. In addition, it’s best practice to generate and use two separate CAs - one for signing host certificates, one for signing user certificates. This is because you don’t want the same processes that add hosts to your fleet to also be able to add users (and vice versa). Using separate CAs also means that in the event of a private key being compromised, you only need to reissue the certificates for either your hosts or your users, not both at once. As such, we’ll also generate a user_ca with this command: $ ssh-keygen -t rsa -b 4096 -f user_ca -C user_ca The user_ca file is the user CA’s private key and should also be protected in the same way as the host CA’s private key. Issuing host certificates (to authenticate hosts to users) In this example, we’ll generate a new host key (with no passphrase), then sign it with our CA. You can also sign the existing SSH host public key if you’d prefer not to regenerate a new key by copying the file (/etc/ssh/ssh_host_rsa_key.pub) from the server, signing it on your CA machine, and then copying it back. $ ssh-keygen -f ssh_host_rsa_key -N &#39;&#39; -b 4096 -t rsa $ ls -l -rw------- 1 ec2-user ec2-user 3247 Mar 17 14:49 ssh_host_rsa_key -rw-r--r-- 1 ec2-user ec2-user 764 Mar 17 14:49 ssh_host_rsa_key.pub $ ssh-keygen -s host_ca -I host.example.com -h -n host.example.com -V +52w ssh_host_rsa_key.pub Enter passphrase: # the passphrase used for the host CA Signed host key ssh_host_rsa_key-cert.pub: id &#34;host.example.com&#34; serial 0 for host.example.com valid from 2020-03-16T15:00:00 to 2021-03-15T15:01:37 $ ls -l -rw------- 1 ec2-user ec2-user 3247 Mar 17 14:49 ssh_host_rsa_key -rw-r--r-- 1 ec2-user ec2-user 2369 Mar 17 14:50 ssh_host_rsa_key-cert.pub -rw-r--r-- 1 ec2-user ec2-user 764 Mar 17 14:49 ssh_host_rsa_key.pub ssh_host_rsa_key-cert.pub contains the signed host certificate. Here’s an explanation of the flags used: -s host_ca: specifies the filename of the CA private key that should be used for signing. -I host.example.com: the certificate’s identity - an alphanumeric string that will identify the server. I recommend using the server’s hostname. This value can also be used to revoke a certificate in future if needed. -h: specifies that this certificate will be a host certificate rather than a user certificate. -n host.example.com: specifies a comma-separated list of principals that the certificate will be valid for authenticating - for host certificates, this is the hostname used to connect to the server. If you have DNS set up, you should use the server’s FQDN (for example host.example.com) here. If not, use the hostname that you will be using in an ~/.ssh/config file to connect to the server. -V +52w: specifies the validity period of the certificate, in this case 52 weeks (one year). Certificates are valid forever by default - expiry periods for host certificates are highly recommended to encourage the adoption of a process for rotating and replacing certificates when needed. Configuring SSH to use host certificates You also need to tell the server to use this new host certificate. Copy the three files you just generated to the server, store them under the /etc/ssh directory, set the permissions to match the other files there, then add this line to your/etc/ssh/sshd_config file: HostCertificate /etc/ssh/ssh_host_rsa_key-cert.pub Once this is done, restart sshd with systemctl restart sshd. Your server is now configured to present a certificate to anyone who connects. For your local ssh client to make use of this (and automatically trust the host based on the certificate’s identity), you will also need to add the CA’s public key to your known_hosts file. You can do this by taking the contents of the host_ca.pub file, adding @cert-authority *.example.com to the beginning, then appending the contents to ~/.ssh/known_hosts: @cert-authority *.example.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDwiOso0Q4W+KKQ4OrZZ1o1X7g3yWcmAJtySILZSwo1GXBKgurV4jmmBN5RsHetl98QiJq64e8oKX1vGR251afalWu0w/iW9jL0isZrPrmDg/p6Cb6yKnreFEaDFocDhoiIcbUiImIWcp9PJXFOK1Lu8afdeKWJA2f6cC4lnAEq4sA/Phg4xfKMQZUFG5sQ/Gj1StjIXi2RYCQBHFDzzNm0Q5uB4hUsAYNqbnaiTI/pRtuknsgl97xK9P+rQiNfBfPQhsGeyJzT6Tup/KKlxarjkMOlFX2MUMaAj/cDrBSzvSrfOwzkqyzYGHzQhST/lWQZr4OddRszGPO4W5bRQzddUG8iC7M6U4llUxrb/H5QOkVyvnx4Dw76MA97tiZItSGzRPblU4S6HMmCVpZTwva4LLmMEEIk1lW5HcbB6AWAc0dFE0KBuusgJp9MlFkt7mZkSqnim8wdQApal+E3p13d0QZSH3b6eB3cbBcbpNmYqnmBFrNSKkEpQ8OwBnFvjjdYB7AXqQqrcqHUqfwkX8B27chDn2dwyWb3AdPMg1+j3wtVrwVqO9caeeQ1310CNHIFhIRTqnp2ECFGCCy+EDSFNZM+JStQoNO5rMOvZmecbp35XH/UJ5IHOkh9wE5TBYIeFRUYoc2jHNAuP2FM4LbEagGtP8L5gSCTXNRM1EX2gQ== host_ca The value *.example.com is a pattern match, indicating that this certificate should be trusted for identifying any host which you connect to that has a domain of *.example.com - such as host.example.com above. This is a comma-separated list of applicable hostnames for the certificate, so if you’re using IP addresses or SSH config entries here, you can change this to something like host1,host2,host3 or 1.2.3.4,1.2.3.5 as appropriate. Once this is configured, remove any old host key entries for host.example.com in your ~/.ssh/known_hosts file, and start an ssh connection. You should be connected straight to the host without needing to trust the host key. You can check that the certificate is being presented correctly with a command like this: $ ssh -vv host.example.com 2&gt;&amp;1 | grep &#34;Server host certificate&#34; debug1: Server host certificate: [email protected] SHA256:dWi6L8k3Jvf7NAtyzd9LmFuEkygWR69tZC1NaZJ3iF4, serial 0 ID &#34;host.example.com&#34; CA ssh-rsa SHA256:8gVhYAAW9r2BWBwh7uXsx2yHSCjY5OPo/X3erqQi6jg valid from 2020-03-17T11:49:00 to 2021-03-16T11:50:21 debug2: Server host certificate hostname: host.example.com At this point, you could continue by issuing host certificates for all hosts in your estate using your host CA. The benefit of doing this is twofold: you no longer need to rely on the insecure trust on first use (TOFU) model for new hosts, and if you ever redeploy a server and therefore change the host key for a certain hostname, your new host could automatically present a signed host certificate and avoid the dreaded WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! message. Issuing user certificates (to authenticate users to hosts) In this example, we’ll generate a new user key and sign it with our user CA. It’s up to you whether you use a passphrase or not. $ ssh-keygen -f user-key -b 4096 -t rsa $ ls -l -rw-r--r--. 1 gus gus 737 Mar 19 16:33 user-key.pub -rw-------. 1 gus gus 3369 Mar 19 16:33 user-key $ ssh-keygen -s user_ca -I [email protected] -n ec2-user,gus -V +1d user-key.pub Enter passphrase: # the passphrase used for the user CA Signed user key user-key-cert.pub: id &#34;[email protected]&#34; serial 0 for ec2-user,gus valid from 2020-03-19T16:33:00 to 2020-03-20T16:34:54 $ ls -l -rw-------. 1 gus gus 3369 Mar 19 16:33 user-key -rw-r--r--. 1 gus gus 2534 Mar 19 16:34 user-key-cert.pub -rw-r--r--. 1 gus gus 737 Mar 19 16:33 user-key.pub user-key-cert.pub contains the signed user certificate. You’ll need both this and the private key (user-key) for logging in. Here’s an explanation of the flags used: -s user_ca: specifies the CA private key that should be used for signing -I [email protected]: the certificate’s identity, an alphanumeric string that will be visible in SSH logs when the user certificate is presented. I recommend using the email address or internal username of the user that the certificate is for - something which will allow you to uniquely identify a user. This value can also be used to revoke a certificate in future if needed. -n ec2-user,gus: specifies a comma-separated list of principals that the certificate will be valid for authenticating, i.e. the *nix users which this certificate should be allowed to log in as. In our example, we’re giving this certificate access to both ec2-user and gus. -V +1d: specifies the validity period of the certificate, in this case +1d means 1 day. Certificates are valid forever by default, so using an expiry period is a good way to limit access appropriately and ensure that certificates can’t be used for access perpetually. If you need to see the options that a given certificate was signed with, you can use ssh-keygen -L: $ ssh-keygen -L -f user-key-cert.pub user-key-cert.pub: Type: [email protected] user certificate Public key: RSA-CERT SHA256:egWNu5cUZaqwm76zoyTtktac2jxKktj30Oi/ydrOqZ8 Signing CA: RSA SHA256:tltbnMalWg+skhm+VlGLd2xHiVPozyuOPl34WypdEO0 (using ssh-rsa) Key ID: &#34;[email protected]&#34; Serial: 0 Valid: from 2020-03-19T16:33:00 to 2020-03-20T16:34:54 Principals: ec2-user gus Critical Options: (none) Extensions: permit-X11-forwarding permit-agent-forwarding permit-port-forwarding permit-pty permit-user-rc Configuring SSH for user certificate authentication Once you’ve signed a certificate, you also need to tell the server that it should trust certificates signed by the user CA. To do this, copy the user_ca.pub file to the server and store it under /etc/ssh, fix the permissions to match the other public key files in the directory, then add this line to /etc/ssh/sshd_config: TrustedUserCAKeys /etc/ssh/user_ca.pub Once this is done, restart sshd with systemctl restart sshd. Your server is now configured to trust anyone who presents a certificate issued by your user CA when they connect. If you have a certificate in the same directory as your private key (specified with the -i flag, for example ssh -i /home/gus/user-key [email protected]), it will automatically be used when connecting to servers. Checking logs If you look in your server’s sshd log (for example, by running journalctl -u sshd), you will see the name of the certificate being used for authentication, along with the fingerprint of the signing CA: sshd[14543]: Accepted publickey for ec2-user from 1.2.3.4 port 53734 ssh2: RSA-CERT ID [email protected] (serial 0) CA RSA SHA256:tltbnMalWg+skhm+VlGLd2xHiVPozyuOPl34WypdEO0 If the user tries to log in as a principal (user) which they do not have permission to use (for example, their certificate grants ec2-user but they try to use root), you’ll see this error in the logs: sshd[14612]: error: key_cert_check_authority: invalid certificate sshd[14612]: error: Certificate invalid: name is not a listed principal If the certificate being presented has expired, you’ll see this error in the logs: sshd[14240]: error: key_cert_check_authority: invalid certificate sshd[14240]: error: Certificate invalid: expired One way that you could make further use of user certificates is to set up a script which will use your CA to issue a certificate to log into production servers, valid only for the next two hours. Every use of this script or process could add logs as to who requested a certificate and embed their email address into the certificate. After this is done, every time the user uses that certificate to access a server (regardless of which principal they log in as), their email address will be logged. This can provide a useful part of an audit trail. There are many other useful things you can do with SSH certificates, such as forcing a specific command to be run when presenting a certain certificate, or denying the ability to forward ports, X11 traffic or SSH agents. Take a look at man ssh-keygen for some ideas. Enforce the use of a bastion host Another way to improve your SSH security is to enforce the use of a bastion host - a server which is specifically designed to be the only gateway for access to your infrastructure. Lessening the size of any potential attack surface through the use of firewalls enables you to keep a better eye on who is accessing what. Switching to the use of a bastion host doesn’t have to be an arduous task, especially if you’re using SSH certificates. By setting up your local ~/.ssh/config file, you can automatically configure all connections to hosts within a certain domain to go through the bastion. Here’s a very quick example of how to force SSH access to any host in the example.com domain to be routed through a bastion host, bastion.example.com: Host *.example.com ProxyJump bastion.example.com IdentityFile ~/user-key Host bastion.example.com ProxyJump none IdentityFile ~/user-key To make this even simpler, if you add user-key to your local ssh-agent with ssh-add user-key, you can also remove the IdentityFile entries from the SSH config file. Once you’re using the bastion host for your connections, you can use iptables (or another *nix firewall configuration tool of your choosing) on servers behind the bastion to block all other incoming SSH connections. Here’s a rough example using iptables: $ iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT $ iptables -A INPUT -p tcp --dport 22 -s &lt;public IP of the bastion&gt; -j ACCEPT $ iptables -A INPUT -p tcp --dport 22 -j DROP It’s a good idea to leave a second SSH session connected to the bastion while running these commands so that if you inadvertently input the wrong IP address or command, you should still have working access to the bastion to fix it via the already-established connection. Add 2-factor authentication to your SSH logins 2-factor authentication makes it more difficult for bad actors to log into your systems by enforcing the need for two different “factors” or methods to be able to successfully authenticate. This usually comes down to needing both “something you know” (like a password, or SSH certificate in our example) and “something you have” (like a token from an app installed on your phone, or an SMS with a unique code). One other possibility is requiring the use of “something you are” - for example a fingerprint, or your voice. In this example, we’ll install the google-authenticator pluggable authentication module, which will require users to input a code from the Google Authenticator app on their phone in order to log in successfully. You can download the app for iOS here and Android here. As a general note, it’s always important to consider the user experience when enforcing security measures. If your measures are too draconian then users may attempt to find ways to defeat and work around them, which will eventually reduce the overall security of your systems and lead to the creation of back doors. To give our users a reasonable experience in this example, we are only going to require 2-factor authentication to be able to log into our bastion host. Once authenticated there, users should be able to log into other hosts simply by using their valid SSH certificate. This combination should give an acceptable level of security without interfering too much with user workflows. With this in mind, however, it is always prudent and appropriate to enforce extra security measures in specific environments which contain critical production data or sensitive information. Install google-authenticator On RHEL/CentOS based systems, you can install the google-authenticator module from the EPEL repository: $ sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # for RHEL/CentOS 7, change for other versions $ sudo yum install google-authenticator For Debian/Ubuntu-based systems, this is available as the libpam-google-authenticator package: $ sudo apt-get install libpam-google-authenticator The google-authenticator module has many options you can set which are documented here. In the interest of saving time, we are going to use some sane defaults in this example: disallow reuse of the same token twice, issue time-based rather than counter-based codes, and limit the user to a maximum of three logins every 30 seconds. To set up Google 2-factor authentication with these settings, a user should run this command: $ google-authenticator -d -f -t -r 3 -R 30 -W You can also run google-authenticator with no flags and answer some prompts to set up interactively if you prefer. This will output a QR code that the user can scan with the app on their phone, plus some backup codes which they can use if they lose access to the app. These codes should be stored offline in a secure location. Scan the generated QR code for your user now with the Google Authenticator app and make sure that you have a 6-digit code displayed. If you need to edit or change any settings in future, or remove the functionality completely, the configuration will be stored under ~/.google_authenticator. Configure PAM for 2-factor authentication To make the system enforce the use of these OTP (one-time password) codes, we’ll first need to edit the PAM configuration for the sshd service (/etc/pam.d/sshd) and add this line to the end of the file: auth required pam_google_authenticator.so nullok The nullok at the end of this line means that users who don’t have a second factor configured yet will still be allowed to log in so that they can set one up. Once you have 2-factor set up for all your users, you should remove nullok from this line to properly enforce the use of a second factor. We also need to change the default authentication methods so that SSH won’t prompt users for a password if they don’t present a 2-factor token. These changes are also made in the /etc/pam.d/sshd file: On RHEL/CentOS-based systems, comment out auth substack password-auth by adding a # to the beginning of the line: #auth substack password-auth On Debian/Ubuntu-based systems, comment out @include common-auth by adding a # to the beginning of the line: #@include common-auth Save the /etc/pam.d/sshd file once you’re done. Configure SSH for 2-factor authentication We also need to tell SSH to require the use of 2-factor authentication. To do this, we make a couple of changes to the /etc/ssh/sshd_config file. Firstly, we need to change ChallengeResponseAuthentication no to ChallengeResponseAuthentication yes to allow the use of PAM for credentials. We also need to set the list of acceptable methods for authentication by adding this line to the end of the file (or editing the line if it already exists): AuthenticationMethods publickey,keyboard-interactive This tells SSH that it should require both a public key (which we are going to be satisfying using an SSH certificate) and a keyboard-interactive authentication (which will be provided and satisfied by the sshd PAM stack). Save the /etc/ssh/sshd_config file once you’re done. At this point, you should restart sshd with systemctl restart sshd. Make sure to leave an SSH connection open so that you can fix any errors if you need to. Restarting SSH will leave existing connections active, but new connections may not be allowed if there is a configuration problem. Test it out Connect to your bastion host directly and you should see a prompt asking you for your 2-factor code: $ ssh bastion.example.com Verification code: Type the code presented by your Google Authenticator app and your login should proceed normally. If you check the sshd log with journalctl -u sshd, you should see a line indicating that your login succeeded: Mar 23 16:51:13 ip-172-31-33-142.ec2.internal sshd[29340]: Accepted keyboard-interactive/pam for gus from 1.2.3.4 port 42622 ssh2 Every other week we&#39;ll send a newsletter with the latest cybersecurity news and Teleport updates. Conclusion In conclusion, the recommended industry best practices for SSH security are: Use SSH certificates Enforce the use of bastion hosts Add 2-factor authentication to your SSH logins The methods above give practical examples of several ways in which you can improve the security of your SSH infrastructure, all while giving users the flexibility to keep using the tools they’re familiar with. This blog post was written by Gus, who works on Teleport, the open-source SSH access tool which implements the industry-best practices for SSH access by default, and requires minimal configuration. Related Posts SSH Handshake Explained | What is SSH Handshake? Restricted Shell | Restricted commands for SSH How to Use Certificate Pinning to Improve UX ssh teleport security     </description>
      <pubDate>01 Apr 20 17:29 EDT</pubDate>
      <guid>https://gravitational.com/blog/how-to-ssh-properly/</guid>
    </item>
    <item>
      <title></title>
      <link>https://letterstoanewdeveloper.com/2019/12/19/cultivate-the-skill-of-undivided-attention-or-deep-work/</link>
      <description>&lt;a href=&#34;https://letterstoanewdeveloper.com/2019/12/19/cultivate-the-skill-of-undivided-attention-or-deep-work/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This is a guest post from Josh Thompson. Enjoy. Dear New Developer, You know that there’s a chasm between your skill level and that of the mythical “senior software developer”. If you build a list of topics you encounter on your job that, if learned to a deep enough level, would put you on the same level as a senior developer, you’ll end up even more demoralized than before compiling that list. No need to assemble this list yourself! I’ve done it for you. Here’s the list of topics that I’d need to dedicate significant time to, in order to close the gap between me and the senior developers on our team, that I’ve encountered in my last two days of work: Breaking complex unknowns into simpler unknowns that can be further split into individual tickets Adding tests to complex, legacy code, to guide further refactoring of said code. Using `grep` to comb through server logs to diagnose a hard-to-identify-and-reproduce problem Provisioning new servers Building bash scripts to automate complex workflows Digging into gem source code to can shed gem dependencies while maintaining certain features Understanding indexing well enough to see that certain queries that we thought were using indexes were not, and fix this oversight index on the fly, without causing any blips in availability Each of these line-items has many books written about the topic. It seems like you could fill a bookshelf with books that address knowledge senior developers have available to them inside their own heads. It takes me long enough to work through a single book, so imagining a bookshelf of extra-curricular reading is quite daunting. It might feel daunting for you, too. Leading vs. lagging indicators The above list of skills is a lagging indicator of the underlying knowledge. We should not target improving lagging indicators, we should improve leading indicators. Josh, what is this ‘lagging and leading indicator’ stuff? Great question! A lagging indicator is “evidence that something has already happened.” If you got an A on a test, that is evidence that you learned the material. A leading indicator is “evidence that something will likely happen”. If you want to get an A on a test, you should study in a similar way as others who have gotten an A on that test. Maybe you need ten high-quality hours of study to get an A, so “number of high-quality study hours” would be a leading indicator of your grade. We no longer take tests (phew. I hated taking tests.) but we get mini-tests of our knowledge, daily. We’re paid to solve problems, which often require learning new things. Rather than focusing on a list of things other developers have learned, and targeting that list, I humbly propose that a leading indicator of acquiring this kind of knowledge is “hours per week spent in a state of intentional deep work”. The above list of topics are lagging indicators of a high degree of technical knowledge. Someone acquires the knowledge, then, and only then, can demonstrate that they have it. Leading indicators are “predictive”, in that if you can identify correctly those indicators, you can predict the outcome of the issue at hand. In this case, the issue at hand is “become significantly more experienced in the domain of software development”. I propose that a leading indicator of someone gaining these skills is the amount of time they spend in a state of deep work. I’d encourage you to read Deep Work: Rules for Focused Success in a Distracted World. The author makes a case for deep work being a key role in the success of “knowledge workers” (which includes many types of work, including, of course, software development.) If you’d rather not read the book, here’s the gist, from this summary of the book: In order to produce the absolute best stuff you’re capable of, you need to commit to deep work. The ability to quickly master hard things and the ability to produce at an elite level, in terms of both quality and speed, are two core abilities for thriving in today’s economy. “To learn hard things quickly, you must focus intensely without distraction.” “Your work is craft, and if you hone your ability and apply it with respect and care, then like the skilled wheelwright you can generate meaning in the daily efforts of your professional life.” “The key to developing a deep work habit is to move beyond good intentions and add routines and rituals to your working life designed to minimize the amount of your limited willpower necessary to transition into and maintain a state of unbroken concentration.” Imagine two equally knowledgeable early-career software developers. They have the exact same skills on January 1, 2020. If the first software developer spends four hours a week doing deep work, while the second software developer spends fifteen hours a week doing deep work, their trajectories will be quite different, and that second developer will quickly gain technical knowledge and proficiencies. So, if you’re an early-career software developer, track the time you spend doing deep work. That has you focusing on a leading indicator of growing in your skills. At that point, you’ll benefit from Peter Drucker’s assessment: What is measured, improves. You’ll track how many hours you spend doing deep work, and by tracking it, you’ll do more and more of it. In conclusion Do more deep work, and over a year or two years, your skills will grow much faster than those doing less deep work. Eventually, you might find that you’re doing the work of a senior developer! Good luck! -Josh Josh looks forward to being a senior developer some day. He’s only a few years into his career in the software development industry, but enjoys getting to bring knowledge and skills from his prior careers into his current role. He lives in (and works remotely from) Golden, CO, with his wife and loves to rock climb and read books, and can often be spotted in Denver-area climbing gyms or local crags. </description>
      <pubDate>24 Mar 20 22:13 EDT</pubDate>
      <guid>https://letterstoanewdeveloper.com/2019/12/19/cultivate-the-skill-of-undivided-attention-or-deep-work/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.kalzumeus.com/greatest-hits/</link>
      <description>&lt;a href=&#34;https://www.kalzumeus.com/greatest-hits/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I’ve written for 16 years, 572 essays, and 2.9 million words and counting. You can read a quick intro or my best work, which I curate below. Most Popular Salary Negotiation Instrumentally probably the most useful thing I have ever written. Salary negotiation advice, originally written for engineers in a good market but I’m told broadly applicable. According to reports from people this is responsible for ~$9 million a year in marginal improvement to compensation. Personal favorite. Don’t Call Yourself A Programmer, And Other Career Advice.  Career advice for engineers, but widely applicable, or so I’m told.  Personal favorite. Some Perspective On The Japanese Earthquake.  My (very personal) take on Japan’s response to disaster management after the Touhoku earthquake in 2011.  Got covered in the NYT, Australian ABC, BBC, etc. Falsehoods Programmers Believe About Names. I identified a common class of bugs in the design of applications. This spawned something of a genre. Running A Software Business on 5 Hours A Week.  Time management and productivity tips.  Personal favorite. See also about using metrics for personal productivity. Why I’m Done Making Desktop Applications.  Personal favorite. How To Compete With Open Source.  (日本語版もあります！） The IE7 CSS Bug Which Cost Me A Month’s Salary.  It was a learning experience… a very expensive learning experience. Strategic SEO for Startups.  Personal favorite, and probably my best “high level” take on SEO strategy for businesses which are not mine.  See also follow-up discussion. The Hardest Adjustment To Self Employment. Building Highly Reliable Websites.  Personal favorite. Marketing In a nutshell: Every software company should assign full-time engineers to working on their marketing funnel. Virtually no one does. Most of my largest career wins are shipping relatively simple engineering artifacts (like e.g. automated drip email campaigns) which directly affect funnel math by 3~15%. Drip email campaigns: My most common consulting engagement was delivering a lifecycle email campaign, often for first-time users of the software. This was because it routinely increased conversion by ~15%. Due to SaaS math that rounds to a 15% increase in enterprise value for one to two weeks of work. (I have a video course on this.) Only enough time to write one email? Send all customers on a month-to-month SaaS plan an email offering to upgrade them to the annual plan for a modest discount (10% off or one month free). You can do this in an hour; it routinely gets 20%+ uptake and is great for your cash flow and churn rate. Here’s one annotated example. Conversion Optimization: I wrote the book on A/B testing in Rails (if by book one means OSS software). It was adapted for Khan Academy’s metrics infrastructure. I used this extensively for testing marketing copy, landing pages, design tweaks, the purchasing page, and in-app funnels. Minor tactical conversion tweaks with big results.  Four years later I’m paid to consult with companies you’ve heard of and it is disgusting how many need to be told some of them.  (Big buttons!) How to sell more software.  Personal favorite. Practical Conversion Tips for Selling Software.  Personal favorite. The importance of always giving the user prominent options which advance them towards conversion. Tips for landing page design.  They worked pretty decently for me. AdWords: Early impressions on how to be successful with AdWords.  Hilariously, at the time I was very opposed to the Content Network, which is now 90% of my AdWords business. Using Placement Preference to decrease costs.  I don’t do it anymore — Conversion Optimizer is far superior for cost performance. Why I don’t use Yahoo Search Marketing, or whatever it is called these days.  Fool me once, bad on you, fool me twice… After being an early adopter of Conversion Optimizer with significant success I was ranking #3 for it (under Google itself), which might have been why they did a case study with me.  It remains my favorite AdWords feature. Measurement: Instrumenting a downloadable free trial to tell what is causing conversions. Using Google Analytics to see what people are clicking on.  (CrazyEgg is a much better option in every way.) Miscellaneous: Engineering Your Way To Marketing Success.  Partially practical tips, partially part of my ongoing campaign to convince programmers that marketing is a worthwhile skill they can’t afford to reflexively dismiss or be terrible at. Why posting on forums to move product at retail is probably a waste of your time. Blogging as personal marketing.  (I was starting to be the go-to guy for SEO a year and change into my business, when I was 25.) Using incentivized surveys to get customer feedback.  Personal favorite.  It taught me some interesting things. Career Advice Don’t Call Yourself A Programmer.  Personal favorite.  My advice to young engineers on positioning themselves for current and future career growth. My thoughts on salary negotiation, particularly useful for engineers. Personal favorite. For freelancers/consultants: how to get your first client and how to set prices (interviews conducted with Ramit Sethi). Weird Hobbies I have some weird hobbies. Some people have, probably justly, accused me of having a hobby of having weird hobbies. Occasionally I write what I learn from them: I used to ghostwrite letters to banks to resolve consumer credit problems, which prepared me very well for when Equifax had their data security breach. Writeup: Identity, Credit Reports, and You I researched and wrote about the economics of the discount brokerage industry. How Discount Brokerages Make Money Conference Talks I present at conferences about 4~5 times a year, and generally request that the talk be made available after the conference. I’ve spoken at Microconf almost every year and consider that community my home-away-from-home. You should absolutely attend if you are at all interested in running a software business. MicroConf recommends my talks in this order, but chronologically works best for some people so: 2011: A Software Business on 5 Hours a Week (not recorded, but I have the slides) 2012: How to Engineer Marketing Success. Explains some of my common tactics, and broader mindset, on building engineering artifacts to influence marketing / sales outcomes. 2013: Building Things to Help You Sell the Things You Build, again on the mechanics of engineering sales and marketing. 2014: I’ve always said that the only thing that could keep me away from Microconf was the birth of a child. In 2014, we were blessed by the birth of our daughter Lillian. 2015: Leveling Up, on how entrepreneurship effectively has a career ladder, where what you learn doing one company can be used to help jumpstart the next. Slides 2016: I attended Microconf but was hip-deep in Starfighter and didn’t present. 2017: Paint by Numbers: From Productized Consulting to SaaS. This is the glide path for bootstrapping a software business off of a consulting/infoproduct sort of offering, and helps you avoid the long slow SaaS ramp of death. Slides. 2018: Your First 60 Days, on booting up a new Internet business. Covers marketing from a cold start, how to prioritize product development and other tasks, and what the minimum viable backoffice work is. Slides 2019: The Ethos of MicroConf, a bit of a personal reflection on life and business. Slides I also periodically speak at other conferences, on a potpurri of topics. You can find many of my presentations on Slideshare or SpeakerDeck. Podcast I am a frequent guest on podcasts. I also occasionally host my own podcast. Some recent episodes: Kalzumeus Podcast Episode 14: Running A Business Portfolio with Jonathan Siegel Kalzumeus Podcast Episode 13: Selling Online Businesses With Thomas Smale Kalzumeus Podcast Episode 12: Salary Negotiation with Josh Doody Videos Andrew Warner interviewed me on Mixergy.  (About one hour, comes with transcript half-written by me.)  Andrew is, by the way, the best interviewer in technology today.  You cannot do better than some of the insights he teases out of guests, and he has a wonderful way of making people so comfortable they forget to not answer the tough questions he slides in there. Gabriel Weinberg interviewed me with specific regards to SEO, mini-sites, and conversion optimization.  (About one hour, comes with transcript written by me.) I did a 7.5 minute lightning talk on selling software to underserved markets at Business of Software 2010. I spoke on Productizing Twilio Applications at TwilioConf 2011. Google brought me in to do a tech talk about What Engineers Don’t Know We Know About Marketing. SEO Content Creation: Before launch (July 1, 2006) I had a rough cut of my content creation strategy and figured out there would be seasonal elements to it. Right after launch, I stumbled on what turned into my first major SEO opportunity: Dolch Sight Words.  This started working rather quickly (was my main source of sales for almost a year), both for traffic and for backlinks. Optimizing your website for snowflake queries: the ones Google only sees once.  This eventually formed the core of my content creation strategy: as many pages as possible, each targeted at one specific, narrow interest. Most recommended series: After seeing the results of doing content creation by hand in notepad, I started trying to scale it up using Rails and get the content written by freelancers.  This ended up getting early positive results and eventually virtually taking over the business (now accounting for some 50% of sales and 75% of profits, give or take).  I eventually distilled this strategy into a presentation on SEO for software companies. Using evergreen content (that which is perpetually useful to consistent, unchanging needs of your customers) to make sales.  (Note: anti-pattern of a 10 year old blog post outranking product site happens frequently in consulting!)  My little brother busy trying to break into comic book writing advice also has thoughts on this here. Mini-sites: I had a variety of mini-sites focused on my best performing single pieces of content, beginning with an experimental one for Christmas back in 2008. They tended to work exceptionally well in their second and third years. This is probably not worth doing anymore since SEO changes over time. Relatedly, I wrote up some tips on how to do holiday promotions. Link Building: See everything I write about content creation, as they’re deeply entwined for me. Tactics and strategy for more effective link building. On Page SEO: An early take on on-page optimizations. Miscellaneous It took 11 days to rank for my product name and a month until organic SEO eclipsed PPC as a source of traffic for me. Coding for SEO on Rails. I tried buying links when I was young and stupid (slightly before Google came down hard on the practice).  It didn’t work out well. Put your blog in a subdirectory of the product domain. The right way to do a 301 redirect in Apache. Why I don’t recommend hiring SEO consultants.  Personal favorite.  (Ironic, since I have worked as one.) Tough To Categorize But Still Useful Why I don’t write any Top Ten Ways To Sell A Widget articles.  Personal favorite. Dealing with market seasonality. Year in Review Posts: 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015 (skipped), and 2016. Since late 2016 I’ve been working at Stripe. (I can’t show you our stats unless you come work with us on growing the GDP of the Internet.) </description>
      <pubDate>01 Feb 21 12:19 EST</pubDate>
      <guid>https://www.kalzumeus.com/greatest-hits/</guid>
    </item>
    <item>
      <title>How we reduced deployment times by 95%</title>
      <link>https://blog.plaid.com/how-we-reduced-deployment-times-by-95/</link>
      <description>&lt;a href=&#34;https://blog.plaid.com/how-we-reduced-deployment-times-by-95/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;As Plaid grows, so does the scale of our infrastructure. We currently run over 20 internal services and deploy over 50 code commits per day across our core services. Minimizing deployment time is therefore of vital importance to maximizing our iteration velocity. A fast deployment process allows us to rapidly ship bug fixes and run a smooth continuously deployed system.A couple months ago, we noticed that slow deploys of our bank integration service were affecting our team&#39;s ability to ship code. Engineers would spend at least 30 minutes building, deploying, and monitoring their changes through multiple staging and production environments, which consumed a lot of valuable engineering time. This became increasingly unacceptable as the team grew larger and we shipped more code daily.While we had plans to implement long-term improvements like moving our Amazon ECS-based service infrastructure onto Kubernetes, a fix was warranted to increase our iteration speed in the short-term. We set out to score a quick win by implementing a custom &#34;fast deployments&#34; mechanism.High latency in Amazon ECS deploysOur bank integration service consists of 4,000 Node.js processes running on dedicated docker containers managed and deployed on ECS, Amazon’s container orchestration service. After profiling our deployment process, we narrowed down the increased deployment latencies to three distinct components:Starting up tasks incurs latency. In addition to application startup time, there is also latency from the ECS health check, which determines when containers are ready to start handling traffic. The three parameters that control this process are interval, retries, and startPeriod. Without careful health check tuning, containers can be stuck in the &#34;starting&#34; state even after they&#39;re ready to serve traffic.Shutting down tasks incurs latency. When we run an ECS Service Update, a SIGTERM signal is sent to all our running containers. To handle this, we have some logic in our application code to drain any extant resources before completely shutting down the service.The rate at which we can start tasks restricts the parallelism of our deploy. Despite us setting the MaximumPercent parameter to 200%, the ECS start-task API call has a hard limit of 10 tasks per call, and it is rate-limited. We need to call it 400 times to place all our containers in production.Approaches exploredWe considered and experimented with a few different potential solutions to chip away at the global objective:Reduce the total number of containers running in production. This was certainly feasible, but it involved a significant overhaul of our service architecture in order for it to handle the same request throughput, and more research needed to be done before such a change could be made.Tweak our ECS configuration by modifying the health check parameters. We experimented with tightening the health check by reducing the interval and startPeriod values, but ECS would then erroneously mark healthy containers as unhealthy when they started, causing our service to never fully stabilize at 100% health. Iterating on these parameters was a slow and arduous process due to the root issue, slow ECS deployments.Spin up more instances in the ECS cluster so that we can start more tasks simultaneously during a deployment. This worked to reduce deploy times, but not by very much. It also isn’t cost-effective in the long run.Optimize service restart time by refactoring initialization and shutdown logic. We were able to shave around 5 seconds per container with a few minor changes.Although these changes improved the overall deploy time by a few minutes, we still needed to improve the timing by at least an order of magnitude for us to consider the problem solved. This would require a fundamentally different solution.Preliminary solution: utilizing the node require cache to “hot reload” application codeThe Node require cache is a JavaScript object that caches modules when they are required. This means that executing require(&#39;foo&#39;) or import * as foo from &#39;foo&#39; multiple times will only require the foo module the first time. Magically, deleting an entry in the require cache (which we can access using the global require.cache object) will force Node to re-read the module from disk when it’s next imported.To circumvent the ECS deployment process, we experimented with utilizing Node’s require cache to perform a “hot reload” of application code at runtime. On receiving an external trigger — we implemented this as a gRPC endpoint on the bank integration service — the application would download new code to replace the existing build, clear the require cache, and thereby force all relevant modules to be re-imported. With this approach, we were able to eliminate much of the latency present in ECS deploys and fine-tune our entire deployment process.Over Plaiderdays — our internal hackathon — a group of engineers across various teams got together to implement an end-to-end proof of concept for what we termed &#34;Fast Deploys&#34;. As we hacked a prototype together, one thing seemed amiss: if the Node code that downloaded new builds also tried to invalidate the cache, it wasn’t clear how the downloader code itself would be reloaded. (There is a way around this with the Node EventEmitter, but it would add considerable complexity to the code). More importantly, there was also some risk of running versions of code that were not in sync, which could cause our application to fail unexpectedly. As we weren’t willing to compromise on the reliability of our bank integration service, this complication warranted rethinking our “hot reloading” approach.Final solution: reloading the processIn the past, in order to run a series of uniform initialization tasks across all our services, we wrote our own process wrapper, which is aptly named Bootloader. At its core, Bootloader contains logic to setup logging pipes, forward signals, and read ECS metadata. Every service is started by passing the application executable’s path to Bootloader, along with a series of flags, which Bootloader then executes as a subprocess after performing the initialization steps.Instead of clearing Node&#39;s require cache, we updated our service to call process.exit with a special exit code after downloading the intended deployment build. We also implemented custom logic in Bootloader to trigger a process reload of any child process that exits with this code. Similar to the “hot reload” approach, this enables us to bypass the cost of ECS deploys and quickly boostrap new code, while avoiding the pitfalls of “hot reloading”. Furthermore, having this &#34;Fast Deploy&#34; logic at the Bootloader layer allows us to generalize it to any other service we run at Plaid.Here&#39;s what the final approach looks like:Our Jenkins deployments pipeline sends an RPC request to all instances of our bank integration service, instructing them to &#34;Fast Deploy&#34; a specific commit hashThe application receives a gRPC request for a fast deployment and downloads a tarball of the build from Amazon S3, keyed on the received commit hash. It then replaces the existing build on the file system and exits with the special exit code that Bootloader recognizes.Bootloader sees that the application exited with this special &#34;Reload&#34; exit code, and restarts the application.Lo and Behold, the service now runs new code!Here’s a very simplified diagram of what happens during this process.ResultsWe were able to ship this “Fast Deployments” project within 3 weeks and reduce our deployment times from more than 30 minutes to 1.5 minutes across 90% of our containers in production.The graph above shows the number of deployed containers for our bank integration service, color-coded by their commits. If you focus on the yellow line graph, you can observe a leveling off in the increase at around 12:15, which represents the long tail of our containers which are still draining their resources.This project has greatly increased the velocity of Plaid&#39;s integrations work, allowing us to ship features and bug fixes more quickly, and minimize engineering time wasted context switching and monitoring dashboards. It is also a testament to our engineering culture of shipping materially impactful projects, embodied by ideas that come out of hackathons.Want to work on impactful projects like Fast Deployments? We&#39;re hiring!</description>
      <pubDate>24 Mar 20 12:27 EDT</pubDate>
      <guid>https://blog.plaid.com/how-we-reduced-deployment-times-by-95/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.ets.org/research/topics/as_nlp/writing_quality/</link>
      <description>&lt;a href=&#34;https://www.ets.org/research/topics/as_nlp/writing_quality/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; To advance quality and equity in education by providing fair and valid assessments, research and related services. Our products and services measure knowledge and skills, promote learning and performance, and support education and professional development for all people worldwide. </description>
      <pubDate>25 Jun 20 07:54 EDT</pubDate>
      <guid>https://www.ets.org/research/topics/as_nlp/writing_quality/</guid>
    </item>
    <item>
      <title>Partying over Internet: Technological Aspects</title>
      <link>http://250bpm.com/blog:158</link>
      <description>&lt;a href=&#34;http://250bpm.com/blog:158&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;← Now, with billion people locked down in their homes, social contact over Internet becomes an increasingly important topic. Not only it allows people to stay in touch, it also lowers the incentives to leave one&#39;s home and meet people in person and thus contributes to the public health. I threw an online birthday party few days ago and in this article I would like to share some of my observations about how a party over Internet differs from one in the physical world and point out some implications and possible improvements for videoconferencing software. To start with, existing videoconferencing software is geared towards business meetings. And the differences between a business meeting and a party are easy to spot: At a party, there&#39;s much less structure. While at a meeting it&#39;s typically just one person that speaks and everybody else listens, at a party people tend to speak in parallel. At a party, non-verbal communication (facial expressions, gestures) is crucial. At a business meeting, not so much. At a meeting one wants to maintain self-control. At a party, one often rather wants to get rid of it. And so on and so forth. So, let&#39;s move directly to my observations: Everyone should be visible Unlike with Google Hangouts where the person speaking is displayed in full-screen view and everyone else as a small icon on the side, at a party a want to see everyone in parallel. Sometimes, it&#39;s someone&#39;s non-verbal accompaniment of someone else&#39;s speech that&#39;s the real fun. But even when that&#39;s not the case, I still want to follow how everyone reacts to what is being said, whether they are laughing, paying attention, not paying attention, preparing a snack, drinking, smoking or whether they have left for a moment, leaving just a lonely chair visible in the window. In short, I want to see all the participants of the party, side by side, all of them in equally sized windows. Size of the window is too expensive a piece of psychological real estate to waste on such an obvious thing as &#34;who&#39;s speaking at the moment&#34;. More about that below. Looking at people Maybe the most unpleasant part of social videoconferencing is not being able to follow who&#39;s looking at whom. Everyone just stares, indiscriminately, at the camera. I have no idea of how reliably the existing eye-tracking software works. After browsing the web for a minute it seems that eye tracking is used mostly for… optimizing ads??? What a waste of resources! Anyway, it doesn&#39;t matter. Whether the software uses webcam to trace your gaze or whether you use mouse to explicitly point to whomever you are looking at the implications are more or less the same: First, I want the window I am looking at to get larger. Second, I want the person on the other side to know that I am looking at them - not necessarily in any obtrusive manner, I just want them to be aware of it, presumably by making my image slightly larger. To give a practical example: One of the participants at the party wears an amusing mask. I point to the corresponding window which makes it larger and allows me to inspect the details of the mask. The person in question sees that I am looking at them and that, possibly, others are looking as well. They may choose to react to that. Taken together, one would see large image of the person they are looking at, somewhat smaller images of people looking at them and small images of everyone else. That, I think, more of less reflects how people perceive each other in real-world social interactions. Eye contact If I look and someone and that person looks back at me, that&#39;s a powerful social signal and it should be reflected by the software. For example, in such a case we could get a special communication channel, where not only we see each other in large windows but also the talk by other people can be muffled so that we can hear each other well. (By the way, this protocol of looking and looking back is based on &#34;cabeceo&#34; as practiced when dancing tango: You can invite a person to dance with you by looking at them and they may accept by looking back or refuse by looking away.) In this post I am not going to speak about larger parties but the problem there is obvious: With many people present there are going to be many parallel conversations, resulting it too much noise and not being able to hear each other properly. Clearly, some kind of fluent separation of the party into subgroups is needed. And while this seems to be a hard problem, the eye contact protocol described above can be used at least as a starting point: Cabeceo protocol allows to create groups of two people. Can it be somehow generalized for groups of three or more? Kissing In many cultures, kissing is an important part of social interaction. However, it doesn&#39;t lend itself well to communication over Internet. Skype allows you to send an emoticon (e.g. a symbol of a heart) to your counterpart. But that doesn&#39;t really convey that feel of intimacy that the real kiss does. After all, emoticons were invented to be a substitute for non-verbal communication in the situations where people don&#39;t see each other (SMS, email) which makes them, in the context of videoconferencing, redundant at best and embarrassing at worst. Obviously, sending a kiss through video can&#39;t be solved by purely technical means. It would require some social innovation. If Eskimos can kiss by rubbing their noses, why can&#39;t we devise a special Internet kiss? But if we do, the experience can be greatly improved by the software. Consider the cabeceo protocol above. What if a kiss worked like this: I look at the other person. The other person looks at me. Our windows get larger, everyone else fades into background. Surrounding noises are muffled. Then we both touch cameras with our noses. It would have to be tested in practice, but it kind of looks like it could feel quite intimate. In the physical world there are many more ways to send social signals. Consider, for example, the seating order. If I sit next to someone it may mean that I want to speak with them, or I may want to signal that I belong to a certain subgroup or maybe it was just the only empty chair left. (Note that many social signals are weak and ambiguous. But that&#39;s a feature, not a bug!) Now, I&#39;ve used seating order just as an example. Not everything that exists at a physical party has to be necessarily mimicked in an online party. However, if the developers of the videoconferencing software decide that seating order is worth mimicking, then it has implications for the product design. For example, ordering of windows on the screen would have to be the same for everyone. It can&#39;t be that you can rearrange them arbitrarily. Should sitting next to someone else come with some extras? Maybe I can whisper to a person sitting besides me. Etc. Miscellaneous There&#39;s a natural worry that seeing only people&#39;s faces is going to prevent a lot of non-verbal communication (gestures). However, my anecdotal evidence is that people, especially as the party progresses, subconsciously tend to move away from the camera making their entire upper body visible and thus improving non-verbal communication channels. Partying online is not all downsides. One upside is that, being at home, people have all kind of inventory at their disposal, which, at a meatspace party, they don&#39;t. In our case we&#39;ve used a plague mask, a peacock feather, a bandana. There may be no technological implications, but I think the behavior is worth mentioning anyway. It&#39;s unclear whether you should see yourself among the participants of the party. On one hand, seeing yourself leads to more self-control, which is not that desirable at a party. On the other hand, people do like the feature and would probably feel awkward if it was removed. Feedback from a friend: &#34;If I am not on the screen it would feel like I am not present at the party.&#34; Conclusion These are just some random thoughts that I&#39;m giving free to anyone who fancies to implement them. However, if you do, please do let me know! It would be really interesting to see how this works in practice. April 5th, 2020Discussion Forum </description>
      <pubDate>05 Apr 20 22:30 EDT</pubDate>
      <guid>http://250bpm.com/blog:158</guid>
    </item>
    <item>
      <title>Best practices can slow your application down</title>
      <link>https://stackoverflow.blog/2021/03/03/best-practices-can-slow-your-application-down/?cb=1&amp;_ga=2.145693744.523477250.1614802183-1247573525.1613934766</link>
      <description>&lt;a href=&#34;https://stackoverflow.blog/2021/03/03/best-practices-can-slow-your-application-down/?cb=1&amp;_ga=2.145693744.523477250.1614802183-1247573525.1613934766&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; [Ed. note: While we take some time to rest up over the holidays and prepare for next year, we are re-publishing our top ten posts for the year. Please enjoy our favorite work this year and we’ll see you in 2022.] Update: I realize we didn’t add a lot of context here when telling the story of the engineering decisions we made years ago, and why we’re moving away from some of them now. This is an attempt to fix that omission.Over the past 13 years, we have progressively changed priority as a business. Early on, scaling to millions of users was our main concern. We made some tough calls, and consciously decided to trade off testability for performance. After successfully achieving that scale, much of the context has changed: we have a much faster base framework now, given all the latest improvements in the .NET world, meaning we don’t have to focus as much on the raw performance of our application code. Our priorities have since steered towards testability. We got away with “testing in production” for a long time, largely due to our (very active) meta community. But now that we’re supporting paying customers, identifying bugs early on reduces the cost of fixing them, and therefore the cost of business. Paying the accumulated tech debt takes time, but it’s already helping us get to more reliable and testable code. It’s a sign of the company maturing and our engineering division re-assessing its goals and priorities to better suit the business that we’re building for.  In software engineering, a number of fairly non-controversial best practices have evolved over the years, which include decoupled modules, cohesive code, and automated testing. These are practices that make for code that’s easy to read and maintain. Many best practices were developed by researchers like David Parnas as far back as the 1970s, people who thought long and hard about what makes maintainable high quality systems.  But in building the codebase for our public Stack Overflow site, we didn’t always follow them.  The Cynefin framework can help put our decision into context. It categorizes decisions into obvious, complicated, complex, and chaotic. From today’s perspective, building a Q&amp;A site is a pretty well-defined—obvious—problem and a lot of best practices emerged over the past years. And if you’re faced with a well-defined problem, you should probably stick to those best practices. But back in 2008, building a community-driven Q&amp;A site at this scale was far from being obvious. Instead, it fell somewhere in the “complex” quadrant (with some aspects in the “complicated” quadrant, like tackling the scaling issues we had). There were no good answers on how to build this yet, no experts who could show us the way. Only a handful of people out there faced the same issues. For over a decade, we addressed our scaling issues by prioritizing performance everywhere. As one of our founders, Jeff Atwood, has famously said, “Performance is a feature.” For much of our existence, it has been the most important feature. As a consequence, we glossed over other things like decoupling, high cohesion, and test automation—all things that have become accepted best practices. You can only do so much with the time and resources at hand. If one thing becomes super important, others have to be cut back.  In this article, we walk through the choices we made and the tradeoffs they entailed. Sometimes we opted for speed and sacrificed testing. With more than a decade of history to reflect on, we can examine why best practices aren’t always the best choice for particular projects. In the beginning… When Stack Overflow launched in 2009, it ran on a few dedicated servers. Because we went with the reliability of a full Microsoft stack—.NET, C#, and MSSQL—our costs grew with the number of instances. Each server required a new license. Our scaling strategy was to scale up, not scale out. Here’s what our architecture looks like now. To keep costs down, the site was engineered to run very fast, particularly in accessing the database. So we were very slim then, and we still are—you can run Stack Overflow in a single web server. The first site was a small operation put together by less than half a dozen people. It initially ran on two rented servers in a colocation facility: one for the site and one for the database. That number soon doubled: In early 2009, Atwood hand-built servers (two web, one utility, one database) and shipped them to Corvallis, OR. We rented space in the PEAK datacenter there, which is where we ran Stack Overflow from for a long time. The initial system design was very slim, and they stayed that way for most of the site’s history. Eventually, maintaining a fast and light site design became a natural obsession for the team.  Safety’s off If you look at the programming languages that are used today, they fall on a spectrum of high and low-level based on how much that language abstracts the bare metal functionality. On the upper end of high-level languages, you have JavaScript: memory allocation, call stacks, and anything related to native machine code is handled transparently. On the other end, you have C: allocate and free memory for variables manually, no garbage collection, and doesn’t really handle vectorized operations. High-level languages provide safety but have a lot more runtime overhead, so can be slower.  Our codebase works the same way. We’ve optimized for speed, so some parts of our codebase used to look like C, because we used a lot of the patterns that C uses, like direct access to memory, to make it fast. We use a lot of static methods and fields as to minimize allocations whenever we have to. By minimizing allocations and making the memory footprint as slim as possible, we decrease the application stalls due to garbage collection. A good example of this is our open source StackExchange.Redis library.    To make sure regularly accessed data is faster, we use both memoization and caching. Memoization means we store the results of expensive operations; if we get the same inputs, we return the stored values instead of running the function again. We use a lot of caching (in different levels, both in-process and external, with Redis) as some of the SQL operations can be slow, while Redis is fast. Translating from relational data in SQL to object oriented data in any application can be a performance bottleneck, so we built Dapper, a high performance micro-ORM that suits our performance needs. We use a lot of tricks and patterns—memoization, static methods, and other tricks to minimize allocations—to make our code run fast. As a trade-off, it often makes it harder to test and harder to maintain. One of the most noncontroversial good practices in the industry is automated tests. We don’t write a lot of these because our code doesn’t follow standard decoupling practices; while those principles make for easy to maintain code for a team, they add extra steps during runtime, and allocate more memory. It’s not much on any given transaction, but over thousands per second, it adds up. Things like polymorphism and dependency injection have been replaced with static fields and service locators. Those are harder to replace for automated testing, but save us some precious allocations in our hot paths Similarly, we don’t write unit tests for every new feature. The thing that hinders our ability to unit test is precisely the focus on static structures. Static methods and properties are global, harder to replace at runtime, and therefore, harder to “stub” or “mock.” Those capabilities are very important for proper isolated unit testing. If we cannot mock a database connection, for instance, we cannot write tests that don’t have access to the database. With our code base, you won’t be able to easily do test driven development or similar practices that the industry seems to love. That does not mean we believe a strong testing culture is a bad practice. Many of us have actually enjoyed working under test-first approaches before. But it’s no silver bullet: your software is not going to crash and burn if you don’t write your tests first, and the presence of tests alone does not mean you won’t have maintainability issues. Currently, we’re trying to change this. We’re actively trying to write more tests and make our code more testable. It’s an engineering goal we aim to achieve, but the changes needed are significant. It was not our priority early on. Now that we have had a product up and running successfully for many years, it’s time to pay more attention to it. Best practices, not required practices So, what’s the takeaway from our experience building, scaling, and ensuring Stack Overflow is reliable for the tens of millions who visit every day?The patterns and behaviors that have made it into best practices in the software engineering industry did so for a reason. They make building software easier, especially on larger teams. But they are best practices, not required practices.  There’s a school of thought that believes best practices only apply to obvious problems. Complex or chaotic problems require novel solutions. Sometimes you may need to intentionally break one of these rules to get the specific results that your software needs.  Special thanks to Ham Vocke and Jarrod Dixon for all their input on this post.  Tags: best practices, engineering, performance </description>
      <pubDate>08 Mar 21 09:12 EST</pubDate>
      <guid>https://stackoverflow.blog/2021/03/03/best-practices-can-slow-your-application-down/?cb=1&amp;_ga=2.145693744.523477250.1614802183-1247573525.1613934766</guid>
    </item>
    <item>
      <title>Educational Resources That Get Students Up to Speed on Advanced Manufacturing and Programming Languages</title>
      <link>https://spectrum.ieee.org/the-institute/ieee-member-news/educational-resources-that-get-students-up-to-speed-on-advanced-manufacturing-and-programming-languages</link>
      <description>&lt;a href=&#34;https://spectrum.ieee.org/the-institute/ieee-member-news/educational-resources-that-get-students-up-to-speed-on-advanced-manufacturing-and-programming-languages&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Educational Resources That Get Students Up to Speed on Advanced Manufacturing and Programming LanguagesShareFOR THE TECHNOLOGY INSIDER Topics SectionsMoreFor IEEE Members For IEEE MembersIEEE SpectrumFollow IEEE SpectrumSupport IEEE SpectrumIEEE Spectrum is the flagship publication of the IEEE — the world’s largest professional organization devoted to engineering and applied sciences. Our articles, podcasts, and infographics inform our readers about developments in technology, engineering, and science. CloseEnjoy more free content and benefits by creating an accountCreate an account to access more content and features on IEEE Spectrum, including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, consider Joining IEEE. These websites teach the skills needed in a competitive job market 19 Aug 2019 2 min read Illustration: Shutterstock </description>
      <pubDate>24 Mar 20 12:27 EDT</pubDate>
      <guid>https://spectrum.ieee.org/the-institute/ieee-member-news/educational-resources-that-get-students-up-to-speed-on-advanced-manufacturing-and-programming-languages</guid>
    </item>
    <item>
      <title></title>
      <link>https://hhexiy.github.io/</link>
      <description>&lt;a href=&#34;https://hhexiy.github.io/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; He He (How to pronounce?) Assistant Professor of Computer Science and Data Science CILVR / ML2 60 Fifth Ave 605 hhe@nyu.edu [CV] [Google Scholar] My goal is to enable reliable communication in natural language between machines and humans. Recent research directions include: (1) Text generation: How do we ensure that the generated text are not just coherent, but also factually correct? Beyond factuality, how do we generate novel, creative text in a controllable way? (2) Robust language understanding: Statistical machine learning systems suffer from spurious correlations in the data, resulting in biased prediction and catastrophic errors. How can we learn models that are robust to dataset biases? (3) Dialogue systems: Building an effective dialogue agent requires understanding in a broad sense, e.g., how to reason about structured knowledge and learn efficient strategies for a specific task? Prospective students: I&#39;m always looking for motivated students. If you are a prospective PhD student, please apply to either the PhD program in Computer Science or PhD program in Data Science and mention my name in your application. If you are an undergraduate or MS student at NYU and have taken ML/NLP courses, please drop me an email with your CV and transcript. If you are interested in a short-term internship for at least 6 months, please send me your CV and transcript. PhD students and post-docs Richard Pang (co-advised with Kyunghyun Cho) Vishakh Padmakumar Nitish Joshi Nicholas Lourie (co-advised with Kyunghyun Cho) Chen Zhao (post-doc, co-advised with Kyunghyun Cho) Visitors and MS students Johnny Ma Xiang Pan Saranya Venkatraman Alumni Udit Arora (MS 2021, now Machine Learning Engineer at Google) Aniket Bhatnagar (MS 2021, now Machine Learning Engineer at Verneek) Zhiliang Tian (Visiting student 2020--2021, HKUST PhD) Teaching CSCI-GA.2590 Natural Language Processing [fall20] [fall21] DS-GA.1003 Machine Learning [spring20] [spring21] [spring22] Publications </description>
      <pubDate>29 Nov 20 11:11 EST</pubDate>
      <guid>https://hhexiy.github.io/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.damninteresting.com/radical-solutions/</link>
      <description>&lt;a href=&#34;https://www.damninteresting.com/radical-solutions/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;© All Rights Reserved. Please do not distribute without written permission from Damn Interesting. Paris, 29 May 1832. All through the night, a young Frenchman named Évariste Galois stayed awake, quill in hand, frantically scrawling notes and equations across dozens of sheets of paper. He had only been studying mathematics seriously for a few years, but he had proven to be a veritable prodigy. After quickly exhausting the knowledge of his teachers, he’d branched out into his own research, extraordinarily prescient. By all rights, Galois ought to have been lauded and laurelled by the scientific community for his work. Above all, he should have been recognised and rewarded by France’s prestigious Academy of Sciences. But Galois⁠—at least, by his own reckoning⁠—had received little but dismissal from the mathematics community. Now he sat feverishly scribbling a letter to his best friend, trying to commit as many of his recent ideas to paper as possible. Finally, in the wee hours of the morning, Galois had sketched out most of what he felt able to capture. “You know … that these aren’t the only subjects I’ve explored,” he wrote. “But I don’t have time”. Twenty-year-old Galois fully expected that he was about to be shot to death. Évariste Galois was born on 25 October 1811 in the town of Bourg-la-Reine, today part of the southern suburbs of Paris. Although his parents ran a well-regarded boarding school of their own, they sent young Évariste to study in Paris shortly before he turned twelve to improve his social opportunities. Galois’s new school, the Collège royal de Louis-le-Grand was and remains prestigious, but in the early 19th century, it boasted not only an unparalleled list of alumni⁠—among them such luminaries as Voltaire and Charles-Marie de La Condamine⁠—but also a fearsomely draconian atmosphere. Meals were meagre, facilities failing, cold constant, rats regular, and punishments painful, with the students under constant surveillance. The punitive environment⁠—plus homesickness and health issues⁠—took its toll on Galois. In his third year, his grades began to drop, and he earned a reputation as a loner and a troublemaker. One teacher labelled him a “chatterer” who “has, I believe, taken upon himself the task of wearing me out”. That said, Galois’s time at the school would soon lead to two major upheavals in the 14-year-old’s life⁠—one political, the other intellectual. Evariste Galois, drawn from memory by his brother Galois grew up at the tail-end of France’s revolutionary and Napoleonic years, which had infused the country’s intellectual bloodstream with liberal ideas. Despite the restored monarchy’s attempts to crack down on this unseemly radicalism, the school had developed a liberal reputation. A new school principal, Nicolas Berthot, thought this merited a course correction, and set about inaugurating new rules harkening back to the school’s harsh old Jesuit roots. In response, students undertook a campaign of nonviolent resistance⁠—when asked to sing hymns, speak in class, or toast the King at meals, they remained mute. Berthot, apparently not one for half measures, simply expelled the students⁠—over 100 of them. While Galois was not directly involved, he was appalled by Berthot’s peremptory reaction. It was in this atmosphere of injustice and oppression that young Galois began his shift from mere liberal-minded student to full-blown anti-authoritarian. In contrast, Galois’s academic upheaval was due to a happy convergence between an academic restructuring and his own academic failure. With his grades plummeting, he was forced to repeat his entire third year. This cannot have been welcome news, but there was an unexpected upside. The third-year curriculum had just been changed, and would now introduce students to arithmetic and geometry alongside their continued study of the classics. Galois was about to meet mathematics. Few blind dates have gone so well. Instantly sparked, the 15-year-old devoured entire textbooks on algebra, calculus, and geometry. Galois’s other classes, already somewhat neglected, fell off his radar almost entirely. Faculty soon abandoned all hope of getting any other subject into his brain. Galois leapt straight into a heavily intuitive, original approach to tackling big unanswered questions. His concerned teachers suggested that he outline his problem-solving more methodically, or at least follow the basics of showing his work, but he was not interested in such elementary claptrap. Within mere months, he had outgrown his coursework and reached the extremities of contemporary mathematical knowledge. Fully aware of his own preternatural talent, Galois set his sights on attending the École polytechnique⁠—France’s foremost technical school⁠—as soon as possible. As he was nearing 16, the minimum age of admission, he registered for the entrance exam immediately. Nearly as immediately, he failed. Still, failing the entrance exam was nothing to be ashamed of. Many students went into their first attempts seeking only to get a sense of how the exam worked⁠—which made sense, as it was a fast-paced verbal interrogation at a blackboard. Just one in three examinees passed on their first try. Though Galois’s failure forced him to remain at Louis-le-Grand, he could at least console himself that this was only a temporary setback. A further consolation appeared in the form of an enthusiastic new mathematics teacher who grasped the scope of Galois’s ideas and quickly became his mentor. Under this mentor’s guidance, in April 1829, 17-year-old Galois published a short paper on repeating fractions in a respected scholarly journal. Impressive though this was, for Galois this was only a side-project, dwarfed by breakthroughs he was making in the area of polynomial equations. This was a big deal. Mathematicians had collectively hit a wall with polynomials late in the 18th century, and Galois was about to suggest a way over. For people who remember algebra classes, the most familiar type of polynomial is the quadratic equation, which can be useful in calculating areas, solving some accounting tasks, and addressing some physics problems. It is generally written as: ax2 + bx + c = 0 “X” is the single unknown value to solve for. There are multiple ways of figuring out the possible value(s) for x, but one reliable way is by taking a, b, and c and plugging them into a formula that was discovered by Spanish mathematician Abraham bar Hiyya Ha-Nasi around the year 1100 AD. It is fittingly known as the quadratic formula: What makes a quadratic equation ‘quadratic’ is that x is squared, or to the power of 2. So a quadratic equation is said to have a degree of 2. An equation where x is to the power of 3 (but nothing more) has a degree of 3, and these are called cubic equations, which look like this: ax3 + bx2 + cx + d = 0 The quadratic formula doesn’t help with cubic equations, but a working cubic formula was discovered sequentially by three Italian mathematicians in the 16th century. A student of one of theirs went one further, literally, managing to find an enormously complicated general formula for quartic equations⁠—those of degree 4. The outstanding question Galois wished to tackle was: Is there a formula to solve quintic equations, i.e. those of degree 5? Prominent mathematicians of the day suspected that a general solution for quintic equations simply did not exist. However, a mere failure to find such a formula was not evidence. Italian mathematician Paolo Ruffini came close to proving that no such general solution existed for 5th-degree equations, which he published in a paper in 1799. Picking up more or less where Ruffini left off, 17-year-old Galois set out to prove that no general formula existed for quintic equations. More broadly, Galois took an interest in an overarching question: what determined whether or not a general formula exists for a given degree? Galois’s approach to untangling the matter was stunningly original. He tied equations to several major new conceptual frames. He identified groups: sets of entities linked by a specific set of certain properties. Then permutations: all the different ways of ordering the members of a set. And symmetries: ways in which entities look like other parts of themselves. There was so much uncharted territory here that Galois found himself inventing a brand-new approach to algebra. The system he devised for describing the subtle internal characteristics of groups⁠—group theory⁠—was almost unprecedented, but so versatile that it could capture the behaviour of not only numbers, but all sorts of grouped items and ways in which their components showed self-similarity. When it came to polynomials, Galois’s major insight was that solvability of a given equation ultimately had far less to do with the equation’s degree, and far more to do with its internal properties related to symmetry. Astrophysicist and popular-mathematics author Mario Livio offers this analogy: “Classifying equations by their degree is analogous to grouping the wooden building blocks in a toy box according to their sizes. Galois’s classification by symmetry properties is equivalent to the realization that the shape of the blocks⁠—round, square, or triangular⁠—was a more important characteristic.” Using his own new group theory to break a complicated polynomial equation into smaller pieces and test those for solvability, Galois showed definitively that quintic equations, considered as a set, would not resolve this way. His results gave mathematics a way of determining whether a particular polynomial can be solved through a formula. All polynomials of degrees 2, 3, and 4 qualified. Starting at degree 5, however, some did and others did not⁠—and thus there was no way a general solution could apply to all of them. A depiction of Paris, 1829, by Giuseppe Canella Galois’s mentor Louis Richard, for one, was well-aware that the boy’s ideas were visionary. Richard decided to help his brilliant protégé submit two papers to the Academy of Sciences in the spring of 1829. Getting the Academy’s attention was an essential step for any aspiring mathematician in France at the time. It was the most prestigious scientific organisation in the country, and having a paper accepted there would be a spectacular feather in Galois’s cap. It wouldn’t hurt when it came to his second attempt at being admitted to the Polytechnique, either. Only members of the Academy could present new findings to the august body, so Galois needed to find a willing sponsor to present papers on his behalf. Richard reached out to the revered Augustin-Louis Cauchy, likely because 15 years earlier, Cauchy had published two papers on general theories of permutations. Cauchy, one of the most prolific mathematicians in history, rarely had time to read and endorse other people’s work⁠—but in this case, against the odds, he agreed. In May and June 1829, he presented a pair of complementary papers by Galois to the Academy of Sciences, and made plans to present a third in January 1830. But the thrill was about to be brutally crushed. A new priest had been assigned to Galois’s hometown of Bourg-la-Reine, one who immediately clashed with its liberal and warmhearted mayor⁠—who happened to be Galois’s father, Nicolas-Gabriel Galois. Determined to save the souls of his parish from the insidious influences of broad-mindedness and insufficient monarchism, the priest devised a plan to undermine the elder Galois and force him out of office. The well-liked Nicolas-Gabriel had a fondness for playful writing at times – delighting the townsfolk with short coupled rhymes. Taking note of this idiosyncracy, the priest began to author his own rhyming couplets in the mayor’s characteristic style, making them mean-spirited rather than playful, and signing them with Nicolas-Gabriel’s name. The slanderous counterfeits spread. Ultimately, the plot proved even more successful than the priest had hoped. In July 1829, devastated by the impersonation and by the loss of his good name, Nicolas-Gabriel Galois took his own life. He had been the mayor of the town for 15 years. The truth quickly emerged. Astoundingly, the priest attempted to take part in the funeral ceremony⁠—only to find himself fleeing a mob of furious townspeople and volleys of stones. Galois witnessed the entire scene, and his grief and rage can only be imagined. He had already had reason to resent and oppose his country’s religious right wing: the political had now become intensely personal. And then, with spectacularly bad timing, the next round of entrance exams for the École polytechnique was upon him. Galois⁠—short-tempered at the best of times, labouring under the emotional toll of his father’s death, convinced of his own genius, already bitter at the perceived injustice of having been rejected the first time around⁠—was in no state to deal with a second high-stakes examination at a blackboard. He was never at his best explaining ideas verbally in the first place, and he had spent the year working on original research rather than preparing for the examination. On top of that, Galois’s examiner was a man known for asking extremely simple questions⁠—not to test candidates’ knowledge, but to gauge their reaction to being asked them. Faced with an examiner who struck him as blitheringly ignorant, Galois did not do well. Unverified legend has long claimed that the exam came to an end when the candidate flung the eraser at the examiner’s face. The gates of the Polytechnique in Galois&#39;s time Whether it was his mathematics or his temper that sunk him, Galois was again denied admission to the Polytechnique. He turned his attention to a backup option: the École préparatoire (Preparatory School), whose main job was training teachers. It had its own set of entrance exams⁠—across a range of subjects, including several that were not mathematics. Although the application deadline for the school had passed, Galois wrote a letter to the administration asking them to let him apply anyway. His letter suggests that recent events had not compromised his cockiness. The examiners allowed him to proceed, but the science adjudicator in particular was less than impressed, commenting dryly that Galois “knows absolutely nothing”: “I was told this student had an aptitude for Mathematics; this surprises me greatly, as based on his examination, I think he possesses very little intelligence, or, at least, it’s so well hidden that I was unable to discover it[.]” Nevertheless, Galois’s scores for mathematics were high enough⁠—and his answers, for a change, expressed clearly enough⁠—that he was admitted to the École préparatoire in November 1829. Galois had a way forward. However, his fledgling mathematics career ran into turbulence shortly after takeoff, due to the untimely death of a Norwegian mathematician named Niels Henrik Abel. Amiable but hapless, Abel had been unsuccessfully clamoring for attention from French mathematicians for years. He had even travelled to Paris to seek recognition, and sent one of his own papers to Augustin-Louis Cauchy. Alas, Cauchy had failed to get around to reading it. Even Abel’s defeated letter to Cauchy several years later, requesting the return of his manuscript, went unacknowledged. Then, only 26 years old, Abel succumbed to tuberculosis. When news of Abel’s death reached the Academy in June 1829, Cauchy scrambled to defend himself⁠—awkwardly and unconvincingly⁠—for having neglected the young Norwegian. He rushed to read Abel’s three-year-old manuscript and present it to the Academy, just a few weeks after his second presentation on Galois’s work. It was immediately apparent that Abel had reached many of the same conclusions as Galois, and done so earlier⁠—publishing a proof that there could be no general formula for polynomials of any individual degree of 5 or above. That autumn, the Bulletin de Férussac published both an obituary of Abel and a detailed analysis of one of his earlier papers. For those in the know, this underscored the fact that while Galois had made some phenomenal breakthroughs, Abel had reached some of the same insights first. Their methods were entirely distinct, but when it came to the conclusions, Galois⁠—nine years younger⁠—had been provably pre-empted. Unsurprisingly, Cauchy’s planned third presentation of Galois’s research did not go ahead as planned in 1830⁠—though, as science historian René Taton points out, the normally irascible Galois did not complain about this, which suggests that he voluntarily withdrew his work given the unintended overlap with Abel’s findings. Niels Henrik Abel That said, Galois could still salvage the considerable original components of his own work. Abel had not scooped him on group theory. While keeping up with his regular schoolwork, Galois compiled his research into a manuscript and submitted it at the end of February 1830 to the Academy’s inaugural Grand Prize in mathematics. Despite a few setbacks, Galois was carving himself a respectable place in the mathematical world. If he were to win the Grand Prize, it would cement his status as a leading light of his generation. On 28 June 1830, the results of the Grand Prize were announced, and Galois’s name was nowhere to be seen. The jury chose to award the prize to two mathematicians for their separate work on elliptic functions. One was the German Carl Gustav Jacob Jacobi; the other, posthumously, was Abel. A clause in the contest rules specified that the jury could award the prize to any paper published in the previous year, not just official entries; the Academy thus made amends to the memory of the young man it had ignored in life. Another young man, however, was available to feel disregarded in Abel’s place. Galois could not possibly have objected to the winners on mathematical grounds; he strongly identified with Abel, and Jacobi appears to have been one of the few other mathematicians he respected. What was galling for Galois was that his manuscript was not even listed as an official contest entry. Indeed, when he asked to have his paper returned, it could not be found. This turned out to be simple bad luck. The Permanent Secretary of the Academy, Joseph Fourier, had served on the jury for the Grand Prize, and had taken Galois’s manuscript home with him to read. Unfortunately, he died in mid-May; Galois’s manuscript was lost somewhere in the shuffle of his papers, and it never resurfaced. Galois already had a sizable chip on one shoulder, and was fast developing one on the other as well. He was increasingly convinced that the Academy was deliberately shunning him. Perhaps, he thought, they were too incompetent to understand his dramatically forward-thinking mathematical ideas and too stodgy to approve of his dramatically forward-thinking political convictions. Galois’s resentment can only have been compounded when, in late July 1830, he took an exam on differential and integral calculus and placed only fourth out of eight students⁠—a shockingly low rank for someone who was having papers published in the same journal as Cauchy. On top of it all, he was still irritated at having twice been denied the chance to study at the Polytechnique, both for the quality of its mathematics instruction and for its opportunities in the way of anti-monarchist political activism. The second of these was about to be on full display as the tectonic plates of French politics shifted in the summer of 1830. King Charles X of France King Charles X, to put it mildly, was extremely conservative. The ministers he chose were even more so. All of them remained traumatised by the French Revolution of 1789, which had resulted in the mass decapitation of their peers⁠—including more than a few members of Charles’s family. As a result, they were dedicated to doing away with those ridiculous ideas of liberty, equality, and fraternity that the revolution had propagated. When the results of an election in the summer of 1830 favoured the liberal left, the government’s ossified leaders retaliated with a particularly boneheaded decision. A series of decrees dissolved the newly elected legislature before it had even met, reduced the assembly’s size by 170 seats, drastically curtailed voting rights in favour of the wealthy, censored opposition publications, and called for a do-over election in September. The government must have expected that the liberals would be unhappy, but as the prime minister reported that he received regular visitations from the Virgin Mary and she had told him everything would be fine, they went ahead anyway. What they didn’t take into account was that essentially abolishing the printing industry would also make the typographers unhappy. With their livelihoods gone, they spilled into the streets, quickly followed by their working-class peers, and then the furious middle class. Riots became the lightning-speed July Revolution, which quickly became mythologised as the Trois Glorieuses, the “Three Glorious Days”, where all classes had united to throw out a tyrannous regime. Members of the democratically organised National Guard⁠—which had been founded during the Revolution but that Charles had abolished⁠—pulled out their old uniforms from the cupboard and spontaneously reformed the militia. They came to be known as the heroes of the hour⁠—along with the students of the Polytechnique, who also jumped into the fray. Their counterparts at the École préparatoire, on the other hand, were shut out. Or, more precisely, shut in. As the battle raged on the streets of Paris, the school’s administrators locked the doors to keep their students from taking part. The school’s new principal, Joseph-Daniel Guigniault, twice threatened to call in the military to keep order among his pupils. (This might not have accomplished anything. The royalist army was a bit preoccupied, trying to control the streets while upper-storey inhabitants of buildings showered them with furniture, including the occasional piano.) Guigniault made matters worse by uttering condescending comments about the revolutionaries. As it was, he could at least claim that he kept his students safe. The only one of them who seems to have been at any risk of injury during the Trois Glorieuses was Galois, who by most reports was so eager to get involved with the rioting that he tried to climb over the schoolyard wall. His participation was unnecessary. Charles X faced the facts and went off into exile, while France proclaimed a new constitutional monarchy under a new king, Louis-Philippe. The National Guard became a key part of the imagery surrounding the change, their blue-white-red uniforms matching the newly restored tricolour flag flapping everywhere. Prise de l&#39;Hôtel de ville : le Pont d&#39;Arcole by Amédée Bourgeois The new regime, however, did not please everyone: it was a compromise that did not go far enough for the left and was anathema to the right. Paris, which set the tone for the whole country, remained a cramped medieval city with an underpaid, underfed working class. There were ongoing flare-ups of both rioting and disease. For Galois, the revolution also meant the loss of his best source of support at the Academy of Sciences: Cauchy, who was a right-wing Catholic and diehard supporter of the old monarchy, had left the country rather than take the required oath of loyalty to the new king. Galois’s own path, naturally, was diametrically opposed. He joined the newly founded Société des amis du peuple, a republican group so radical that it was banned altogether in October 1830, after which it became a (theoretically) secret society. While the Polytechnique was being laurelled for its active liberalism, Galois was stuck behind locked doors with its despised principal Guigniault⁠—his best chance at fighting the Establishment being stifled by the Establishment. To Galois, the principal’s behaviour was infuriatingly hypocritical. At first, Guigniault had mocked and dismissed the revolutionaries⁠—only to drape himself ostentatiously in the tricolour of those same revolutionaries at the battle’s end. Guigniault also made the empty gesture of changing the name of the school itself back to its Napoleon-era name of École normale, while at the same time continuing to insist that “good students should not be interested in politics”. Galois exchanged an increasingly testy series of letters with school administrators, criticising Guigniault’s response to the riots and revolution. When Galois published an “anonymous” letter in a prominent education-focused journal, spilling the matter into the press and thus the public, Guigniault simply expelled him. With school off the table, nothing prevented Galois from getting in on the glory of the National Guard. The young mathematician enlisted in one of the Guard’s artillery batteries, one known to be a hotbed of republican sentiment. A good chance lay ahead for the republicans to demonstrate their continued displeasure with current affairs. Several of Charles X’s ministers were on trial, and the consensus on the left was that anything less than death sentences would be tantamount to acquittal⁠—and grounds for another uprising. Wearing a newly purchased uniform, Galois would have been among the artillerymen stationed at the Louvre on 21 December 1830 when the sentences were announced. The ministers were sentenced to life in prison rather than death. The situation was precarious for days, and several members of the artillery were arrested for seditious behaviour. King Louis-Philippe, no fool, realised that leaving a bunch of radical republicans in charge of cannons might not be the wisest course. On 31 December, he dissolved the National Guard’s artillery units, outlawing the wearing of their uniforms. Galois’s military career had lasted, at most, three weeks. Untethered from all institutions, Galois floundered, but the Academy of Sciences extended an olive branch. One of its most respected mathematicians, Siméon Denis Poisson⁠—who had shared journal space with both Galois and Cauchy just a few months earlier⁠—requested that Galois send them a new paper. Galois wrote a new manuscript⁠—probably a re-creation of his lost submission to the Grand Prize⁠—and submitted it on 17 January 1831. After two months passed with no word, Galois followed up with a snide letter to the President of the Academy, suggesting there was something fishy in Poisson’s delay. Pointedly rehashing his experience, Galois topped it off with a direct accusation of ulterior motives: “…the examining committee decided a priori that I could not have resolved this problem, firstly because my name was Galois, and moreover because I was a student. And the committee misplaced my paper. And I was told that my paper was misplaced. This lesson should have sufficed me […] to this day, my research has met more or less the same fate […] Will the analogy be pursued to the end? Please […] invite Messrs. Lacroix and Poisson to declare whether they have misplaced my paper, or whether they intend to report on it to the Academy.” Needless to say, this was not a good strategy for building a career. Other mathematicians were left shaking their heads at Galois’s behaviour. Rumours flew that he was losing his mind. On 16 April 1831, nineteen of Galois’s fellow republicans were acquitted of sedition charges. Overjoyed, their allies carried them home in triumph, and quickly decided to hold a celebratory banquet in their honour. There was no right to free association in France at the time, but even the most dictatorial regimes knew better than to get between the French and a good meal. A banquet was therefore one of very few legal ways of gathering a large group of like-minded people to exchange ideas (or to collectively decide to defenestrate a bust of the king, or both). The venue was a restaurant, but not one known for its cuisine. Rather, it was the largest space in Paris that a group could book easily. A year earlier, it had hosted a liberal banquet that had laid the groundwork for the end of Charles X’s rule. Now Galois looked forward to a similar event⁠—and in preparation, he visited a local knife-maker and very eagerly ordered a ‘folding dagger’. Alexandre Dumas ca. 1831 One of the other revolutionaries in attendance at the banquet was Alexandre Dumas. The future creator of the Three Musketeers and the Count of Monte Cristo, Dumas was already a renowned playwright; only a week earlier, one of his plays had become the theatrical sensation of the year. He had also been an artilleryman in the same National Guard battery as Galois and many of the nineteen acquitted republicans. Recalling the event much later in his Memoirs, Dumas wrote that “it would have been difficult to find two hundred people more hostile to the government in all of Paris”. Even so, Galois eventually managed to find a way to stand out. As the number of emptied champagne bottles grew and the banqueters began to forego their promise not to make any unapproved toasts, Dumas suddenly became aware that “[a]n extremely animated scene was taking place fifteen or twenty places down from me. A young man, holding his raised glass and an open dagger in the same hand, was striving to make himself heard. It was Évariste Galois, […] one of the most ardent republicans.” Ardent and, it must be said, drunk. Galois later admitted to a friend that if he’d been sober, he would never have behaved as he did. Dumas could not hear over the immediate roar of the crowd, but he did work out that the words “Louis-Philippe” had been uttered, that Galois’s open dagger was unambiguous, and that there were limits to his own radicalism. Catching each other’s eye, Dumas and his neighbour hopped out a window and skedaddled⁠—which, from a banquet on the ground floor, was admittedly easier than it might have been. What Galois had done was to openly call for regicide⁠—a crime so unthinkable that the criminal code classified it as being as unnatural as parricide. Dumas knew well that plays could be banned simply for alluding to it. This action was too radical even for the radicals: republicans did not want their movement reminding the nation of the constant guillotining that had marked the 1789 Revolution. But Galois’s fiery outburst was soon all over the newspapers, causing considerable embarrassment. The morning after the banquet, Galois was arrested at his mother’s house. Galois’s supporters latched onto the fact that the threat had technically been conditional⁠—“To Louis-Philippe, should he betray [his oath to uphold the constitution]”⁠—with the crowd’s noise burying the second half. Astonishingly, at his own trial on 15 June 1831, Galois did not take advantage of this escape rope. Instead, he doubled down in every possible way. Not only did he not blame drunkenness, he insisted that he had intended what he’d said, conditionals be damned. In open court, with a gobsmacked audience looking on, Galois confirmed that it had been an assassination threat, that he had not just been expressing his personal opinion, and that he was attempting to goad others into making attempts on the king’s life. Indeed, Galois went on, in his opinion Louis-Philippe probably already had betrayed his oath. Around this point, the judge cut the interrogation short⁠—either to keep a lid on Galois’s outrageous sentiments, or simply to stop him from digging himself even more deeply into a hole. King Louis-Philippe I Surprisingly, Galois was acquitted. Dumas’s explanation was that the jurors either agreed with Galois, or simply thought he was beyond sanity. Most other sources agree that the judge and jury took pity on him because he was so young⁠—still only 19. Against the odds, Galois had slipped out of the political noose. Not only that, but the publicity dislodged the apparent impasse at the Academy of Sciences. On the day his trial started, the newspaper Le Globe published a lengthy letter detailing Galois’s experiences with the Academy. Specifically, the publication argued that Poisson⁠—who had requested the new paper from Galois in the first place⁠—had really dropped the ball. Perhaps encouraged to hurry, Poisson and his co-referee submitted their report a few weeks after Galois’s acquittal, on 4 July 1831, then presented it publicly to the Academy a week later. A months-long wait for a response to a manuscript was not actually anything unusual. The Academy was swamped with submissions. Mathematical historian Caroline Ehrhardt reports that two-thirds of the papers submitted to the Academy never received a report at all⁠—and of those that did, few got more than a few terse sentences. Galois’s complaint of neglect in March was baseless. Fortunately, the Academy did not dismiss him out-of-hand for his impudence⁠—which is just as well, for this manuscript contained what astrophysicist/author Mario Livio later called “one of the most imaginative breakthroughs in the history of algebra”⁠—the core of his invention of group theory. To their credit, the reviewers produced a lengthy, thorough, and detailed report on Galois’s manuscript. It was clear that they saw and appreciated the links between Galois’s ideas and Abel’s. But their evaluation was not what Galois had hoped for: overall, Poisson and Lacroix confessed that they were baffled. The math itself was not the problem; rather, they were not always able to follow the argumentation. “[…] we have made every effort to understand Mr. Galois’s demonstration. His reasoning is neither clear enough not developed enough for us to have been able to judge its exactness […] The author announces that this proposition […] is part of a general theory liable of many other applications. It is often the case that the different parts of a theory, by mutually illuminating one another, are easier to grasp when taken together rather than in isolation. We can therefore wait for the author to have published his work in its entirety before coming to a definitive opinion […]” Several contextual factors worked against Galois. He still did not have training in the conventions of writing out mathematical discoveries⁠—a skill he could have picked up at the Polytechnique. Added to this, his work was pure mathematics; this would have resonated with the absent Cauchy, but for the other members of the Academy at the time, what was important was applied mathematics. Plus, algebra was still thought of as a tool rather than a subfield of mathematics in and of itself; Poisson and Lacroix would have been judging Galois’s manuscript through the lens of mathematical analysis⁠—what we now call differential calculus⁠—and looking for its practical potential. Galois’s ideas likely struck them as a pointless excursion into terra incognita. With the benefit of hindsight, Mario Livio slams the reviewers for reacting so lukewarmly to Galois’s manuscript. However, in modern academic parlance, the referees’ report is much more “revise and resubmit” than it is an outright rejection. More than one modern mathematician has admitted they would likely have made the same decision in response to the manuscript as it was. None of this counted for Galois. Furious, he became convinced once and for all that the Academy was out to get him. And as usual, mathematical disappointment dovetailed with political trouble. Only days later, on 14 July 1831, the police set out very early for Galois’s home. Knowing him to be a troublemaker, they were hoping to nab him in a pre-emptive roundup of republican rowdies known to have plans for commemorating the fall of the Bastille. No Évariste Galois was found⁠—because he had already left home that morning. The police eventually caught up with him and his friend Ernest Duchâtelet. The young men were going to an illegal march, with Galois illegally wearing his National Guard uniform⁠—not to mention carrying a rifle, a couple of pistols, and a dagger. Placed under arrest, the pair complied calmly enough that the officers didn’t think to confiscate their prisoners’ rifles until they were halfway to the police station. Sainte-Pélagie prison Galois was thrown in jail to await trial, but his stay in Sainte-Pélagie prison got off to an eventful start. As the anniversary of the July Revolution approached, the republican prisoners got more and more excited. On 28 July 1831, they happily spent the day yelling anti-royalist slogans, insulting passers-by, and throwing things out of windows. Towards evening, a loaf of bread hit a woman on the head so hard that she was left bruised and bleeding (a testament to the sort of food the inmates were given). Half an hour later, one of Galois’s cellmates began to caterwaul La Marseillaise as he undressed for bed in front of the window. Outside, an exasperated local citizen sick of the ruckus let off a round of buckshot that hit the man clear in the face. Inside, panic erupted. The guards on the scene managed to regain control only by throwing three of the inmates⁠—including both Galois and Monsieur La Marseillaise⁠—into the dungeon. This action itself nearly caused a riot, and rumours flew that the shot had been fired by a prison employee at the governor’s order. Galois was only in the dungeon for three days, but it was three months before he came to trial. This time around, Galois backed down and feebly claimed that he hadn’t realised that wearing his uniform was illegal. The judge was unconvinced. Probably feeling that Galois’s previous acquittal had failed to teach him a much-needed lesson, he sentenced the young man to a further six months in jail. This sentence was disproportionately long, especially considering that Duchâtelet⁠—who had sketched a guillotine on his cell wall and added verses that threatened Louis-Philippe⁠—would have to do only three. Imprisonment did not suit Galois. Still, the dampness and discipline of Sainte-Pélagie was probably more comfortable than boarding school had been. Prisoners could receive visitors every Thursday and Sunday, chat among themselves, walk about. Nevertheless, Galois soon fell into despair. Some of his co-inmates began referring to him as “an old man of twenty”. His sister Nathalie, who visited constantly, thought the same, finding him as hollow-eyed as someone 30 years older. Galois had already proven at the banquet in May that he could drink beyond his limits⁠—an ability that he now had cause to demonstrate repeatedly. Fellow inmate François-Vincent Raspail, the president of the banned Société des amis du peuple, describes how prisoners taunted Galois by calling him a water-drinker. In response, Galois began downing entire bottles of brandy in one go, with predictable consequences. His mental state deteriorated, reopening his grief over the death of his father. He may even have attempted suicide, prevented only by Raspail’s intervention. But Galois also spent a great deal of time in prison pacing and doing mathematics in his head. He even worked out a plan to bypass the Academy and publish two manuscripts privately with the help of his friend Auguste Chevalier. One of these papers⁠—”On the Conditions for Solubility of Equations by Radicals”⁠—was a revision of the paper that the Academy had balked at; the other was a new work on “Primitive Equations Solvable by Radicals”, which Galois (now particularly adept at anything having to do with radicals) seems to have undertaken starting in the summer of 1830. Before he got out of prison, he had put considerable energy into drafting a preface to the two works. This was a five-page manifesto dripping with vitriol and heavy-handed sarcasm, along with self-congratulatory sentiments about his own independence from the Academy and “how lowly I esteem my adversaries”. Among other things, he was still stewing over his lost paper. “I must mention how manuscripts most often end up lost in the folders of Messrs. the members of the Institute, though truly I can’t conceive of such carelessness on the part of men who have the death of Abel on their conscience.” He snarkily outlines how he could have clogged his manuscript with useless details to make it easier to understand and appease his reviewers’ request for additional information. He witheringly suggests that the examiners in charge of testing candidates for the Polytechnique deserve to be members of the Academy, given that “they certainly have no place in posterity”. He presents himself as a martyr figure, “knowingly exposing myself to the mockery of dunces”, and even obliquely lambastes the Academy for favouring applied over pure mathematics. Clearly, prison had done nothing to temper either Galois’s simmering rage or his self-esteem⁠—nor, for that matter, his politics. In spite of this, Galois was released from prison in March, before he had completed either his manuscripts or his sentence. The authorities sent him to a small halfway house run by a man named Denis Faultrier to recover his broken health. The transfer dramatically changed things. Back in prison, Galois had told Raspail that the one thing he truly lacked was someone he could love “with his heart alone”. Now that lack was about to be rectified⁠—but it would not end well. &#39;Évariste&#39; superimposed on &#39;Stéphanie&#39; at top left, and E and S monograms bottom left After his transfer, Galois became acquainted with a woman named Stéphanie, and reacted to her rather as he had to his first encounter with mathematics. It turned out that Galois fell in love as wildly as he did everything else. Even as he went over old papers, presumably still working on his pair of manuscripts, he doodled Stéphanie’s name in the margins. On one page, he superimposed the names “Évariste” and “Stéphanie”; on another, he sketched out some rather elegant monograms combining the initials “E” and “S”. Clearly, Galois had it bad. The object of his fixation was almost certainly Stéphanie Félicité Poterin du Motel, a cousin of Denis Faultrier’s. Nearly 20 years old, she did not seem to reciprocate Galois’s affections. On 14 May 1832, she wrote to him: “Let us please make an end of this matter. I do not have sufficient wit to follow a correspondence of this type, but I will try to have enough to converse with you as I did before anything happened… no longer think about things that could not exist and that never will exist.” Another excerpt was even more crushing: “…be persuaded, Sir, it would doubtless never have been more; you are assuming wrongly and your regrets are ill-founded.” She ruled out even the possibility of a friendship, telling Galois that he was wrong to believe that men and women could ever be true friends. A heartbroken Galois swirled downwards into an emotional whirlpool. His friend Auguste Chevalier grew alarmed, thinking that Galois was revelling in his own misery; Chevalier accused him of “being drunk on the putrefied muck of a rotten world infecting [his] heart, [his] mind, and [his] hands.” In a consoling letter, Chevalier apparently urged him, not for the first time, to seek refuge in religion. On 25 May 1832, Galois wrote back. “[H]ow can one destroy the traces of emotions as violent as those I have passed through? How can one console oneself for having exhausted in one month the greatest source of bliss available to man, to have exhausted it without bliss, without hope, certain one has drained it dry for life? […] for your part, you feel obliged to do your best to convert me. But it is my duty to warn you, as I’ve done a hundred times, that your efforts are in vain. […] I’m disenchanted with everything, even the love of glory.” Still, this all-encompassing existential loathing did not stop Galois from looking forward to reuniting with Chevalier in a few days. But the trip never happened. What actually happened in the next four days is unknown, but the night of the 29th found Galois desperately dashing off additional letters instead of sleeping. One was addressed to a pair of his republican friends: “I have been challenged [to a duel] by two patriots…it was impossible for me to refuse.” Officially, duelling was illegal at the time, but Galois was right. Like most of the rest of Europe, plus the New World (hello Mr. Hamilton, Mr. Burr), France was in the grip of a duelling frenzy. If anything, the historical reality makes the way the Three Musketeers duel at the drop of a hankie seem downright restrained. More than 200 people died in duels in France between 1826 and 1834 alone. Alexandre Dumas knew this firsthand: he had won his first duel when he was 22 (though his trousers had fallen down in the process). Politicians blew out the brains of other politicians, journalists from opposing sides of the political divide killed each other with pistol or sword, authors and literary critics could be found on the fighting fields, young men took aim at each other over women, professional killers went around challenging people for jostling them in the street. In Bordeaux, a duelling club was founded whose members swore to only ever fight to the death: the association lasted three years, until a newcomer systematically killed all twelve surviving members. In 1837, two law professors took up swords over whether a certain passage in the 6th-century Digest of Justinian should end with a colon or a semicolon. (Professor Semicolon won by sticking three inches of steel into Professor Colon’s arm.) The poet Lamartine, who ended up in a duel in 1825 after one of his poems included a mildly uncomplimentary line about Italy, summed up the ethos by remarking that “It takes more courage to refuse one duel than to fight ten”. In this atmosphere, there was never any question as to whether Galois would accept the challenge: any young man refusing would forever be branded a coward, and a fellow of above-average hotheadedness was unlikely to consider the possibility anyway. But Galois did not think he was fighting a duel for a particularly glorious cause. In a letter he addressed to “all republicans”, he wrote: A page from Galois&#39;s memoir/article, with &#34;I don&#39;t have time&#34; scrawled in the margin “I die the victim of a shameless flirt and her two dupes. My life is being extinguished in a miserable bit of gossip. Oh! Why die for something so small⁠—die for something so contemptible!” He closed by describing himself in Latin: “Nitens lux, horrenda procella, tenebris æternis involuta”, which translates to “a brilliant light, swallowed by an awful tempest, wrapped in eternal darkness.” Id est, the turducken of melodrama. As Samuel Johnson said, the certainty of being hanged in the morning concentrates the mind wonderfully. The possibility of being shot did precisely the same to Galois. A few days earlier, he had been doubting his ability to ever do math again; now he began a letter to Auguste Chevalier. “My dear Friend,” he wrote, “I’ve done several new things in analysis.” Then, in small, neat handwriting, over seven pages, he frantically set about getting down as many of his original ideas as he could. These provided enough material, Galois claimed, for three manuscripts. The first was the one the Academy had shrugged at; Galois insisted he stood by it, with only a small number of corrections. He then sketched out the most important parts of the other two manuscripts that he envisioned. Only at the end of the letter did some sense of finality come into play: “You know, my dear Auguste, that these are not the only topics that I have explored […] But I don’t have time and my ideas are not yet very well developed in this immense area.” He concluded by asking Chevalier to publish the entire text of the letter itself in the Revue encyclopédique, and to publicly ask mathematicians Carl Gustav Jacob Jacobi or Carl Friedrich Gauss, or both, “to give their opinion, not on the truth, but on the importance of these theorems”. He also defended himself against the risk of again being accused of cursory mathematical argumentation: “In my life, I’ve often taken the risk of advancing propositions about which I wasn’t certain. But everything I’ve written here has been in my mind for almost a year, and it is too much in my interest not to make mistakes for anyone to suspect that I’ve here formulated theorems for which I don’t have complete demonstrations.” And yet, on the night of the 29th, Galois also revisited the copy of the manuscript that the Academy had returned to him. At one point, he scrawled in a margin, “There is something to be completed in this proof. I don’t have time”⁠—a poignant admission that reviewers’ calls for greater clarity had not been entirely undue. At the scheduled time, on the outskirts of Paris, with a handful of witnesses, Galois and his challenger chose their pistols. It is possible that only one of them was loaded⁠—an uncommon but not unknown option that added an element of Russian roulette to the mix. The two men walked to twenty-five paces, turned to face one another, and shot. Hit in the abdomen, Galois dropped to the ground. Someone⁠—possibly an onlooker, possibly a passerby⁠—transported him to the nearby Cochin Hospital. Conscious, but badly wounded, Galois lay in a room with four or five other patients. The only family member to have heard about the duel was his younger brother Alfred, who raced to the hospital and soon became despondent. Galois’s wound was not only severe, but also oddly positioned. It was as if he had not tried to minimise his chances of being shot; he might not even have turned to the side as he faced his foe as was customary. The injury was extensive, and already infected. The hospital surgeon and both brothers all knew that there was little to be done. Alfred was in tears, but Galois stoically instructed his brother, “Don’t cry. I need all of my courage to die at the age of twenty.” He characteristically rejected an attempt to have a priest attend to him, then, at 10:00 a.m. on 31 May 1832, Évariste Galois died in his brother’s arms. Pistol duel in the XIX century by Bauce and Rouget Legend has long held that a large crowd attended Galois’s funeral that Saturday. The Prefect of Police claimed much later in his memoirs that some two to three thousand people were there. However, on the day itself, the same man’s report to the Minister of the Interior stated that the actual number was around 150, mostly other republicans. They set off at 11:30 a.m. on 2 June to accompany Évariste Galois from Cochin Hospital to the Montparnasse graveyard. After some good old fiery political speeches, Galois’s body became the 18th of 21 to be placed in a common grave⁠—after which the attendees passed the hat around to pay for the funeral costs. Perhaps more than 150 had intended to be there. Paris was (again) on the verge of insurrection, and the republicans were looking for an excuse to gather and start a riot. Galois’s funeral might have fit the bill perfectly⁠—except that the day before saw the death of General Jean Maximilien Lamarque. Lamarque was a prominent advocate for the wretched, miserable poor⁠—who, despite what certain modern stage productions suggest, were not inclined to be tuneful about their state. The republicans quickly realised that Lamarque’s funeral would be the higher-profile event, and rescheduled the rebellion. Thus, Évariste Galois was once again shoved aside and left out, even in death. His duel deprived him by just a few days of the opportunity to die a glorious republican martyr in the ill-fated 1832 June Rebellion. His friends of the Société des amis du peuple would appear thinly disguised in one of the best-selling novels of all time, but there are no mathematicians among the students in Victor Hugo’s Les Misérables. Adding insult to fatal injury, among those who fought heroically on the June barricades, and survived, was a certain Étienne-François Pecheux d’Herbenville⁠—who most likely fired the bullet that prevented Galois from ever getting to join a revolution. The identity of Galois’s opponent has long been one of the many mysteries surrounding the duel. Alexandre Dumas specifically named him as d’Herbenville, but Dumas is not infallible, and other evidence seemed to suggest otherwise. Two days after the duel, a newspaper from the town of Lyon, Le Précurseur, reported on the duel, describing the victor as “one of [Galois’s] old friends, like him a very young man, like him a member of the Société des amis du peuple, and who had … also been a figure in a political trial”. The article attributed the duel to a romantic argument, and identified Galois’s foe by the initials “L.D.” The Précurseur article is full of small inaccuracies, but even taking these into account, it is tricky to make their depiction match Dumas’s identification. On the one hand, d’Herbenville⁠—a charming young man who liked to wrap his cartridges in pink silk paper⁠—was indeed a republican. In fact, he was one of the nineteen republicans whose acquittal was celebrated with the notorious banquet that ended with Galois’s drunken oath and Dumas’s autodefenestration. But beyond this there was no evidence of any connection between the two, let alone of old friendship. Given that Galois did not have many friends, other candidates have been sought. Mario Livio, among others, has argued for Galois’s friend and fellow prisoner Ernest Duchâtelet, who has the advantage of both being a friend and having at least one of the right initials. Recent discoveries, however, make it almost certain that Dumas was right after all. One key piece of evidence is a copy of France’s 1791 revolutionary constitution. Among its owners was a Swiss medical student named Larguier, who wrote on it, “This manuscript was given to me by Gallois killed in a duel by Pécheux d’Herbinville”. Meanwhile, Olivier Courcelle’s research has discovered that d’Herbenville’s names were rarely spelt the same way twice in the press. Among the variations were forms such as “Lepescheux” and “Dherbinville”⁠—these would give us the “L. D.” reported by Le Précurseur. Most tellingly of all, recent close analysis of Galois’s manuscripts has shown that, though crossed out, d’Herbenville’s name appears in Galois’s papers, proving that the two were indeed connected somehow. How, and as of when, is unknown, but Courcelle has discovered that d’Herbenville took classes at Louis-le-Grand at the same time that Galois resided at the school. Moreover, d’Herbenville studied mathematics at school before turning his attention elsewhere, and eventually became an engineer. In d’Herbenville, Galois would have found a radical republican he could talk shop with. But knowing the identity of Galois’s killer does not solve the greater mystery of why the duel was fought at all, nor explain its oddities. The confusion over who took Galois to the hospital suggests that he went into the duel without any witnesses of his own⁠—a baffling choice given that one of the witnesses’ duties was to ensure prompt medical care for the wounded. Between this and Galois’s uncommon injury, several commentators have suggested that Galois went into the duel intending to die. One theory even holds that he was sacrificing himself for his political cause. But this is difficult to reconcile with his open letter to “all republicans”, in which he begs “my friends the patriots not to reproach me for dying for something other than the country”. Instead, he attributes the death he foresees for himself to “a miserable bit of gossip”. Moreover, one of Galois’s last-minute letters suggests that the challenger(s) “charged me on my honour not to inform any patriot” (italics in original). Galois’s devastated brother Alfred, meanwhile, thought Galois had been shot by secret police agents acting on behalf of the king. This is profoundly unlikely. Even if Louis-Philippe’s regime had been prone to assassinating people⁠—which it does not appear to have been⁠—Galois was simply not important enough to warrant eliminating, particularly in such a convoluted manner. A French postage stamp commemorating Évariste Galois More likely, it was simply a ‘matter of honour’⁠—and the generally accepted explanation is that the honour in question was Stéphanie du Motel’s. Most commentators identify the ‘shameless flirt’ of Galois’s letter as Stéphanie, though whether she deserved that appellation is entirely unknowable⁠—as is whether the challengers were in any way ‘dupes’. Quite possibly, Galois had simply importuned her so much that she had no option but to ask for assistance. Certainly, Galois indicates that he insulted his challengers, and did so to their faces: he wrote in his open letter that he “repent[s] having spoken a fateful truth to men who were so poorly prepared to hear it coolly”. With Galois’s temper, it seems unlikely that any of it was done coolly. As research continues into Galois’s papers and what he crossed out where, answers may yet come to light. Recent findings have confirmed that there was a police report about the duel; perhaps it will be found in an archive somewhere. Alfred Galois and Auguste Chevalier were left with the weighty task of doing justice to Évariste Galois’s mathematical legacy, and it took time. It was not until 1843 that Galois’s luck changed, when his work reached French mathematician (and member of the Academy) Joseph Liouville. As he worked through Galois’s papers, Liouville noted their brevity and relative opacity, but realised that what Galois had proposed years before was both mathematically rigorous and far ahead of its time. In 1846, Liouville published Galois’s “ingenious and profound” work in his own internationally renowned journal, and thus it reached the wider mathematical community at last. Within 15 years, what came to be known as Galois theory was being taught in algebra classes. Historian Amir Alexander says that Galois “had become an iconic figure of the field, a revered martyr to mathematics”. It is a curious fact that what seems at first glance to be nothing but “pure” mathematics often later turns out to have important applied uses. While the Academy of Sciences could not see the practical side at the time, Galois’s new ways of approaching symmetries, permutations, and groups turned out to apply to, well, basically everything. Subtle symmetry appears to play a profound, central role in the laws of physics as we understand them. It applies just as well to the miniscule (particle physics) as to the humongous (cosmology), and is scattered throughout just about everything in nature that shows organised behaviour. One can only imagine what Galois might have been able to contribute with more than just a few years of research. The story of Évariste Galois⁠—a revolutionary in every sense⁠—has become something of a legend in the last 150 years, not least because of the dual figure he presents as mathematical visionary and political lightning-rod. Early obituaries all focused on him as a republican. As early as 1846, however, Liouville could dismiss Galois’s political activities as nothing more than “a pity”, and for several decades this was the common verdict. Neither of these is the full story. Galois’s mathematical thought and his political thinking are deeply intertwined. In one of his draft papers, an equation that cannot be broken down further leads him to write the word “Indivisible”, and beneath that, “Indivisibility of the republic”, followed on a new line by “Liberty, equality, fraternity, or death”. Among the scrawls on the same page are the words “Une femme” (“a woman”)⁠—and, deeply scrawled out and now visible only with specialised equipment, the name of Pecheux d’Herbenville. Galois closed his final mathematical statement with the bitter hope that after Jacobi and Gauss had given their opinion on the importance of his theorems, “there will be, I hope, some people who will find it to their advantage to decipher all this mess.” There were, and they did. </description>
      <pubDate>29 Mar 20 16:28 EDT</pubDate>
      <guid>https://www.damninteresting.com/radical-solutions/</guid>
    </item>
    <item>
      <title>You Don’t Need All That Complex/Expensive/Distracting Infrastructure</title>
      <link>https://blog.usejournal.com/you-dont-need-all-that-complex-expensive-distracting-infrastructure-a70dbe0dbccb</link>
      <description>&lt;a href=&#34;https://blog.usejournal.com/you-dont-need-all-that-complex-expensive-distracting-infrastructure-a70dbe0dbccb&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Infrastructure for your next side-project?How do you run your web products?Kubernetes clusters? Load balancers across multiple cloud regions? Zero-click, Blue-Green deployment based on real-time metrics from a continuous deployment pipeline?It’s all really cool. And if you spend much time on Hacker News, or reading the latest treatises from FAANG DevOps wizards, you’re convinced it’s all absolutely essential for your site. But it’s all a nightmare. Setting it all up is a complicated time-sink. And if you don’t want to roll it all yourself, it’s alarmingly expensive. Even then, it’s still a nightmare, you’ve just paid for the privilege of this particular nightmare.The good news? You don’t need any of it*.(* well you need some of it … but so much less than you think)On Twitter Pieter Levels — multiple Product Hunt Maker of the Year winner, founder of a growing list of sites generating thousands a month in revenue and basically the god of the “Indie Maker” community — was asked what kind of infrastructure he used to host all of his wildly successful indie projects?.The answer? Simple … a single Linode VPS.https://twitter.com/levelsio/status/1101581928489078784Now I’m not saying I know exactly what infrastructure Pieter has in place. But it’s a pretty firm guess that there’s no complicated auto-scaling groups, no fleet of expensive cloud machines to pay for, no complex Kubernetes clusters.Want to read this story later? Save it in Heyday.Your goal, when launching a product, is to build a product that solves a problem for your users. Not build the fanciest deployment pipelines, or multi-zone, multi-region, multi-cloud Nuclear Winter proof high availability setup. I’ve seen the idea that every minute spent on infrastructure is a minute less spent shipping features. Now obviously that’s not entirely true; you can’t ship features unless you have a server, or some way to get your features onto your server (and preferably keep them there, for a bit at least). But it’s not a bad mindset. You need the most simple, least time consuming infrastructure that gets you to that point.When I was building my side project Curated Design Boutique, I spent hours setting up a simple and free continuous deployment pipeline. Commit to git, Semaphore CI builds the Docker image, uploads the image to a container registry, then pulls &amp; restarts the container on my server. It’s really cool. I felt like a wizard once I got it working … and it was free! I actually sat down here to write all about how to do it yourself, until I realised how long I spent on it and how much value it delivered to my users (and as a result: how much money it put into my bank account) — absolutely nothing. Sure, automatically building &amp; uploading to a registry? Great! Saves me a lot of repetitive error prone tagging &amp; typing. But the really time consuming, fiddly &amp; unreliable part of the system was the automatic deployment. Manually running the `docker-compose` script on my server costs nothing (and I’m paying attention when I do it, so I don’t need the extra automatic monitoring infrastructure to tell me when I’ve screwed it all up).Your users to don’t care how your code gets onto your server. 99.9% of the time they don’t care about your fancy high availability setup either. Obviously if you’re FAANG-level or some established site where that 0.1% downtime translates into vast quantities of cash disappearing from your books, this stuff is all great. You have the funds to do things “right”. Your fancy zero-click continuous deployment system saves you thousands/millions a year. But at indie maker, “Hey look at this cool thing I built … please, please someone look at it (and upvote it on Product Hunt too, thx)” scale — the scale of almost every single site on the net — that 0.1% is vanishingly insignificant.‘Engineers get sidetracked by things that make engineers excited, not that solve real problems for users’ — we’ve heard it all before. There’s no shocking revelation here (you are on Medium.com after all …). But there seems to be a pervasive sense that you can’t launch a product without a K8s cluster or two, load balanced across a couple of regions, oh and if you have to deploy anything manually how can you possibly expect to turn a profit?So, finally getting that next Unicorn idea down into code? You don’t need all of that infrastructure you have planned out.</description>
      <pubDate>11 Mar 21 09:36 EST</pubDate>
      <guid>https://blog.usejournal.com/you-dont-need-all-that-complex-expensive-distracting-infrastructure-a70dbe0dbccb</guid>
    </item>
    <item>
      <title>MapFilterFold&#xA;Meta recommendations from HN</title>
      <link>https://mapfilterfold.com/</link>
      <description>&lt;a href=&#34;https://mapfilterfold.com/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; For early stage founders Communication soft skills Oditties and fun Up next: systems thinking hits NonfictionFictionBoth 84 hits Thinking, Fast and Slow by Daniel Kahneman IanCal: A stunningly good book about cognitive biases, with fairly understated claims and backed up with studies. Excellent advice for life and it&#39;s changed how I view decisions and interactions. read comments in: nonfiction | psychology | business | economics | mental models 78 hits How to Win Friends and Influence People by Dale Carnegie beat: Not creepy at all, despite how the title sounds in today&#39;s language. This book is the bible of how to get along with others. It&#39;s been in continuous print since before WWII, for good reason. read comments in: nonfiction | business | psychology | leadership | communication | management | self improvement 74 hits Sapiens: A Brief History of Humankind by Yuval Noah Harari air7: Hands down the book that most influenced me. The book had (for me) not one but several simple-yet-profound ideas that were forever inserted into the foreground of how I make sense of the world. read comments in: nonfiction | history | philosophy | anthropology 68 hits Gödel, Escher, Bach by Douglas R. Hofstadter mck-: What a unique masterpiece. Covers a wide range fascinating concepts through the three geniuses in Math, Art, and Music... read comments in: nonfiction | philosophy | math | mental models 62 hits Meditations by Marcus Aurelius jonathansorum: Meditations easily has my highest rate of highlighted words in relation to total book length. Seems like every page (almost) has some eternal and profound in it. read comments in: nonfiction | philosophy | history | spirituality | psychology | leadership | self improvement 60 hits The Selfish Gene by Richard Dawkins sorenn111: The Selfish Gene has been the most influential book on my life. Especially when Dawkins makes the point about pre-darwininan philosophy needing rethinking. His point being... read comments in: nonfiction | biology | evolution | philosophy 51 hits The Pragmatic Programmer by Andy Hunt BFatts: A fantastic language-agnostic manual that still applies heavily today. read comments in: nonfiction | programming | computer science 47 hits Zen and the Art of Motorcycle Maintenance by Robert M. Pirsig Roelven: It has shaped my thinking on &#39;what is good&#39; or &#39;what does quality&#39; mean. As an engineer it is easy to appreciate the author slowly going insane about the details he keeps coming back to... read comments in: fiction | philosophy | spirituality 46 hits The Design of Everyday Things by Donald A. Norman davidgh: A masterpiece. The age of the book proves it. It is as relevant today as it was when written 30 years ago. The only downside to the book is it will ruin every elevator, door handle and... read comments in: nonfiction | design | business | psychology 45 hits Zero to One: Notes on Startups, or How to Build the Future by Peter Thiel shawn: I think it&#39;s a good one because it&#39;s a mix of analysis and history. Thiel had a unique vantage point, and he shares it well. It also challenges you to be ambitious, which is becoming a rare sentiment. read comments in: nonfiction | business | entrepreneurship 42 hits Antifragile: Things That Gain from Disorder by Nassim Nicholas Taleb jurgenwerk: Man, reading this book really put a fire under my ass. I realized how much more I could be getting out of life by pursuing optionality and using the barbell strategy. read comments in: nonfiction | philosophy | business | economics | psychology 42 hits Man&#39;s Search for Meaning by Viktor E. Frankl bradbatt: Amazingly powerful read. It is simultaneously completely saddening to read what some humans are capable of doing to others, but also inspiring to see those who were victims of the holocaust... read comments in: nonfiction | psychology | philosophy | history | memoir 40 hits Deep Work: Rules for Focused Success in a Distracted World by Cal Newport jacobkg: Starts with the thesis that a generation of workers have forgotten how to concentrate on mentally challenging tasks. Full of ideas and inspiration for rebuilding your stamina for intense focused thought. read comments in: nonfiction | business | productivity | psychology | self improvement 39 hits Structure and Interpretation of Computer Programs by Harold Abelson throwaway124567: Very good. It was MITs old CS textbook, it’s still highly relevant. It takes a while to get through and you probably would get the most value out of it if you already have a lot of programming experience. read comments in: nonfiction | programming | computer science 37 hits The Mythical Man-Month: Essays on Software Engineering by Frederick P. Brooks Jr. ereyes01: One of the most important books ever written on software engineering practice. Author Frederick Brooks won the Turing Award for this book and for his work on IBM&#39;s System/360... read comments in: nonfiction | programming | business | management | computer science 36 hits Siddhartha by Hermann Hesse acrodrig: I think it&#39;s the closest I have come to understanding &#34;enlightenment&#34; (whatever it may mean for each person). Give it a try. read comments in: fiction | philosophy PreviousNext Page123…189 </description>
      <pubDate>27 Mar 20 14:57 EDT</pubDate>
      <guid>https://mapfilterfold.com/</guid>
    </item>
    <item>
      <title>Some advice on writing well for NLP</title>
      <link>https://users.umiacs.umd.edu/~resnik/writing_advice.html</link>
      <description>&lt;a href=&#34;https://users.umiacs.umd.edu/~resnik/writing_advice.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; My former grad student Chris Dyer wrote to me recently to ask if I could remind him of some of the useful editorial advice I&#39;d given him while he was writing his dissertation. Made me feel all warm and fuzzy to think it was valuable enough that he wants to pass it on to his own students. A lot of my process for helping students improve their writing happens in the moment, very much on a case by case basis. But here are a few principles that I think are worth noting. Strong writing Feeling/thinking verbs. Avoid &#34;we think&#34;, &#34;we believe&#34;, etc. If you&#39;re putting it in your paper, it&#39;s because you believe it or think it&#39;s true, and these do nothing but weaken or hedge. &#34;Present&#34; verbs. If I present an algorithm to you, am I presenting someone else&#39;s work that existed before, or my own novel contribution? Avoid wording that is ambiguous and aim for strong verbs that emphasize your particular contribution. Using strong verbs. For algorithms, models, etc., it&#39;s must stronger to introduce something new than simply to propose it, although propose is good early in the paper for a hypothesis that you then support with results, allowing you to claim that you have validated, verified, or demonstrated. In general for results, it&#39;s strong to demonstrate and show, although there are also appropriate places for, say, having found something to be true or, in the context of something more exploratory or inductive, having seen some behavior or pattern. Unless it&#39;s an actual proof, avoid prove, and unless you&#39;re Columbus, avoid transitive discover NP, although with a sentential complement discovered that [clause] is similar to having seen. Passive voice. There is nothing the least bit wrong with passive voice when it is used appropriately. (For example, there is no reason whatsoever for me to modify the previous sentence to say &#34;when one uses it appropriately&#34;.) For chapter and verse on this, see Pullum, &#34;Confusion over avoiding the passive&#34;, http://www.lel.ed.ac.uk/grammar/passives.html; it&#39;s a must-read. Academic &#34;we&#34;. This is somewhere between a matter of taste and a religious issue, so your mileage may vary. However, if you&#39;re doing a practice presentation or defense (or sometimes even the real thing) and a certain colleague of mine is in the audience, and you use &#34;we&#34;, you can expect a question about which pieces of the contribution are yours and which should be attributed to your advisor. Personally, I prefer &#34;I&#34; for dissertations and in single-authored papers I tend to avoid the issue when possible by using alternative phrasing, e.g. passive (&#34;a corpus of 200M words was obtained by...&#34;), non-animate subjects (&#34;The results of Experiment 1 demonstrate...&#34;), nominalizations (&#34;After sentence-breaking and tokenization...&#34;), etc. That said, I do think it&#39;s fine to use an inclusive &#34;we&#34; to provide an informal tone that brings together author and audience, e.g. &#34;When we take a look at the output of Algorithm 1, ...&#34;. (Notice that if past tense took had been used instead of present tense take, this would have been an academic rather than inclusive &#34;we&#34;.) The &#34;story&#34; in a paper should be organized logically, not chronologically. Nobody needs to know that you actually executed Experiment 1 a month after Experiment 3. The logic of the argument in the paper should dictate the structure. There are exceptions, e.g. perhaps analysis of Experiment 2 led to some new or expanded ideas that were then tested in Experiment 3, but notice that in this case the logical progression and the chronological progression coincide. Nobody cares about debugging or implementation. Implementation details belong in documentation or, if they&#39;re really salient for the paper, in an appendix. Unless the paper is about data structures, programming language choice, etc., go with Marr&#39;s computational or algorithmic levels in your description, not the physical/implementation level. Be generous in your citations. People who have done related work might well be your reviewers. Plus it&#39;s the right thing to do. &#39;Nuff said. Eschew obfuscation. Yes, you can save a whole lot of space by condensing a ton in between \begin{algorithm} and \end{algorithm}. But not everyone (read: not every reviewer) enjoys having to work through the line-by-line details of an algorithm. Make sure the text has plenty of plain language and be generous with your prose explanation of what&#39;s going on. And try to avoid any Greek letters that people might not know how to pronounce. Be explicit about having separated training and test data. This is a pet peeve of mine. Yes, everyone is supposed to remember this. But make sure your description makes it clear that you did. Explain why you chose the parameters you chose. You used 50 topics for LDA? Why not 20 or 100? Oh, and if the answer is not that you either decided a priori or tuned on held-out data, but rather that it gave you the best results on your test data, you&#39;d better see the previous point: you are reporting a tainted experiment. You can un-taint it somewhat by reporting the results for the other values you tried also -- but then you should be prepared for a reviewer to ask why we should believe 50 will be the best value on the next, previously unseen dataset. (Better yet, do the next experiment fixing 50 in advance and show that the choice generalized to another case.) If all else fails, appeal to previous literature and choose parameter values that can be described as typical in prior work. At this point I realized I was veering into &#34;grumpy old man&#34; territory and decided to stop... </description>
      <pubDate>29 Nov 20 11:11 EST</pubDate>
      <guid>https://users.umiacs.umd.edu/~resnik/writing_advice.html</guid>
    </item>
    <item>
      <title>Tutorial #2: few-shot learning and meta-learning I</title>
      <link>https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/</link>
      <description>&lt;a href=&#34;https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Introduction Humans can recognize new object classes from very few instances. However, most machine learning techniques require thousands of examples to achieve similar performance. The goal of few-shot learning is to classify new data having seen only a few training examples. In the extreme, there might only be a single example of each class (one shot learning). In practice, few-shot learning is useful when training examples are hard to find (e.g., cases of a rare disease), or where the cost of labelling data is high. Few-shot learning is usually studied using N-way-K-shot classification. Here, we aim to discriminate between $N$ classes with $K$ examples of each. A typical problem size might be to discriminate between $N=10$ classes with only $K=5$ samples from each to train from. We cannot train a classifier using conventional methods here; any modern classification algorithm will depend on far more parameters than there are training examples, and will generalize poorly. If the data is insufficient to constrain the problem, then one possible solution is to gain experience from other similar problems. To this end, most approaches characterize few-shot learning as a meta-learning problem. The meta learning framework In the classical learning framework, we learn a how to classify from training data and evaluate the results using test data. In the meta-learning framework, we learn how to learn to classify given a set of training tasks and evaluate using a set of test tasks (figure 1); In other words, we use one set of classification problems to help solve other unrelated sets. Figure 1. Meta-learning framework. An algorithm is trained using a series of training tasks. Here, each task is a 3-way-2-shot classification problem because each training task contains a support set with three different classes and two examples of each. During training the cost function assesses performance on the query set for each task in turn given the respective support set. At test time, we use a completely different set of tasks, and evaluate performance on the query set, given the support set. Note that there is no overlap between the classes in the two training tasks {cat, lamb, pig}, {dog, shark, lion} and between those in the test task {duck, dolphin, hen}, so the algorithm must learn to classify image classes in general rather than any particular set. Here, each task mimics the few-shot scenario, so for N-way-K-shot classification, each task includes $N$ classes with $K$ examples of each. These are known as the support set for the task and are used for learning how to solve this task. In addition, there are further examples of the same classes, known as a query set, which are used to evaluating the performance on this task. Each task can be completely non-overlapping; we may never see the classes from one task in any of the others. The idea is that the system repeatedly sees instances (tasks) during training that match the structure of the final few-shot task, but contain different classes. At each step of meta-learning, we update the model parameters based on a randomly selected training task. The loss function is determined by the classification performance on the query set of this training task, based on knowledge gained from its support set. Since the network is presented with a different task at each time step, it must learn how to discriminate data classes in general, rather than a particular subset of classes. To evaluate few-shot performance, we use a set of test tasks. Each contains only unseen classes that were not in any of the training tasks. For each, we measure performance on the query set based on knowledge of their support set. Approaches to meta-learning Approaches to meta-learning are diverse and there is no consensus on the best approach. However, there are three distinct families, each of which exploits a different type of prior knowledge: Prior knowledge about similarity: We learn embeddings in training tasks that tend to separate different classes even when they are unseen. Prior knowledge about learning: We use prior knowledge to constrain the learning algorithm to choose parameters that generalize well from few examples. Prior knowledge of data: We exploit prior knowledge about the structure and variability of the data and this allows us to learn viable models from few examples. An overview these methods can be seen in figure 2. In this review, we will consider each family of methods in turn.  Figure 2. Few-shot learning methods can be divided into three families. The first family learns prior knowledge about the similarity and dissimilarity of classes (in the form of embeddings) from training tasks. The second family exploits prior knowledge about how to learn that it has garnered from training tasks. The third family exploits prior knowledge about the data and its likely variation that is has learned from training tasks. Prior knowledge of similarity This family of algorithms aims to learn compact representations (embeddings) in which the data vector is mostly unaffected by intra-class variations but retains information about class membership. Early work focused on pairwise comparators which aim to judge whether two data examples are from the same or different classes, even though the system may not have seen these classes before. Subsequent research focused on multi-class comparators which allow assignment of new examples to one of several classes. Pairwise comparators Pairwise comparators take two examples and classify them as either belonging to the same or different classes. This differs from the standard N-way-K-shot configuration and does not obviously map onto the above description of meta-learning although as we will see later there is in fact a close relationship. Siamese networks Koch et al. (2015) trained a model that outputs the probability $Pr(y_a=y_{b})$ that two data examples $\mathbf{x}_{a}$ and $\mathbf{x}_{b}$ belong to the same class (figure 3a). The two examples are passed through identical multi-layer neural networks (hence Siamese) to create two embeddings. The component-wise absolute distance between the embeddings is computed and passed to a subsequent comparison network that reduces this distance vector to a single number. This is passed though a sigmoidal output for classification as being the same or different with a cross-entropy loss. Figure 3. Pairwise comparators. a) Siamese networks take two examples $\mathbf{x}_{a}$ and $\mathbf{x}_{b}$ and return the probability $Pr(y_{a}=y_{b})$ that they are the same class. They do this by passing each example through an identical network (hence Siamese) and then using the pairwise difference between the embeddings as the basis of the decision. b) Triplet networks take two examples of the same class $\mathbf{x}_{a}$ and $\mathbf{x}_{+}$ and one of a different class $\mathbf{x}_{-}$ and pass all three through identical networks to create three embeddings. The triplet loss encourages the embeddings of examples from the same class to be closer together than those from different classes. c) In the test phase for triplet networks, we pass two examples $\mathbf{x}_{a}$ and $\mathbf{x}_{b}$ through the same network and judge whether they come from the same class or not based on the distance. During training, each pair of examples are randomly drawn from a super-set of training classes. Hence, the system learns to discriminate between classes is general, rather than two classes in particular. In testing, completely different classes are used. Although this does not have the formal structure of the N-way-K-shot task, the spirit is similar. Triplet networks Triplet networks (Hoffer &amp; Ailon 2015) consist of three identical networks that are trained by triplets $\{\mathbf{x}_{+},\mathbf{x}_{a},\mathbf{x}_{-}\}$ of the form (positive, anchor, negative). The positive and anchor samples are from the same class, whereas the negative sample is from a different class. The learning criterion is triplet loss which encourages the anchor to be closer to the positive example than it is to the negative example in the embedding space (figure 3b). Hence it is based on two pairwise comparisons. After training, the system can take two examples and establish whether they are from the same or different classes, by thresholding the distance in the learned embedding space. This was employed in the context of face verification by Schroff et al. (2015). This line of work is part of a greater literature on learning distance metrics (see Suarez et al. 2018 for overview). Multi-class comparators Pairwise comparators can be adapted to the N-way-K-shot setting by assigning the class for an example in the query set based on its maximum similarity to one of the examples in the support set. However, multi-class comparators attempt to do the same thing in a more principled way; here the representation and final classification are learned in an end-to-end fashion. In this section, we&#39;ll use the notation $\mathbf{x}_{nk}$ to denote the $k$th support example from the $n$th class in the N-Way-K-Shot classification task, and $y_{nk}$ to denote the corresponding label. For simplicity, we&#39;ll assume there is a single query example $\hat{\mathbf{x}}$ and the goal is to predict the associated label $\hat{y}$. Matching Networks Matching networks (Vinyals et al. 2016) predict the one-hot encoded query-set label $\hat{\mathbf{y}}$ as a weighted sum of all of the one-hot encoded support-set labels $\{\mathbf{y}_{nk}\}_{n,k=1}^{NK}$. The weight is based on a computed similarity $a[\hat{\mathbf{x}},\mathbf{x}_{nk}]$ between the query-set data $\hat{\mathbf{x}}$ and each training example $\{\mathbf{x}_{nk}\}_{n,k=1}^{N,K}$. \begin{equation}     \hat{\mathbf{y}} = \sum_{n=1}^{N}\sum_{k=1}^{K} a[\mathbf{x}_{nk},\hat{\mathbf{x}}]\mathbf{y}_{nk} \tag{1.1} \end{equation} where the similarities have been constrained to be positive and sum to one.  To compute the similarity $a[\hat{\mathbf{x}},\mathbf{x}_{nk}]$, they pass each support example $\mathbf{x}_{nk}$ through a network $\mbox{ f}[\bullet]$ to produce an embedding and pass the query example $\hat{\mathbf{x}}$ through a different network $\mbox{ g}[\bullet]$ to produce a different embedding. They then compute the cosine similarity between these embeddings (figure 5a) \begin{equation}      d[\mathbf{x}_{nk}, \hat{\mathbf{x}}] = \frac{\mbox{ f}[\mathbf{x}_{nk}]^{T}\mbox{ g}[\hat{\mathbf{x}}]} {||\mbox{ f}[\mathbf{x}_{nk}]||\cdot||\mbox{ g}[\hat{\mathbf{x}}]||}, \tag{1.2} \end{equation} and normalise using a softmax function: \begin{equation}     a[\hat{\mathbf{x}}_{nk},\mathbf{x}] = \frac{\exp[d[\mathbf{x}_{nk},\hat{\mathbf{x}}]]}{\sum_{n=1}^{N}\sum_{k=1}^{K}\exp[d[\mathbf{x}_{nk},\hat{\mathbf{x}}]]}. \tag{1.3} \end{equation} to produce positive similarities that sum to one. This system can be trained end to end for the N-way-K-shot learning task.1 At each learning iteration, the system is presented with a training task; the predicted labels are computed for the query set (the calculation is based on the support set) and the loss function is the cross entropy of the ground truth and predicted labels. Matching networks compute similarities between the embeddings of each support example and the query example. This has the disadvantage that the algorithm is not robust to data imbalance; if there are more support examples for some classes than others (i.e., we have departed from the N-way-K-shot scenario), the ones with more frequent training data may dominate. Prototypical Networks Prototypical networks (Snell et al. 2017) are robust to data imbalance by construction; they average the embeddings $\{\mathbf{z}_{nk}\}_{k=1}^{K}$ of the examples for class $n$ to compute their mean embedding or prototype $\mathbf{p}_{n}$. They then use the similarity between each prototype and the query embedding (figures 4 and 5 b) as a basis for classification. Figure 4. Prototypical networks. The support examples $\mathbf{x}_{nk}$ are all mapped to the embedding space to create embedding $\mathbf{z}_{nk}$ (coloured circles). All of the embeddings for class $k$ are averaged to create a prototype $\mathbf{p}_{n}$. To classify query examples $\hat{\mathbf{x}}$, we first compute its embedding $\hat{\mathbf{z}}$ and then base the decision on the relative distance to the prototypes. The similarity is computed as a negative multiple of the Euclidean distance (so that larger distances now give smaller numbers). They pass these similarities to a softmax function to give a probability over classes. This model effectively learns a metric space where the average of a few examples of a class is a good representation of that class and class membership can be assigned based on distance. They noted that (i) the choice of distance function is vital as squared Euclidean distance outperformed cosine distance, (ii) having a higher number of classes in the support set helps to achieve better performance, and that (iii) the system works best when the support size of each class is matched in the training and test tasks. Ren et al. (2018) extended this system to take advantage of additional unlabeled data which might be from the test task classes or from other distractor classes. Oreshkin et al. (2018) extended this approach by learning a task-dependent metric on the feature space, so that the distance metric changes from place to place in the embedding space. Relation Networks Matching networks and prototypical networks both focus on learning the embedding and compare examples using a pre-defined metric (cosine and Euclidean distance, respectively). Relation networks (Santoro et al. 2016) also learn a metric for comparison of the embeddings (figure 5c). Similarly to prototypical networks, the relation network averages the embeddings of each class in the support set together to form a single prototype. Each prototype is then concatenated with the query embedding and passed to a relation module. This is a learnable non-linear operator that produces a similarity score between 0 and 1 where 1 indicates that the query example belongs to this class prototype. This approach is clean and elegant and can be trained end-to-end. Comparison between models All of the pairwise and multi-class comparators are closely related to one another. Each learns an embedding space for data examples. In matching networks, there are different embeddings for support and query examples, but in the other models, they are the same. For prototypical networks and relation networks, multiple embeddings from the same class are averaged to form prototypes. Distances between support set embeddings/prototypes and query set embeddings are computed using either pre-determined distance functions such as Euclidean or cosine distance (triplet networks, matching networks, prototypical networks) or by learning a distance metric (Siamese networks and relation networks). Figure 5. Multi-class comparators. a) Matching networks compute separate embeddings for support examples (here $\mathbf{x}_{11},\mathbf{x}_{12},\mathbf{x}_{21},\mathbf{x}_{22}$) and the query example $\hat{\mathbf{x}}$. Here $\mathbf{x}_{nk}$ is the $k$th example from the $n$th class. They compute the cosine similarity between each support embedding and the the query embedding, and then use these similarities to choose the class. This has the disadvantage that if there are many more examples of one class than the others, the relatively abundant class may be chosen too frequently. b) Prototypical networks embed the query and support examples using the same network, but average together support embeddings to make prototypes for each class, and so it doesn&#39;t matter if the numbers are unbalanced. The Euclidean distance between query embeddings and prototypes is used to support classification. c) Relation networks replace this Euclidean distance with a learned non-linear distance metric. The multi-class networks have the advantage that they can be trained end-to-end for the N-way-K-shot classification task. This is not true for the pairwise comparators which are trained to produce a similarity or distance between pairs of data examples (which could itself subsequently be used to support multi-class classification). Although it is not obvious how the pairwise comparators map to the meta-learning framework, it is possible to consider their data as consisting of minimal training and test tasks. For Siamese networks, each pair of examples is a training task, consisting of one support example and one query example, where their classes may not necessarily match. For triplet networks, there are two support examples (from different classes) and one query example (from one of the classes). Conclusion In part I of this tutorial we have described the few-shot and meta-learning problems and introduced a taxonomy of methods. We have also discussed methods that use a series of training tasks to learn prior knowledge about the similarity and dissimilarity of classes that can be exploited for future few-shot tasks. This knowledge takes the form of data embeddings that reduce within-class variance relative to between-class variance, and hence make it easier to learn from just a few data points. In part II of this tutorial, we&#39;ll discuss methods that incorporate prior knowledge about how to learn models, and that incorporate prior knowledge about the data itself. 1Vinyals et al. (2016). also introduced a novel context embedding method which took the full context of the support set $\mathcal{S}$ into account so that $\mbox{ g}[\bullet] = \mbox{ g}[\mathbf{x}, \mathcal{S}]$. Here, the support set was considered as a sequence and encoded by a bi-directional LSTM. Snell et al. (2017) later argued that this context embedding was problematic and redundant. </description>
      <pubDate>18 Jul 20 15:47 EDT</pubDate>
      <guid>https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/</guid>
    </item>
    <item>
      <title>The YCombinator Experience - Remote Edition</title>
      <link>https://magicbell.io/blog/the-ycombinator-experience-remote-edition</link>
      <description>&lt;a href=&#34;https://magicbell.io/blog/the-ycombinator-experience-remote-edition&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I have been an entrepreneur for 15 years now, building my third company - MagicBell. I bootstrapped the last one, SupportBee, to over half a million in ARR. However, I decided to trade it for a new company and follow the venture-backed route this time around. The first stop on this journey naturally seemed like YCombinator, and I am happy I applied and was accepted.In this post, I want to talk about my experience in the W21 batch that&#39;s still underway as I write this. TLDR; It&#39;s worth it - apply!Why I applied to YCombinatorWhile I bring a ton of technical and business experience to a new venture, the fact that I never worked in a FAANG or venture-backed business means I don&#39;t have the network to raise money. Some people around me felt that my experience bootstrapping my last business should be enough to close seed funding, but the only thing that helps close funding is the fear of missing out on your deal. I witnessed this first hand after we launched to #2 on Product Hunt. We got some good inbound investor interest, but I couldn&#39;t create the pressure needed to close funding - there was no forcing function like the demo day. We had already applied to YC by this time (and were invited for an interview), but this experience reaffirmed my desire to get into YC.Preconceived notions about YCombinatorYC has been around for a long time, and so have I. Naturally, I had some ideas about the program that weren&#39;t necessarily accurate. The biggest one was that Ycombinator is all about raising money, and if you are not in the mindset of &#39;raise as much as you can, as often as you can,&#39; you won&#39;t fit in. Even in the first few weeks in the program, it was evident that YC advocates building a solid business - ideally without funding. Ironically, that leads to investors wanting to invest in you. However, YC never pushes you to raise money or raise more than you need.I had some concerns about the fully remote batch, but in the end, the remote setup turned out to be a really positive thing in many unexpected ways. More about that further down in the post.Pitfalls to avoid when applyingYcombinator pioneered the open application model of startup funding. I have applied before - several times, in fact. However, I was never accepted. In fact, I was never even invited for an interview. Looking back, I realize I made a few mistakes with my application process. Some of these are surprisingly common:Talking about my idea only from a customer&#39;s perspective Solely focusing on how customers find your product useful is a mistake. Customers make the startups tick, but early-stage startups are about the vision of what the business could become one day if everything goes right. You use the customer traction to back that story up. It&#39;s important to write in your application why this can become a large business and what you envision that journey to be. Michael Siebel has a video on it.Writing to sound impressiveIt&#39;s much better to write clearly and let your traction and your background do your bidding instead. You don&#39;t have to convince people that your business cannot fail - investors understand the nature of bets. However, you have to convince them that the resulting company would be massive if everything goes well.Stopping to apply after a couple of rejections If your business is making progress and you think YC can benefit, keep applying. A lot of companies get in after multiple attempts and go on to do very well.Not getting feedback on your applicationOne of the biggest mistakes I made in the past is not getting feedback from YC alumni and ex-partners on my application. Many proactively offer to help with your application on Twitter. Take them up on it! You may have a certain writing style, but these people understand how YC thinks (and reads) and can help you craft a narrative that can help your application resonate with the partners. If you receive an interview invite, seek out help with mock interviews too, but avoid doing too many of them as they can make you sound overly rehearsed.You can read our YC W21 application if you are so inclined :)Finally, let&#39;s talk about the YC experience, and then we can touch upon the remote experience.Structure of the batchThe batches keep getting bigger and going remote has only accelerated this trend. However, YC is very software-driven, and the structure of the batch facilitates scaling up. In fact, Startup School is YC&#39;s approach on a much larger scale.  The batch is divided into several groups, and the groups are divided into several sections. Each group has four partners. This structure has implications for your interaction with your peers and partners, as I&#39;ll take about in the next section. You&#39;ll end up getting to know the companies in your section very well. YC tries their best to match you up with companies in your timezone, industry, and stage. Program StructureThe first week of the program is the Bootcamp - a lot of talks on product development, customer development, and sales. There are also special workshops for Biotech companies.  From the second week onwards, things fall into a predictable routine - Tuesdays are talks from successful alumni, Thursdays are group events, and Fridays are group office hours. You are encouraged to book 1-1 office hours with your partners every 7-10 days, but they aren&#39;t mandatory.The various stages of the programThe program itself is different than I had imagined. Like I mentioned earlier, it&#39;s a lot more focused on building a great product and a business that can run without raising money, and then using that momentum to raise money. This leads to two distinct phases. In the first one, which lasts roughly two months, most of the talks are focused on building a great product and company. Some founders do use YC to raise money, but that&#39;s certainly not what the partners encourage. The office hours are about discussing your demo day goals and the roadblocks in achieving them.The next phase kicks in the final month, and then everything is about the demo day and fundraising. In the group events on Thursdays, the alumni talk about their fundraising experience, the office hours deal with fundraising questions, and the Slack chats are about fundraising too :)Why I found the program helpfulApart from getting help in setting up the company, the resources (AWS credits, great deals on other software), I found a few things very useful about the experienceAdvice from the partnersThe number one benefit  is the advice you receive from the partners.  Partners at YC  have worked with hundreds of startups and have good advice. You may not agree with everything and that&#39;s ok - but they can be a good sounding board nevertheless. The Tuesday talksThe talks by successful alumni are very inspiring. While I had read about some of these alumni and seen their talks on Youtube, the Tuesday talks are much more candid since they are completely off the record. People share the ups and downs of their journey in ways they can&#39;t do otherwise. This time many of them called in from their homes and so the talks (and the people) felt even more relatable. Recommendations from the networkWhen building a startup, you need to get things done, quickly. The YC community is a great resource for finding service providers to work with - lawyers, accountants, software vendors, domain brokers. You are usually one post away from getting trusted recommendations. Peer pressure - the good kindGiven that YC gets tens of thousands of applications and accepts a small percentage of them, your peers are very smart and ambitious. Everyone is working hard and trying to make a lot of progress on some very ambitious demo day goals. In group office hours, everyone shares their progress and seeing your peers make a ton of progress week on week is very inspiring.Support for women foundersYC is very supportive of women founders and minorities. There are several female partners, and as far as I know, every group has at least one. My group&#39;s partner Reshma has been very supportive, checking in proactively at times to make sure I am doing well.The Remote ExperienceIf you have read this far, you can tell that most of the benefits I have talked about are easily leveraged in a remote setting. The Tuesday talks are probably even better in a remote setting because you get to see successful entrepreneurs in their homes - away from their offices&#39; glitz, increasing the relatability.The office hours work very well remotely, too, much like the other meetings you are having. Finally, not moving to the Bay area and looking for an expensive short-term rental was a relief. I&#39;d love to hang in the Bay Area, but I much prefer to do it after product-market fit!One interesting side effect of the remote batch was that I got to see a big group interact remotely. I have worked remotely since 2015 but never had hundreds of people on our Slack. With tools like Donut to facilitate serendipitous connections, the Bookface platform to share knowledge and book time with the partners, and Zoom meetups, the remote experience works great for a large group. Since we aspire to be a large remote company one day, it is useful for me to have a mental model of how it would feel. Obviously, remote has some cons too. Most notably, the timezone issues. It can be challenging to participate in some events or book office hours with the timezone gap. If you are in the EU, it&#39;s not too bad, but anything further east is much worse. There is a non-zero probability that you&#39;ll be added to a group because the companies are in the same timezone and not necessarily in the same industry/stage.I hope this post gives you a good sense of the YC experience, and sheds some light on how things work remotely. I&#39;ll be happy to answer any questions on Twitter. Follow the discussion on Hacker News. Hacker news power user? Try MagicBell for HN Chrome extension to see your comment on page. Featured BlogpostsYour Go-to Guide to Notification Customization with MagicBellBuilding a React Notification System: What You Need to KnowWhy You Don’t Really Need to Build React Web Notifications for Your AppCan you build a complete notification system without breaking a sweat?</description>
      <pubDate>16 Mar 21 09:10 EDT</pubDate>
      <guid>https://magicbell.io/blog/the-ycombinator-experience-remote-edition</guid>
    </item>
    <item>
      <title>Natural Language Processing: the age of Transformers</title>
      <link>https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/</link>
      <description>&lt;a href=&#34;https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This article is the first installment of a two-post series on Building a machine reading comprehension system using the latest advances in deep learning for NLP. Stay tuned for the second part, where we&#39;ll introduce a pre-trained model called BERT that will take your NLP projects to the next level!In the recent past, if you specialized in natural language processing (NLP), there may have been times when you felt a little jealous of your colleagues working in computer vision. It seemed as if they had all the fun: the annual ImageNet classification challenge, Neural Style Transfer, Generative Adversarial Networks, to name a few. At last, the dry spell is over, and the NLP revolution is well underway! It would be fair to say that the turning point was 2017, when the Transformer network was introduced in Google&#39;s Attention is all you need paper. Multiple further advances followed since then, one of the most important ones being BERT - the subject of our next article.To lay the groundwork for the Transformer discussion, let&#39;s start by looking at one of the common categories of NLP tasks: the sequence to sequence (seq2seq) problems. They are pretty much exactly what their name suggests: both the inputs and the outputs of a seq2seq task are sequences. In the context of NLP, there are typicaly additional restrictions put in place: The elements of the sequence are tokens corresponding to some set vocabulary (often including an Unknown token for the out-of-vocabulary words) The order inside the sequence matters. Next we shall take a moment to remember the fallen heros, without whom we would not be where we are today. I am, of course, referring to the RNNs - Recurrent Neural Networks, a concept that became almost synonymous with NLP in the deep learning field. 1. The predecessor to Transformers: the RNN Encoder-Decoder This story takes us all the way back to 2014 (Ref, another Ref), when the idea of approaching seq2seq problems via two Recurrent Neural Networks combined into an Encoder-Decoder model, was born. Let&#39;s demonstrate this architecture on a simple example from the Machine Translation task. Take a French-English sentence pair, where the input is &#34;je suis étudiant&#34; and the output &#34;I am a student&#34;. First, &#34;je&#34; (or, most likely, a word embedding for the token representing &#34;je&#34;), often accompanied by a constant vector hE0 which could be either learned or fixed, gets fed into the Encoder RNN. This results in the output vector hE1 (hidden state 1), which serves as the next input for the Encoder RNN, together with the second element in the input sequence &#34;suis&#34;. The output of this operation, hE2, and &#34;étudiant&#34; are again fed into the Encoder, producing the last Encoded hidden state for this training sample, hE3. The hE3 vector is dependent on all of the tokens inside the input sequence, so the idea is that it should represent the meaning of the entire phrase. For this reason it is also referred to as the context vector. The context vector is the first input to the Decoder RNN, which should then generate the first element of the output sequence &#34;I&#34; (in reality, the last layer of the Decoder is typically a softmax, but for simplicity we can just keep the most likely element at the end of every Decoder step). Additionally, the Decoder RNN produces a hidden state hD1. We feed hD1 and the previous output I back into the Decoder to hopefully get &#34;am&#34; as our second output. This process of generating and feeding outputs back into the Decoder continues until we produce an &lt;EOS&gt; - the end of the sentence token, which signifies that our job here is done. The RNN Encoder-Decoder model in action. To avoid any confusion, there is something that I would like to draw your attention to. The multiple RNN blocks appear in the Figure because of the multiple elements of the sequence that get fed into / generated by the networks, but make no mistake - there is only one Encoder RNN and one Decoder RNN at play here. It may help to think of the repeated blocks as the same RNN at different timesteps, or as multiple RNNs with shared weights, that are envoked one after another.This architecture may seem simple (especially until we sit down to actually write the code with LSTMs or GRUs thrown in for good measure), but it actually turns out to be remarkably effective for many NLP tasks. In fact, Google Translate has been using it under the hood since 2016. However, the RNN Encoder-Decoder models do suffer from certain drawbacks:1a. First problem with RNNs: Attention to the rescueThe RNN approach as described above does not work particularly well for longer sentences. Think about it: the meaning of the entire input sequence is expected to be captured by a single context vector with fixed dimensionality. This could work well enough for &#34;Je suis étudiant&#34;, but what if your input looks more like this:&#34;It was a wrong number that started it, the telephone ringing three times in the dead of night, and the voice on the other end asking for someone he was not.&#34;Good luck encoding that into a context vector! However, there turns out to be a solution, known as the Attention mechanism.Schematics of (left) a conventional RNN Encoder-Decoder and (right) an RNN Encoder-Decoder with AttentionThe basic idea behind Attention is simple: instead of passing only the last hidden state (the context vector) to the Decoder, we give it all the hidden states that come out of the Encoder. In our example that would mean hE1, hE2 and hE3. The Decoder will determine which of them gets attended to (i.e., where to pay attention) via a softmax layer. Apart from adding this additional structure, the basic RNN Encoder-Decoder architecture remains the same, yet the resulting model performs much better when it comes to longer input sequences. 1b. Second problem with Recurrent NNs: they are (surprise!) Recurrent The other problem plaguing RNNs has to do with the R inside the name: the computation in a Recurrent neural network is, by definition, sequential. What does this property entail? A sequential computation cannot be parallelized, since we have to wait for the previous step to finish before we move on to the next one. This lengthens both the training time, and the time it takes to run inference. One of the ways around the sequential dilemma is to use Convolutional neural networks (CNNs) instead of RNNs. This approach has seen its share of success, until it got outshone by the &lt;drumroll&gt; ... 2. Attention is All You Need (c) Google, 2017 The Transformer architecture was introduced in the paper whose title is worthy of that of a self-help book: Attention is All You Need. Again, another self-descriptive heading: the authors literally take the RNN Encoder-Decoder model with Attention, and throw away the RNN. Attention is all you need! Well, it ends up being quite a bit more complicated than that in practice, but that is the basic premise. How does this work? To start with, each pre-processed (more on that later) element of the input sequence wi gets fed as input to the Encoder network - this is done in parallel, unlike the RNNs. The Encoder has multiple layers (e.g. in the original Transformer paper their number is six). Let us use hi to label the final hidden state of the last Encoder layer for each wi. The Decoder also contains multiple layers - typically, the number is equal to that of the Encoder. All of the hidden states hi will now be fed as inputs to each of the six layers of the Decoder. If this looks familiar to you, it is for a good reason: this is the Transformer&#39;s Encoder-Decoder Attention, which is rather similar in spirit to the Attention mechanism that we discussed above. Before we move on to how the Transformer&#39;s Attention is implemented, let&#39;s discuss the preprocessing layers (present in both the Encoder and the Decoder as we&#39;ll see later). There are two parts to preprocessing: first, there is the familiar word embedding, a staple in most modern NLP models. These word embeddings could be learned during training, or one could use one of the existing pre-trained embeddings. There is, however, a second part that is specific to the Transformer architecture. So far, no where have we provided any information on the order of the elements inside the sequence. How can this be done in the absence of the sequential RNN architecture? Well, we have the positions, let&#39;s encode them inside vectors, just as we embedded the meaning of the word tokens with word embeddings. The resulting post-processed vectors, carrying information about both the word&#39;s meaning and its position in the sentence, are passed on to the Encoder and Decoder layers. An Encoder with two layers, processing a three element input sequence (w1, w2, and w3) in parallel. Each input element&#39;s Encoder also receives information about the other elements via its Self-Attention sublayers, allowing the relationships between words in the sentence to be captured.2a. Attention, the linear algebra prospective I come from a quantum physics background, where vectors are a person&#39;s best friend (at times, quite literally), but if you prefer a non linear algebra explanation of the Attention mechanism, I highly recommend checking out The Illustrated Transformer by Jay Alammar. Let&#39;s use X to label the vector space of our inputs to the Attention layer. What we want to learn during training are three embedding matrices, WK, WV and WQ, which will permit us to go from X to three new spaces: K (keys), V (values) and Q (queries): K = X WK       V = X WV       Q = X WQ The way that these embedded vectors are then used in the Encoder-Decoder Attention is the following. We take a Q vector (a query, i.e., we specify the kind of information that we want to attend to) from the Decoder. Additionally, we take vectors V (values) that we can think of as something similar to linear combinations of vectors X coming from the Encoder (do not take &#34;linear combination&#34; literally however, as the dimensionality of X and V is, in general, different). Vectors K are also taken from the Encoder: each key Kn indexes the kind of information that is captured by the value Vn. To determine which values should get the most attention, we take the dot product of the Decoder&#39;s query Q with all of the Encoder&#39;s keys K. The softmax of the result will give the weights of the respective values V (the larger the weight, the greater the attention). Such mechanism is known as the Dot-product attention, given by the following formula: where one can optionally divide the dot product of Q and K by the dimensionality of key vectors dk. To give you an idea for the kind of dimensions used in practice, the Transformer introduced in Attention is all you need has dq=dk=dv=64 whereas what I refer to as X is 512-dimensional. 2b. What is new: Self-Attention In addition to the Encoder-Decoder Attention, the Transformer architecture includes the Encoder Self-Attention and the Decoder Self-Attention. These are calculated in the same dot-product manner as discussed above, with one crucial difference: for self-attention, all three types of vectors (K, V, and Q) come from the same network. This also means that all three are associated with the elements of the same sequence (input for the Encoder and output for the Decoder). The purpose of introducing self-attention is to learn the relationships between different words in the sentence (this function used to be fulfilled by the sequential RNN). One way of looking at it is a representation of each element of the sequence as a weighted sum of the other elements in the sequence. Why bother? Consider the following two phrases: 1. The animal did not cross the road because it was too tired. 2. The animal did not cross the road because it was too wide. Clearly, it is most closely related to the animal in the first phrase and the road in the second one: information that would be missing if we were to use a uni-directional forward RNN! In fact, the Encoder Self-Attention, that is bi-directional by design, is a crucial part of BERT, the pre-trained contextual word embeddings, that we shall discuss later on. Where are the calculations for the Encoder Self-Attention carried out? Turns out, inside every Encoder layer. This permits the network to pay attention to relevant parts of the input sequence at different levels of abstraction: the values V of the lower Encoder layers will be closest to the original input tokens, whereas Self-Attention of the deeper layers will involve more abstract constructions. 2c. Putting it all together By now we have established that Transformers discard the sequential nature of RNNs and process the sequence elements in parallel instead. We saw how the Encoder Self-Attention allows the elements of the input sequence to be processed separately while retaining each other&#39;s context, whereas the Encoder-Decoder Attention passes all of them to the next step: generating the output sequence with the Decoder. What happens at this stage may not be so clear. As you recall, the RNN Encoder-Decoder generates the output sequence one element at a time. The previously generated output gets fed into the Decoder at the subsequent timestep. Do Transformers really find a way to free us from the sequential nature of this process and somehow generate the whole output sequence at once? Well - yes and no. More precisely, the answer is [roughly] yes when training, and no at inference time. The Transformer architecture featuting a two-layer Encoder / Decoder. The Encoder processes all three elements of the input sequence (w1, w2, and w3) in parallel, whereas the Decoder generates each element sequentially (only timesteps 0 and 1, where the output sequence elements v1 and v2 are generated, are depicted). Output token generation continues until an end of the sentence token &lt;EOS&gt; appears.The inputs to the Decoder come in two varieties: the hidden states that are outputs of the Encoder (these are used for the Encoder-Decoder Attention within each Decoder layer) and the previously generated tokens of the output sequence (for the Decoder Self-Attention, also computed at each Decoder layer). Since during the training phase, the output sequences are already available, one can perform all the different timesteps of the Decoding process in parallel by masking (replacing with zeroes) the appropriate parts of the &#34;previously generated&#34; output sequences. This masking results in the Decoder Self-Attention being uni-directional, as opposed to the Encoder one. Finally, at inference time, the output elements are generated one by one in a sequential manner. Some final remarks before we call it a day: The part of the Decoder that I refer to as postprocessing in the Figure above is similar to what one would typically find in the RNN Decoder for an NLP task: a fully connected (FC) layer, which follows the RNN that extracted certain features from the network&#39;s inputs, and a softmax layer on top of the FC one that will assign probabilities to each of the tokens in the model&#39;s vocabularly being the next element in the output sequence. At that point, we could use a beam search algorithm to keep the top few predictions at each step and choose the most likely output sequence at the end, or simply keep the top choice each time. The Transformer architecture is the driving force behind many of the recent  breakthroughs in the field of NLP. To put some hard numbers on that statement, lets turn to a metric called BLEU, commongly used to evaluate the quality of machine translations. The original Transformer achieved a score of 28.4 BLEU on an English-to-German translation task, and if that does not tell you much, suffices to say that it was better than the exisiting best result by over 2 BLEU!Next, in the coming blog post we will discuss BERT (Bidirectional Encoder Representations from Transformers): contextualized word embeddings based on the Transformer (more precisely, Transformer&#39;s Encoder), and how to train a BERT-based machine reading comprehension model on the Scaleway GPU instances. </description>
      <pubDate>24 Mar 20 12:22 EDT</pubDate>
      <guid>https://blog.scaleway.com/2019/building-a-machine-reading-comprehension-system-using-the-latest-advances-in-deep-learning-for-nlp/</guid>
    </item>
    <item>
      <title></title>
      <link>https://jvns.ca/blog/debugging-attitude-matters/</link>
      <description>&lt;a href=&#34;https://jvns.ca/blog/debugging-attitude-matters/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A while back I wrote What does debugging a program look like? on what to do when debugging (change one thing at a time! check your assumptions!). But I was debugging some CSS last week, and I think that post is missing something important: your attitude. Now – I’m not a very good CSS developer yet. I’ve never written CSS professionally and I don’t understand a lot of basic CSS concepts (I think I finally understood for the first time recently how position: absolute works). And last week I was working on the most complicated CSS project I’d ever attempted. While I was debugging my CSS, I noticed myself doing some bad things that I normally would not! I was: making random changes to my code in the hopes that it would work googling a lot of things and trying them without understanding what they did if something broke, reverting my changes and starting again This strategy was exactly as effective as you might imagine (not very effective!), and it was because of my attitude about CSS! I had this unusual-for-me belief that CSS was Too Hard and impossible for me to understand. So let’s talk about that attitude a bit! the problem attitude: “this is too hard for me to understand” One specific problem I was having was – I had 2 divs stacked on top of one another, and I wanted Div A to be on top of Div B. My model of CSS stacking order at the start of this was basically “if you want Thing A to be on top of Thing B, change the z-index to make it work”. So I changed the z-index of Div A to be 5 or something. But it didn’t work! In Firefox, div A was on top, but in Chrome, Div B was on top. Argh! Why? CSS is impossible!!! (if you want to see the exact actual situation I was in, I reproduced the different-in-firefox-and-chrome thing here after the fact) I googled a bit, and I found out that a possible reason z-index might not work was because Div A and Div B were actually in different “stacking contexts”. If that was true, even if I set the z-index of Div A to 999999 it would still not put it on top of Div B. (here’s a small example of what this z-index problem looks like, though I think my specific bug had some extra complications) I thought “man, this stacking context thing seems really complicated, why is it different between Firefox and Chrome, I’m not going to be able to figure this out”. So I tried a bunch of random things a bunch of blog posts suggested, which as usual did not work. Finally I gave up this “change random things and pray” strategy and thought “well, what if I just read the documentation on stacking order, maybe it’s not that bad”. So I read the MDN page on stacking order, which says: When the z-index property is not specified on any element, elements are stacked in the following order (from bottom to top): 1. The background and borders of the root element 2. Descendant non-positioned blocks, in order of appearance in the HTML 3. Descendant positioned elements, in order of appearance in the HTML This is SO SIMPLE! It just depends on the order in the HTML! I put Div A after Div B in the HTML (as a sibling) and it made everything work in both browsers. better attitude: “let’s learn the basics and see if that helps” This whole stacking problem turned out to really not be that complicated – all I needed to do was read a very short and simple documentation page to understand how stacking works! Of course, computer things are not always this simple (and even in this specific case the rules about what creates a new stacking context are pretty complicated.). But I did not need to understand those more complicated rules in order to put Div A on top of Div B! I only needed to know the much simpler 3 rules above. So – calm down for a second, learn a few of the basics, and see if that helps. watching people who know what they’re doing is inspiring Another area of CSS that I thought was “too hard” for me to understand was this whole position: absolute and position: relative business. I kept seeing (and sometimes using!) examples where people made complicated CSS things with position: absolute but I didn’t understand how they worked. Doesn’t position: absolute mean that the element is always in the same place on the screen? Why are these position: absolute things moving when I scroll like the rest of the document? (spoiler: no, that’s position: fixed.) But last week, I paired with someone who’s a lot better at CSS than me on some code, and I saw that they were just typing in position: absolute and position: relative confidently into their code without seeming confused about it!! Could that be me? I looked up the documentation on MDN on position: absolute, and it said: The element is removed from the normal document flow, and no space is created for the element in the page layout. It is positioned relative to its closest positioned ancestor… Its final position is determined by the values of top, right, bottom, and left. So things with position: absolute are positioned relative to their closest positioned ancestor! And you just use top/bottom/right/left to pick where! That’s so simple! documentation that you can trust makes a big difference I think another big source of my frustration with CSS is that I didn’t have the best grasp of where to find accurate information &amp; advice. I knew that MDN was a reliable reference, but MDN doesn’t really help answer questions like “ok but seriously how do I center a div???” and I found myself reading a lot of random Stack Overflow answers/blog posts that I wasn’t 100% sure were correct. This week I learned about CSS Tricks which has a lot of GREAT articles like Centering in CSS: A Complete Guide which seems very reputable and is written in a super clear way. that’s all! I don’t really know why I started to believe that it was “impossible” to understand basic CSS concepts since I don’t believe that about computers in general. Maybe because I’ve been writing CSS at a beginner level for a very long time but hadn’t ever really tried to do a more involved CSS project than “let’s arrange some divs in a grid with flexbox”! But this attitude really got in the way of me writing the CSS I wanted to write! And once I let go of it and used my normal debugging techniques I was able to get a lot more things to work the way I wanted. </description>
      <pubDate>05 Apr 20 16:17 EDT</pubDate>
      <guid>https://jvns.ca/blog/debugging-attitude-matters/</guid>
    </item>
    <item>
      <title></title>
      <link>https://increment.com/programming-languages/crash-course-in-compilers/</link>
      <description>&lt;a href=&#34;https://increment.com/programming-languages/crash-course-in-compilers/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Late one night on an uncrowded subway car in New York City, I had my laptop open, working on a game whose deadline was drawing near. A gentleman sat next to me and, seeing the walls of colored text on my screen, asked if I was writing C++. I told him I wasn’t, and he was curious to hear what language I was using. I was working on a web game in a programming language I had designed for myself, and I told him so—it was something that I made up, I said. After looking at me for a moment, he asked, “Why would anyone do that?” I started to answer, but alas, we had arrived at his stop, and he disappeared onto the platform before I could explain myself. In many ways, I’ve been trying to answer that man’s question for years now.The thing is, I absolutely love programming languages. I work as a graphics and video game developer, which is thrilling and challenging work, but secretly I would rather be hacking on compilers. I love languages because, of everything I’ve encountered in computing, languages are by far the weirdest. They combine the brain-bending rigor of abstract math, the crushing pressures of capitalistic industry, and the irrational anxiety of a high school prom. The decision to adopt or avoid a language is always a mix of their perceived formal power (“Does this language even have this particular feature?”), employability (“Will this language get me a job?”), and popularity (“Does anyone important use this language anymore?”). I can’t think of another engineering tool that demands similar quasi-religious devotion from its users. Programming languages ask us to reshape our minds, and that makes them deeply personal and subjective.The field of study of programming languages is called programming language theory, or PLT. Software engineers are confronted with programming languages just about every day, but few develop a deep relationship with PLT. Languages are tools, primarily, a means to an end, and most professionals will do fine just learning to use the popular ones well enough to get their jobs done.Diving deeper into PLT, though, is a great way to grow as a developer. Not only is language design a lot of fun, but a deeper understanding of the tools you use every day will give you a better handle on them, and can make learning new languages considerably easier, even if you don’t dream of becoming the next Guido van Rossum or Rich Hickey. And hey, you never know—your personal project could become the next major piece of software engineering infrastructure. It’s happened before.What is a programming language?So, what is a programming language? This might seem like an odd question to ask about tools this ubiquitous, but starting from a definition is often helpful to focus the conversation. A programming language is a formal language used to communicate instructions to a computer. It is formal in that it conforms to a rigid set of rules that determine what is and is not allowed. It is a means of communication in that the primary goal of the tool is to translate ideas in a programmer’s head into a form that a computer can act on. The fact that you are communicating with a computer is significant. Unlike other forms of language, or even instructional arts like musical composition or screenwriting, the final agent fulfilling the instructions is not human. The result is that qualities that other forms of communication tend to depend on—like intuition, common sense, and context—are not available.The decisive factor in what makes something a programming language (or not) is known as Turing completeness. Alan Turing’s seminal work in the 1940s included the definition of the Turing machine, a mathematical description of an abstract computer that became foundational for our understanding of how algorithms work. A Turing machine can, provably, implement any computable algorithm, and any system that can simulate the Turing machine can do so as well. Such a system is deemed Turing complete, and most programming languages have this status as a basic goal (though there are some interesting languages that do not). A deep dive into computability theory is beyond the scope of this article, but suffice it to say that a language with some notion of state (often variables or argument passing) and conditionals is most likely Turing complete. This leaves out markup languages like HTML and configuration languages like YAML or JSON, but includes a hilarious collection of systems that are accidentally Turing complete (including an abuse of HTML and CSS).In practice, you interact with programming languages via computer programs or software libraries into which you feed code in order to produce an effect. They come in two broad manifestations: as compilers and as interpreters. Each approach has its advantages and disadvantages, and the line between the two can be quite blurry, with frameworks like Mono going so far as to offer both simultaneously.An interpreter’s job is to take source code and immediately implement its effects. An interpreter turns source code into an internal representation that it can use to carry out the computation the source code describes. This representation will include the functions, variables, expressions, statements, and all other semantics of the source language. You can think of source code as an extreme, Turing-complete configuration file that controls the interpreter’s behavior. My first foray into language design was based on Peter Norvig’s excellent Lispy interpreter in Python, and the more recent MAL project has amassed implementations in 72 languages. The advantages of interpreters include their simplicity, the fact that they can often start executing faster than compilers, and their ability to run in environments where compiling new code is prohibited (like on iOS or most video game consoles).This piece, however, will focus on compilers. The job of a compiler is to take source code and translate it into a target code with the same meaning. Often that target code is in a lower-level language like machine code, but that isn’t always the case. The generated target code can then be evaluated in order to carry out the computation of the original source code. Compilers can be thought of as a pipeline of transformations, starting with the programmer’s source code and proceeding through a series of internal representations that end in the desired target code, after which it is handed off to another system for evaluation.The classic example is a compiler for the C programming language, where source code written in C is compiled into machine code that a computer’s hardware can execute directly. In this case, a higher-level language is compiled into a lower-level one. C# and Java are similar, but they compile into bytecodes that are executed by the Common Language Runtime (CLR) and the Java virtual machine (JVM), respectively, as opposed to physical hardware. Virtual machines like the CLR and the JVM provide cross-platform environments that handle a lot of low-level details for you while providing additional functionality like garbage collection and a type system. There are even cases where it is desirable to compile a lower-level language into a higher-level one. To run in the browser, the JSIL project compiles C# bytecode into JavaScript so it can run on the web, and Emscripten does the same for C and C++. There are also situations where the same language is both the source and target language. The so-called transpilers Babel and Closure compile JavaScript into JavaScript in order to access new features of the language and implement optimizations, respectively.How does a compiler work?Compilers tend to proceed in a linear sequence of phases, each phase providing the next with its input. Even wildly different languages will broadly have the same structure. Comparing the compilation steps of different languages is a useful way to get a handle on the general process, and to begin to grok how a compiler works.ParsingThe first question a compiler has to answer is, “What did the programmer say?” This step in the compiler pipeline is usually called parsing. The user prepares source code that is valid in the language they are programming in. Source code is often text, but it doesn’t have to be—take the visual languages Scratch, Pure Data (Pd), and Max/MSP, for example. Once the programmer has prepared their source code, the compiler’s first task is to turn it into a data structure that is useful to later stages of the compiler. This is the stage where errors specific to the syntax are reported, like missing semicolons or unmatched braces. This is done differently from language to language, but in two broad categories: Lisp reading and scanning/parsing.Languages in the Lisp family are notorious for their simple syntaxes. The simplicity is a result of deliberate design, but also a side-effect of a property that Lisp programmers take very seriously: Lisp source code is a literal representation of Lisp data. Put another way, Lisp source code is homoiconic with Lisp data. To that end, the first step in a Lisp compiler is to turn source code text into data structures that the language understands. Historically this has included lists, numbers, and symbols, known collectively as “symbolic expressions” or “s-expressions,” but modern Lisps like Clojure include hashmaps, vectors, and sets in their syntax. Lisps traditionally call this step “reading” instead of parsing (which is where the R in REPL comes from, a Lisp idea). Lisp readers are simple enough that they tend to be written by hand. Clojure’s reader is handwritten in Java and contains a combination of regular expressions and string operations to convert text into data structures, even matching against string literals when it needs to.Languages with more complex syntax require more work. The majority of mainstream languages require a two-step process: scanning followed by parsing. A scanner (also known as a lexical analyzer) reads source text and produces a linear stream of tokens; the parser reads the stream of tokens and recognizes patterns to transform into nodes in an abstract syntax tree that the next step of the pipeline will deal with. The complexity of this step depends on the complexity of the syntax of the language. Some languages will use handwritten scanners and parsers, while others will depend on parser generators like Lex/Yacc or Flex/Bison, which take as input a specification of the desired grammar of the language, and produce as output a scanner and parser for that language.TypeScript’s scanner is handwritten and features recognizable constructs like mapping from keywords to token types and a large statement switching on character codes to determine what to scan next. The tokens allow the parser to reason with higher-level constructs like SyntaxKind.​AsteriskToken and SyntaxKind.​OpenBraceToken as in the parse​Import​Declaration​Or​Import​Equals​Declaration function. CoffeeScript relies on Jison, a JavaScript port of Bison, for its parsing. We can see the language described as a grammar with declarative rules, like the rules for if expressions. Ruby’s Yacc grammar is a favorite of mine: In order to implement Ruby’s famously appealing syntax, the grammar comes out to a colossal 11,400+ lines of Yacc code!AnalysisOnce parsing is complete, the compiler must analyze the parsed code into an abstract syntax tree, or AST. Analysis answers the question, “What did the user mean?” Languages in the Lisp family will usually take an additional step to go from the s-expressions the reader produced to an initial AST, while the parsers of languages outside the Lisp family will usually produce an AST directly. This is where the semantic features of the language are implemented, like name resolution, control flow, and function invocation. Additionally, analysis is a phase where optimizations can begin to happen, by transforming the AST into semantically equivalent ASTs that perform better. This is likely the most varied phase between compilers, and each language will be radically different here. There aren’t really any libraries or APIs to lean on here, and it’s up to the language implementer to derive this meaning themselves.In languages with types, this is where type information is inferred, flowed, and validated. Even dynamically typed languages can flow type information in order to gain performance. For example, ClojureCLR uses reflection to determine the type of its static method invocations and static field lookups. This information is used to generate better bytecode and compiler errors. Languages like TypeScript provide a type system to a target that is dynamically typed by thoroughly checking types in the analysis phase and issuing a warning if types do not line up. Type-safe languages like Haskell will dedicate a large portion of their analysis phase to type checking.EmissionOnce an AST is produced and settled on, the final step is to emit the target code. When targeting machine code, modern languages will most often use the LLVM toolchain. LLVM is an exciting project because it unifies various hardware platforms and optimizations under one target. It specifies its own intermediate representation (LLVM IR) that a compiler would emit. IR code then goes through the same parse-analyze-emit pipeline described in this article to turn into machine code. The benefit is that LLVM presents a more straightforward assembly language that is still very low level without concerning the language developer with platform-specific quirks. Targeting IR means your language can take advantage of optimizations written for C and C++ with no additional effort on your part. LLVM exposes both a C++ and a C API to generate IR. The C API means bindings to other languages are possible (I’ve successfully used them in Node and C#). LLVM can even be found in compilers for dynamic languages like Julia.Virtual machine targets like the CLR and the JVM are similar, but each exposes a bytecode language that is at an even higher level than LLVM IR. C#’s standard library provides a very robust namespace specifically for generating bytecode that exposes an object-oriented interface to emit assemblies, types, methods, and every other aspect of bytecode. Java does not have a comparable namespace in its own standard library, but third-party libraries like ASM or BCEL can fill this gap. These APIs can be seen in somewhat wrapped form in Clojure’s JVM and CLR compilers.If the target is source code in a high-level language, emission might actually involve concatenating strings together. There often isn’t an existing API to generate source code in a high-level programming language—the expectation is that a human programmer will manually type it all out. This is an issue for languages that compile to JavaScript, as is evident in the ClojureScript and TypeScript compilers. Some languages, like Carp, treat C as their compile target, resulting in similar-looking emission phases.Tooling and ecosystemsAt this point, formally speaking, you’re done! The compiler has transformed code from the source language into the target language and achieved its basic goal. In practice, however, the job of a language designer is just beginning. Languages are more than their compilers, and the day-to-day experience of working with a language actually involves myriad developer tools acting in concert. Once a language’s compiler is working, the question then becomes one of editor integration, debugger support, documentation, a community, and a library ecosystem. Most of this takes considerable time to develop, and this is what gives existing languages inertia over new ones.Historically, languages had not directly addressed the task of managing third-party libraries, or packages. In the pre-web, pre-open source days, when languages like C++ arrived, the issue of integrating with a stranger’s code was nowhere near as complicated as it is now. Even languages that appeared in the 1990s tended to not include package managers, with Ruby’s RubyGems not landing until eight years after Ruby itself. Post-web languages are more likely to include a package manager as part of their standard tooling, as Elm and Rust do. Most package managers are specific to their languages, custom built, and require server infrastructure, though generic solutions like Gx and Nix are available as well. Gx is interesting because it operates over IPFS, a peer-to-peer protocol that requires no central server coordination. Nix is the result of Eelco Dolstra’s PhD thesis, “The Purely Functional Software Deployment Model,” and is primarily used in the NixOS operating system. It’s purely functional and, as a result, provides very reproducible deployments.Integrating with editors has also been a pain, traditionally. Programmers expect good syntax highlighting, completion, and other features all in their favorite editor. It was usually up to the community to provide these bindings, leading to an uneven developer experience across editors. Recently, Microsoft has put out what they call the Language Server Protocol to help address these issues and make it easier to integrate new programming languages with text editors. It’s essentially a network protocol for a text editor. Your language only needs to implement the protocol once, and then every editor that supports it (which is most major editors) can communicate with your language to get autocomplete and other features.Why anyone would do thisIf you’re reading this, gentleman from the subway, I hope it has begun to answer your question about why anyone would make up a programming language. It’s a wonderful puzzle to solve, and more approachable than it may seem at first. Languages represent different ideas of how to capture human creativity on a machine, and I’ve never been disappointed by pulling the curtain back on an implementation to see how it ticks. Seeing common patterns across different languages and getting a sense of their trade-offs also gives you a new perspective when picking up new languages, something every working programmer will have to do at some point in their career.Whether you’re building the next chapter in the history of software engineering or just peeking under the hood of a machine that you use every day, the world of programming languages is yours to explore. It will expand your mind and make you a better programmer—and you might not even be the strangest person on the train.</description>
      <pubDate>02 Apr 20 21:24 EDT</pubDate>
      <guid>https://increment.com/programming-languages/crash-course-in-compilers/</guid>
    </item>
    <item>
      <title>Silly job interview questions in Haskell</title>
      <link>https://chrispenner.ca/posts/interview</link>
      <description>&lt;a href=&#34;https://chrispenner.ca/posts/interview&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Today I thought it&#39;d be fun to take a look at a few common &amp; simple &#34;interview questions&#34; in Haskell. These sorts of questions are often used to establish whether someone has programming and problem solving skills, and I thought it might be useful for folks to see how they play out in Haskell since our beloved language&#39;s solutions tend to follow a different paradigm than most other languages do. I&#39;ll withhold any judgement on whether these questions are in any way helpful in determining programming skill whatsoever 😅; please don&#39;t @ me about it. Palindromes Let&#39;s start off nice and easy with the standard &#34;is it a palindrome&#34; question! The task is to write a function which determines whether a given string is a palindrome (i.e. whether it reads the same in both reverse and forwards) isPalindrome :: String -&gt; Bool isPalindrome str = str == reverse str &gt;&gt;&gt; isPalindrome &#34;racecar&#34; True &gt;&gt;&gt; isPalindrome &#34;hello world!&#34; False That&#39;ll do it! Not much to say about this one, it&#39;s nice that our definition roughly matches an English sentence describing the problem &#34;does a given string equal itself in reverse&#34;. I&#39;ll leave it as an exercise for the reader to expand it to handle differences in capitalization however you like. Fizz Buzz Next up is the infamous Fizz Buzz! For the 3 of you who are unfamiliar, for each number from 1 to 100 we need to print out &#34;Fizz&#34; if it&#39;s divisible by 3, &#34;Buzz&#34; if it&#39;s divisible by 5, and &#34;Fizz Buzz&#34; if it&#39;s divisible by both 3 AND 5! Otherwise we print the number itself. Let&#39;s see it! import Data.Foldable fizzle :: Int -&gt; String fizzle n | n `mod` 3 == 0 &amp;&amp; n `mod` 5 == 0 = &#34;Fizz Buzz!&#34; | n `mod` 3 == 0 = &#34;Fizz!&#34; | n `mod` 5 == 0 = &#34;Buzz!&#34; | otherwise = show n main :: IO () main = do for_ [1..100] (putStrLn . fizzle) &gt;&gt;&gt; main 1 2 Fizz! 4 Buzz! Fizz! 7 8 Fizz! Buzz! 11 Fizz! 13 14 Fizz Buzz! 16 -- ...you get the idea I write a helper function &#34;fizzle&#34; here which converts a number into its appropriate string so I can keep the &#34;printing&#34; logic separate, which is good programming style in Haskell as it makes things easier to both test and reason about. We can see that &#34;case analysis&#34; is very helpful for these sorts of problems, I&#39;m using &#34;pattern guards&#34; to do a sort of multi-way if statement. Since &#34;divisible by both 3 &amp; 5&#34; overlaps with the other conditions and also is the most restrictive, we check for that one first, then check the other two cases falling back on returning the string version of the number itself. It all works beautifully! I really enjoy looking at this problem as an example of how Haskell is different from other languages. Most things in Haskell are functions, even our loops are just higher-order functions! The nice thing about that is that functions are composable and have very clean boundaries, which means we don&#39;t need to intermingle the syntax of a for-loop with our logic. It&#39;s these same principles which allow us to easily separate our effectful printing logic from our function which computes the output string. The next difference we can see is that we use pattern-matching, specifically &#34;pattern guards&#34;, which allow us to select which definition of a function we want to use. It looks a bit like a glorified if-statement, but I find it&#39;s less syntactic noise once you get used to it, and there are many more things pattern guards can do! All that&#39;s left is to loop over all the numbers and print them out one by one, which is a snap thanks to the for_ function! Next! Sum up to N problem Here&#39;s a less-common problem that nonetheless I&#39;ve still heard a few times! I think it was in one of my algorithms assignments back in the day... The task is to take a list of numbers and find any combinations of 3 numbers which add up to a specified total. For instance, if we want to determine all combinations of 3 numbers which add up to 15, we&#39;d expect our result to look something like this: &gt;&gt;&gt; sumToN 15 [2, 5, 3, 10, 4, 1, 0] [[2,3,10],[5,10,0],[10,4,1]] Notice how each inner list sums to 15? We only care about combinations here, not permutations, so we have [2, 3, 10], but don&#39;t bother with [3, 2, 10]! So how will we set about implementing an algorithm for this? Well, the first thing to come to mind here is that we&#39;re finding combinations, then we&#39;re filtering them down to match a predicate! In Haskell we like to split problems into smaller composable pieces, the filter part should be pretty easy, so let&#39;s tackle the combinations problem first. After a quick look through hackage it looks like there is a permutations function, but strangely there&#39;s no combinations function! I suppose we could somehow try to de-duplicate the output of permutations, but it&#39;ll be fun to write our own version! combinations are quite nice to compute recursively, so let&#39;s try it that way! combinations :: Int -&gt; [a] -&gt; [[a]] -- Only one way to get zero things combinations 0 _ = [[]] combinations n (x:xs) = -- Get all combinations containing x by appending x to all (n-1) -- combinations of the rest of the list fmap (x:) (combinations (n-1) xs) -- Combine it with all combinations from the rest of the list &lt;&gt; combinations n xs -- No elements means no combinations! combinations _ [] = [] Here we&#39;re using pattern matching and recursion to do our dirty work. First we can confidently say that there&#39;s only ONE way to get 0 elements from any list of elements, so we can fill that in. Next we&#39;ll handle a single step, if we have at least one element left in the list, we can compute all the combinations which contain that element by prepending it to all the combinations of size n-1 from the remainder of the list; and we&#39;ll concatenate that with all the combinations of the rest of the list. Lastly we add one more pattern match which handles all invalid inputs (either negative numbers or empty lists) and simply assert that they have no valid combinations. Let&#39;s try out our implementation before we move on to the next part. &gt;&gt;&gt; combinations 3 [1..5] [[1,2,3],[1,2,4],[1,2,5],[1,3,4],[1,3,5],[1,4,5],[2,3,4],[2,3,5],[2,4,5],[3,4,5]] &gt;&gt;&gt; combinations 2 [1..4] [[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]] Feel free to take the time to convince yourself that these are correct 😀 To finish it off we need to find any of these combinations which add up to our target number. sumNToTotal :: Int -&gt; Int -&gt; [Int] -&gt; [[Int]] sumNToTotal n totalNeeded xs = filter matchesSum (combinations n xs) where matchesSum ys = sum ys == totalNeeded &gt;&gt;&gt; sumNToTotal 3 15 [2, 5, 3, 10, 4, 1, 0] [[2,3,10],[5,10,0],[10,4,1]] Great! We can simply get all possible combinations and filter out the results which don&#39;t properly sum to the expected number. One other nifty thing here is that, because Haskell is lazy, if we only need to find the first valid combination, we could just grab the first result of the list and Haskell won&#39;t do any more work than absolutely necessary. But wait! There&#39;s a surprise part two of this problem: We now have to find all combinations of ANY length which sum to a target number, lucky for us, that&#39;s pretty easy for us to adapt for! sumAnyToTarget :: Int -&gt; [Int] -&gt; [[Int]] sumAnyToTarget totalNeeded xs = foldMap (\n -&gt; sumNToTotal n totalNeeded xs) [0..length xs] &gt;&gt;&gt; sumAnyToTarget 15 [2, 5, 3, 10, 4, 1, 0] [ [5,10] , [2,3,10] , [5,10,0] , [10,4,1] , [2,3,10,0] , [10,4,1,0] , [2,5,3,4,1] , [2,5,3,4,1,0] ] This new version re-uses the sumNToTotal function we wrote in the previous step! It iterates over each possible length of combination and finds all the winning combinations using sumNToTotal, then concatenates them using foldMap! Works out pretty cleanly if I do say so myself! Check if two strings are anagrams For whatever reason, interviewers LOVE string manipulation questions; so let&#39;s try another one! Here our task is to determine whether two strings are anagrams of each other. I&#39;d say the difficulty for this one comes from thinking up your strategy rather than the implementation itself. Here&#39;s how I&#39;d give this a go in Haskell! import Data.Function (on) isAnagram :: String -&gt; String -&gt; Bool isAnagram = (==) `on` sort &gt;&gt;&gt; isAnagram &#34;elbow&#34; &#34;below&#34; True &gt;&gt;&gt; isAnagram &#34;bored&#34; &#34;road&#34; False &gt;&gt;&gt; isAnagram &#34;stressed&#34; &#34;desserts&#34; True Here we&#39;re using a funky higher-order function called on; on takes two functions, AND THEN takes two arguments! In this case it calls &#34;sort&#34; on both arguments, then checks if the sorted results are equal! It turns out this is sufficient to know if two strings are anagrams! But wait! What&#39;s that? What if they&#39;re in differing cases! Okay fine! import Data.Char (toLower) isAnagram :: String -&gt; String -&gt; Bool isAnagram a b = (==) `on` (sort . map toLower) Happy now? No? What&#39;s that? It seems non-performant? Well yes, but actually no! While it&#39;s true that sort has an O(nlogn) performance profile, one interesting thing here is that sorting is lazy in Haskell! This means that if our two strings are unequal, they will only be sorted far enough to determine inequality! In fact, if the first elements of each sorted string aren&#39;t equal to each other, then we won&#39;t bother sorting any more. Sure, our function isn&#39;t perfect, but it&#39;s not bad, especially since this is the first approach that came to mind. Compare our 2 line solution with the Java Solution provided in the post which gave me the idea for this problem. It might be more performant (though to be honest I haven&#39;t benchmarked them), but if I&#39;m going to be reading this code often in the future, I&#39;d much prefer the clearest version which performs at an adequate level. Min and Max Here&#39;s a problem! Given a list of elements, find the smallest and largest element of that list! I&#39;ll show and discuss three different strategies for this one. Here&#39;s the first: simpleMinMax :: Ord a =&gt; [a] -&gt; (a, a) simpleMinMax xs = (minimum xs, maximum xs) &gt;&gt;&gt; simpleMinMax [3, 1, 10, 5] (1,10) This is the simplest way we could imagine doing this sort of thing; and indeed it does work! Unfortunately, there are few skeletons from &#34;legacy&#34; haskell that are hidden in this closet. Look what happens if we try it on an empty list! &gt;&gt;&gt; simpleMinMax [] (*** Exception: Prelude.minimum: empty list Oops... Haskell isn&#39;t supposed to throw exceptions! That&#39;s okay though, there are some other good ways to accomplish this which won&#39;t blow up in our faces! Time for the next one! boundedMinMax :: (Bounded a, Ord a) =&gt; [a] -&gt; (a, a) boundedMinMax xs = coerce $ foldMap (\x -&gt; (Min x, Max x)) xs &gt;&gt;&gt; boundedMinMax [4, 1, 23, 7] :: (Int, Int) (1,23) &gt;&gt;&gt; boundedMinMax [] :: (Int, Int) (9223372036854775807,-9223372036854775808) This implementation might be a bit confusing if you haven&#39;t learned enough about Semigroups and Monoids, but don&#39;t let that scare you! These are both very common abstractions in Haskell and are used very often and to great effect! A Semigroup is a type of interface which provides an implementation which lets us combine multiple elements together. Haskell has two semigroup type-wrappers which provide specific behaviour to whichever type we wrap: Min and Max! These types define a combining operation which, any time we combine two elements, will keep only the smallest or largest value respectively! I&#39;m using foldMap here to project each list element into a tuple of these two types which, when the list is collapsed by foldMap, will all combine together and will include the lowest and highest elements, all in a single pass! So what&#39;s up with the second example? Well, it&#39;s a bit unexpected, but not necessarily wrong. When we&#39;re missing any elements to compare foldMap will use the default value for each of our type wrappers, which it can do if they&#39;re monoids. For Min and Max the default value is the &#34;smallest&#34; and &#34;largest&#34; value of the wrapped type, which is defined by the Bounded interface that we require in the type signature. This works okay, and behaves as expected under most circumstances, but maybe we can try one more time: import Data.Semigroup minMax :: Ord a =&gt; [a] -&gt; Maybe (a, a) minMax xs = case foldMap (\a -&gt; Just (Min a, Max a)) xs of Just (Min x, Max y) -&gt; Just (x, y) _ -&gt; Nothing &gt;&gt;&gt; minMax [4, 1, 9, 5] Just (1,9) &gt;&gt;&gt; minMax [] Nothing Okay! This is pretty much the same, but we needed an explicit way to correctly handle an empty list of values. In this case, by wrapping our tuple in Just we invoke the Maybe monoid, and remember that foldMap is smart enough to return the &#34;empty&#34; element of that monoid if our list is empty! That means we get Nothing back if there are no elements. This may seem like &#34;magic&#34; at first, but all of these typeclasses have laws which dictate their behaviour and make them predictable. I suggest learning more about monoids if you have time, they&#39;re fascinating and useful! This is a very &#34;safe&#34; implementation, in fact much safer than most languages would offer. We explicitly return Nothing in the case that the list is empty, and the Maybe return type requires the caller to handle that case. I mentioned earlier how functions are composable, and it turns out that data-types are too! If we pair two objects with a semigroup together in a tuple, that tuple has a semigroup instance too, which combines respective element together when we combine tuples! Word Frequency This is a pretty popular one too! The challenge this time is, given a block of text, find the most common word! Ultimately, this comes down to an understanding of data-structures. import Data.List (maximumBy) import Data.Function (on) import qualified Data.Map as M mostCommonWord :: String -&gt; Maybe String mostCommonWord str = if null wordCounts then Nothing else Just . fst . maximumBy (compare `on` snd) . M.toList $ wordCounts where wordCounts = M.unionsWith (+) . fmap (\w -&gt; M.singleton w 1) . words $ str There&#39;s a bit more going on this time, so let&#39;s break it down a bit! In Haskell, we use &#34;math-style&#34; function composition using ., so we read most expressions from right-to-left. Let&#39;s look at the wordCounts binding down in the where clause first. Reading from right to left, first we use the words function from the built-in Prelude to split the incoming stream into a list of words, then we create a key-value map out of each one, consisting of the word as the key with a value of 1 to start. Now we have a list of key-value maps, and can add them up all up key-wise using unionsWith from the Data.Map library, this will count up the number of elements of each key and will result in a key-value mapping where the values represent occurrences. We&#39;ve got a mapping now, so let&#39;s find the largest count! First things first, to be safe we&#39;ll check whether the map has any values at all, if it doesn&#39;t then we&#39;ll return Nothing. Otherwise, we can convert the map into a list of key-value pairs by calling M.toList, then we can use maximumBy to return the biggest element according to a comparison function that we specify! on comes in handy here and we can tell it to compare on the second element, which is the count. That will return us the key-value pair with the largest value, then we just need to grab the key as a result using fst! Ultimately this is a bit of a naive implementation which won&#39;t work well on huge texts, but it should be enough to get you through the whiteboard portion of the interview 😄. Summary That&#39;s all I&#39;ve got for you today, nothing to revolutionary I&#39;m sure, but hopefully you had a bit of fun, or maybe learned a thing or two about what code looks like in Haskell compared to your favourite language 😄 Hopefully you learned something 🤞! If you did, please consider checking out my book: It teaches the principles of using optics in Haskell and other functional programming languages and takes you all the way from an beginner to wizard in all types of optics! You can get it here. Every sale helps me justify more time writing blog posts like this one and helps me to continue writing educational functional programming content. Cheers! </description>
      <pubDate>07 Feb 21 15:01 EST</pubDate>
      <guid>https://chrispenner.ca/posts/interview</guid>
    </item>
    <item>
      <title></title>
      <link>https://jlongster.com/How-I-Became-Better-Programmer</link>
      <description>&lt;a href=&#34;https://jlongster.com/How-I-Became-Better-Programmer&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; How I Became a Better Programmer March 20, 2017 Several people at React Conf asked me for advice on becoming a better programmer. For some reason, people see me as a pretty advanced programmer worth listening to. I thought it would be worthwhile to write down my &#34;mental model&#34; for how I have approached programming over the years. Some details about me: I&#39;m 32 years old and have over 10 years of solid experience. It probably wasn&#39;t until the last few years until I really felt confident in what I was doing. Even now, though, I continually doubt myself. The point is that this feeling doesn&#39;t go away, so just try to ignore it, keep hacking, and keep building experience. Let me be clear that these are only a few tips for improving your skills. Ultimately you need to figure out what works best for you. These are just things that I have found helpful. Find people who inspire you, but don&#39;t idolize them. Over the years there have been many people that I looked up to and watched for new tech. I learned a lot by simply trusting they were right and digging into things they worked on. These people tend to be very productive, brilliant, and inspiring. Find them and let them inspire and teach you. However, make sure not to idolize them. It&#39;s easy to seem intimidating from a twitter feed, but if you look at how they work in real life, you&#39;ll see that they aren&#39;t that different. Hacks everywhere, etc. We&#39;re all just experimenting. Lastly, don&#39;t blindly trust them; if you disagree, engage them and learn from it. Some of my most productive conversations happened this way. My Emacs config is a mess. I don&#39;t know why my OCaml autocompletion is broken (it&#39;s been broken for over a month). I don&#39;t automate stuff and have to dig around in my shell history to find commands I need sometimes. I write the ugliest code at first. I stick things on the global object until I know what I&#39;m doing. The most experienced programmer uses hacks all the time; the important part is that you&#39;re getting stuff done. Don&#39;t devalue your work. Newer programmers tend to feel like their work isn&#39;t worth much because they are new. Or maybe you are an experienced programmer, but working in a new area that makes you uncomfortable. In my opinion, some of the best ideas come from newer programmers who see improvements to existing tech that those who have already-formed opinions don&#39;t see. Your work is worthwhile, no matter what. In the worst case, if your idea doesn&#39;t work out, the community will have learned better why that approach doesn&#39;t make sense. (A note to the community: it&#39;s up to us to execute on this and be welcoming to newcomers.) Don&#39;t feel pressured to work all the time. With new tech coming out every day, it can feel like the world will move on without you if you take a night off. That&#39;s not true. In fact, you will do better work if you disengage a lot. Your perspective will be fresh, and I find myself subconsciously coming up with new ideas when I&#39;m not working. The majority of the stuff being released every day is just a rehash of the same ideas. Truly revolutionary stuff only happens every few years. A good talk to watch on this subject is Hammock Driven Development. Ignore fluff. One of the biggest ways you can objectively get better faster is by ignoring &#34;fluff&#34; that won&#39;t actually improve your skills very much. Another way to say this is &#34;use your time wisely&#34;. You only have so many hours in the day and if you spend it on deeper things you will see a big difference over time. So what is &#34;fluff&#34;? It&#39;s up to you, but I can give you some examples of what I consider fluff: language syntax, library APIs, and configuring build tooling. Learning a new ES7 JS syntax won&#39;t make you a better programmer nearly as much as learning how compilers work, for example. Adopting a new library that implements the same idea but with a new API isn&#39;t that interesting. All of those things are important, of course, but I recommend spending more time learning deeper concepts that will reward you for years. Here&#39;s a question I like to ask: do you spend most of your time making your code look &#34;nice&#34;? If so, I recommend not focusing on it so much. Your code is going to change a lot over time anyway. It&#39;s better to focus hard on the core problems you&#39;re trying to solve and think hard about your layers of abstractions. After you&#39;ve nailed all of that you can spend a little time polishing your code. (This also applies to the DRY principle. Don&#39;t worry about it so much. Feel free to duplicate.) Dig into past research. If you&#39;re excited about an idea, it&#39;s super tempting to sit down an immediately get going. But you shouldn&#39;t do that until you&#39;ve done some cursory research about how people have solved it before. Spending a few days researching the topic always completely changes how I am going to solve it. It&#39;s valuable to learn how to read academic papers. I don&#39;t know anything about denotational/operational/etc semantics so there are a lot of papers I can&#39;t read. But there are many that use code instead of math and aren&#39;t too hard to read. There is a huge amount of knowledge sitting in papers from the last 30 years. If you get good at extracting this, you&#39;ll be a thought-leader in no time. Prettier is a perfect example of this. I knew what I wanted but I had no idea how to implement it. After a little research I found this paper and after a few days I knew exactly what I needed to do. I had something basic working in a week. If I ignored previous research it would have taken a lot longer. If you&#39;re looking for papers, the Papers We Love GitHub repo is a great place to start. Take on big projects. Get uncomfortable. There&#39;s nothing better than experience. Not everyone is in the position to experiment, but if you have time, try and take on some big projects. You don&#39;t even need to finish them. Just trying to tackle something like writing a compiler will teach you tons in the first few weeks. I honestly hate the feeling where I have no idea how to solve a complex problem. It&#39;s uncomfortable. I know I&#39;ll have to do a lot of research and learning before I&#39;m even close to a solution. But I&#39;m always a much better programmer afterwards. Start with learning a new language. It&#39;s the most effective way to force you out of your current habits and see things in a new light. For me, the best thing I did as a young programmer was learn Scheme. It&#39;s an extremely simple language and forces you to do everything in a functional style, and really learn the fundamentals of how code works. The few years I spent in Scheme are still paying off today; the way I see code is fundamentally changed. (I even named my company Shift Reset LLC after the shift/reset operators from Scheme.) Here&#39;s a list of a few things I would recommend doing. These are all things that had huge impacts on my programmer career. Most of them continue to pay off to this day in subtle ways and help me to deconstruct new ideas mentally. You don&#39;t need to do these to become a good programmer, and there are many other things you can learn to improve yourself, but these are what helped me. Learn C - Just the basics, if you don&#39;t already. I think it&#39;s valuable to understand why everyone complains about it. Write a compiler - Perhaps the best way to get uncomfortable and learn. Check out the super tiny compiler. Learn macros - See Scheme, Lisp, or Clojure(Script). Macros will really change how you see code. SICP - SICP is an old book that I think is still relevant today (some people disagree). It assumes very little programming knowledge and walks you all the way up to implementing a meta-circular evaluator and compiler. Another book I really enjoyed and goes a lot deeper in compilers is Lisp In Small Pieces. Understand continuations - Continuations are a low-level control flow mechanism. Scheme is the only language to implement them, and while you will never use them in production, they will change how you think about control flow. I wrote a blog post trying to explain them. If anything, just try a new language - Regardless of what you do, you really should explore other languages. I would recommend any of the following: Clojure, Rust, Elm, OCaml/Reason, Go, or Scheme. All of them have unique features and will force you to learn a new way of thinking. James Long is a developer &amp; designer with over a decade of experience building large-scale applications. Get in touch © James Long 2020 </description>
      <pubDate>24 Mar 20 19:25 EDT</pubDate>
      <guid>https://jlongster.com/How-I-Became-Better-Programmer</guid>
    </item>
    <item>
      <title></title>
      <link>https://mtlynch.io/</link>
      <description>&lt;a href=&#34;https://mtlynch.io/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I&#39;m Michael Lynch, software developer and blogger. I used to work as a software engineer at large companies, but now I run small software businesses of my own and blog about the process. Most Popular Articles Why I Quit Google to Work for Myself TinyPilot: Build a KVM Over IP for Under $100 How I Stole Your Siacoin My Second Year as a Solo Developer Articles about Software Development How to Make Your Code Reviewer Fall in Love with You How to Do Code Reviews Like a Human Why Good Developers Write Bad Unit Tests End-to-End Testing Web Apps: The Painless Way Articles about Blogging How I Hired a Freelance Editor for My Blog How to Hire a Cartoonist to Make Your Blog Less Boring Hiring Content Writers: A Guide for Small Businesses </description>
      <pubDate>01 Feb 21 12:22 EST</pubDate>
      <guid>https://mtlynch.io/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.pnas.org/content/114/25/6521</link>
      <description>&lt;a href=&#34;https://www.pnas.org/content/114/25/6521&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Research Article Language from police body camera footage shows racial disparities in officer respect , Nicholas P. Camp, Vinodkumar Prabhakaran, William L. Hamilton, Rebecca C. Hetey, Camilla M. Griffiths, David Jurgens, Dan Jurafsky, and Jennifer L. EberhardtaDepartment of Linguistics, Stanford University, Stanford, CA 94305;bDepartment of Psychology, Stanford University, Stanford, CA 94305;cDepartment of Computer Science, Stanford University, Stanford, CA 94305 See allHide authors and affiliations Contributed by Jennifer L. Eberhardt, March 26, 2017 (sent for review February 14, 2017; reviewed by James Pennebaker and Tom Tyler) SignificancePolice officers speak significantly less respectfully to black than to white community members in everyday traffic stops, even after controlling for officer race, infraction severity, stop location, and stop outcome. This paper presents a systematic analysis of officer body-worn camera footage, using computational linguistic techniques to automatically measure the respect level that officers display to community members. This work demonstrates that body camera footage can be used as a rich source of data rather than merely archival evidence, and paves the way for developing powerful language-based tools for studying and potentially improving police–community relations.AbstractUsing footage from body-worn cameras, we analyze the respectfulness of police officer language toward white and black community members during routine traffic stops. We develop computational linguistic methods that extract levels of respect automatically from transcripts, informed by a thin-slicing study of participant ratings of officer utterances. We find that officers speak with consistently less respect toward black versus white community members, even after controlling for the race of the officer, the severity of the infraction, the location of the stop, and the outcome of the stop. Such disparities in common, everyday interactions between police and the communities they serve have important implications for procedural justice and the building of police–community trust.racial disparitiesnatural language processingprocedural justicetraffic stopspolicingOver the last several years, our nation has been rocked by an onslaught of incidents captured on video involving police officers’ use of force with black suspects. The images from these cases are disturbing, both exposing and igniting police–community conflict all over the country: in New York, Missouri, Ohio, South Carolina, Maryland, Illinois, Wisconsin, Louisiana, Oklahoma, and North Carolina. These images have renewed conversations about modern-day race relations and have led many to question how far we have come (1). In an effort to increase accountability and transparency, law enforcement agencies are adopting body-worn cameras at an extremely rapid pace (2, 3).Despite the rapid proliferation of body-worn cameras, no law enforcement agency has systematically analyzed the massive amounts of footage these cameras produce. Instead, the public and agencies alike tend to focus on the fraction of videos involving high-profile incidents, using footage as evidence of innocence or guilt in individual encounters.Left unexamined are the common, everyday interactions between the police and the communities they serve. By best estimates, more than one quarter of the public (ages 16 y and over) comes into contact with the police during the course of a year, most frequently as the result of a police-initiated traffic stop (4, 5). Here, we examine body-worn camera footage of routine traffic stops in the large, racially diverse city of Oakland, CA.Routine traffic stops are not only common, they are consequential, each an opportunity to build or erode public trust in the police. Being treated with respect builds trust in the fairness of an officer’s behavior, whereas rude or disrespectful treatment can erode trust (6, 7). Moreover, a person’s experiences of respect or disrespect in personal interactions with police officers play a central role in their judgments of how procedurally fair the police are as an institution, as well as their willingness to support or cooperate with the police (8, 9).Blacks report more negative experiences in their interactions with the police than other groups (10). Across numerous studies, for example, blacks report being treated less fairly and respectfully in their contacts with the police than whites (6, 11). Indeed, some have argued that racial disparities in perceived treatment during routine encounters help fuel the mistrust of police in the controversial officer-involved shootings that have received such great attention. However, do officers treat white community members with a greater degree of respect than they afford to blacks?We address this question by analyzing officers’ language during vehicle stops of white and black community members. Although many factors may shape these interactions, an officer’s words are undoubtedly critical: Through them, the officer can communicate respect and understanding of a citizen’s perspective, or contempt and disregard for their voice. Furthermore, the language of those in positions of institutional power (police officers, judges, work superiors) has greater influence over the course of the interaction than the language used by those with less power (12⇓⇓⇓–16). Measuring officer language thus provides a quantitative lens on one key aspect of the quality or tone of police–community interactions, and offers new opportunities for advancing police training.Previous research on police–community interactions has relied on citizens’ recollection of past interactions (10) or researcher observation of officer behavior (17⇓⇓–20) to assess procedural fairness. Although these methods are invaluable, they offer an indirect view of officer behavior and are limited to a small number of interactions. Furthermore, the very presence of researchers may influence the police behavior those researchers seek to measure (21).In study 1, human participants rated officer utterances on several overlapping dimensions of respect. With a high degree of agreement, participants inferred these dimensions from officer language. Even though they were not told the race of the stopped driver, participants judged officer language directed toward black motorists to be less respectful than language directed toward whites. In study 2, we build statistical models capable of predicting aspects of respect based on linguistic features derived from theories of politeness, power, and social distance. We discuss the linguistic features that contribute to each model, finding that particular forms of politeness are implicated in perceptions of respect. In study 3, we apply these models to all vehicle stop interactions between officers of the Oakland Police Department and black/white community members during the month of April 2014. We find strong evidence that utterances spoken to white community members are consistently more respectful, even after controlling for contextual factors such as the severity of the offense or the outcome of the stop.DataOur dataset consists of transcribed body camera footage from vehicle stops of white and black community members conducted by the Oakland Police Department during the month of April 2014. We examined 981 stops of black (N = 682) and white (N = 299) drivers from this period, 68.1% of the 1,440 stops of white and black drivers in this period. These 981 stops were conducted by 245 different officers (see SI Appendix, Data Sampling Process for inclusion criteria). Per Oakland Police Department policy, officers turn on their cameras before making contact with the driver and record for the duration of the stop. From the 183 h of footage in these interactions, we obtain 36,738 usable officer utterances for our analysis.Study 1: Perceptions of Officer Treatment from Language.We first test whether human raters can reliably judge respect from officers’ language, and whether these judgments reveal differences in officer respect toward black versus white community members.Respect is a complex and gradient perception, incorporating elements of a number of correlated constructs like friendliness and formality. Therefore, in this study, we ask participants to rate transcribed utterances spoken by officers along five conceptually overlapping folk notions related to respect and officer treatment. We randomly sampled 414 unique officer utterances (1.1% of all usable utterances in the dataset) directed toward black (N = 312) or white (N = 102) community members. On each trial, participants viewed the text of an officer utterance, along with the driver’s utterance that immediately preceded it. All proper names and places were anonymized, and participants were not told the race or gender of the driver. Participants indicated on four-point Likert scales how respectful, polite, friendly, formal, and impartial the officer was in each exchange. Each utterance was rated by at least 10 participants.Could participants reliably glean these qualities from such brief exchanges? Previous work has demonstrated that different perceivers can arrive at similar judgments from “thin slices” of behavior (22). In a similar vein, participants showed consistency in their perceptions of officer language, with reliability for each item ranging from moderate (Cronbach’s α = 0.73) to high (α = 0.91) agreement (see SI Appendix, Annotator Agreement). These results demonstrate that transcribed language provides a sufficient and consensual signal of officer communication, enough to gain a picture of the dynamics of an interaction at a given point in time.To test whether participant ratings uncovered racial group differences, we averaged scores across raters to calculate a single rating on each dimension for each utterance, then built a linear mixed-effects regression model to estimate the fixed effect of community member race across interactions, controlling for variance of a random effect at the interaction level. Officer utterances directed toward black drivers were perceived as less respectful [b = −0.23, 95% confidence interval (−0.34, −0.11)], polite [b = −0.23 (−0.35, −0.12)], friendly [b = −0.24 (−0.36, −0.12)], formal [b = −0.16 (−0.30, −0.03)], and impartial [b = −0.26 (−0.39, −0.12)] than language directed toward white drivers (Fig. 1). These differences persisted even when controlling for the age and sex of the driver (see SI Appendix, Model Outputs for Each Rated Dimension).Download figure Open in new tab Download powerpoint Fig. 1.(Left) Differences in raw participant ratings between interactions with black and white community members. (Right) When collapsed to two uncorrelated components, Respect and Formality, we find a significant difference for Respect but none for Formality. Error bars represent 95% confidence intervals. PC, principal component.Given the expected conceptual overlap in the five perceptual categories we presented to the participants, we used principal component analysis to decompose the ratings into their underlying components. Two principal components explained 93.2% of the variance in the data (see SI Appendix, Principal Component Analysis (PCA) Loadings for loadings). The first component, explaining 71.3% of the variance and composed of positive loadings on the impartial, respectful, friendly, and polite dimensions with some loading on the formal dimension, we characterize as Respect, broadly construed. The second, explaining 21.9% of the variance and composed primarily of a very high positive loading on the formal dimension and a weak negative loading on the friendly dimension, we characterize as Formality. This component captures formality as distinct from respect more generally, and is likely related to social distance.Standardizing these factor scores as outcome variables in mixed-effects models, we find that officers were equal in Formality with white and black drivers [β = −0.01 (−0.19, 0.16)], but higher in Respect with white drivers [β = 0.17 (0.00, 0.33)] (Fig. 1).Study 1 demonstrates that key features of police treatment can be reliably gleaned from officer speech. Participant ratings from thin slices of police–community interactions reveal racial disparities in how respectful, impartial, polite, friendly, and formal officers’ language to community members was perceived. Such differences were driven by differences in the Respect officers communicated toward drivers rather than the Formality with which officers addressed them.Study 2: Linguistic Correlates of Respect.The methods of study 1 (human coding of 414 individual utterances), although effective at discovering racial disparities in officer respect toward community members in our dataset, cannot offer a general solution to the analysis of body camera data. One problem is scale: Each year, on the order of 26 million vehicle stops are made (5). Furthermore, using only a small sample of individual utterances makes it impossible to study how police treatment varies over officers, or how the interaction progresses across time in each stop.In this study, we therefore develop computational linguistic models of respect and formality and tune them on the 414 individual utterances; in study 3, we apply these models to our full dataset of 36,738 utterances. Our method is based on linguistic theories of respect that model how speakers use respectful language (apologizing, giving agency, softening of commands, etc.) to mitigate “face-threatening acts.” We use computational linguistic methods (e.g., refs. 23⇓⇓–26) to extract features of the language of each officer utterance. The log-transformed counts of these features are then used as independent variables in two linear regression models predicting the perceptual ratings of Respect and Formality from study 1.Our model-assigned ratings agree with the average human from study 1 about as well as humans agree with each other. Our model for Respect obtains an adjusted R2 of 0.258 on the perceptual ratings obtained in study 1, and a root-mean-square error (RMSE) of 0.840, compared with an RMSE of 0.842 for the average rater relative to other raters. Our model for Formality obtains an adjusted R2 of 0.190, and an RMSE of 0.882 compared with 0.764 for the average rater (see SI Appendix, Model Comparison to Annotators for more details on how these values were calculated). These results indicate that, despite the sophisticated social and psychological cues participants are likely drawing upon in rating officers’ utterances, a constrained set of objectively measurable linguistic features can explain a meaningful portion of the variance in these ratings.Fig. 2 lists the linguistic features that received significant weights in our model of Respect (arranged by their model coefficients). For example, apologizing, gratitude, and expressions of concern for citizen safety are all associated with respect. The bars on the right show the log-odds of the relative proportion of interactions in our dataset taken up by each feature, where negative numbers mean that a feature comprised a larger proportion of officers’ speech in interactions with black community members and positive numbers mean the same for interactions with white community members. Example utterances containing instances of the highest-weighted features for the Respect model are shown in Fig. 3. See SI Appendix, Study 2 for full regression outputs and more detailed discussion of particular linguistic findings.Download figure Open in new tab Download powerpoint Fig. 2.(Left) Respect weights assigned by final model to linguistic features and (Right) the corresponding log-odds of those features occurring in officer speech directed toward black versus white community members, calculated using Fisher’s exact test. †P &lt; 0.1; ∗P &lt; 0.05; ∗∗P &lt; 0.01; ∗∗∗P &lt; 0.001.Download figure Open in new tab Download powerpoint Fig. 3.Sample sentences with automatically generated Respect scores. Features in blue have positive coefficients in the model and connote respect, such as offering reassurance (“no problem”) or mentioning community member well-being (“drive safe”). Features in red have negative coefficients in the model and connote disrespect, like informal titles (“my man”), or disfluencies (“that- that’s”).Study 3: Racial Disparities in Respect.Having demonstrated that people can reliably infer features of procedural justice from officer speech (study 1), and that these ratings can be reliably predicted from statistical models of linguistic features (study 2), we are now able to address our central question: Controlling for contextual factors of the interaction, is officers’ language more respectful when speaking to white as opposed to black community members?We apply our models from study 2 to the entire corpus of transcribed interactions to generate predicted scores for Respect and Formality for each of the 36,738 utterances in our dataset. We then build linear mixed-effects models for Respect and Formality over these utterances. We include, as covariates in our primary model, community member race, age, and gender; officer race; whether a search was conducted; and the result of the stop (warning, citation, or arrest). We include random intercepts for interactions nested within officers.Controlling for these contextual factors, utterances spoken by officers to white community members score higher in Respect [β = 0.05 (0.03, 0.08)]. Officer utterances were also higher in Respect when spoken to older [β = 0.07 (0.05, 0.09)] community members and when a citation was issued [β = 0.04 (0.02, 0.06)]; Respect was lower in stops where a search was conducted [β = −0.08 (−0.11, −0.05)]. Officer race did not contribute a significant effect. Furthermore, in an additional model on 965 stops for which geographic information was available, neither the crime rate nor density of businesses in the area of the stop were significant, although a higher crime rate was indicative of increased Formality [β = 0.03 (0.01, 0.05)].One might consider the hypothesis that officers were less respectful when pulling over community members for more severe offenses. We tested this by running another model on a subset of 869 interactions for which we obtained ratings of offense severity on a four-point Likert scale from Oakland Police Department officers, including these ratings as a covariate in addition to those mentioned above. We found that the offense severity was not predictive of officer respect levels, and did not substantially change the results described above.To consider whether this disparity persists in the most “everyday” interactions, we also reran our analyses on the subset of interactions that did not involve arrests or searches (N = 781), and found the results from our earlier models were fundamentally unchanged. Full regression tables for all models described above are given in SI Appendix, Study 3.Another hypothesis is that the racial disparities might have been caused by officers being more formal to white community members, and more informal or colloquial to black community members. However, we found that race was not associated with the formality of officers’ utterances. Instead, utterances were higher in Formality in interactions with older [β = 0.05 (0.03, 0.07)] and female [β = 0.02 (0.00, 0.04)] community members.Are the racial disparities in the respectfulness of officer speech we observe driven by a small number of officers? We calculated the officer-level difference between white and black stops for every officer (N = 90) in the dataset who had interactions with both blacks and whites (Fig. 4). We find a roughly normal distribution of these deltas for officers of all races. This contrasts with the case of stop-and-frisk, where individual outlier officers account for a substantial proportion of racial disparities (27); the disparities we observe here cannot be explained by a small number of extreme officers.Download figure Open in new tab Download powerpoint Fig. 4.Kernel density estimate of individual officer-level differences in Respect when talking to white as opposed to black community members, for the 90 officers in our dataset who have interactions with both blacks and whites. More positive numbers on the x axis represent a greater positive shift in Respect toward white community members.Because our model is able to generate scores across all utterances in our dataset, we can also consider aspects of the trajectory of interactions beyond the mean level of respect (Fig. 5). Growth-curve analyses revealed that officers spoke with greater Respect [b = 0.35 (0.29, 0.40)] and reduced Formality [b = −0.57 (−0.62, −0.53)] as interactions progressed. However, these trajectories varied by community member race: Although stops of white and black drivers converged in the Formality expressed during the interaction [b = −0.09 (−0.13, −0.05)], the gap in Respect increased over time [b = 0.10 (0.05, 0.15)]. That is, officer Respect increased more quickly in interactions with white drivers [b = 0.45 (0.38, 0.54)] than in interactions with black drivers [b =  0.24 (0.19, 0.29)].Download figure Open in new tab Download powerpoint Fig. 5.Loess-smoothed estimates of the (Left) Respect and (Right) Formality of officers’ utterances relative to the point in an interaction at which they occur. Respect tends to start low and increase over an interaction, whereas the opposite is true for Formality. The race discrepancy in Respect is consistent throughout the interactions in our dataset.Discussion.Despite the formative role officer respect plays in establishing or eroding police legitimacy (7), it has been impossible to measure how police officers communicate with the public, let alone gauge racial disparities in officer respect. However, body-worn cameras capture such interactions every day. Computational linguistic techniques let us examine police–community contacts in a manner powerful enough to scale to any number of interactions, but sensitive enough to capture the interpersonal qualities that matter to the police and public alike.In doing so, we first showed that people make consistent judgments about such interactions from officers’ language, and we identified two underlying, uncorrelated constructs perceived by participants: Respect and Formality. We then built computational linguistic models of these constructs, identifying crucial positive and negative politeness strategies in the police–community interactional context. Applying these models to an entire month of vehicle stops, we showed strong evidence for racial disparities in Respect, but not in Formality: Officers’ language is less respectful when speaking to black community members.Indeed, we find that white community members are 57% more likely to hear an officer say one of the most respectful utterances in our dataset, whereas black community members are 61% more likely to hear an officer say one of the least respectful utterances in our dataset. (Here we define the top 10% of utterances to be most respectful and the bottom 10% to be least respectful.)This work demonstrates the power of body camera footage as an important source of data, not just as evidence, addressing limitations with methodologies that rely on citizens’ recollection of past interactions (10) or direct researcher observation of police behavior (17⇓⇓–20). However, studying body camera footage presents numerous hurdles, including privacy concerns and the raw scale of the data. The computational linguistic models presented here offer a path toward addressing both these concerns, allowing for the analysis of transcribed datasets of any size, and generating reliable ratings of respect automatically. These models have the potential to allow for useful information about an interaction to be extracted while maintaining officer and community member privacy.The racial disparities in officer respect are clear and consistent, yet the causes of these disparities are less clear. It is certainly possible that some of these disparities are prompted by the language and behavior of the community members themselves, particularly as historical tensions in Oakland and preexisting beliefs about the legitimacy of the police may induce fear, anger, or stereotype threat. However, community member speech cannot be the sole cause of these disparities. Study 1 found racial disparities in police language even when annotators judged that language in the context of the community member’s utterances. We observe racial disparities in officer respect even in police utterances from the initial 5% of an interaction, suggesting that officers speak differently to community members of different races even before the driver has had the opportunity to say much at all.Regardless of cause, we have found that police officers’ interactions with blacks tend to be more fraught, not only in terms of disproportionate outcomes (as previous work has shown) but also interpersonally, even when no arrest is made and no use of force occurs. These disparities could have adverse downstream effects, as experiences of respect or disrespect in personal interactions with police officers play a central role in community members’ judgments of how procedurally fair the police are as an institution, as well as the community’s willingness to support or cooperate with the police (8, 9).We now have a method for quantifying these troubled interactions. Although the circumstances of any particular stop can vary dramatically, our approach allows us to measure aggregate department-level trends, revealing disparities across hundreds of interactions. These disparities are part of a constellation of differences in officer language spoken toward black versus white community members; a simple classifier trained on only the words used by officers is able to correctly predict the race of the community member in over two thirds of the interactions (see SI Appendix, Linguistic Classification Accuracy of Race).Future research could expand body camera analysis beyond text to include information from the audio such as speech intonation and emotional prosody, and video, such as the citizen’s facial expressions and body movement, offering even more insight into how interactions progress and can sometimes go awry. In addition, footage analysis could help us better understand what linguistic acts lead interactions to go well, which can inform police training and quantify its impacts over time.The studies presented here open a path toward these future opportunities and represent an important area of research for the study of policing: Computational, large-scale analyses of language give us a way to examine and improve police–community interaction that we have never had before.Materials and MethodsData and Processing.The video for each traffic stop was transcribed into text by professional transcribers, who transcribed while listening to audio and watching the video. Extensive measures were taken to preserve privacy; data were kept on a central server, and transcribers (as well as all researchers) underwent background checks with the Oakland Police Department. Transcribers also “diarized” the text (labeling who was speaking at each time point). We used the diarization to automatically remove all officer speech to the dispatcher or to other officers, leaving only speech from the officer directed toward the community member. After transcription, transcripts were manually cleaned up, heuristically fixing transcriber diarization errors, and correcting typographical errors involving utterance timing so that all transcripts were automatically readable. Every utterance in the dataset was processed with Stanford CoreNLP 3.4.1 (28) to generate sentence and word segmentation, part-of-speech tags, and dependency parses used for feature extraction and analysis.The raw video footage associated with this paper was available for our research purposes with the cooperation of the Oakland Police Department, and naturally cannot be publicly distributed. However, we make available deidentified data frames for each study described here, so that other researchers can replicate our results. We also release all of the code for the computational linguistic models, as well as pretrained models that can be run on arbitrary text.Human Annotation of Utterances.A subset of 420 exchanges, consisting of one officer utterance (defined as a “turn” of one or more sentences by transcribers) and, if applicable, the immediately preceding community member utterance were sampled from the corpus for annotation. Utterances were sampled with the constraint that at least 15 words were spoken between the two speakers, and that at least five words were spoken by the officer. These utterances were grouped into seven “batches” of 60 utterances apiece. Due to a data error, six duplicate utterances were annotated, but were excluded from subsequent analyses, resulting in 414 unique utterances toward black (N = 312) and white (N = 102) community members.Each of 70 participants (39 female, Mage = 25.3) rated a batch of 60 of these utterances, such that each utterance was rated by at least 10 participants. On each trial, participants viewed the text of an exchange between a police officer and a community member: the text of the officer utterance, as well as the text of the community member utterance that immediately preceded it, if there was one. They then indicated, on four-point bipolar Likert scales, how respectful, polite, friendly, formal, and impartial the officer was in each exchange. Participants were allowed to indicate that they could not rate an utterance on a particular dimension, but were encouraged to nonetheless indicate their best guess. Participants had no other information about the interaction besides the officer’s utterance and the immediately preceding community member utterance.All research was approved by the Stanford University Institutional Review Board, and written informed consent was obtained from all raters before their participation.Computational Annotation of Utterances.Our model draws on linguistic theories of politeness; the technical term “politeness” refers to how concepts like respect, formality, and social distance take shape in language. These theories suggest that speakers use polite or respectful language to mitigate face-threatening acts (29⇓–31).Negative politeness is used to mitigate direct commands or other impositions that limit the freedom of action of the listener, for example, by minimizing the imposition or emphasizing the agency of the interlocutor. Such strategies are central to police–community interactions because of the inherently coercive nature of a traffic stop. For instance, the use of the word “please” can soften requests and provide a sense of agency or choice; apologizing (“sorry,” “excuse me”) can admit regret on the part of the officer that some request is necessary; the use of hedges (“may,” “kinda,” “probably”) may reduce the perception of imposition.Positive politeness is used to show that the speaker values the interlocutor and their interests, or to minimize the impact of actions that could damage such a perception. Positive politeness strategies are also crucial for police–community interactions, where the inherently unequal social roles at play may necessitate a particular sensitivity to the community member’s positive face. For instance, greetings and introductions can establish a friendly context at the beginning of an interaction and convey openness. Expressions of reassurance (“no big deal,” “don’t worry”) seek to assuage the community member’s potential concerns in tense circumstances, and expressions of gratitude (“thank you”) serve to reduce the perceived power differential by deferring to the actions of the community member. Mentions of safety (“Drive safely now”) explicitly acknowledge concern for the community member’s personal well-being. Referring expressions are another important component of positive politeness; formal titles (“sir,” “ma’am,” “Mr.,” “Ms.”) and surnames may convey a contrast with informal titles (“dude,” “bro,” “bud”) and first names (31⇓–33).We also include features we expect to capture officer anxiety, such as speech disfluencies (“w- well”) and commands to keep “hands on the wheel,” which may contribute to a community member’s perception of disrespect. These are of a different character than the politeness strategies discussed above, but we found that all analyses presented here hold true even if these features are not included.We use standard techniques to automatically extract features from the text of each utterance (23⇓⇓–26). These features include lexicons (lists of words). For example, to detect informal titles, we used an augmented version of a word list from ref. 34. We also used regular expressions, such as for detecting tag questions (“do that for me, will you?”), and syntactic parse features, such as a feature that detects when “just” is used in constructions as an adverbial modifier.Features were modeled as log-transformed counts in each utterance, and were used as independent variables in two linear regression models predicting the human perceptual ratings of respect and formality obtained in study 1. They were introduced into the regression using stepwise forward selection by R2 to remove features that don’t substantially contribute to the model’s accuracy.AcknowledgmentsThis research was supported by the John D. and Catherine T. MacArthur Foundation, with additional support from the Stanford Institute for Research in the Social Sciences, the Stanford School of Humanities and Sciences, and the Stanford Data Science Initiative. We also thank the City of Oakland and the Oakland Police Department for their support and cooperation.FootnotesAuthor contributions: R.V., N.P.C., D. Jurafsky, and J.L.E. designed research; R.V. and N.P.C. performed research; V.P., W.L.H., R.C.H., C.M.G., and D. Jurgens contributed new reagents/analytic tools; R.V. and N.P.C. analyzed data; R.V., N.P.C., D. Jurafsky, and J.L.E. wrote the paper; and D. Jurafsky and J.L.E. served as PI on this project.Reviewers: J.P., University of Texas at Austin; and T.T., Yale Law School.Conflict of interest statement: J.L.E. was invited by a federal judge and monitor to serve as a Subject Matter Expert to assist with the Oakland Police Department’s reform efforts. The assignment began prior to the studies reported here.This article contains supporting information online at www.pnas.org/lookup/suppl/doi:10.1073/pnas.1702413114/-/DCSupplemental. </description>
      <pubDate>12 Jun 20 12:04 EDT</pubDate>
      <guid>https://www.pnas.org/content/114/25/6521</guid>
    </item>
    <item>
      <title></title>
      <link>https://livingafi.com/2021/03/17/the-2021-early-retirement-update/</link>
      <description>&lt;a href=&#34;https://livingafi.com/2021/03/17/the-2021-early-retirement-update/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I’ve had a difficult time finding a way to create a new entry on a blog that has essentially been dead since 2015.  Yet, I have had a persistent feeling that I owe my readers an update. Consider this an effort to encapsulate the last five or six years into a single post containing fewer than seven eight nine ten thousand thousand words so you can hopefully finish reading it in under half an hour. 2015 I retired early on a shoestring, something called lean-FIRE as dubbed by the early-retirement geeks: a sub 4% withdrawal rate (appx 30Kish a year spend — renting, no home ownership) taken from a nest egg of approximately 950K. I followed a plan I’d outlined in previous posts. I had my asset sheet ready. During the year, I went on a few modest vacations and enjoyed myself. I decompressed from work, stopped worrying about how productive I was, and more-or-less became accustomed to living out my days while not worrying too much about status.  Or getting ahead. Or anything at all, really. Despite the mental challenges, I was overwhelmingly happy for the duration. 2016 My blogging alter-ego, Dr. Doom, looks like this when he is happy.  Kind of frightening, I know. What can I say?  It was a great year. More vacations and travel. A trip to Hawaii. Another to Portugal and France, taken together, two straight months out of the country.  My wife has family in Portugal close to Lisbon, where we stayed for free, treating them to meals out and paying for the cost of various leisure activities for them in return for lodging — a fantastic trade. I sucked down an awful lot of media — movies, video games, books. Way, way too much to document here. A day would often consist of exercising for an hour or two, spending some time my partner, and weaving reading/movie/videogames/tv stuff and/or listening to new music and/or playing guitar throughout the rest of it.  A favorite activity was just laying on the couch with my wife, both of us reading for hours, facing each other while leaning on opposite arms of the sofa, with our legs intertwined. I also continued to spend a lot of time with family — my aging mother, my wife’s nieces and nephews and parents, talking on the phone with folks. I saw old friends, getting together with them usually during a lunch hour where they’d stolen away from their own employer for a while just to see me and grab Thai food or something. Toward the end of the year, I decided I wanted to pursue writing in a more serious way. During this year I was overwhelmingly very, very happy. I think it was the best year of my entire life. I’d decompressed from work during the previous year and so this year I sort of let it all hang out. My wife and I were excited about the changes we’d made and felt closer than ever.  Fantastic. 2017 Uh, we’re still friends, right? A lot of writing and reading. The majority of my day, most days.  It’s terrific. At the same time, I have a growing sense of disconnect from some of my old friends. Time spent with family continues to be, you know, family time. Fine. Sometimes boring, sometimes dramatic, sometimes painful — particularly when a family member is having difficulty with something in their life and I can’t help.  That kind of thing is just hard. But most of the time it’s awesome.  Laughing and reminiscing and sharing meals and watching dumb comedies together and feeling close and happy. But peers — people my age that are still working that I’ve been friends with for decades.  That situation is another matter entirely. I find this year that I am losing a sense of intimacy with some of them. They work a lot — I do not. We are starting to lose some common interests and activities that helped to create our friendships in the first place.  Some of the threads that bound us together for 20 plus years were perhaps unravelling.   Not going to lie.  These developments bothered me.  The first couple of years it didn’t seem to matter, the differences in our lives.  Here in the third year, they do. I speculate that initially they viewed my change of lifestyle as temporary, but after a while, it stuck — they realized that I would not be working again and I am therefore, ahem, officially and forever different.  (This is no temporary thing.)  I feel sure that some of them also felt that they were, you know, continuing to move and shake and improve their lives whereas, from their perspective, I’d sort of given up — my life appeared to be a static thing, unchanging, and therefore pretty fucking boring. I should point out that this (the growing feeling of disconnection) wasn’t true of all of my friends.  With some of them I felt as close as ever.  For example with my three best friends there have been very few changes to our relationship dynamics.  They are the kind of people that don’t care whether or not you have recently updated your kitchen, if you catch my drift.  But virtually everyone else — my more casual, normal-type friends — these are the people that I have more trouble relating to as time passes.   At any rate, I didn’t realize that this happened to so many people:  Once they have been established in their careers for a decade or more, they 1) feel secure in their job 2) have accumulated some money and 3) feel compelled to start spending.   And once everyone in your peer group is spending more money — well, then everyone is looking over their shoulder at everyone else’s spending and this is really how the whole Keeping Up game begins.  This tendency overtook even people I would not have suspected to be vulnerable to the treadmill — one of my old friends from college for example, who grew up poor and didn’t spend much back in the day, now has a summer home in New Hampshire and a 4000 sq foot McMansion and all the trimmings.  There is no going back.  It’s strange to watch your own friends change their spending and living patterns right in front of your eyes as you get older.  It fills in some of the blanks that you had when you were younger, looking up at the older generation, wondering how people and families turned into what they were.  Also throughout this year, my partner and I perhaps felt less close to one another.  She was restless and according to my journal entries for the year I suspected she was growing depressed but trying to hide it from me.  Late in the year, I confronted her on my observations and she didn’t deny it — she was having trouble with the new lifestyle.  She also indicated that maybe she wanted to go back to work. This year we are mostly doing well, particularly in the first half.  But there is a gradual and ominous shift to something else, something not so great, and we can both feel it. During the year, I try to support her:  I encourage her to either find employment again or look for something that might make her happier but she refuses both options.  She said this despite telling me that she felt she was happier back when she was working.  (She said this in Oct of 2017).  A large part of our relationship in the final quarter of this year is examining things that might make her feel better, because she is not doing great anymore.  (She rejected the idea of seeing a therapist to help her think things through.) More travel: Multiple Canadian cities, Niagra Falls, Austin TX, a ton of little 1-off trips to drive-able cities that I’d never before been to: Portland Maine, Rockland Maine for the annual Lobster Fest, Albany NY. We saw at least 20 museums this year, too many to list.  We walked outside for an hour and a half nearly every day, smack in the middle of it, when normal people were at work.  And there are just a bazillion little fun things that we did that would take too much space to document.  This year we took full advantage of our freedom. I am still fairly happy this year although toward the last couple of months I can see that things are perhaps beginning to move in the wrong direction. 2018 My blogging alter ego, Victor Von Doom, at his desk writing crappy fiction:  “It was a Dark and Batman-y Knight.  Plus, Stormy, because Storm was summoning a typhoon.”   This is how cringey Marvel-DC Universe fan fiction crossovers begin. The writing stalls. In 2017, it was fun and I did it for its own sake.  Here in 2018, the practice is still enjoyable but I have more difficulty completing stories. Or feeling like I am making measurable progress toward any definite milestones or goals.  I suddenly feel like it’s important to get some kind of external validation for my work:  I submit completed work to publications and am rejected.  Most of the time I feel as though I’m adrift at sea with this ancient fantasy of mine — the fantasy that I might be a writer.  I start to think: I’m not good enough.  I’m too old.  I will never achieve mastery, and even if I do, no one will notice. My brain tells me it’s over — that my window of opportunity has passed — with that familiar subjective (and often wrong) complete certainty that your internal voice loves to provide. I am perfectly aware that it took most established writers years and years and years of practice before anything significant happened – a published story, finding a publicist that believed in them, readers who clamored for more.  And I know that most people who want to be writers wind up writing a lot and never going anywhere.  Further, I also know that many writers who are Quite Fucking Good go completely unnoticed throughout their lives.  As is the case with virtually everything in the arts, it’s not exactly an easy thing to do — to be externally successful. So this year I’m frequently wondering if I have the stamina to make it through this phase, the middle-territory, where I’m reasonably competent but not good enough to do anything other than amateur hour shit that lives in a file on my computer. I start to understand I need new friends — people that are into what I’m into.  I sign up for Creative Writing evening classes at a local state college and make it through the semester just fine and my professor loves me but I’m underwhelmed by the whole experience and don’t make friends.  (I have a lot to say about this but won’t unpack it here because I’m trying to keep this post at a high-level-summary for the most part.) My partner and I are doing well overall, particularly in the first half of the year.  She’s still struggling to be happier, but on balance our relationship continues to be pretty awesome.  We continue to spend a lot of time together, taking trips, laughing about dumb shit, going on daily walks, sexy times, cuddling, all of the good stuff that makes relationships great. Toward the end of 2018 though, in December, we had an argument that still haunts me.  She said she wasn’t “kind of sort of” unhappy anymore.  Nope.  Now it was official.  Unhappy with a capital U.   She revealed that she felt like she wasn’t “going anywhere.” And when we were both working and getting richer, she felt like we were secretly getting ahead — making forward progress of sorts — even though nobody else could see it because we weren’t broadcasting our early retirement goals and corresponding asset sheet. (See: Stealth Wealth.) The sharp yearly increases in account balances we experienced during our working years gave her life some sense of momentum that had been since lost since we stopped receiving paychecks.  She also said that she now felt her friends’ lives were better than ours. This was her phrasing. I disagreed:  They’re working, we are not.  I said that I personally love the hell out of this, that we’ve stopped performing activities which so frequently made us miserable.  I see our current lives as better.  Theirs appear to be a constant struggle most of the time. It didn’t matter. Her comments were telling. She felt like she was ‘falling behind’ and this sense made her anxious and uncomfortable — it contributed to feelings of worthlessness, and she felt like she was doing something wrong with her life.  Another phrase, similar to the first one but turned on its head: ‘We should be further along in our lives.’    In other words, she still felt pressure to keep up with the mythical Joneses.  She was no longer on the treadmill of work.  She was instead standing still on the sidelines, watching all her friends.  They moved their legs on the exercise machines and money came out of the side of them.  They moved a little more and new hardwood flooring appeared in their bedroom at home.  More yet and they’re rewarded with a brand new sunroom.   To me, these people on their exercise machines were standing still — nothing was happening in their lives.  But to her, they appeared to be going somewhere.  Because: Instagram and Facebook and posts about their Very Awesome Timeshares and summer homes and expensive vacations and whatever else they were blowing money on.  Their acquisitions signaled progress to her in a way that they simply did not for me.  I suppose this is is a testament to the different reactions one can have to the same experience — the experience of watching your friends work away at through their jobs while they spend money on &lt;stuff&gt;. Something else that I’ve come to believe is this:  She felt that her friends were jumping upward in class.  It wasn’t just about the material stuff — and it also wasn’t about the fancy vacations per se.  It was the fact that upper-middle and lower-upper class people talk a lot about these things — the homes, the upgrades, the travel.  It’s their language.  Then they weave the material stuff into a narrative about how they’ve earned it, how their life is a struggle and they’ve done incredibly well to pull off this level of comfort, and it all somehow has to do with how incredible they are as people.  There’s a self-satisfaction in the conversational rituals that she was missing out on.   Perhaps by extension, she was lower class by not being able to participate directly in these spending patterns and conversational games. This is no longer as simple as FOMO.  This is feeling estranged and lost — excluded and isolated.  I wrote about this sort of problem way back in this ancient post. My hypothesis in that post:  Certain people are just wired to be like everyone else.  To move with the pack, stay with the herd socially and so on.  Others don’t feel as constrained and aren’t as bothered when they find themselves fifty yards away from where everyone else is.  These tendencies and preferences are part of our core personality makeup. I have to say — my spouse’s behavior was on-track with everything I wrote in that post.  She’s a Rational and a Guardian rolled into one.  I tried my best to convert her — to make her more comfortable being different and pursuing her own path, but in the end, she had too much Guardian in her.  Instead she wanted to follow the trail that was blazed by so many of our peers.  She needed to stay with them and not be so different. If you want to read some details of our conversations, you can. I don’t want to clutter this supposedly brief summary with too much of the nitty gritty.  2019 Batman is experiencing some serious Back Bain. ED Diagnosis It’s not the ED you are thinking of — I’m not going to get gross in this blog, relax. In February of this year, I blew my back out.  Normally when someone fucks up their back it gets better in a couple of weeks.  Mine did not. I go through the health-care rigmarole to try to figure out what is wrong.  Appointments with a lot of different doctors, who are all having trouble understanding what’s wrong.  It’s terrifying to be honest:  At times I feel like I will never be OK again. In April, after a couple of months of diagnostics, I have a genetic test for the connective tissue disorder Ehlers-Danlos-Syndrome.  ED, or EDS to be more complete.  Confirmed.  My tissues and joints are loose, everywhere.  I’ve known my whole life that I was flexible and had more discomfort than most people in certain parts of my body but always chalked it up to being double jointed and it didn’t seem to seriously impact my life until this particular physical trauma. My actual hands. I feel zero discomfort doing this.  Yeah I know it’s not natural and therefore gross and a lot of people have strong reactions to this sort of thing but I wanted to show, with a single picture, what this thing, Ehlers-Danlos, really is. So look, it’s a rough start to the year. I spend a lot of time doing physical therapy, building up my muscles so I can reduce stress on my joints, which don’t hold themselves together as well on me as they do on normal people — my tendons are too long, my skin is too elastic.  It can be a struggle to get through days.  I have a lot of appointments with doctors.  My life basically sucks during this period. So around late May of 2019, I’m finally on my feet again.  (My core is amazing at this point, btw, I basically built it up so that there’s a lot less stress on my spine and hips.)  I feel more or less all right.  I still have to do close to an hour a day of exercises to keep my body stable so I don’t fall apart again.  I don’t care.  It’s a small price to pay for being able to have a normal life. The Breakup During this time of physical rehabilitation, while I was struggling with the physical issues, my partner finally figured out what would make her happier.  It was a relationship with another man.  Disgusted and disappointed &lt;a million unwritten feelings and descriptions go here&gt; I take steps to end our relationship — this is June of 2019. For anyone who thinks that we could have worked it out, I must say that I’m one of those people who won’t get past infidelity because it is a core trust issue.  After that detail came to light, I lost all interest in trying to reconcile.  And she wasn’t trying that hard anyway, because she had acquired, somehow, New Life Dreams, which had to do with Conspicuous Consumption and Keeping Up and being Visibly Awesome — dreams which are at odds with my own.  Her (updated) definition of best life and my own definition of best life were irreconcilable — the disconnect was real.  On top of it all, she decided I was going to be crippled the rest of my life, and couldn’t stomach the thought of being a caretaker.  (I am not crippled, I look pretty good at 5’10 155lbs, and am healthy, still muscular, totally functional, and do not need anyone to care for me.  I just have to do an enormous amount of daily physical maintenance in order to stay this way.)  Anyway, I’ll stop there, I don’t want this to turn into bitching about my failed long-term relationship.  The bottom line is that by August of 2019, we were done.  (The 2021 update on her is that she moved out of state to be with that guy for a year and then dumped him just a month ago.) I lost my running group.  I was an avid runner for decades but I can’t do this form of activity anymore.  Running was a major source of release for me — a social outlet, a way to burn energy, be outdoors, and feel good about life in general.  Gone. After the breakup — this is late summer 2019 — I find myself to also be Officially Unhappy, with the capital U and everything.  I felt isolated and alone.  I questioned everything. Early retirement. The whole financial independence (FI) goal. They say that if you aren’t thinking about happiness then you are probably happy. This, the end of 2019, is the first year since retiring that I think about my happiness — or lack thereof — an awful fucking lot.  So there you go. I consider how much luck plays into all of this. My Early Retirement idols — MMM, ERE — never ran into a life changing health problem. Me? My own genetic gods were not so kind. The New Job In other news, my previously calculated FI math suddenly no longer works because I am single and it’s more expensive generally to be single than part of a couple with shared expenses.  Plus I’m spending more — quite a bit more actually — because of the health issues. Toward the end of the year I began working again in an attempt to fix this problem.  I was on my own. Not-So-Fun Fact: My relationship status and ED altered the Early Retirement Math. I had to confront this head-on and make some changes. I was still quite well-off but I wasn’t in great shape to go the rest of my life without adding a bit to my nest egg. Conclusion: I needed income again. Luckily, the job market was hot and as such my time away from the workforce wasn’t a deterrent for employers. At the end of the year, I get a job consulting in my old field — Information Technology, Software, Infrastructure-as-a-Service, DevOps for those in the know. The job isn’t bad — I don’t have a manager to directly report to, I work from home 4 out of 5 days a week, (5/5 now due to Covid) there’s no travel, the pay is good, the people are all right. Very occasionally I have a stressful day or two when I can’t figure something out or someone is leaning on me and I sweat things. Overall, though, I find I’m mostly okay with working again, especially because I suddenly need and value the money. Back in 2015, when I first quit, I had trouble understanding why I was bothering to earn money, since we had “enough” to retire on, given our spending as a couple. This is no longer the case — I, as a single entity, suddenly needed and valued the income and health-care benefits.  I also should point out that working helped to reduce my feelings of isolation and loneliness — it gave me something concrete to do, people to interact with, that kind of thing. It provided problems and situations to keep my brain busy in the wake of the split up so I couldn’t ruminate/obsess on it endlessly.  This might sound like I’m trying to hard to find positives about working again but holy shit I am not.  Staying busy helped me to get through this period of my life. I don’t view this — going back to work — as a “give-up” move. It was pragmatic and necessary. I also valued the low-deductible health care — I continue to have physical therapy and checkpoints with specialists for the Ehlers-Danlos condition.  For example, I will continue to need MRIs a couple of times a year which cost 2-3K a pop. All that being said — all of the positives and so on — work still sucks.  I am good at what I do and I sometimes feel good about accomplishing stuff but I don’t particularly enjoy it in and of itself.  It’s great in the sense that it passes time and gives me some social interactions and feelings of accomplishment — plus money of course — but I still do not have any sort of sense that this is my calling.  I would certainly not do this for free. Also:  The CV-19 driven zoom meeting craze in 2020 can go fuck itself.   Attempts to Reach Out In the second half of this year I decided to, for the second time since leaving work in 2015, take classes in writing at a (different) local state college and the experience was again overwhelmingly negative for surprisingly similar reasons to the first attempt.  My thought was that maybe the socialization and structure would help to jump-start my brain, getting me back in the mindset to be more productive with the pen again.  Maybe it’d even inspire me.  But it didn’t work out. I also decided to see a therapist again, for the first time in about seven years, and the guy I selected turned out to be close to useless. After five sessions I realized he’s rather unimaginative and cannot comprehend the sort of challenges I have in my own life, which are: I am returning to work unexpectedly after years off (unusual for a previously high earning dude), I am single and lonely, I am kind of mentally shot and discouraged about my newfound physical problems, I’m extremely disappointed in both myself and my ex for complicated reasons, and I am having major issues with the sort of “what is the point of my life exactly, again?” kind of questions that of course periodically plague us all but are, at this particular point in this particular year, not even close to periodic, they’re absolutely unending — so I dump him after just a month. I call around and find another therapist and the exercise is almost exactly the same. (This is not good. I start to think that I’m just massively damaged and can’t relate to anyone anymore.  Even professionals.) Living A FI I start to have disturbing Thoughts about This Blog Specifically, a lot of shame. I barely thought about this thing in 2015, 2016, 2017, 2018 when I was relatively happy. But here in late 2019, I begin to think: Holy shit, maybe I’m steering people in the wrong direction. There’s so much left of life, so much that will change, so much that will deviate from peoples’ original plans.  Most folks think they’re going to live and love without major hurdles or changes.  Maybe the vast majority of them won’t.  But surely some of them will have similar challenges.  Am I giving folks terrible advice?  Telling people to just say fuck it and quit because their financial numbers “work” based on their current situation?  I looked at other FI bloggers who quit work and retired.  They all appeared to be blissful. Stoic.  Confident and without reservations.  Since I ran into problems myself, I started to feel like I was defective.  Like something was wrong with me and that’s why it didn’t work so well.  Maybe it has to do with my personality (a nerdy introvert).  Or it could be because I’m not trying to sell product and make money off of my choices, like almost everyone else who blogs about FI seems to.  Maybe I needed to feel like reaching FI itself was going to help me to continue to make money or socialize  — maybe publicizing it and earning some dough as a result and getting together with groups of people who shared my goal would have helped me to retain a sense of progression and momentum in life, along with more connection.  I’m not negatively judging those other bloggers — I know it sounds like I am but I’m not.  I was exploring why they seemed to be doing better and I think there’s something to these ideas. But instead I was anonymous.  Just a random dude with a blog who quit working.  I wasn’t holding conferences with disciples.  I wasn’t trying to turn this shit into a hustle.  Becoming financially independent — it was just something I did, as a life choice.  I didn’t consider it to be who I was.  It wasn’t a new kind of career or something.   It wasn’t and isn’t something I’m trying to push on people.  Maybe that was my mistake.  Maybe I should have embraced this thing as my identity instead of just being my regular ‘ol self.   I can barely stand to think about it. Specifically what I can’t stand to think about is this idea: I retired on a sort of “lean-FIRE” budget, confidently told the world I’d be just fine, and five years or so later, I’m a wreck, I’d lost my partner, and I had to make sizable changes.  I kept feeling that I’d failed somehow.  Like I’d done something wrong — I felt guilty. Nerd Rapper MF Doom:  Just since some people, wear a mask — Don’t mean they did something (RIP) I thought I’d be fine forever after I quit.  And I wasn’t. Still, I don’t think that I am example of a stereotypical “Early Retirement Fail” exactly. There are threads in certain internet forums (MMM for example) where this topic is discussed and the focus seems to be on financial ruin.  This is not me. Look, I need to be clear about a couple of things: Nothing terrible happened to me financially. My relationship went south and the numbers no longer worked so I found employment again. I can’t consider this a failure of the whole financial independence goal. It was instead a result of my unexpectedly changing life, plain and simple. And that failure forced me to re-examine the overall plan. I’m in good shape generally, but I’ll be in better shape if I continue to work and save for a few more years. Yeah, without my former partner, I became depressed and anxious and again struggled with one of the great questions that terrorizes us all:  Purpose.  But let’s characterize the failure.  I didn’t have difficulty filling my days with stuff to do.  I didn’t start drinking or smoking a lot of weed or fall out of shape or gain a bunch of weight.  What did happen was that I realized I needed more out of life.  The discomfort — that growing sense of unhappiness, the creeping edges of depression just out of my direct line of sight as though it’s hiding in the periphery at all times — well.  That discomfort did exactly what it was supposed to do.  It prompted me to make some major changes that moved me in the right direction. As far as failures go, it was a good one. Trying Again Despite all of my issues and some amount of depression, in November of 2019, I found the energy to mess around with dating apps, the Tinders and the Bumbles and the eHarmonys. (Special thanks to my good friend Testosterone for the motivation.  You’re my homey, 4ever.) After a number of horrific experiences, going out on first dates over and over again with women that I knew instantly were the wrong match for me, people I would never be comfortable with (this is women who are heavily invested in social media and status, broadcasting a lot of details about their life and so on, which is fine for a lot of men, but simply not a good fit for me due to my personality), early in 2020, I finally met a wonderful, kind, grounded person: A reader, a writer, a nerd, a librarian.  Barbara Gordon was a librarian.  Just sayin’. We hit it off. (Thank fucking god..)  And life gradually starts feeling better.  2020 The year of Covid-19. This year my net worth starts climbing crazily, despite the bomb-out in March of 2020. I manage to hold on without selling.  Barely. Watching the totals drop is wrenching, a total gut-check. It reminded me a lot of 2008 meltdown that I wrote about in this post, except the drop was more sudden. I was tempted to sell constantly — every day, every hour! — but I didn’t.  I had to re-read John Bogle’s Common Sense on Mutual Funds to help me stay the course.   During this tumultuous period, I find myself being incredibly grateful that I have a job that allows me to work remotely so I can continue to earn income and have access to the health care benefits that I need. When things start climbing again with the US DOM S&amp;P 500, I can hardly believe it. First back up to break even, then upward and onward and what the fuck is happening type-shit. Note: This is why you just stay in the market. You don’t know what is going to happen and nobody does. Stick to your Stock/Bond asset allocation and keep dumping money in and tune out the financial chatter and let it go. You will do OK.  (Probably.  According to history, anyway.) I continue to spend a lot of time this year with my new partner. Our relationship flourishes.  Despite CV-19, we spend many weekends embarking on short trips, often after taking a covid test, which are free in our state and help set us at ease.  I am painting a little bit again. I start writing again as well.  I do my physical therapy and submit a bunch of good work for my employer.  I also take care of my mother when she needs it — she is in my Covid bubble. I persist in doing the things that I enjoy, despite all the bullshit in the world. I read more books this year than I can remember reading in any year in my entire life. I expand to new authors.  I read books that my new partner recommends, things I might have never been exposed to otherwise, Shirley Jackson and May Sarton and Margaret Atwood and Rachel Cusk.    In the middle of the year I try again to find a therapist and this time I find someone that is much better.  It’s hard to express why but we clicked and I felt she understood me in ways the previous two did not.  She puts me on an antidepressant. (I have a family history of depression.  Bet you didn’t see that coming &lt;/sarcasm&gt;)  This is the first time that a therapist firmly told me to take medication.  I agreed because I wanted to feel better and was willing to try things. It helps — I feel as though it’s prompting me somehow to get unstuck, to some extent. Despite CV-19 and political unrest and a lot of economic uncertainty, despite my continuing health problems and slowly grinding myself out of the fog of what was clearly, in hindsight, depression, it’s a good year all things considered, and a year that gets steadily better as it goes.  By the end of it, I realize it had been terrific overall, if you want to know the full truth.  I feel a little bad having such a wonderful year when so many are suffering but that’s how it ended up. The most important cog in the machinery of my personal happiness turned out to be a simple one:  Having a wonderful partner.  This will sound like a dumb 90s pop song written by a boy band but whatever, I’m head over heels for her and that makes everything better, despite my physical issues, despite CV-19, despite fucking everything.  I have meaning and love in my life — daily meaning, meaning that’s easy to define and pays off constantly — and it has nothing to do with employment and money.   It’s been said a billion times that money doesn’t make you happy. People do. Connections and relationships do. Purpose helps, too. This year makes me a believer in all of the above. 100%.  If that’s cheesy, so be it — I’m the Mayor of Cheese. Odds and Ends   The White Space Issue I admit, all that white doesn’t look like much, but it’s everything. The English author Martin Amis once wrote:  Happiness writes white. We all sort of instinctively know this.  If this post consisted of all of the joy I took from not working for several years – the food in France, the mansions in Newport RI, the lazy mornings, the walks around ponds with adorable ducklings following me around hoping for breadcrumbs, the billion details I can’t even remember myself — it’d frankly be intolerable to read.  It’s not nearly as fun or interesting to read about someone’s satisfactions than someone’s struggles.  So please consider this:  The fact I had almost nothing to say about 2015, 2016, 2017, and most of 2018 is a tremendous indication that I was quite happy overall, and simply didn’t want to share the specifics.  I was happier during these years than I was in the years prior when I was working, easily, not-even-close, full stop. Besides, we don’t learn a hell of a lot from other peoples’ happiness very often, do we?  Changes Holy shit do things change. The best laid plans, right? I thought I had everything set. I had a financial drawdown schematic in place. I knew what my yearly spend rate was. My partner was on-board and excited. I absolutely had plans — lots and lots of plans — for how to spend my time. I got older. My long-time lover and partner-in-crime and I split at the end of summer in 2019 — I mean we’d been together for two decades. Who isn’t going to get down about a relationship of that length ending, regardless of the circumstances? Plus, my monthly expenses went up somewhat as a result. I was diagnosed with a health condition that requires constant care and feeding (and cash). I met a new person that I feel certain I am going to spend the rest of my life with.  (We’re currently engaged — awesome!)  I went back to work because circumstances indicated this was the right move to make, for the time being. Is this an early-retirement fail? I don’t think so. I was met with challenges and I adapted to them. I’m generally happy now. I just had no idea what sorts of things might happen after I retired. The bottom line is that I made a financial plan that would have worked out just fine had I not had a crushing relationship termination plus the discovery of a permanent and expensive health condition. The initial plan itself was fine. But the plot twisted: My life decided it didn’t want to conform to the plan. If you are yourself working on becoming FI and you have any specific takeaway from this post, let it be this:  You are making future plans based on what your current life looks like. Your current job, your current income, your current partner, your current percentage of savings, your expected market return, your housing costs, your location and so-on. You’re assuming large parts of your life will remain static over the next X years, where, for many early-retiree hopefuls, X is 30+ years, perhaps even fifty. They may not be static.  It might be a mistake to think that things will be as smooth as you believe they will be. The ability to recover from changes and disruptions — to be adaptable and resilient in the face of adversity — will show itself to be perhaps the most critical Early Retirement skill of them all.  I hope sincerely that nothing changes for everyone on this path — that your happiness meter goes up to max and stays there.  But for some percentage of us, like myself — well.  We will hit bumps in the road, potholes, areas of the street that are flooded out where you need to slow down or even turn around and find another route.  And we will all need to be flexible enough to deal with it.   I now view my personal setbacks as a net-positive, no doubt.  I mean, I found a new partner who is a much closer match to my values.  Her idea of a nice car, for example, is the Ecto-1.  The Lego version, specifically. The original Ghostbusters is in the top 10 of 80s movies, easily. Still holds up in 2021, too. Mostly, she says, she just wants to be comfy in a nice hobbit-hole with me and some friends and family around us.  We’re also almost certain to buy a house and have a kid together, too, and this is something I couldn’t do with my ex for &lt;reasons&gt;.   I find all of this to be incredibly exciting and amazing. Bilbo’s place, at perhaps 1300 sq feet, would be priced at 1.6 Mil in my area, because it’s in a Good Neighborhood.  (I’m in an extremely high COLA.) Extremely Rough Financials I retired early 2015 with about 950 in assets.  60K of this was cash.  I spent about 30K a year.  (My spouse also spent 30k, making our combined total 60k /yr — but the focus of this section is how my own net worth fared over the last 6 years, so we need to focus on that 30K number.) Had things gone exactly according to plan, I would have spent about 180K over the duration as a result (6 years at my planned 30K/yr withdrawal amount).  But truth be told, I spent a bunch over that due to problems I’d listed above in this post.  So I came in at closer to 240K — 40k annually — 33% higher, a 10k/yr difference. Of that 890K that wasn’t cash, I had a 70/30 stock bond split as per my investment policy statement. Very long story short, I spent an average of 40k a year, I rebalanced yearly to retain my 70/30 allocation, and I wound up with about 1.3 million. This seems like a ton of money but keep in mind that 1.300K in 2021 dollars equals approximately 1.150K in 2015 when I retired. So we can say that inflation adjusted, my net worth has gone up by about 20%.  There’s some rounding and fudging here as I don’t care to crunch everything exactly for the purposes of this blog but it’s a good enough percentage to ballpark it for the readers that care (and yeah, I know some do.  Full disclosure, I didn’t even bother to run these numbers until writing this blog post — my personal give-a-shit meter on this topic wasn’t high enough.) You read that right.  My net worth has gone up despite not working for nearly 5 years.  I can’t quite believe it myself.  And I thought I was retiring at a market peak and had virtually no hope of getting this kind of performance over the next  5-6 years. It just goes to show:  You just never know how things are going to go. Regrets Only one.  At the time I quit working in 2015, I had to leave my job for a shit-ton of reasons that are well-documented in the historical pages of this blog.  Mostly I was just tired of continually working.  I’d been working 48 out of 52 weeks a year or more for 18 straight years.  Make no mistake about it:  People aren’t meant to work like this.  It’s totally absurd. So I don’t regret going for something along the lines of lean-FIRE and I don’t regret trying to live a life where I wasn’t spending much money and I don’t think that being frugal had any impact on my happiness overall.  I don’t regret quitting when I did, and I don’t regret taking time off from work.  Quite the opposite.  I needed the break, the change of pace, the new experiences and the perspective. I am sorry that things didn’t work out between my (ex) wife and I and sometimes I wonder if I could have done some things differently that might have held us together. (Almost definitely I could have.) But in the end I know that I am a kind and caring person — a person who tried to communicate with her, to support and help her to be happy, no matter what she felt she needed.  She said earnestly, hundreds of times, that she wanted to take this journey with me, and she changed her mind a few years in.  Then, before we could work it out, there was that little adultery thing I’d mentioned before, at which point we really had no path forward together — she closed off any possibility of sharing a future with me.  I’m upset about all of this, of course.  But I’m not sure I have any regrets.  I tried my best.  I believe that.   So far I don’t regret going back to work in my old field/industry although I don’t love it and I am aware of the irony given how much I have complained about work in the pages of this blog and insisted I would never go back. It’s also worth pointing out that all of my saving and investing in my 20s and 30s has positioned me to do whatever I want, more or less.  It has given me freedom to make choices, exactly as I’d hoped it would all along.  I still have an amazing stash of money.  I mean, I was able to take close to five straight years off work and still have more than I started with, inflation-adjusted.  How many people get to take this amount of time off work when they’re 40?  As of this writing, after a year and a half or so of additional work, I now have about 20% more, inflation adjusted, than when I initially quit.  This stash still gives me options and freedom that most people don’t have.  I could, for example, leave the country, go to a cheaper place, and live out the rest of my days there comfortably with virtually no financial concerns. It just so happens that I’d rather stay in Massachusetts (US) with my new partner.  I want to make a life together here, surrounded by our families and friends and familiar roads and shops and communities, without worrying too much about money.  The stash still absolutely allows me to do this and I’m grateful for this every single day — still grateful for the actions that I took when I was younger that allowed the current version of me, at age 43 here in 2021, to be as free and confident as I am now. So what was that one regret?  It’s that I didn’t take my partner’s initial unhappiness more seriously.  I encouraged her to explore her own life and find activities and goals that would help her feel better.  I suggested therapy and offered to go with her.  I was clear that if she wanted to go back to work I was eager to support her in this.  I wanted her to do anything that might help.  But my suggestions and support weren’t enough — I never could figure out what she wanted or how I could help — from my perspective it seemed as though she rejected most of my attempts to talk about this.  But I still have this sense that I could have and should have done better here. Bottom line:  We didn’t communicate effectively in this area, and of course that’s at least 50% my fault.  I wish I knew what I could have done differently here but it’s clear to me that I didn’t do enough.  By the time I understood what a huge problem this was for her – the lack of direction/progress/purpose in her life – that she just wasn’t excited about anything anymore — it was too late.  I’ll never have full closure on the relationship but I no longer care.  Part of being a healthy adult is sometimes accepting the behavior of people without understanding all of the reasons behind their actions.  Closure is great, of course, but we don’t always get it.     Looking Ahead It’s sort of exhausting to think about coming up with a new plan to quit work and retire again.   But it will happen at some point, for sure.  I’m currently working on finding a place to live with my partner.  We may get married.  We may have a kid together.  These things — the house, a possible child — they will surely change the retirement math yet again. My fiancé is a librarian and she doesn’t make a ton of money.  This drastically changes the early-retirement numbers for the two of us.  My ex had a lot of scratch — a roughly similar asset sheet to my own.  However, although my fiancé is frugal and debt free, she does not have a large nest egg nor an enormous earning potential — she frankly needs some help in order to do this thing with me.  And says she doesn’t want to stop working, regardless .  She actually — honestly and truly — loves her job.  (She helps individual people on a day to day basis and this is instantly gratifying and awesome for her — it sort of continually fills her daily energy meter, if I can do a video game analogy.  In contrast, the societal benefits of my own work are so heavily abstracted as to be basically unrecognizable and unmeasurable — I sure as hell don’t see the results myself.  If there are any, at all.) Anyway — bottom line is that my own asset sheet funds the lion’s share of our future together.  (I don’t care in the slightest, I’m just stating facts.  And I did not plan for this, or expect this.  It’s just the way it is, now.) Additionally, I’m spending more than that 30K/yr I had estimated back in 2015.  I’m at 40K now and we’ll be at 55K together I think, once we consolidate assets and buy a home.  It’s also possible that I might spend more in the future, too.  Not ridiculous amounts.  Not on cars or excessive housing or anything stupid.  I enjoy being fairly minimalist, not just because it saves me money, but also because it reduces my negative impact on the world (less energy consumption and waste, etc.) But I really want to travel some with her — at least a couple of substantial international trips a year.  And my window to visit other countries — to experience life and be out-and-about, walking and sightseeing and behaving like a normal person, may be closing a lot sooner than I previously thought, due to the whole ED condition.  I might not be fully mobile in a decade.  (I should be — I hope to be!  — But I don’t know for sure.)  Travel isn’t free.  While we won’t be taking luxury vacations, neither do I want to blink if we want to, say, eat at a cute café or take a ferry to an island or pay money to get into any and all museums or whatever.  I want to go and do the things we want to do instead of agonizing over relatively small dollar amounts.  I want to be present in the moment and not restrict our activities out of some desire to not spend as much.  And I don’t want to talk or think about money all that much while traveling, either.  We will think about money while we do the planning, absolutely — set a budget, spending targets and so on — but once we’re out and about, no thank you.  I feel like if you are so paranoid about being cheap while you are on vacation that you forget to enjoy yourself, you’ve missed the point. Anyway.  The bottom line is that I won’t quit my current gig at least until we’ve gotten to a place where I feel we’re stable.  The housing thing in particular — it’s a huge variable.  At current mortgage interest rates (3%) I have decided it’s better to carry housing debt than pay off a residence.  But it’s much easier to get a mortgage if you have a job with the accompanying paystubs.  So I’m sure I will at least carry my job until we find a house together and the mortgage/finances are accounted for — this will help me to project our future together and create a financial plan that works for our family.  Aside:  COVID and low interest rates have really made housing prices in suburbs of major cities spike to absurd levels, which is where we’re looking, and it is a problem.  The home I sold in mid 2015 for example for 850K just sold again in 2020 for a whopping 1.150 Million.  A 35% increase in 5 years.  Raise your hand if your salary went up 35% during that time.  I didn’t think so. Another Aside:  When I was younger, I often read on financial forums that so-called comfort spending would probably go up as I became older — that it’s hard to maintain the same level of vigilance with regard to being frugal as the years pass.  You get tired.  And at the same time, your asset sheet (probably) increases.  You want to spend a little more in order to have a few more pleasures here and there.  I brushed this shit off at the time.  I thought comfort spending was silly and dumb.  And yet here I am, at 43, with slightly higher year-over-year spending than I projected.  I must admit, this is a direct result of comfort spending.  I turn the heat slightly higher, especially if my partner is uncomfortable, despite my thoughts about both the money and the increased CO2 emissions we’re producing.  (I am a huge climate change nerd, I just don’t write about it much because it’s so polarizing).  I don’t worry as much about going out to eat once in a while, particularly if we are both exhausted.   Part of me hates myself even as I make these decisions, because I know I could be doing better — I could be more efficient!  I could have a lighter impact on the planet! — but despite the awareness, I find myself making the easier choices more and more.  I feel embarrassed to admit this but at the same time I wanted to get these facts about my life out there, in the interest of full disclosure.  I mean this is a blog in which I admitted my partner cheated on me — could I possibly reveal anything worse?  A meth addiction?  I’m not sure…  At any rate, the years have changed me somewhat.  I’m not a different person exactly — but my edges have become more rounded and smooth.  I view this as a positive development. On another note, at this point, time is my enemy, much more so than money.  Luckily, because of my role at my current company, I can take weeks off (unpaid) between projects, and this time will allow me to do the things I want to do with my significant other.  I should be able to balance things together in a way that allows me to be free and live my life my own way. So you want a date as to when I’ll quit and retire again? Look, I do too.  But looking forward to a life without work isn’t as important to me as it used to be.   It helps now that I know what it’s like.  (It also helps that I’m not burnt out anymore.) Not working is awesome, for sure — I was able to do whatever I wanted with my time, and I didn’t miss work at all.  (I know a lot of people who retire wind up missing work but honestly, this just never happened for me.  Sure, other life-type problems cropped up that I had to deal with but I never once thought for example I really wish I was broadcasting my recent corporate achievements to a manager in a fake-hierarchy instead of  pursuing my own interests during the 4.75 years without employment.  I just… didn’t.  Didn’t miss the job, didn’t miss the function, didn’t miss working.) Still, I learned that having freedom in and of itself didn’t automatically bring me happiness.  Happiness is more complicated than that. So in the short term, it’s easier to get a check and work the job and focus on my life with my partner instead of the future. I simply no longer see working as a huge impediment to my overall happiness. And by extension, I no longer see quitting my job and retiring early as the most direct path to bliss.  It was five years ago, but now?  Happiness is not thinking too carefully about the finances.  Happiness is spending as much time as I possibly can with my partner.   Happiness is thinking about growth and joy and changes in the days to come. Happiness is a mix of thinking about now and ten or twenty years from now — pleasure in the moment, satisfaction through the week, and some sense that you have a lot to look forward to as the world continues to turn. There’s no point to Early Retirement if you haven’t properly positioned yourself toward the light of future happiness. So people might say:  Retire again.  Retire now livingafi.  You have the money.  Do it.  You’ve said in the past that you hated work.  And you said yourself that you have a bit more money now than you did five years ago.  You know what you want to retire TO.  You want to be a a reader, a writer, a husband, a family-man.  You can do it right now.  Quit again. And so I could.  But I’m not so much worried about a life without work as I am a life without meaning or purpose or love. So I will work until I am sure that all of these things can exist in harmony, and without a ton of financial stress. It is going to be a couple more years, given the abundance of unknowns. </description>
      <pubDate>22 Mar 21 15:53 EDT</pubDate>
      <guid>https://livingafi.com/2021/03/17/the-2021-early-retirement-update/</guid>
    </item>
    <item>
      <title>Deep Learning Illustrated: Building Natural Language Processing Models</title>
      <link>https://blog.dominodatalab.com/deep-learning-illustrated-building-natural-language-processing-models/</link>
      <description>&lt;a href=&#34;https://blog.dominodatalab.com/deep-learning-illustrated-building-natural-language-processing-models/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Many thanks to Addison-Wesley Professional for providing the permissions to excerpt &#34;Natural Language Processing&#34; from the book, Deep Learning Illustrated by Krohn, Beyleveld, and Bassens. The excerpt covers how to create word vectors and utilize them as an input into a deep learning model. A complementary Domino project is available. Introduction While the field of computational linguistics, or Natural Language Processing (NLP), has been around for decades, the increased interest in and use of deep learning models has also propelled applications of NLP forward within industry. Data scientists and researchers require an extensive array of techniques, packages, and tools to accelerate core work flow tasks including prepping, processing, and analyzing data. Utilizing NLP helps researchers and data scientists complete core tasks faster. As Domino is committed to accelerating data science work flows, we reached out to Addison-Wesley Professional (AWP) for permissions to excerpt the extensive “Natural Language Processing” chapter from the book, Deep Learning Illustrated. We appreciate AWP Pearson for providing the permissions to excerpt the work and enabling us to provide a complementary Domino project. Chapter Introduction: Natural Language Processing In Chapter 2 [in the book], we introduced computational representations of language, particularly highlighting word vectors as a potent approach for quantitatively capturing word meaning. In the present chapter [excerpt], we cover code that will enable you to create your own word vectors as well as to provide them as an input into a deep learning model. The natural language processing models you build in this chapter will incorporate neural network layers we’ve applied already: dense layers from Chapters 5 through 9 [in the book], and convolutional layers from Chapter 10 [in the book]. Our NLP models will also incorporate new layer types—ones from the family of recurrent neural networks. RNNs natively handle information that occurs in sequences such as natural language, but they can, in fact, handle any sequential data—such as financial time series or temperatures at a given geographic location—so they’re quite versatile. The chapter concludes with a section on deep learning networks that process data via multiple parallel streams—a concept that dramatically widens the scope for creativity when you design your model architectures and, as you’ll see, can also improve model accuracy. Preprocessing Natural Language Data There are steps you can take to preprocess natural language data such that the modeling you carry out downstream may be more accurate. Common natural language preprocessing options include: Tokenization: This is the splitting of a document (e.g., a book) into a list of discrete elements of language (e.g., words), which we call tokens. Converting all characters to lowercase. A capitalized word at the beginning of a sentence (e.g., She) has the same meaning as when it’s used later in a sentence (She). By converting all characters in a corpus to lowercase, we disregard any use of capitalization. Removing stop words: These are frequently occurring words that tend to contain relatively little distinctive meaning, such as the, at, which, and of. There is no universal consensus on the precise list of stop words, but depending on your application it may be sensible to ensure that certain words are (or aren’t!) considered to be stop words. For example, in this chapter, we’ll build a model to classify movie reviews as positive or negative. Some lists of stop words include negations like didn’t, isn’t, and wouldn’t that might be critical for our model to identify the sentiment of a movie review, so these words probably shouldn’t be removed. Removing punctuation: Punctuation marks generally don’t add much value to a natural language model and so are often removed. Stemming: Stemming is the truncation of words down to their stem. For example, the words house and housing both have the stem hous. With smaller datasets in particular, stemming can be productive because it pools words with similar meanings into a single token. There will be more examples of this stemmed token’s con- text, enabling techniques like word2vec or GloVe to more accurately identify an appropriate location for the token in word-vector space (see Figures 2.5 and 2.6) [in the book]. [Note: Lemmatization, a more sophisticated alternative to stemming, requires the use of a reference vocabulary. For our purposes in this book, stemming is a sufficient approach for considering multiple related words as a single token.] Handling n-grams: Some words commonly co-occur in such a way that the combination of words is better suited to being considered a single concept than several separate concepts. As examples, New York is a bigram (an n-gram of length two), and New York City is a trigram (an n-gram of length three). When chained together, the words new, york, and city have a specific meaning that might be better captured by a single token (and therefore a single location in word-vector space) than three separate ones. Depending on the particular task that we’ve designed our model for, as well as the dataset that we’re feeding into it, we may use all, some, or none of these data preprocessing steps. As you consider applying any preprocessing step to your particular problem, you can use your intuition to weigh whether it might ultimately be valuable to your downstream task. We’ve already mentioned some examples of this: Stemming may be helpful for a small corpus but unhelpful for a large one. Likewise, converting all characters to lowercase is likely to be helpful when you’re working with a small corpus, but, in a larger corpus that has many more examples of individual uses of words, the distinction of, say, general (an adjective meaning “widespread”) versus General (a noun meaning the commander of an army) may be valuable. Removing punctuation would not be an advantage in all cases. Consider, for example, if you were building a question-answering algorithm, which could use question marks to help it identify questions. Negations may be helpful as stop words for some classifiers but probably not for a sentiment classifier, for example. Which words you include in your list of stop words could be crucial to your particular application, so be careful with this one. In many instances, it will be best to remove only a limited number of stop words. If you’re unsure whether a given preprocessing step may be helpful or not, you can investigate the situation empirically by incorporating the step and observing whether it impacts the accuracy of your deep learning model downstream. As a general rule, the larger a corpus becomes, the fewer preprocessing steps that will be helpful. With a small corpus, you’re likely to be concerned about encountering words that are rare or that are outside the vocabulary of your training dataset. By pooling several rare words into a single common token, you’ll be more likely to train a model effectively on the meaning of the group of related words. As the corpus becomes larger, however, rare words and out-of-vocabulary words become less and less of an issue. With a very large corpus, then, it is likely to be helpful to avoid pooling several words into a single common token. That’s because there will be enough instances of even the less-frequently-occurring words to effectively model their unique meaning as well as to model the relatively subtle nuances between related words (that might otherwise have been pooled together). To provide practical examples of these preprocessing steps in action, we invite you to check out our Natural Language Preprocessing Jupyter notebook [or the complementary Domino project]. It begins by loading a number of dependencies: import nltk from nltk import word_tokenize, sent_tokenize from nltk.corpus import stopwords from nltk.stem.porter import * nltk.download(&#39;gutenberg&#39;) nltk.download(&#39;punkt&#39;) nltk.download(&#39;stopwords&#39;) import string import gensim from gensim.models.phrases import Phraser, Phrases from gensim.models.word2vec import Word2Vec from sklearn.manifold import TSNE import pandas as pd from bokeh.io import output_notebook, output_file from bokeh.plotting import show, figure %matplotlib inline Most of these dependencies are from nltk (the Natural Language Toolkit) and gensim (another natural language library for Python). We explain our use of each individual dependency when we apply it in the example code that follows. Tokenization The dataset we used in this notebook is a small corpus of out-of-copyright books from Project Gutenberg. [Note: Named after the printing-press inventor Johannes Gutenberg, Project Gutenberg is a source of tens of thousands of electronic books. These books are classic works of literature from across the globe whose copyright has now expired, making them freely available. See gutenberg.org.] This corpus is available within nltk so it can be easily loaded using this code: from nltk.corpus import gutenberg This wee corpus consists of a mere 18 literary works, including Jane Austen’s Emma, Lewis Carroll’s Alice in Wonderland, and three plays by a little-known fellow named William Shakespeare. (Execute gutenberg.fileids() to print the names of all 18 documents.) By running len(gutenberg.words()), you can see that the corpus comes out to 2.6 million words—a manageable quantity that means you’ll be able to run all of the code examples in this section on a laptop. To tokenize the corpus into a list of sentences, one option is to use nltk’s sent_tokenize() method: gberg_sent_tokens = sent_tokenize(gutenberg.raw() Accessing the first element of the resulting list by running gberg_sent_tokens[0], you can see that the first book in the Project Gutenberg corpus is Emma, because this first element contains the book’s title page, chapter markers, and first sentence, all (erroneously) blended together with newline characters (\n): &#39;[Emma by Jane Austen 1816]\n\nVOLUME I\n\nCHAPTER I\n\n\nEmma Wood- house, handsome, clever, and rich, with a comfortable home\nand happy disposition, seemed to unite some of the best blessings\nof existence; and had lived nearly twenty-one years in the world\nwith very little to distress or vex her.&#39; A stand-alone sentence is found in the second element, which you can view by executing gberg_sent_tokens[1]: &#34;She was the youngest of the two daughters of a most affectionate, \nindulgent father; and had, in consequence of her sister&#39;s marriage,\nbeen mistress of his house from a very early period.&#34; You can further tokenize this sentence down to the word level using nltk’s word_tokenize() method word_tokenize(gberg_sent_tokens[1]) This prints a list of words with all whitespace, including newline characters, stripped out (see Figure 11.1). The word father, for example, is the 15th word in the second sentence, as you can see by running this line of code: word_tokenize(gberg_sent_tokens[1])[14] Although the sent_tokenize() and word_tokenize() methods may come in handy for working with your own natural language data, with this Project Gutenberg corpus, you can instead conveniently employ its built-in sents() method to achieve the same aims in a single step: gberg_sents = gutenberg.sents() This command produces gberg_sents, a tokenized list of lists. The higher-level list consists of individual sentences, and each sentence contains a lower-level list of words within it. Appropriately, the sents() method also separates the title page and chapter markers into their own individual elements, as you can observe with a call to gberg_sents[0:2]: [[&#39;[&#39;, &#39;Emma&#39;, &#39;by&#39;, &#39;Jane&#39;, &#39;Austen&#39;, &#39;1816&#39;, &#39;]&#39;], [&#39;VOLUME&#39;, &#39;I&#39;], [&#39;CHAPTER&#39;, &#39;I&#39;]] Because of this, the first actual sentence of Emma is now on its own as the fourth element of gberg_sents, and so to access the 15th word (father) in the second actual sentence, we now use gberg_sents[4][14]. Converting All Characters to Lowercase For the remaining natural language preprocessing steps, we begin by applying them iteratively to a single sentence. As we wrap up the section later on, we’ll apply the steps across the entire 18-document corpus. Looking back at Figure 11.1, we see that this sentence begins with the capitalized word She. If we’d like to disregard capitalization so that this word is considered to be identical to she, then we can use the Python lower() method from the string library, as shown in Example 11.1. Example 11.1 Converting a sentence to lowercase [w.lower() for w in gberg_sents[4]] This line returns the same list as in Figure 11.1 with the exception that the first element in the list is now she instead of She. Removing Stop Words and Punctuation Another potential inconvenience with the sentence in Figure 11.1 is that it’s littered with both stop words and punctuation. To handle these, let’s use the + operator to concatenate together nltk’s list of English stop words with the string library’s list of punctuation marks: stpwrds = stopwords.words(&#39;english&#39;) + list(string.punctuation) If you examine the stpwrds list that you’ve created, you’ll see that it contains many common words that often don’t contain much particular meaning, such as a, an, and the. [Note These three particular words are called articles, or determiners. However, it also contains words like not and other negative words that could be critical if we were building a sentiment classifier, such as in the sentence, “This film was not good.”] In any event, to remove all of the elements of stpwrds from a sentence we could use a list comprehension  as we do in Example 11.2, which incorporates the lowercasing we used in Example 11.1. Example 11.2 Removing stop words and punctuation with a list comprehension [w.lower() for w in gberg_sents[4] if w.lower() not in stpwrds] Relative to Figure 11.1, running this line of code returns a much shorter list that now contains only words that each tend to convey a fair bit of meaning: [&#39;youngest&#39;, &#39;two&#39;, &#39;daughters&#39;, &#39;affectionate&#39;, &#39;indulgent&#39;, &#39;father&#39;, &#39;consequence&#39;, &#39;sister&#39;, &#39;marriage&#39;, &#39;mistress&#39;, &#39;house&#39;, &#39;early&#39;, &#39;period&#39;] Stemming To stem words, you can use the Porter algorithm [Note: Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14, 130–7.] provided by nltk. To do this, you create an instance of a PorterStemmer() object and then add its stem() method to the list comprehension you began in Example 11.2, as shown in Example 11.3. Example 11.3 Adding word stemming to our list comprehension [stemmer.stem(w.lower()) for w in gberg_sents[4] if w.lower() not in stpwrds] This outputs the following: [&#39;youngest&#39;, &#39;two&#39;, &#39;daughter&#39;, &#39;affection&#39;, &#39;indulg&#39;, &#39;father&#39;, &#39;consequ&#39;, &#39;sister&#39;, &#39;marriag&#39;, &#39;mistress&#39;, &#39;hous&#39;, &#39;earli&#39;, &#39;period&#39;] This is similar to our previous output of the sentence except that many of the words have been stemmed: daughters to daughter (allowing the plural and singular terms to be treated identically) house to hous (allowing related words like house and housing to be treated as the same) early to earli (allowing differing tenses such as early, earlier, and earliest to be treated as the same) These stemming examples may be advantageous with a corpus as small as ours, because there are relatively few examples of any given word. By pooling similar words together, we obtain more occurrences of the pooled version, and so it may be assigned to a more accurate location in vector space (Figure 2.6). With a very large corpus, however, where you have many more examples of rarer words, there might be an advantage to treating plural and singular variations on a word differently, treating related words as unique, and retaining multiple tenses; the nuances could prove to convey valuable meaning. Handling n-grams To treat a bigram like New York as a single token instead of two, we can use the Phrases() and Phraser() methods from the gensim library. As demonstrated in Example 11.4, we use them in this way: Phrases() to train a “detector” to identify how often any given pair of words occurs together in our corpus (the technical term for this is bigram collocation) relative to how often each word in the pair occurs by itself Phraser() to take the bigram collocations detected by the Phrases() object and then use this information to create an object that can efficiently be passed over our corpus, converting all bigram collocations from two consecutive tokens into a single token Example 11.4 Detecting collocated bigrams phrases = Phrases(gberg_sents) bigram = Phraser(phrases) By running bigram.phrasegrams, we output a dictionary of the count and score of each bigram. The topmost lines of this dictionary are provided in Figure 11.2. Each bigram in Figure 11.2 has a count and a score associated with it. The bigram two daughters, for example, occurs a mere 19 times across our Gutenberg corpus. This bigram has a fairly low score (12.0), meaning the terms two and daughters do not occur together very frequently relative to how often they occur apart. In contrast, the bigram Miss Taylor occurs more often (48 times), and the terms Miss and Taylor occur much more frequently together relative to how often they occur on their own (score of 453.8). Scanning over the bigrams in Figure 11.2, notice that they are marred by capitalized words and punctuation marks. We’ll resolve those issues in the next section, but in the meantime let’s explore how the bigram object we’ve created can be used to convert bigrams from two consecutive tokens into one. Let’s tokenize a short sentence by using the split() method on a string of characters wherever there’s a space, as follows: tokenized_sentence = &#34;Jon lives in New York City&#34;.split() If we print tokenized_sentence, we output a list of unigrams only: [&#39;Jon&#39;, &#39;lives&#39;, &#39;in&#39;, &#39;New&#39;, &#39;York&#39;, &#39;City&#39;]. If, however, we pass the list through our gensim bigram object by using bigram[tokenized_sentence], the list then contains the bigram New York: [&#39;Jon&#39;, &#39;lives&#39;, &#39;in&#39;, &#39;New_York&#39;, &#39;City&#39;]. After you’ve identified bigrams across your corpus by running it through the bigram object, you can detect trigrams (such as New York City) by passing this new, bigram-filled corpus through the Phrases() and Phraser() methods. This could be repeated again to identify 4-grams (and then again to identify 5-grams, and so on); however, there are diminishing returns from this. Bigrams (or at most trigrams) should suffice for the majority of applications. By the way, if you go ahead and detect trigrams with the Project Gutenberg corpus, New York City is unlikely to be detected. Our corpus of classic literature doesn’t mention it often enough. Having run through some examples of preprocessing steps on individual sentences, we now compose some code to preprocess the entire Project Gutenberg corpus. This will also enable us to collocate bigrams on a cleaned-up corpus that no longer contains capital letters or punctuation. Later on in this chapter, we’ll use a corpus of film reviews that was curated by Andrew Maas and his colleagues at Stanford University to predict the sentiment of the reviews with NLP models. [Note: Maas, A., et al. (2011). Learning word vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, 142–50.] During their data preprocessing steps, Maas and his coworkers decided to leave in stop words because they are “indicative of sentiment.” [Note: This is in line with our thinking, as we mentioned earlier in the chapter.] They also decided not to stem words because they felt their corpus was sufficiently large that their word-vector-based NLP model “learns similar representations of words of the same stem when the data suggest it.” Said another way, words that have a similar meaning should find their way to a similar location in word-vector space (Figure 2.6) [in the book] during model training. Following their lead, we’ll also forgo stop-word removal and stemming when preprocessing the Project Gutenberg corpus, as in Example 11.5. Example 11.5 Removing capitalization and punctuation from Project Gutenberg corpus lower_sents = [] for s in gberg_sents: lower_sents.append([w.lower() for w in s if w.lower()not in list(string.punctuation)]) In this example, we begin with an empty list we call lower_sents, and then we append preprocessed sentences to it using a for loop. [Note: If you’re preprocessing a large corpus, we’d recommend using optimizable and parallelizable functional program- ming techniques in place of our simple (and therefore simple-to-follow) for loop.] For preprocessing each sentence within the loop, we used a variation on the list comprehension from Example 11.2, in this case removing only punctuation marks while converting all characters to lowercase. With punctuation and capitals removed, we can set about detecting collocated bigrams across the corpus afresh: lower_bigram = Phraser(Phrases(lower_sents)) Relative to Example 11.4, this time we created our gensim lower_bigram object in a single line by chaining the Phrases() and Phraser() methods together. The top of the output of a call to lower_bigram.phrasegrams is provided in Figure 11.3: Comparing these bigrams with those from Figure 11.2, we do indeed observe that they are all in lowercase (e.g., miss taylor) and bigrams that included punctuation marks are nowhere to be seen. Examining the results in Figure 11.3 further, however, it appears that the default minimum thresholds for both count and score are far too liberal. That is, word pairs like two daughters and her sister should not be considered bigrams. To attain bigrams that we thought were more sensible, we experimented with more conservative count and score thresholds by increasing them by powers of 2. Following this approach, we were generally satisfied by setting the optional Phrases() arguments to a min(imum) count of 32 and to a score threshold of 64, as shown in Example 11.6. Example 11.6 Detecting collocated bigrams with more conservative thresholds lower_bigram = Phraser(Phrases(lower_sents, min_count=32, threshold=64)) Although it’s not perfect, [Note: These are statistical approximations, of course!] because there are still a few questionable bigrams like great deal and few minutes, the output from a call to lower_bigram.phrasegrams is now largely defensible, as shown in Figure 11.4. Armed with our well-appointed lower_bigram object from Example 11.6, we can at last use a for loop to iteratively append for ourselves a corpus of cleaned-up sentences, as in Example 11.7. Example 11.7 Creating a “clean” corpus that includes bigrams clean_sents = [] for s in lower_sents: clean_sents.append(lower_bigram[s]) Creating Word Embeddings with word2vec With the cleaned corpus of natural language clean_sents now available to us, we are well positioned to embed words from the corpus into word-vector space (Figure 2.6). As you’ll see in this section, such word embeddings can be produced with a single line of code. This single line of code, however, should not be executed blindly, and it has quite a few optional arguments to consider carefully. Given this, we’ll cover the essential theory behind word vectors before delving into example code. The Essential Theory Behind word2vec In Chapter 2, we provided an intuitive understanding of what word vectors are. We also discussed the underlying idea that because you can “know a word by the company it keeps” then a given word’s meaning can be well represented as the average of the words that tend to occur around it. word2vec is an unsupervised learning technique—that is, it is applied to a corpus of natural language without making use of any labels that may or may not happen to exist for the corpus. This means that any dataset of natural language could be appropriate as an input to word2vec. [Note: Mikolov, T., et al. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781] When running word2vec, you can choose between two underlying model architectures—skip-gram (SG) or continuous bag of words (CBOW; pronounced see-bo)— either of which will typically produce roughly comparable results despite maximizing probabilities from “opposite” perspectives. To make sense of this, reconsider our toy-sized corpus from Figure 2.5: you shall know a word by the company it keeps In it, we are considering word to be the target word, and the three words to the right of it as well as the three words to the left of it are considered to be context words. (This corresponds to a window size of three words—one of the primary hyperparameters we must take into account when applying word2vec.) With the SG architecture, context words are predicted given the target word. [Note: In more technical machine learning terms, the cost function of the skip-gram architecture is to maximize the log probability of any possible context word from a corpus given the current target word.] With CBOW, it is the inverse: The target word is predicted based on the context words. [Note: Again, in technical ML jargon, the cost function for CBOW is maximizing the log probability of any possible target word from a corpus given the current context words. ] To understand word2vec more concretely, let’s focus on the CBOW architecture in greater detail (although we equally could have focused on SG instead). With CBOW, the target word is predicted to be the average of all the context words considered jointly. “Jointly” means “all at once”: The particular position of context words isn’t taken into consideration, nor whether the context word occurs before or after the target word. That the CBOW architecture has this attribute is right there in the “bag of words” part of its name: We take all the context words within the windows to the right and the left of the target word. We (figuratively!) throw all of these context words into a bag. If it helps you remember that the sequence of words is irrelevant, you can even imagine shaking up the bag. We calculate the average of all the context words contained in the bag, using this average to estimate what the target word could be. If we were concerned about syntax—the grammar of language (see Figure 2.9 for a refresher on the elements of natural language)—then word order would matter. But because with word2vec we’re concerned only with semantics—the meaning of words— it turns out that the order of context words is, on average, irrelevant. Having considered the intuitiveness of the “BOW” component of the CBOW moniker, let’s also consider the “continuous” part of it: The target word and context word windows slide continuously one word at a time from the first word of the corpus all the way through to the final word. At each position along the way, the target word is estimated given the context words. Via stochastic gradient descent, the location of words within vector space can be shifted, and thereby these target-word estimates can gradually be improved. In practice, and as summarized in Table 11.1, the SG architecture is a better choice when you’re working with a small corpus. It represents rare words in word-vector space well. In contrast, CBOW is much more computationally efficient, so it is the better option when you’re working with a very large corpus. Relative to SG, CBOW also represents frequently occurring words slightly better. [Note: Regardless of whether you use the SG or CBOW architecture, an additional option you have while running word2vec is the training method. For this, you have two different options: hierarchical softmax and negative sampling. The former involves normalization and is better suited to rare words. The latter, on the other hand, forgoes normalization, making it better suited to common words and low-dimensional word-vector spaces. For our purposes in this book, the differences between these two training methods are insignificant and we don’t cover them further.] Although word2vec is comfortably the most widely used approach for embedding words from a corpus of natural language into vector space, it is by no means the only approach. A major alternative to word2vec is GloVe—global vectors for word representation—which was introduced by the prominent natural language researchers Jeffrey Pennington, Richard Socher, and Christopher Manning. [Note: 15. Pennington, J., et al. (2014). GloVe: Global vectors for word representations. Proceedings of the Conference on Empirical Methods in Natural Language Processing.] At the time—in 2014—the three were colleagues working together at Stanford University. GloVe and word2vec differ in their underlying methodology: word2vec uses predictive models, while GloVe is count based. Ultimately, both approaches tend to pro- duce vector-space embeddings that perform similarly in downstream NLP applications, with some research suggesting that word2vec may provide modestly better results in select cases. One potential advantage of GloVe is that it was designed to be parallelized over multiple processors or even multiple machines, so it might be a good option if you’re looking to create a word-vector space with many unique words and a very large corpus. The contemporary leading alternative to both word2vec and GloVe is fastText. [Note: The open-source fastText library is available at fasttext.cc. Joulin, A., et al. (2016). Bag of tricks for efficient text classification. arXiv: 1607.01759. Bojanowski, P., et al. (2016). Enriching word vectors with subword information. arXiv: 1607.04606. Note that the lead author of the landmark word2vec paper, Tomas Mikolov, is the final author of both of these landmark fastText papers.] This approach was developed by researchers at Facebook. A major benefit of fastText is that it operates on a subword level—its “word” vectors are actually subcomponents of words. This enables fastText to work around some of the issues related to rare words and out-of-vocabulary words addressed in the preprocessing section at the outset of this chapter. Evaluating Word Vectors However you create your word vectors—be it with word2vec or an alternative approach—there are two broad perspectives you can consider when evaluating the quality of word vectors: intrinsic and extrinsic evaluations. Extrinsic evaluations involve assessing the performance of your word vectors within whatever your downstream NLP application of interest is—your sentiment-analysis classifier, say, or perhaps your named-entity recognition tool. Although extrinsic evaluations can take longer to carry out because they require you to carry out all of your downstream processing steps—including perhaps training a computationally intensive deep learning model—you can be confident that it’s worthwhile to retain a change to your word vectors if they relate to an appreciable improvement in the accuracy of your NLP application. In contrast, intrinsic evaluations involve assessing the performance of your word vectors not on your final NLP application, but rather on some specific intermediate sub- task. One common such task is assessing whether your word vectors correspond well to arithmetical analogies like those shown in Figure 2.7. For example, if you start at the word-vector location for king, subtract man, and add woman, do you end up near the word-vector location for queen? [Note: A test set of 19,500 such analogies was developed by Tomas Mikolov and his colleagues in their 2013 word2vec paper. This test set is available at download.tensorflow.org/data/questions-words.txt.] Relative to extrinsic evaluations, intrinsic tests are quick. They may also help you better understand (and therefore troubleshoot) intermediate steps within your broader NLP process. The limitation of intrinsic evaluations, however, is that they may not ultimately lead to improvements in the accuracy of your NLP application downstream unless you’ve identified a reliable, quantifiable relationship between performance on the intermediate test and your NLP application. Running word2vec As mentioned earlier, and as shown in Example 11.8, word2vec can be run in a single line of code—albeit with quite a few arguments. Example 11.8 Running word2vec model = Word2Vec(sentences=clean_sents, size=64, sg=1, window=10, iter=5, min_count=10, workers=4) Here’s a breakdown of each of the arguments we passed into the Word2Vec() method from the gensim library: sentences: Pass in a list of lists like clean_sents as a corpus. Elements in the higher-level list are sentences, whereas elements in the lower-level list can be word- level tokens. size: The number of dimensions in the word-vector space that will result from running word2vec. This is a hyperparameter that can be varied and evaluated extrinsically or intrinsically. Like other hyperparameters in this book, there is a Goldilocks sweet spot. You can home in on an optimal value by specifying, say, 32 dimensions and varying this value by powers of 2. Doubling the number of dimensions will double the computational complexity of your downstream deep learning model, but if doing this results in markedly higher model accuracy then this extrinsic evaluation suggests that the extra complexity could be worthwhile. On the other hand, halving the number of dimensions halves computational complexity downstream: If this can be done without appreciably decreasing your NLP model’s accuracy, then it should be. By performing a handful of intrinsic inspections (which we’ll go over shortly), we found 64 dimensions to provide more sensible word vectors than 32 dimensions for this particular case. Doubling this figure to 128, however, provided no noticeable improvement. sg: Set to 1 to choose the skip-gram architecture, or leave at the 0 default to choose CBOW. As summarized in Table 11.1, SG is generally better suited to small datasets like our Gutenberg corpus. window: For SG, a window size of 10 (for a total of 20 context words) is a good bet, so we set this hyperparameter to 10. If we were using CBOW, then a window size of 5 (for a total of 10 context words) could be near the optimal value. In either case, this hyperparameter can be experimented with and evaluated extrinsically or intrinsically. Small adjustments to this hyperparameter may not be perceptibly impactful, however. iter: By default, the gensim Word2Vec() method iterates over the corpus fed into it (i.e., slides over all of the words) five times. Multiple iterations of word2vec is analogous to multiple epochs of training a deep learning model. With a small corpus like ours, the word vectors improve over several iterations. With a very large corpus, on the other hand, it might be cripplingly computationally expensive to run even two iterations—and, because there are so many examples of words in a very large corpus anyway, the word vectors might not be any better. min_count: This is the minimum number of times a word must occur across the corpus in order to fit it into word-vector space. If a given target word occurs only once or a few times, there are a limited number of examples of its contextual words to consider, and so its location in word-vector space may not be reliable. Because of this, a minimum count of about 10 is often reasonable. The higher the count, the smaller the vocabulary of words that will be available to your downstream NLP task. This is yet another hyperparameter that can be tuned, with extrinsic evaluations likely being more illuminating than intrinsic ones because the size of the vocabulary you have to work with could make a considerable impact on your downstream NLP application. workers: This is the number of processing cores you’d like to dedicate to training. If the CPU on your machine has, say, eight cores, then eight is the largest number of parallel worker threads you can have. In this case, if you choose to use fewer than eight cores, you’re leaving compute resources available for other tasks. In our GitHub repository, we saved our model using the save() method of word2vec objects: model.save(&#39;clean_gutenberg_model.w2v&#39;) Instead of running word2vec yourself, then, you’re welcome to load up our word vectors using this code: model = gensim.models.Word2Vec.load(&#39;clean_gutenberg_model.w2v&#39;) If you do choose the word vectors we created, then the following examples will produce the same outputs. [Note: Every time word2vec is run, the initial locations of every word of the vocabulary within word-vector space are assigned randomly. Because of this, the same data and arguments provided to Word2Vec() will nevertheless produce unique word vectors every time, but the semantic relationships should be similar.] We can see the size of our vocabulary by calling len(model.wv.vocab). This tells us that there are 10,329 words (well, more specifically, tokens) that occur at least 10 times within our clean_sents corpus. [Note: Vocabulary size is equal to the number of tokens from our corpus that had occurred at least 10 times, because we set min_count=10 when calling Word2Vec() in Example 11.8.] One of the words in our vocabulary is dog. As shown in Figure 11.6, we can output its location in 64-dimensional word-vector space by running model.wv[&#39;dog&#39;]. As a rudimentary intrinsic evaluation of the quality of our word vectors, we can use the most_similar() method to confirm that words with similar meanings are found in similar locations within our word-vector space. [Note: Technically speaking, the similarity between two given words is computed here by calculating the cosine similarity.] For example, to output the three words that are most similar to father in our word-vector space, we can run this code: model.wv.most_similar(&#39;father&#39;, topn=3) This outputs the following: [(&#39;mother&#39;, 0.8257375359535217),(&#39;brother&#39;, 0.7275018692016602),(&#39;sister&#39;, 0.7177823781967163)] This output indicates that mother, brother, and sister are the most similar words to father in our word-vector space. In other words, within our 64-dimensional space, the word that is closest. [Note: That is, has the shortest Euclidean distance in that 64-dimensional vector space.] to father is the word mother. Table 11.2 provides some additional examples of the words most similar to (i.e., closest to) particular words that we’ve picked from our word- vector vocabulary, all five of which appear pretty reasonable given our small Gutenberg corpus. [Note that the final test word in Table 11.2—ma’am—is only available because of the bigram collocation (see Examples 11.6 and 11.7).] Suppose we run the following line of code: model.wv.doesnt_match(&#34;mother father sister brother dog&#34;.split()) We get the output dog, indicating that dog is the least similar relative to all the other possible word pairs. We can also use the following line to observe that the similarity score between father and dog is a mere 0.44: model.wv.similarity(&#39;father&#39;, &#39;dog&#39;) This similarity score of 0.44 is much lower than the similarity between father and any of mother, brother, or sister, and so it’s unsurprising that dog is relatively distant from the other four words within our word-vector space. As a final little intrinsic test, we can compute word-vector analogies as in Figure 2.7. For example, we can execute this code: model.wv.most_similar(positive=[&#39;father&#39;, &#39;woman&#39;], negative=[&#39;man&#39;]) The top-scoring word comes out as mother, which is the correct answer to the analogy. Suppose we likewise execute this code: model.wv.most_similar(positive=[&#39;husband&#39;, &#39;woman&#39;], negative=[&#39;man&#39;]) In this case, the top-scoring word comes out as wife, again the correct answer, therebysuggesting that our word-vector space may generally be on the right track. A given dimension within an n-dimensional word-vector space does not necessarily represent any specific factor that relates words. For example, although the real-world differences in meaning of gender or verb tense are represented by some vector direction (i.e., some movement along some combination of dimensions) within the vector space, this meaningful vector direction may only by chance be aligned—or perhaps correlated—with a particular axis of the vector space. This contrasts with some other approaches that involve n-dimensional vector spaces, where the axes are intended to represent some specific explanatory variable. One such approach that many people are familiar with is principal component anal- ysis (PCA), a technique for identifying linearly uncorrelated (i.e., orthogonal) vectors that contribute to variance in a given dataset. A corollary of this difference between information stored as points in PCA versus in word-vector space is that in PCA, the first principal components contribute most of the variance, and so you can focus on them and ignore later principal components; but in a word-vector space, all of the dimensions may be important and need to be taken into consideration. In this way, approaches like PCA are useful for dimensionality reduction because we do not need to consider all of the dimensions. Plotting Word Vectors Human brains are not well suited to visualizing anything in greater than three dimensions. Thus, plotting word vectors—which could have dozens or even hundreds of dimensions—in their native format is out of the question. Thankfully, we can use techniques for dimensionality reduction to approximately map the locations of words from high- dimensional word-vector space down to two or three dimensions. Our recommended approach for such dimensionality reduction is t-distributed stochastic neighbor embedding (t-SNE; pronounced tee-snee), which was developed by Laurens van der Maaten in col- laboration with Geoff Hinton (Figure 1.16). [Note: van der Maaten, L., Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9, 2579–605.] Example 11.9 provides the code from our Natural Language Preprocessing notebook for reducing our 64-dimensional Project Gutenberg-derived word-vector space down to two dimensions, and then storing the resulting x and y coordinates within a Pandas DataFrame. There are two arguments for the TSNE() method (from the scikit-learn library) that we need to focus on: n_components is the number of dimensions that should be returned, so setting this to 2 results in a two-dimensional output, whereas 3 would result in a three- dimensional output. n_iter is the number of iterations over the input data. As with word2vec (Example 11.8), iterations are analogous to the epochs associated with training a neural network. More iterations corresponds to a longer training time but may improve the results (although only up to a point). Example 11.9 t-SNE for dimensionality reduction tsne = TSNE(n_components=2, n_iter=1000) X_2d = tsne.fit_transform(model.wv[model.wv.vocab]) coords_df = pd.DataFrame(X_2d, columns=[&#39;x&#39;,&#39;y&#39;]) coords_df[&#39;token&#39;] = model.wv.vocab.keys() Running t-SNE as in Example 11.9 may take some time on your machine, so you’re welcome to use our results if you’re feeling impatient by running the following code: coords_df = pd.read_csv(&#39;clean_gutenberg_tsne.csv&#39;) [Note: We created this CSV after running t-SNE on our word-vectors using this command: coords_df.to_csv(&#39;clean_gutenberg_tsne.csv&#39;, index=False). Note that because t-SNE is stochastic, you will obtain a unique result every time you run it.] Whether you ran t-SNE to produce coords_df on your own or you loaded in ours, you can check out the first few lines of the DataFrame by using the head() method: coords_df.head() Our output from executing head() is shown in Figure 11.7. Example 11.10 provides code for creating a static scatterplot (Figure 11.8) of the two-dimensional data we created with t-SNE (in Example 11.9). Example 11.10 Static two-dimensional scatterplot of word-vector space _ = coords_df.plot.scatter(&#39;x&#39;, &#39;y&#39;, figsize=(12,12),marker=&#39;.&#39;, s=10, alpha=0.2) On its own, the scatterplot displayed in Figure 11.8 may look interesting, but there’s little actionable information we can take away from it. Instead, we recommend using the bokeh library to create a highly interactive—and actionable—plot, as with the code provided in Example 11.11. [Note: In Example 11.11, we used the Pandas sample() method to reduce the dataset down to 5,000 tokens, because we found that using more data than this corresponded to a clunky user experience when using the bokeh plot interactively.] Example 11.11 Interactive bokeh plot of two-dimensional word-vector data output_notebook() subset_df = coords_df.sample(n=5000) p = figure(plot_width=800, plot_height=800) _ = p.text(x=subset_df.x, y=subset_df.y, text=subset_df.token) show(p) The code in Example 11.11 produces the interactive scatterplot in Figure 11.9 using the x and y coordinates generated using t-SNE. By toggling the Wheel Zoom button in the top-right corner of the plot, you can use your mouse to zoom into locations within the cloud so that the words become legible. For example, as shown in Figure 11.10, we identified a region composed largely of items of clothing, with related clusters nearby, including parts of the human anatomy, colors, and fabric types. Exploring in this way provides a largely subjective intrinsic evaluation of whether related terms—and particularly synonyms—cluster together as you’d expect them to. Doing similar, you may also notice particular shortcomings of your natural-language preprocessing steps, such as the inclusion of punctuation marks, bigrams, or other tokens that you may prefer weren’t included within your word-vector vocabulary. The Area under the ROC Curve Our apologies for interrupting the fun, interactive plotting of word vectors. We need to take a brief break from natural language-specific content here to introduce a metric that will come in handy in the next section of the chapter, when we will evaluate the performance of deep learning NLP models. Up to this point in the book, most of our models have involved multiclass outputs: When working with the MNIST digits, for example, we used 10 output neurons to rep- resent each of the 10 possible digits that an input image could represent. In the remaining sections of this chapter, however, our deep learning models will be binary classifiers: They will distinguish between only two classes. More specifically, we will build binary classifiers to predict whether the natural language of film reviews corresponds to a favorable review or negative one. Unlike artificial neural networks tasked with multiclass problems, which require as many output neurons as classes, ANNs that are acting as binary classifiers require only a single output neuron. This is because there is no extra information associated with having two output neurons. If a binary classifier is provided some input x and it calculates some output [latex]\hat{y}[/latex] for one of the classes, then the output for the other class is simply 1 - [latex]\hat{y}[/latex]. As an example, if we feed a movie review into a binary classifier and it outputs that the probability that this review is a positive one is 0.85, then it must be the case that the probability of the review being negative is 1 − 0.85 = 0.15. Because binary classifiers have a single output, we can take advantage of metrics for evaluating our model’s performance that are sophisticated relative to the excessively black-and-white accuracy metric that dominates multiclass problems. A typical accuracy calculation, for example, would contend that if [latex]\hat{y}[/latex] &gt; 0.5 then the model is predicting that the input x belongs to one class, whereas if it outputs anything less than 0.5, it belongs to the other class. To illustrate why having a specific binary threshold like this is overly simplistic, consider a situation where inputting a movie review results in a binary classifier outputting [latex]\hat{y}[/latex] = 0.48: A typical accuracy calculation threshold would hold that—because this [latex]\hat{y}[/latex] is lower than 0.5--it is being classed as a negative review. If a second film review corresponds to an output of [latex]\hat{y}[/latex] = 0.51, the model has barely any more confidence that this review is positive relative to the first review. Yet, because 0.51 is greater than the 0.5 accuracy threshold, the second review is classed as a positive review. The starkness of the accuracy metric threshold can hide a fair bit of nuance in the quality of our model’s output, and so when evaluating the performance of binary classifiers, we prefer a metric called the area under the curve of the receiver operating characteristic. The ROC AUC, as the metric is known for short, has its roots in the Second World War, when it was developed to assess the performance of radar engineers’ judgment as they attempted to identify the presence of enemy objects. We like the ROC AUC for two reasons: It blends together two useful metrics—true positive rate and false positive rate—into a single summary value. It enables us to evaluate the performance of our binary classifier’s output across the full range of [latex]\hat{y}[/latex], from 0.0 to 1.0. This contrasts with the accuracy metric, which evaluates the performance of a binary classifier at a single threshold value only— usually [latex]\hat{y}[/latex] = 0.50. The Confusion Matrix The first step toward understanding how to calculate the ROC AUC metric is to under- stand the so-called confusion matrix, which—as you’ll see—isn’t actually all that confusing. Rather, the matrix is a straightforward 2 × 2 table of how confused a model (or, as back in WWII, a person) is while attempting to act as a binary classifier. You can see an example of a confusion matrix in Table 11.3. To bring the confusion matrix to life with an example, let’s return to the hot dog / not hot dog binary classifier that we’ve used to construct silly examples over many of the preceding chapters: When we provide some input x to a model and it predicts that the input represents a hot dog, then we’re dealing with the first row of the table, because the predicted y = 1. In that case, True positive: If the input is actually a hot dog (i.e., actual y = 1), then the model correctly classified the input. False positive: If the input is actually not a hot dog (i.e., actual y = 0), then the model is confused. False negative: If the input is actually a hot dog (i.e., actual y = 1), then the model is also confused in this circumstance. True negative: If the input is actually not a hot dog (i.e., actual y = 0), then the model correctly classified the input.When we provide some input x to a model and it predicts that the input does not represent a hot dog, then we’re dealing with the second row of the table, because predicted y = 0. In that case, Calculating the ROC AUC Metric Briefed on the confusion matrix, we can now move forward and calculate the ROC AUC metric itself, using a toy-sized example. Let’s say, as shown in Table 11.4, we provide four inputs to a binary-classification model. Two of these inputs are actually hot dogs (y = 1), and two of them are not hot dogs (y = 0). For each of these inputs, the model outputs some predicted [latex]\hat{y}[/latex], all four of which are provided in Table 11.4. To calculate the ROC AUC metric, we consider each of the [latex]\hat{y}[/latex] values output by the model as the binary-classification threshold in turn. Let’s start with the lowest [latex]\hat{y}[/latex], which is 0.3 (see the “0.3 threshold” column in Table 11.5). At this threshold, only the first input is classed as not a hot dog, whereas the second through fourth inputs (all with [latex]\hat{y}[/latex] &gt; 0.3) are all classed as hot dogs. We can compare each of these four predicted classifications with the confusion matrix in Table 11.3: True negative (TN): This is actually not a hot dog (y = 0) and was correctly predicted as such. True positive (TP): This is actually a hot dog (y = 1) and was correctly predicted as such. False positive (FP): This is actually not a hot dog (y = 0) but it was erroneously predicted to be one. True positive (TP): Like input 2, this is actually a hot dog (y = 1) and was correctly predicted as such. The same process is repeated with the classification threshold set to 0.5 and yet again with the threshold set to 0.6, allowing us to populate the remaining columns of Table 11.5. As an exercise, it might be wise to work through these two columns, comparing the classifications at each threshold with the actual y values and the confusion matrix (Table 11.3) to ensure that you have a good handle on these concepts.Finally, note that the highest [latex]\hat{y}[/latex] value (in this case, .09) can be skipped as a potential threshold, because at such a high threshold we’d be considering all four instances to not be hot dogs, making it a ceiling instead of a classification boundary. The next step toward computing the ROC AUC metric is to calculate both the true positive rate (TPR) and the false positive rate (FPR) at each of the three thresholds. Equations 11.1 and 11.2 use the “0.3 threshold” column to provide examples of how to calculate the true positive rate and false positive rate, respectively. Shorthand versions of the arithmetic for calculating TPR and FPR for the thresholds 0.5 and 0.6 are also provided for your convenience at the bottom of Table 11.5. Again, perhaps you should test if you can compute these values yourself on your own time. The final stage in calculating ROC AUC is to create a plot like the one we provide in Figure 11.11. The points that make up the shape of the receiver operating characteristic (ROC) curve are the false positive rate (horizontal, x-axis coordinate) and true positive rate (vertical, y-axis coordinate) at each of the available thresholds (which in this case is three) in Table 11.5, plus two extra points in the bottom-left and top-right corners of the plot. Specifically, these five points (shown as orange dots in Figure 11.11) are: (0, 0) for the bottom-left corner (0, 0.5) from the 0.6 threshold (0.5, 0.5) from the 0.5 threshold (0.5, 1) from the 0.3 threshold (1, 1) for the top-right corner In this toy-sized example, we only used four distinct [latex]\hat{y}[/latex] values so there are only five points that determine the shape of the ROC curve, making the curve rather step shaped. When there are many available predictions providing many distinct [latex]\hat{y}[/latex] values—as is typically the case in real-world examples—the ROC curve has many more points, and so it’s much less step shaped and much more, well, curve shaped. The area under the curve (AUC) of the ROC curve is exactly what it sounds like: In Figure 11.11, we’ve shaded this area in orange and, in this example, the AUC constitutes 75 percent of all the possible area and so the ROC AUC metric comes out to 0.75. A binary classifier that works as well as chance will generate a straight diagonal running from the bottom-left corner of the plot to its top-right corner, so an ROC AUC of 0.5 indicates that the classifier works as well as flipping a coin. A perfect ROC AUC is 1.0, which is attained by having FPR = 0 and TPR = 1 across all of the available [latex]\hat{y}[/latex] thresholds. When you’re designing a binary classifier to perform well on the ROC AUC metric, the goal is thus to minimize FPR and maximize TPR across the range of [latex]\hat{y}[/latex] thresholds. That said, for most problems you encounter, attaining a perfect ROC AUC of 1.0 is not possible: There is usually some noise—perhaps a lot of noise—in the data that makes perfection unattainable. Thus, when you’re working with any given dataset, there is some (typically unknown!) maximum ROC AUC score, such that no matter how ideally suited your model is to act as a binary classifier for the problem, there’s an ROC AUC ceiling that no model can crack through. Over the remainder of this chapter we use the illuminating ROC AUC metric, alongside the simpler accuracy and cost metrics you are already acquainted with, to evaluate the performance of the binary-classifying deep learning models that we design and train. Natural Language Classification with Familiar Networks In this section, we tie together concepts that were introduced in this chapter—natural language preprocessing best practices, the creation of word vectors, and the ROC AUC metric—with the deep learning theory from previous chapters. As we already alluded to earlier, the natural language processing model you’ll experiment with over the remainder of the chapter will be a binary classifier that predicts whether a given film review is a positive one or a negative one. We begin by classifying natural language documents using types of neural networks that you’re already familiar with—dense and convolutional— before moving along to networks that are specialized to handle data that occur in a sequence. Loading the IMDb Film Reviews As a performance baseline, we’ll initially train and test a relatively simple dense network. All of the code for doing this is provided within our Dense Sentiment Classifier Jupyter notebook [or the complementary Domino project]. Example 11.12 provides the dependencies we need for our dense sentiment classifier. Many of these dependencies will be recognizable from previous chapters, but others (e.g., for loading a dataset of film reviews, saving model parameters as we train, calculating ROC AUC) are new. As usual, we cover the details of these dependencies as we apply them later on. Example 11.12 Loading sentiment classifier dependencies import keras from keras.datasets import imdb # new! from keras.preprocessing.sequence import pad_sequences # new! from keras.models import Sequential from keras.layers import Dense, Flatten, Dropout from keras.layers import Embedding # new! from keras.callbacks import ModelCheckpoint # new! import os # new! from sklearn.metrics import roc_auc_score, roc_curve # new! import pandas as pd import matplotlib.pyplot as plt # new! %matplotlib inline It’s a good programming practice to put as many hyperparameters as you can at the top of your file. This makes it easier to experiment with these hyperparameters. It also makes it easier for you (or, indeed, your colleagues) to understand what you were doing in the file when you return to it (perhaps much) later. With this in mind, we place all of our hyperparameters together in a single cell within our Jupyter notebook. The code is provided in Example 11.13. Example 11.13 Setting dense sentiment classifier hyperparameters # output directory name: output_dir = &#39;model_output/dense&#39; # training: epochs = 4 batch_size = 128 # vector-space embedding: n_dim = 64 n_unique_words = 5000 n_words_to_skip = 50 max_review_length = 100 pad_type = trunc_type = &#39;pre&#39; # neural network architecture: n_dense = 64 dropout = 0.5 Let’s break down the purpose of each of these variables: output_dir: A directory name (ideally, a unique one) in which to store our model’s parameters after each epoch, allowing us to return to the parameters from any epoch of our choice at a later time. epochs: The number of epochs that we’d like to train for, noting that NLP models often overfit to the training data in fewer epochs than machine vision models. batch_size: As before, the number of training examples used during each round of model training (see Figure 8.5). n_dim: The number of dimensions we’d like our word-vector space to have. n_unique_words: With word2vec earlier in this chapter, we included tokens in our word-vector vocabulary only if they occurred at least a certain number of times within our corpus. An alternative approach—the one we take here—is to sort all of the tokens in our corpus by the number of times they occur, and then only use a certain number of the most popular words. Andrew Maas and his coworkers [Note: We mentioned Maas et al. (2011) earlier in this chapter. They put together the movie-review corpus we’re using in this notebook.] opted to use the 5,000 most popular words across their film-review corpus and so we’ll do the same.[Note: This 5,000-word threshold may not be optimal, but we didn’t take the time to test lower or higher values. You are most welcome to do so yourself!] n_words_to_skip: Instead of removing a manually curated list of stop words from their word-vector vocabulary, Maas et al. made the assumption that the 50 most frequently occurring words across their film-review corpus would serve as a decent list of stop words. We followed their lead and did the same.[Note: Note again that following Maas et al.’s lead may not be the optimal choice. Further, note that this means we’ll actually be including the 51st most popular word through to the 5050th most popular word in our word-vector vocabulary.] max_review_length: Each movie review must have the same length so that TensorFlow knows the shape of the input data that will be flowing through our deep learning model. For this model, we selected a review length of 100 words.[Note: You are free to experiment with lengthier or shorter reviews.] Any reviews longer than 100 are truncated. Any reviews shorter than 100 are padded with a special padding character (analogous to the zero padding that can be used in machine vision, as in Figure 10.3). pad_type: By selecting &#39;pre&#39;, we add padding characters to the start of every review. The alternative is &#39;post&#39;, which adds them to the end. With a dense network like the one in this notebook, it shouldn’t make much difference which of these options we pick. Later in this chapter, when we’re working with specialized, sequential-data layer types, [Note: For example, RNN, LSTM.] it’s generally best to use &#39;pre&#39; because the content at the end of the document is more influential in the model and so we want the largely uninformative padding characters to be at the beginning of the document. trunc_type: As with pad_type, our truncation options are &#39;pre&#39; or &#39;post&#39;. The former will remove words from the beginning of the review, whereas the latter will remove them from the end. By selecting &#39;pre&#39;, we’re making (a bold!) assumption that the end of film reviews tend to include more information on review sentiment than the beginning. n_dense: The number of neurons to include in the dense layer of our neural network architecture. We waved our finger in the air to select 64, so some experimentation and optimization are warranted at your end if you feel like it. For simplicity’s sake, we also are using a single layer of dense neurons, but you could opt to have several. dropout: How much dropout to apply to the neurons in the dense layer. Again, we did not take the time to optimize this hyperparameter (set at 0.5) ourselves. Loading in the film review data is a one-liner, provided in Example 11.14. Example 11.14 Loading IMDb film review data (x_train, y_train), (x_valid, y_valid) = \ imdb.load_data(num_words=n_unique_words, skip_top=n_words_to_skip) This dataset from Maas et al. (2011) is made up of the natural language of reviews from the publicly available Internet Movie Database (IMDb; imdb.com). It consists of 50,000 reviews, half of which are in the training dataset (x_train), and half of which are for model validation (x_valid). When submitting their review of a given film, users also provide a star rating, with a maximum of 10 stars. The labels (y_train and y_valid) are binary, based on these star ratings: Reviews with a score of four stars or fewer are considered to be a negative review (y = 0). Reviews with a score of seven stars or more, meanwhile, are classed as a positive review (y = 1). Moderate reviews—those with five or six stars—are not included in the dataset, making the binary classification task easier for any model. By specifying values for the num_words and skip_top arguments when calling imdb.load_data(), we are limiting the size of our word-vector vocabulary and removing the most common (stop) words, respectively. In our Dense Sentiment Classifier notebook, we have the convenience of loading our IMDb film-review data via the Keras imdb.load_data() method. When you’re working with your own natural language data, you’ll likely need to preprocess many aspects of the data yourself. In addition to the general preprocessing guidance we provided earlier in this chapter, Keras provides a number of convenient text preprocessing utilities, as documented online at keras.io/preprocessing/text. In particular, the Tokenizer() class may enable you to carry out all of the preprocessing steps you need in a single line of code, including - Tokenizing a corpus to the word level (or even the character level) - Setting the size of your word-vector vocabulary (with num_words) - Filtering out punctuation Converting all characters to lowercase - Converting tokens into an integer index Examining the IMDb Data Executing x_train[0:6], we can examine the first six reviews from the training dataset, the first two of which are shown in Figure 11.12. These reviews are natively in an integer-index format, where each unique token from the dataset is represented by an integer. The first few integers are special cases, following a general convention that is widely used in NLP: 0: Reserved as the padding token (which we’ll soon add to the reviews that are shorter than max_review_length). 1: Would be the starting token, which would indicate the beginning of a review. As per the next bullet point, however, the starting token is among the top 50 most common tokens and so is shown as “unknown.” 2: Any tokens that occur very frequently across the corpus (i.e., they’re in the top 50 most common words) or rarely (i.e., they’re below the top 5,050 most common words) will be outside of our word-vector vocabulary and so are replaced with this unknown token. 3: The most frequently occurring word in the corpus. 4: The second-most frequently occurring word. 5: The third-most frequently occurring, and so on. Using the following code from Example 11.15, we can see the length of the first six reviews in the training dataset. Example 11.15 Printing the number of tokens in six reviews for x in x_train[0:6]: print(len(x)) They are rather variable, ranging from 43 tokens up to 550 tokens. Shortly, we’ll handle these discrepancies, standardizing all reviews to the same length. The film reviews are fed into our neural network model in the integer-index format of Figure 11.12 because this is a memory-efficient way to store the token information. It would require appreciably more memory to feed the tokens in as character strings, for example. For us humans, however, it is uninformative (and, frankly, uninteresting) to examine reviews in the integer-index format. To view the reviews as natural language, we create an index of words as follows, where PAD, START, and UNK are customary for representing padding, starting, and unknown tokens, respectively: word_index = keras.datasets.imdb.get_word_index() word_index = {k:(v+3) for k,v in word_index.items()} word_index[&#34;PAD&#34;] = 0 word_index[&#34;START&#34;] = 1 word_index[&#34;UNK&#34;] = 2 index_word = {v:k for k,v in word_index.items()} Then we can use the code in Example 11.16 to view the film review of our choice—in this case, the first review from the training data. Example 11.16 Printing a review as a character string &#39; &#39;.join(index_word[id] for id in x_train[0]) The resulting string should look identical to the output shown in Figure 11.13. Remembering that the review in Figure 11.13 contains the tokens that are fed into our neural network, we might nevertheless find it enjoyable to read the full review without all of the UNK tokens. In some cases of debugging model results, it might indeed even be practical to be able to view the full review. For example, if we’re being too aggressive or conservative with either our n_unique_words or n_words_to_skip thresholds, it might become apparent by comparing a review like the one in Figure 11.13 with a full one. With our index of words (index_words) already available to us, we simply need to download the full reviews: (all_x_train,_),(all_x_valid,_) = imdb.load_data() Then we modify Example 11.16 to execute join() on the full-review list of our choice (i.e., all_x_train or all_x_valid), as provided in Example 11.17. Example 11.17 Print full review as character string &#39; &#39;.join(index_word[id] for id in all_x_train[0]) Executing this outputs the full text of the review of our choice—again, in this case, the first training review—as shown in Figure 11.14. Standardizing the Length of the Reviews By executing Example 11.15 earlier, we discovered that there is variability in the length of the film reviews. In order for the Keras-created TensorFlow model to run, we need to specify the size of the inputs that will be flowing into the model during training. This enables TensorFlow to optimize the allocation of memory and compute resources. Keras provides a convenient pad_sequences() method that enables us to both pad and truncate documents of text in a single line. Here we standardize our training and validation data in this way, as shown in Example 11.18. Example 11.18 Standardizing input length by padding and truncating x_train = pad_sequences(x_train, maxlen=max_review_length, padding=pad_type, truncating=trunc_type, value=0) x_valid = pad_sequences(x_valid, maxlen=max_review_length, padding=pad_type, truncating=trunc_type, value=0) Now, when printing reviews (e.g., with x_train[0:6]) or their lengths (e.g., with the code from Example 11.15), we see that all of the reviews have the same length of 100 (because we set max_review_length = 100). Examining x_train[5]—which previously had a length of only 43 tokens—with code similar to Example 11.16, we can observe that the beginning of the review has been padded with 57 PAD tokens (see Figure 11.15). Dense Network With sufficient NLP theory behind us, as well as our data loaded and preprocessed, we’re at long last prepared to make use of a neural network architecture to classify film reviews by their sentiment. A baseline dense network model for this task is shown in Example 11.19. Example 11.19 Dense sentiment classifier architecture model = Sequential() model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) model.add(Flatten()) model.add(Dense(n_dense, activation=&#39;relu&#39;)) model.add(Dropout(dropout)) # model.add(Dense(n_dense, activation=&#39;relu&#39;)) # model.add(Dropout(dropout)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) Let’s break the architecture down line by line: We’re using a Keras Sequential() method to invoke a sequential model, as we have for all of the models so far in this book. As with word2vec, the Embedding() layer enables us to create word vectors from a corpus of documents—in this case, the 25,000 movie reviews of the IMDb training dataset. Relative to independently creating word vectors with word2vec (or GloVe, etc.) as we did earlier in this chapter, training your word vectors via backpropagation as a component of your broader NLP model has a potential advantage: The locations that words are assigned to within the vector space reflect not only word similarity but also the relevance of the words to the ultimate, specific purpose of the model (e.g., binary classification of IMDb reviews by sentiment). The size of the word-vector vocabulary and the number of dimensions of the vector space are specified by n_unique_words and n_dim, respectively. Because the embedding layer is the first hidden layer in our network, we must also pass into it the shape of our input layer: We do this with the input_length argument. As in Chapter 10, the Flatten() layer enables us to pass a many-dimensional output (here, a two-dimensional output from the embedding layer) into a one- dimensional dense layer. Speaking of Dense() layers, we used a single one consisting of relu activations in this architecture, with applied to it. We opted for a fairly shallow neural network architecture for our baseline model, but you can trivially deepen it by adding further Dense()layers (see the lines that are commented out)./li&gt; Finally, because there are only two classes to classify, we require only a single output neuron (because, as discussed earlier in this chapter, if one class has the probability p then the other class has the probability 1 − p). This neuron is sigmoid because we’d like it to output probabilities between 0 and 1 (refer to Figure 6.9). In addition to training word vectors on natural language data alone (e.g., with word2vec or GloVe) or training them with an embedding layer as part of a deep learning model, pretrained word vectors are also available online. As with using a ConvNet trained on the millions of images in ImageNet (Chapter 10), this natural language transfer learning is powerful, because these word vectors may have been trained on extremely large corpuses (e.g., all of Wikipedia, or the English-language Internet) that provide large, nuanced vocabularies that would be expensive to train yourself. Examples of pretrained word vectors are available in this repo and here. The fast- Text library also offers subword embeddings in 157 languages; these can be downloaded from fasttext.cc. In this book, we don’t cover substituting pretrained word vectors (be they down- loaded or trained separately from your deep learning model, as we did with Word2Vec() earlier in this chapter) in place of the embedding layer, because there are many different permutations on how you might like to do this. See this neat tutorial from François Chollet, the creator of Keras. Executing model.summary(), we discover that our fairly simple NLP model has quite a few parameters, as shown in Figure 11.16: In the embedding layer, the 320,000 parameters come from having 5,000 words, each one with a location specified in a 64-dimensional word-vector space (64 × 5,000 = 320,000). Flowing out of the embedding layer through the flatten layer and into the dense layer are 6,400 values: Each of our film-review inputs consists of 100 tokens, with each token specified by 64 word-vector-space coordinates (64 × 100 = 6,400). Each of the 64 neurons in the dense hidden layer receives input from each of the 6,400 values flowing out of the flatten layer, for a total of 64 × 6,400 = 409,600 weights. And, of course, each of the 64 neurons has a bias, for a total of 409,664 parameters in the layer. Finally, the single neuron of the output layer has 64 weights—one for the activation output by each of the neurons in the preceding layer—plus its bias, for a total of 65 parameters. Summing up the parameters from each of the layers, we have a grand total of 730,000 of them. As shown in Example 11.20, we compile our dense sentiment classifier with a line of code that should already be familiar from recent chapters, except that—because we have a single output neuron within a binary classifier—we use binary_crossentropy cost in place of the categorical_crossentropy cost we used for our multiclass MNIST classifiers. Example 11.20 Compiling our sentiment classifier model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) With the code provided in Example 11.21, we create a ModelCheckpoint() object that will allow us to save our model parameters after each epoch during training. By doing this, we can return to the parameters from our epoch of choice later on during model evaluation or to make inferences in a production system. If the output_dir directory doesn’t already exist, we use the makedirs() method to make it. Example 11.21 Creating an object and directory for checkpointing model parameters after each epoch modelcheckpoint = ModelCheckpoint(filepath=output_dir+ &#34;/weights.{epoch:02d}.hdf5&#34;) if not os.path.exists(output_dir): os.makedirs(output_dir) ###Insert Figure Like the compile step, the model-fitting step (Example 11.22) for our sentiment classifier should be familiar except, perhaps, for our use of the callbacks argument to pass in the modelcheckpoint object. [Note: This isn’t our first use of the callbacks argument. We previously used this argument, which can take in a list of multiple different callbacks, to provide data on model training progress to TensorBoard (see Chapter 9)]. Example 11.22 Fitting our sentiment classifier model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), callbacks=[modelcheckpoint]) As shown in Figure 11.17, we achieve our lowest validation loss (0.349) and highest validation accuracy (84.5 percent) in the second epoch. In the third and fourth epochs, the model is heavily overfit, with accuracy on the training set considerably higher than on the validation set. By the fourth epoch, training accuracy stands at 99.6 percent while validation accuracy is much lower, at 83.4 percent. To evaluate the results of the best epoch more thoroughly, we use the Keras load_ weights() method to load the parameters from the second epoch (weights.02.hdf5) back into our model, as in Example 11.23. [Note: Although the method is called load_weights(), it loads in all model parameters, including biases. Because weights typically constitute the vast majority of parameters in a model, deep learning practitioners often call parameter files “weights” files. Earlier versions of Keras used zero indexing for epochs, but more recent versions index starting at 1.] Example 11.23 Loading model parameters model.load_weights(output_dir+&#34;/weights.02.hdf5&#34; We can then calculate validation set y_hat values for the best epoch by passing the predict_proba() method on the x_valid dataset, as shown in Example 11.24. Example 11.24 Predicting y_hat for all validation y_hat = model.predict_proba(x_valid) With y_hat[0], for example, we can now see the model’s prediction of the sentiment of the first movie review in the validation set. For this review, [latex]\hat{y}[/latex] = 0.09, indicating the model estimates that there’s a 9 percent chance the review is positive and, therefore, a 91 percent chance it’s negative. Executing y_valid[0] informs us that [latex]\hat{y}[/latex] = 0 for this review—that is, it is in fact a negative review—so the model’s [latex]\hat{y}[/latex] is pretty good! If you’re curious about what the content of the negative review was, you can run a slight modification on Example 11.17 to access the full text of the all_x_valid[0] list item, as shown in Example 11.25. Example 11.25 Printing a full validation review &#39; &#39;.join(index_word[id] for id in all_x_valid[0]) Examining individual scores can be interesting, but we get a much better sense of our model’s performance by looking at all of the validation results together. We can plot a histogram of all the validation [latex]\hat{y}[/latex] values by running the code in Example 11.26. Example 11.26 Plotting a histogram of validation data [latex]\hat{y}[/latex] values plt.hist(y_hat)_ = plt.axvline(x=0.5, color=&#39;orange&#39;) The histogram output is provided in Figure 11.18. The plot shows that the model often has a strong opinion on the sentiment of a given review: Some 8,000 of the 25,000 re- views (~32 percent of them) are assigned a [latex]\hat{y}[/latex] of less than 0.1, and ~6,500 (~26 percent) are given a [latex]\hat{y}[/latex] greater than 0.9. The vertical orange line in Figure 11.18 marks the 0.5 threshold above which reviews are considered by a simple accuracy calculation to be positive. As discussed earlier in the chapter, such a simple threshold can be misleading, because a review with a yˆ just be- low 0.5 is not predicted by the model to have much difference in sentiment relative toa review with a [latex]\hat{y}[/latex] just above 0.5. To obtain a more nuanced assessment of our model’s performance as a binary classifier, we can use the roc_auc_score() method from the scikit-learn metrics library to straightforwardly calculate the ROC AUC score across the validation data, as shown in Example 11.27. Example 11.27 Calculating ROC AUC for validation data pct_auc = roc_auc_score(y_valid, y_hat)*100.0 &#34;{:0.2f}&#34;.format(pct_auc) Printing the output in an easy-to-read format with the format() method, we see that the percentage of the area under the receiver operating characteristic curve is (a fairly high) 92.9 percent. To get a sense of where the model breaks down, we can create a DataFrame of y and [latex]\hat{y}[/latex] validation set values, using the code in Example 11.28. Example 11.28 Creating a ydf DataFrame of y and ˆy values float_y_hat = [] for y in y_hat: float_y_hat.append(y[0]) ydf = pd.DataFrame(list(zip(float_y_hat, y_valid)), columns=[&#39;y_hat&#39;, &#39;y&#39;]) Printing the first 10 rows of the resulting ydf DataFrame with ydf.head(10), we see the output shown in Figure 11.19. Querying the ydf DataFrame as we do in Examples 11.29 and 11.30 and then examining the individual reviews these queries surface by varying the list index in Example 11.25, you can get a sense of the kinds of reviews that cause the model to make its largest errors. Example 11.29 Ten cases of negative validation reviews with high [latex]\hat{y}[/latex] scores ydf[(ydf.y == 0) (ydf.y_hat &gt; 0.9)].head(10) Example 11.30 Ten cases of positive validation reviews with low [latex]\hat{y}[/latex] scores ydf[(ydf.y == 0) (ydf.y_hat &gt; 0.9)].head(10) An example of a false positive—a negative review (y = 0) with a very high model score ([latex]\hat{y}[/latex] = 0.97)—that was identified by running the code in Example 11.29 is provided in Figure 11.20. [Note: We output this particular review—the 387th in the validation dataset—by running the following code: &#39; &#39;.join(index_word[id] for id in all_x_valid[386]).] And an example of a false negative—a positive review (y = 1) with a very low model score ([latex]\hat{y}[/latex] = 0.06)—that was identified by running the code in Example 11.30 is provided in Figure 11.21. [Note: Run &#39; &#39;.join(index_word[id] for id in all_x_valid[224]) to print out this same review yourself.] Carrying out this kind of post hoc analysis of our model, one potential shortcoming that surfaces is that our dense classifier is not specialized to detect patterns of multiple tokens occurring in a sequence that might predict film-review sentiment. For example, it might be handy for patterns like the token-pair not-good to be easily detected by the model as predictive of negative sentiment. Convolutional Networks As covered in Chapter 10, convolutional layers are particularly adept at detecting spatial patterns. In this section, we use them to detect spatial patterns among words—like the not-good sequence—and see whether they can improve upon the performance of our dense network at classifying film reviews by their sentiment. All of the code for this ConvNet can be found in our Convolutional Sentiment Classifier notebook. The dependencies for this model are identical to those of our dense sentiment classifier (see Example 11.12), except that it has three new Keras layer types, as provided in Example 11.31. Example 11.31 Additional CNN dependencies from keras.layers import Conv1D, GlobalMaxPooling1D from keras.layers import SpatialDropout1D The hyperparameters for our convolutional sentiment classifier are provided in Example 11.32. Example 11.32 Convolutional sentiment classifier hyperparameters # output directory name: output_dir = &#39;model_output/conv&#39;# training: epochs = 4 batch_size = 128 # vector-space embedding: n_dim = 64 n_unique_words = 5000 max_review_length = 400 pad_type = trunc_type = &#39;pre&#39; drop_embed = 0.2 # new! # convolutional layer architecture: n_conv = 256 # filters, a.k.a. kernels k_conv = 3 # kernel length # dense layer architecture: n_dense = 256 dropout = 0.2 Relative to the hyperparameters from our dense sentiment classifier (see Example 11.13): We have a new, unique directory name (&#39;conv&#39;) for storing model parameters after each epoch of training. Our number of epochs and batch size remain the same. Our vector-space embedding hyperparameters remain the same, except that We quadrupled max_review_length to 400. We did this because, despite the fairly dramatic increase in input volume as well as an increase in our number of hidden layers, our convolutional classifier will still have far fewer parameters relative to our dense sentiment classifier. With drop_embed, we’ll be adding dropout to our embedding layer. Our convolutional sentiment classifier will have two hidden layers after the embedding layer: A convolutional layer with 256 filters (n_conv), each with a single dimension (a length) of 3 (k_conv). When working with two-dimensional images in Chapter 10, our convolutional layers had filters with two dimensions. Natural language—be it written or spoken—has only one dimension associated with it (the dimension of time) and so the convolutional layers used in this chapter will have one-dimensional filters. A dense layer with 256 neurons (n_dense) and dropout of 20 percent. The steps for loading the IMDb data and standardizing the length of the reviews are identical to those in our Dense Sentiment Classifier notebook (see Examples 11.14 and 11.18). The model architecture is of course rather different, and is provided in Example 11.33. Example 11.33 Convolutional sentiment classifier architecture model = Sequential() # vector-space embedding: model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) model.add(SpatialDropout1D(drop_embed)) # convolutional layer: model.add(Conv1D(n_conv, k_conv, activation=&#39;relu&#39;)) # model.add(Conv1D(n_conv, k_conv, activation=&#39;relu&#39;)) model.add(GlobalMaxPooling1D()) # dense layer: model.add(Dense(n_dense, activation=&#39;relu&#39;)) model.add(Dropout(dropout)) # output layer: model.add(Dense(1, activation=&#39;sigmoid&#39;)) Breaking the model down: Our embedding layer is the same as before, except that it now has dropout applied to it. We no longer require Flatten(), because the Conv1D()layer takes in both dimensions of the embedding layer output. We use relu activation within our one-dimensional convolutional layer. The layer has 256 unique filters, each of which is free to specialize in activating when it passes over a particular three-token sequence. The activation map for each of the 256 filters has a length of 398, for a 256×398 output shape. [Note: As described in Chapter 10, when a two-dimensional filter convolves over an image, we lose pixels around the perimeter if we don’t pad the image first. In this natural language model, our one-dimensional convolutional filter has a length of three, so, on the far left of the movie review, it begins centered on the second token and, on the far right, it ends centered on the second-to-last token. Because we didn’t pad the movie reviews at both ends before feeding them into the convolutional layer, we thus lose a token’s worth of information from each end: 400 − 1 − 1 = 398. We’re not upset about this loss. If you fancy it, you’re welcome to add additional convolutional layers, by, for example, uncommenting the second Conv1D() Global max-pooling is common for dimensionality reduction within deep learning NLP models. We use it here to squash the activation map from 256 × 398 to 256 × 1. By applying it, only the magnitude of largest activation for a given convolutional filter is retained by the maximum-calculating operation, and we lose any temporal-position-specific information the filter may have output to its 398-element-long activation map. Because the activations output from the global max-pooling layer are one- dimensional, they can be fed directly into the dense layer, which consists (again) of relu neurons and dropout is applied. The output layer remains the same. The model has a grand total of 435,000 parameters (see Figure 11.22), several hundred thousand fewer than our dense sentiment classifier. Per epoch, this model will nevertheless take longer to train because the convolutional operation is relatively computationally expensive. A critical item to note about this model architecture is that the convolutional filters are not detecting simply triplets of words. Rather, they are detecting triplets of word vectors. Following from our discussion in Chapter 2, contrasting discrete, one-hot word representations with the word-vector representations that gently smear meaning across a high-dimensional space (see Table 2.1), all of the models in this chapter become specialized in associating word meaning with review sentiment—as opposed to merely associating individual words with review sentiment. As an example, if the network learns that the token pair not-good is associated with a negative review, then it should also associate the pair not-great with negative reviews, because good and great have similar meanings (and thus should occupy a similar location in word-vector space). The compile, checkpoint, and model-fitting steps are the same as for our dense sentiment classifier (see Examples 11.20, 11.21, and 11.22, respectively). Model-fitting progress is shown in Figure 11.23. The epoch with the lowest validation loss (0.258) and highest validation accuracy (89.6 percent) was the third epoch. Loading the model parameters from that epoch back in (with the code from Example 11.23 but specifying weights.03.hdf5), we then predict [latex]\hat{y}[/latex] for all validation data (exactly as in Example 11.24). Creating a histogram (Figure 11.24) of these [latex]\hat{y}[/latex] values (with the same code as in Example 11.26), we can see visually that our CNN has a stronger opinion of review sentiment than our dense network did (refer to Figure 11.18): There are about a thousand more reviews with [latex]\hat{y}[/latex] &lt; 0.1 and several thousand more with [latex]\hat{y}[/latex] &gt; 0.9. Calculating ROC AUC (with the code from Example 11.27), we output a very high score of 96.12 percent, indicating that the CNN’s confidence was not misplaced: It is a marked improvement over the already high ~93 percent score of the dense net. Networks Designed for Sequential Data Our ConvNet classifier outperformed our dense net—perhaps in large part because its convolutional layer is adept at learning patterns of words that predict some outcome, such as whether a film review is favorable or negative. The filters within convolutional layers tend to excel at learning short sequences like triplets of words (recall that we set k = 3 in Example 11.32), but a document of natural language like a movie review might contain much longer sequences of words that, when considered all together, would enable the model to accurately predict some outcome. To handle long sequences of data like this, there exists a family of deep learning models called recurrent neural networks (RNNs), which include specialized layer types like long short-term memory units (LSTMs) and gated recurrent units (GRUs). In this section, we cover the essential theory of RNNs and apply several variants of them to our movie-review classification problem. We also introduce attention—an especially sophisticated approach to modeling natural language data that is setting new benchmarks across NLP applications. As mentioned at the start of the chapter, the RNN family, including LSTMs and GRUs, is well suited to handling not only natural language data but also any input data that occur in a one-dimensional sequence. This includes price data (e.g., financial time series, stock prices), sales figures, temperatures, and disease rates (epidemiology). While RNN applications other than NLP are beyond the scope of this textbook, we collate resources for modeling quantitative data over time at jonkrohn.com/resources under the heading Time Series Prediction. Recurrent Neural Networks Consider the following sentences: Jon and Grant are writing a book together. They have really enjoyed writing it. The human mind can track the concepts in the second sentence quite easily. You already know that “they” in the second sentence refers to your authors, and “it” refers to the book we’re writing. Although this task is easy for you, however, it is not so trivial for a neural network. The convolutional sentiment classifier we built in the previous section was able to consider a word only in the context of the two words on either side of it (k_conv = 3, as in Example 11.32). With such a small window of text, that neural network had no capacity to assess what “they” or “it” might be referring to. Our human brains can do it because our thoughts loop around each other, and we revisit earlier ideas in order to in- form our understanding of the current context. In this section we introduce the concept of recurrent neural networks, which set out to do just that: They have loops built into their structure that allow information to persist over time. The high-level structure of a recurrent neural network (RNN) is shown in Figure 11.25. On the left, the purple line indicates the loop that passes information between steps in the network. As in a dense network, where there is a neuron for each input, so too is there a neuron for each input here. We can observe this more easily on the right, where the schematic of the RNN is unpacked. There is a recurrent module for each word in the sentence (only the first four words are shown here for brevity).[Note: This is also why we have to pad shorter sentences during preprocessing: The RNN expects a sequence of a particular length, and so if the sequence is not long enough we add PAD tokens to make up the difference.] However, each module receives an additional input from the previous module, and in doing so the network is able to pass along information from earlier timesteps in the sequence. In the case of Figure 11.25, each word is represented by a distinct timestep in the RNN sequence, so the network might be able to learn that “Jon” and “Grant” were writing the book, thereby associating these terms with the word “they” that occurs later in the sequence. Recurrent neural networks are, computationally, more complex to train than exclusively “feedforward” neural networks like the dense nets and CNNs we’ve used so far in the book. As depicted in Figure 8.6, feedforward networks involve backpropagating cost from the output layer back toward the input layer. If a network includes a recurrent layer (such as SimpleRNN, LSTM, or GRU), then the cost must be backpropagated not only back toward the input layer, but back over the timesteps of the recurrent layer (from later timesteps back toward earlier timesteps), as well. Note that, in the same way that the gradient of learning vanishes as we backpropagate over later hidden layers toward earlier ones (see Figure 8.8), so, too, does the gradient vanish as we backpropagate over later timesteps within a recurrent layer toward earlier ones. Because of this, later timesteps in a sequence have more influence within the model than earlier ones do. [Note: If you suspect that the beginning of your sequences (e.g., the words at the beginning of a movie review) is generally more relevant to the problem you’re solving with your model (sentiment classification) than the end (the words at the end of the review), you can reverse the sequence before passing it as an input into your network. In that way, within your network’s recurrent layers, the beginning of the sequence will be backpropagated over before the end is.] Implementing an RNN in Keras Adding a recurrent layer to a neural network architecture to create an RNN is straightforward in Keras, as we illustrate in our RNN Sentiment Classifier Jupyter notebook [or the complementary Domino project]. For the sake of brevity and readability, please note that the following code cells are identical across all the Jupyter notebooks in this chapter, including the Dense and Convolutional Sentiment Classifier notebooks that we’ve already covered: Loading dependencies (Example 11.12), except that there are often one or two additional dependencies in a given notebook. We’ll note these additions separately—typically when we present the notebook’s neural network architecture. Loading IMDb film review data (Example 11.14). Standardizing review length (Example 11.18). Compiling the model (Example 11.20). Creating the ModelCheckpoint() object and directory (Example 11.21). Fitting the model (Example 11.22). Loading the model parameters from the best epoch (Example 11.23), with the critical exception that the particular epoch we select to load varies depending on which epoch has the lowest validation loss. Predicting [latex]\hat{y}[/latex] for all validation data (Example 11.24). Plotting a histogram of [latex]\hat{y}[/latex] (Example 11.26). Calculating ROC AUC (Example 11.27). The code cells that vary are those in which we: Set hyperparameters Design the neural network architecture The hyperparameters for our RNN are as shown in Example 11.34. Example 11.34 RNN sentiment classifier hyperparameters # output directory name: output_dir = &#39;model_output/rnn&#39; # training: epochs = 16 # way more! batch_size = 128 # vector-space embedding: n_dim = 64 n_unique_words = 10000 max_review_length = 100 # lowered due to vanishing gradient over time pad_type = trunc_type = &#39;pre&#39; drop_embed = 0.2 # RNN layer architecture: n_rnn = 256 drop_rnn = 0.2 Changes relative to our previous sentiment classifier notebooks are: We quadrupled epochs of training to 16 because overfitting didn’t occur in the early epochs. We lowered max_review_length back down to 100, although even this is excessive for a simple RNN. We can backpropagate over about 100 timesteps (i.e., 100 tokens or words in a natural language model) with an LSTM (covered in the next section) before the gradient of learning vanishes completely, but the gradient in a plain old RNN vanishes completely after about 10 timesteps. Thus, max_review_length could probably be lowered to less than 10 before we would notice a reduction in this model’s performance. For all of the RNN-family architectures in this chapter, we experimented with doubling the word-vector vocabulary to 10000 tokens. This seemed to provide improved results for these architectures, although we didn’t test it rigorously. We set n_rnn = 256, so we could say that this recurrent layer has 256 units, or, alternatively, we could say it has 256 cells. In the same way that having 256 convolutional filters enabled our CNN model to specialize in detecting 256 unique triplets of word meaning,43 this setting enables our RNN to detect 256 unique sequences of word meaning that may be relevant to review sentiment. [Note: “Word meaning” here refers to a location in word-vector space] Our RNN model architecture is provided in Example 11.35. Example 11.35 RNN sentiment classifier architecture from keras.layers import SimpleRNN model = Sequential() model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) model.add(SpatialDropout1D(drop_embed)) model.add(SimpleRNN(n_rnn, dropout=drop_rnn)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) In place of a convolutional layer or a dense layer (or both) within the hidden layers of this model, we have a Keras SimpleRNN() layer, which has a dropout argument; as a result, we didn’t need to add dropout in a separate line of code. Unlike putting a dense layer after a convolutional layer, it is relatively uncommon to add a dense layer after a recurrent layer, because it provides little performance advantage. You’re welcome to try it by adding in a Dense() hidden layer anyway. The results of running this model (which are shown in full in our RNN Sentiment Classifier notebook) were not encouraging. We found that the training loss, after going down steadily over the first half-dozen epochs, began to jump around after that. This indicates that the model is struggling to learn patterns even within the training data, which—relative to the validation data—it should be readily able to do. Indeed, all of the models fit so far in this book have had training losses that reliably attenuated epoch over epoch. As the training loss bounced around, so too did the validation loss. We observed the lowest validation loss in the seventh epoch (0.504), which corresponded to a validation accuracy of 77.6 percent and an ROC AUC of 84.9 percent. All three of these metrics are our worst yet for a sentiment classifier model. This is because, as we mentioned earlier in this section, RNNs are only able to backpropagate through ~10 time steps before the gradient diminishes so much that parameter updates become negligibly small. Because of this, simple RNNs are rarely used in practice: More-sophisticated recurrent layer types like LSTMs, which can backpropagate through ~100 time steps, are far more common.[Note: The only situation we could think of where a simple RNN would be practical is one where your sequences only had 10 or fewer consecutive timesteps of information that are relevant to the problem you’re solving with your model. This might be the case with some time series forecasting models or if you only had very short strings of natural language in your dataset.] Long Short-Term Memory Units As stated at the end of the preceding section, simple RNNs are adequate if the space between the relevant information and the context where it’s needed is small (fewer than 10 timesteps); however, if the task requires a broader context (which is often the case in NLP tasks), there is another recurrent layer type that is well suited to it: long short-term memory units, or LSTMs. LSTMs were introduced by Sepp Hochreiter and Jürgen Schmidhuber in 1997,[Note: Hochreiter, S., Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9, 1735–80.] but they are more widely used in NLP deep learning applications today than ever before. The basic structure of an LSTM layer is the same as the simple recurrent layers captured in Figure 11.25. LSTMs receive input from the sequence of data (e.g., a particular token from a natural language document), and they also receive input from the previous time point in the sequence. The difference is that inside each cell in a simple recurrent layer (e.g., SimpleRNN() in Keras), you’ll find a single neural network activation function such as a tanh function, which transforms the RNN cell’s inputs to generate its output. In contrast, the cells of an LSTM layer contain a far more complex structure, as depicted in Figure 11.26. This schematic can appear daunting, and, admittedly, we agree that a full step-by-step breakdown of each component inside of an LSTM cell is unnecessarily detailed for this book. [Note: For a thorough exposition of LSTM cells, we recommend Christopher Olah’s highly visual explainer, which is available at bit.ly/colahLSTM.] That said, there are a few key points that we should nevertheless touch on here. The first is the cell state running across the top of the LSTM cell. Notice that the cell state does not pass through any nonlinear activation functions. In fact, the cell state only undergoes some minor linear transformations, but otherwise it simply passes through from cell to cell. Those two linear transformations (a multiplication and an addition operation) are points where a cell in an LSTM layer can add information to the cell state, information that will be passed onto the next cell in the layer. In either case, there is a sigmoid activation (represented by σ in the figure) before the information is added to the cell state. Because a sigmoid activation produces values between 0 and 1, these sigmoids act as “gates” that decide whether new information (from the current timestep) is added to the cell state or not. The new information at the current timestep is a simple concatenation of the current timestep’s input and the hidden state from the preceding timestep. This concatenation has two chances to be incorporated into the cell state—either linearly or following a nonlin- ear tanh activation—and in either case it’s those sigmoid gates that decide whether the information is combined. After the LSTM has determined what information to add to the cell state, another sigmoid gate decides whether the information from the current input is added to the final cell state, and this results in the output for the current timestep. Notice that, under a different name (“hidden state”), the output is also sent into the next LSTM module (which represents the next timestep in the sequence), where it is combined with the next timestep’s input to begin the whole process again, and that (alongside the hidden state) the final cell state is also sent to the module representing the next timestep. We know this might be a lot to come to grips with. Another way to distill this LSTM content is: The cell state enables information to persist along the length of the sequence, through each timestep in a given LSTM cell. It is the long-term memory of the LSTM. The hidden state is analogous to the recurrent connections in a simple RNN and represents the short-term memory of the LSTM. Each module represents a particular point in the sequence of data (e.g., a particular token from a natural language document). At each timestep, several decisions are made (using those sigmoid gates) about whether the information at that particular timestep in the sequence is relevant to the local (hidden state) and global (cell state) contexts. The first two sigmoid gates determine whether the information from the current timestep is relevant to the global context (the cell state) and how it will be com- bined into that stream. The final sigmoid gate determines whether the information from the current timestep is relevant to the local context (i.e., whether it is added to the hidden state, which doubles as the output for the current timestep). We recommend taking a moment to reconsider Figure 11.26 and see if you can follow how information moves through an LSTM cell. This task should be easier if you keep in mind that the sigmoid gates decide whether information is let through or not. Regardless, the primary take-aways from this section are: Simple RNN cells pass only one type of information (the hidden state) between timesteps and contain only one activation function. LSTM cells are markedly more complex: They pass two types of information between timesteps (hidden state and cell state) and contain five activation functions. Implementing an LSTM with Keras Despite all of their additional computational complexity, as demonstrated within our LSTM Sentiment Classifier notebook, implementing LSTMs with Keras is a breeze. As shown in Example 11.36, we selected the same hyperparameters for our LSTM as we did for our simple RNN, except: We changed the output directory name. We updated variable names to n_lstm and drop_lstm. We reduced the number of epochs of training to 4 because the LSTM begins to overfit to the training data much earlier than the simple RNN. Example 11.36 LSTM sentiment classifier hyperparameters # output directory name: output_dir = &#39;model_output/LSTM&#39; # training: epochs = 4 batch_size = 128 # vector-space embedding: n_dim = 64 n_unique_words = 10000 max_review_length = 100 pad_type = trunc_type = &#39;pre&#39; drop_embed = 0.2 # LSTM layer architecture: n_lstm = 256 drop_lstm = 0.2 Our LSTM model architecture is also the same as our RNN architecture, except that we replaced the SimpleRNN() layer with LSTM(); see Example 11.37. Example 11.37 LSTM sentiment classifier architecture from keras.layers import LSTM model = Sequential() model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) model.add(SpatialDropout1D(drop_embed)) model.add(LSTM(n_lstm, dropout=drop_lstm)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) The results of training the LSTM are provided in full in our LSTM Sentiment Classifier notebook. To summarize, training loss decreased steadily epoch over epoch, suggesting that model-fitting proceeded more conventionally than with our simple RNN. The results are not a slam dunk, however. Despite its relative sophistication, our LSTM per- formed only as well as our baseline dense model. The LSTM’s epoch with the lowest validation loss is the second one (0.349); it had a validation accuracy of 84.8 percent and an ROC AUC of 92.8 percent. Bidirectional LSTMs Bidirectional LSTMs (or Bi-LSTMs, for short) are a clever variation on standard LSTMs. Whereas the latter involve backpropagation in only one direction (typically backward over timesteps, such as from the end of a movie review toward the beginning), bidirectional LSTMs involve backpropagation in both directions (backward and forward over timesteps) across some one-dimensional input. This extra backpropagation doubles computational complexity, but if accuracy is paramount to your application, it is often worth it: Bi-LSTMs are a popular choice in modern NLP applications because their ability to learn patterns both before and after a given token within an input document facilitates high-performing models. Converting our LSTM architecture (Example 11.37) into a Bi-LSTM architecture is painless. We need only wrap our LSTM() layer within the Bidirectional() wrapper, as shown in Example 11.38. Example 11.38 Bidirectional LSTM sentiment classifier architecture from keras.layers import LSTM from keras.layers.wrappers import Bidirectional # new! model = Sequential() model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) model.add(SpatialDropout1D(drop_embed)) model.add(Bidirectional(LSTM(n_lstm, dropout=drop_lstm))) model.add(Dense(1, activation=&#39;sigmoid&#39;)) The straightforward conversion from LSTM to Bi-LSTM yielded substantial performance gains, as the results of model-fitting show (provided in full in our Bi LSTM Sentiment Classifier notebook). The epoch with the lowest validation loss (0.331) was the fourth, which had validation accuracy of 86.0 percent and an ROC AUC of 93.5 per- cent, making it our second-best model so far as it trails behind only our convolutional architecture. Stacked Recurrent Models Stacking multiple RNN-family layers (be they SimpleRNN(), LSTM, or another type) is not quite as straightforward as stacking dense or convolutional layers in Keras—although it certainly isn’t difficult: It requires only specifying an extra argument when the layer is defined. As we’ve discussed, recurrent layers take in an ordered sequence of inputs. The recurrent nature of these layers comes from their processing each timestep in the sequence and passing along a hidden state as an input to the next timestep in the sequence. Upon reaching the final timestep in the sequence, the output of a recurrent layer is the final hidden state. So in order to stack recurrent layers, we use the argument return_sequences=True. This asks the recurrent layer to return the hidden states for each step in the layer’s sequence. The resulting output now has three dimensions, matching the dimensions of the input sequence that was fed into it. The default behavior of a recurrent layer is to pass only the final hidden state to the next layer. This works perfectly well if we’re passing this information to, say, a dense layer. If, however, we’d like the subsequent layer in our network to be another recurrent layer, that subsequent recurrent layer must receive a sequence as its input. Thus, to pass the array of hidden states from across all individual timesteps in the sequence (as opposed to only the single final hidden state value) to this subsequent recurrent layer, we set the optional return_sequences argument to True. [Note: There is also a return_state argument (which, like return_sequences, defaults to False) that asks the network to return the final cell state in addition to the final hidden state. This optional argument is not used as often, but it is useful when we’d like to initialize a recurrent layer’s cell state with that of another layer, as we do in “encoder-decoder” models (introduced in the next section)] To observe this in action, check out the two-layer Bi-LSTM model shown in Example 11.39. (Notice that in this example we still leave the final recurrent layer with its default return_sequences=False so that only the final hidden state of this final recurrent layer is returned for use further downstream in the network.) Example 11.39 Stacked recurrent model architecture from keras.layers import LSTM from keras.layers.wrappers import Bidirectional model = Sequential() model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length)) model.add(SpatialDropout1D(drop_embed)) model.add(Bidirectional(LSTM(n_lstm_1, dropout=drop_lstm, return_sequences=True))) # new! model.add(Bidirectional(LSTM(n_lstm_2, dropout=drop_lstm))) model.add(Dense(1, activation=&#39;sigmoid&#39;)) As you’ve discovered a number of times since Chapter 1 of this book, additional layers within a neural network model can enable it to learn increasingly complex and abstract representations. In this case, the abstraction facilitated by the supplementary Bi-LSTM layer translated to performance gains. The stacked Bi-LSTM outperformed its unstacked cousin by a noteworthy margin, with an ROC AUC of 94.9 percent and validation accuracy of 87.8 percent in its best epoch (the second, with its validation loss of 0.296). The full results are provided in our Stacked Bi LSTM Sentiment Classifier notebook. The performance of our stacked Bi-LSTM architecture, despite being considerably more sophisticated than our convolutional architecture and despite being designed specifically to handle sequential data like natural language, nevertheless lags behind the accuracy of our ConvNet model. Perhaps some hyperparameter experimentation and fine-tuning would yield better results, but ultimately our hypothesis is that because the IMDb film review dataset is so small, our LSTM models don’t have an opportunity to demonstrate their potential. We opine that a much larger natural language dataset would facilitate effective backpropagation over the many timesteps associated with LSTM layers. [Note: If you’d like to test our hypothesis yourself, we provide appropriate sentiment analysis dataset suggestions in Chapter 14.] A relative of the LSTM within the family of RNNs is the gated recurrent unit (GRU). GRUs are slightly less computationally intensive than LSTMs because they involve only three activation functions, and yet their performance often approaches the performance of LSTMs. If a bit more compute isn’t a deal breaker for you, we see little advantage in choosing a GRU over an LSTM. If you’re interested in trying a GRU in Keras anyway, it’s as easy as importing the GRU() layer type and dropping it into a model architecture where you might otherwise place an LSTM() layer. Check out our GRU Sentiment Classifier notebook for a hands-on example. [Note: Cho, K., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv:1406.1078.] Seq2seq and Attention Natural language techniques that involve so-called sequence-to-sequence (seq2seq; pronounced “seek-to-seek”) models take in an input sequence and generate an output sequence as their product. Neural machine translation (NMT) is a quintessential class of seq2seq models, with Google Translate’s machine-translation algorithm serving as an example of NMT being used in a production system. [Note: Google Translate has incorporated NMT since 2016. You can read more about it at bit.ly/translateNMT.] NMTs consist of an encoder-decoder structure, wherein the encoder processes the input sequence and the decoder generates the output sequence. The encoder and decoder are both RNNs, and so during the encoding step there exists a hidden state that is passed between units of the RNN. At the end of the encoding phase, the final hidden state is passed to the decoder; this final state can be referred to as the “context.” In this way, the decoder starts with a context for what is happening in the input sequence. Although this idea is sound in theory, the context is often a bottleneck: It’s difficult for models to handle really long sequences, and so the context loses its punch. Attention was developed to overcome the computational bottleneck associated with context. [Note: Bahdanau, D., et al. (2014). Neural machine translation by jointly learning to align and translate. arXiv:1409.0473]. In a nutshell, instead of passing a single hidden state vector (the final one) from the encoder to the decoder, with attention we pass the full sequence of hidden states to the decoder. Each of these hidden states is associated with a single step in the input sequence, although the decoder might need the context from multiple steps in the input to inform its behavior at any given step during decoding. To achieve this, for each step in the sequence the decoder calculates a score for each of the hidden states from the encoder. Each encoder hidden state is multiplied by the softmax of its score. [Note: Recall from Chapter 6 that the softmax function takes a vector of real numbers and generates a probability distribution with the same number of classes as the input vector.] This serves to amplify the most relevant contexts (they would have high scores, and thus higher softmax probabilities) while muting the ones that aren’t relevant; in essence, attention weights the available contexts for a given timestep. The weighted hidden states are summed, and this new context vector is used to predict the output for each timestep in the decoder sequence. Following this approach, the model selectively reviews what it knows about the input sequence and uses only the relevant information where necessary to inform the output. It’s paying attention to the most relevant elements of the whole sentence! If this book were dedicated solely to NLP, we’d have at least a chapter covering seq2seq and attention. As it stands, we’ll have to leave it to you to further explore these techniques, which are raising the bar of the performance of many NLP applications. Transfer Learning in NLP Machine vision practitioners have for a number of years been helped along by the ready availability of nuanced models that have been pretrained on large, rich datasets. As covered in the “Transfer Learning” section near the end of Chapter 10, casual users can download model architectures with pretrained weights and rapidly scale up their particular vision application to a state-of-the-art model. Well, more recently, such transfer learning has become readily available for NLP, too. [Note:When we introduced Keras Embedding() layers earlier in this chapter, we touched on transfer learning with word vectors. The transfer learning approaches covered in this section—ULMFiT, ELMo, and BERT—are closer in spirit to the transfer learning of machine vision, because (analogous to the hierarchical visual features that are represented by a deep CNN; see Figure 1.17) they allow for the hierarchical representation of the elements of natural language (e.g., subwords, words, and context, as in Figure 2.9). Word vectors, in contrast, have no hierarchy; they capture only the word level of language.] First came ULMFiT (universal language model fine-tuning), wherein tools were described and open-sourced that enabled others to use a lot of what the model learns during pretraining. [Note: Howard, J., and Ruder, S. (2018). Universal language model fine-tuning for text classification. arXiv:1801.06146] In this way, models can be fine-tuned on task-specific data, thus requiring less training time and fewer data to attain high-accuracy results. Shortly thereafter, ELMo (embeddings from language models) was revealed to the world. [Note: Peters, M.E., et al. (2018). Deep contextualized word representations. arXiv:1802.05365.] In this update to the standard word vectors we introduced in this chapter, the word embeddings are dependent not only on the word itself but also on the context in which the word occurs. In place of a fixed word embedding for each word in the dictionary, ELMo looks at each word in the sentence before assigning each word a specific embedding. The ELMo model is pretrained on a very large corpus; if you had to train it yourself, it would likely strain your compute resources, but you can now nevertheless use it as a component in your own NLP models. The final transfer learning development we’ll mention is the release of BERT (bi-directional encoder representations from transformers) from Google. [Note: Devlin, J., et al. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv: 0810.04805.] Perhaps even more so than ULMFiT and ELMo, pretrained BERT models tuned to particular NLP tasks have been associated with the achievement of new state-of-the-art benchmarks across a broad range of applications, while requiring much less training time and fewer data to get there. Non-sequential Architectures: The Keras Functional API To solve a given problem, there are countless ways that the layer types we’ve already covered in this book can be recombined to form deep learning model architectures. For example, see our Conv LSTM Stack Sentiment Classifier notebook, wherein we were extra creative in designing a model that involves a convolutional layer passing its activations into a Bi-LSTM layer.57 Thus far, however, our creativity has been con- strained by our use of the Keras Sequential() model, which requires each layer to flow directly into a following one. Although sequential models constitute the vast majority of deep learning models, there are times when non-sequential architectures—which permit infinite model-design possibilities and are often more complex—could be warranted.58 In such situations, we can take advantage of the Keras functional API, which makes use of the Model class instead of the Sequential models we’ve worked with so far in this book. As an example of a non-sequential architecture, we decided to riff on our highest- performing sentiment classifier, the convolutional model, to see if we could squeeze more juice out of the proverbial lemon. As diagrammed in Figure 11.27, our idea was to have three parallel streams of convolutional layers—each of which takes in word vectors from an Embedding()layer. As in our Convolutional Sentiment Classifier notebook, one of these streams would have a filter length of three tokens. One of the others will have a filter length of two—so it will specialize in learning word-vector pairs that appear to be relevant to classifying a film review as having positive or negative sentiment. The third convolutional stream will have a filter length of four tokens, so it will specialize in detecting relevant quadruplets of word meaning. The hyperparameters for our three-convolutional-stream model are provided in Example 11.40 as well as in our Multi ConvNet Sentiment Classifier Jupyter notebook [or the complementary Domino project]. Example 11.40 Multi-ConvNet sentiment classifier hyperparameters # output directory name: output_dir = &#39;model_output/multiconv&#39; # training: epochs = 4 batch_size = 128 # vector-space embedding: n_dim = 64 n_unique_words = 5000 max_review_length = 400 pad_type = trunc_type = &#39;pre&#39; drop_embed = 0.2 # convolutional layer architecture: n_conv_1 = n_conv_2 = n_conv_3 = 256 k_conv_1 = 3 k_conv_2 = 2 k_conv_3 = 4 # dense layer architecture: n_dense = 256 dropout = 0.2 The novel hyperparameters are associated with the three convolutional layers. All three convolutional layers have 256 filters, but mirroring the diagram in Figure 11.27, the layers form parallel streams—each with a unique filter length (k) that ranges from 2 up to 4. The Keras code for our multi-ConvNet model architecture is provided in Example 11.41. Example 11.41 Multi-ConvNet sentiment classifier architecture from keras.models import Model from keras.layers import Input, concatenate # input layer: input_layer = Input(shape=(max_review_length,), dtype=&#39;int16&#39;, name=&#39;input&#39;) # embedding: embedding_layer = Embedding(n_unique_words, n_dim, name=&#39;embedding&#39;)(input_layer) drop_embed_layer = SpatialDropout1D(drop_embed, name=&#39;drop_embed&#39;)(embedding_layer) # three parallel convolutional streams: conv_1 = Conv1D(n_conv_1, k_conv_1, activation=&#39;relu&#39;, name=&#39;conv_1&#39;)(drop_embed_layer) maxp_1 = GlobalMaxPooling1D(name=&#39;maxp_1&#39;)(conv_1) conv_2 = Conv1D(n_conv_2, k_conv_2, activation=&#39;relu&#39;, name=&#39;conv_2&#39;)(drop_embed_layer) maxp_2 = GlobalMaxPooling1D(name=&#39;maxp_2&#39;)(conv_2) conv_3 = Conv1D(n_conv_3, k_conv_3, activation=&#39;relu&#39;, name=&#39;conv_3&#39;)(drop_embed_layer) maxp_3 = GlobalMaxPooling1D(name=&#39;maxp_3&#39;)(conv_3) # concatenate the activations from the three streams: concat = concatenate([maxp_1, maxp_2, maxp_3]) # dense hidden layers: dense_layer = Dense(n_dense, activation=&#39;relu&#39;, name=&#39;dense&#39;)(concat) drop_dense_layer = Dropout(dropout, name=&#39;drop_dense&#39;)(dense_layer) dense_2 = Dense(int(n_dense/4),activation=&#39;relu&#39;, name=&#39;dense_2&#39;)(drop_dense_layer) dropout_2 = Dropout(dropout, name=&#39;drop_dense_2&#39;)(dense_2) # sigmoid output layer: predictions = Dense(1, activation=&#39;sigmoid&#39;, name=&#39;output&#39;)(dropout_2) # create model: model = Model(input_layer, predictions) This architecture may look a little alarming if you haven’t seen the Keras Model class used before, but as we break it down line-by-line here, it should lose any intimidating aspects it might have: With the Model class, we specify the Input() layer independently, as opposed to specifying it as the shape argument of the first hidden layer. We specified the data type (dtype) explicitly: 16-bit integers (int16) can range up to 32,767, which will accommodate the maximum index of the words we input.59 As with all of the layers in this model, we specify a recognizable name argument so that when we print the model later (using model.summary()) it will be easy to make sense of everything. Every layer is assigned to a unique variable name, such as input_layer, embedding_layer, and conv_2. We will use these variable names to specify the flow of data within our model. The most noteworthy aspect of using the Model class, which will be familiar to developers who have worked with functional programming languages, is the variable name within the second set of parentheses following any layer call. This specifies which layer’s outputs are flowing into a given layer. For example, (input_layer) in the second set of parentheses of the embedding_layer indicates that the output of the input layer flows into the embedding layer. The Embedding() and SpatialDropout1D layers take the same arguments as before in this chapter. The output of the SpatialDropout1D layer (with a variable named drop_embed_layer) is the input to three separate, parallel convolutional layers: conv_1, conv_2, and conv_3. As per Figure 11.27, each of the three convolutional streams includes a Conv1D layer (with a unique k_conv filter length) and a GlobalMaxPooling1D layer. The activations output by the GlobalMaxPooling1D layer of each of the three convolutional streams are concatenated into a single array of activation values by the concatenate() layer, which takes in a list of inputs ([maxp_1, maxp_2, maxp_3]) as its only argument. The concatenated convolutional-stream activations are provided as input to two Dense() hidden layers, each of which has a Dropout() layer associated with it. (The second dense layer has one-quarter as many neurons as the first, as specified by n_dense/4.) The activations output by the sigmoid output neuron ([latex]\hat{y}[/latex] ) are assigned to the variable name predictions. Finally, the Model class ties all of the model’s layers together by taking two arguments: the variable name of the input layer (i.e., input_layer) and the output layer (i.e., predictions). Our elaborate parallel network architecture ultimately provided us with a modest bump in capability to give us the best-performing sentiment classifier in this chapter (see Table 11.6). As detailed in our Multi ConvNet Sentiment Classifier notebook, the lowest validation loss was attained in the second epoch (0.262), and this epoch was associated with a validation accuracy of 89.4 percent and an ROC AUC of 96.2 percent—a tenth of a percent better than our Sequential convolutional model. Summary In this chapter, we discussed methods for preprocessing natural language data, ways to create word vectors from a corpus of natural language, and the procedure for calculating the area under the receiver operating characteristic curve. In the second half of the chapter, we applied this knowledge to experiment with a wide range of deep learning NLP models for classifying film reviews as favorable or negative. Some of these models involved layer types you were familiar with from earlier chapters (i.e., dense and convolutional layers), while later ones involved new layer types from the RNN family (LSTMs and GRUs) and, for the first time in this book, a non-sequential model architecture. A summary of the results of our sentiment-classifier experiments are provided in Table 11.6. We hypothesize that, had our natural language dataset been much larger, the Bi-LSTM architectures might have outperformed the convolutional ones. Domino editorial note: we&#39;ve moved the &#34;footnotes&#34; to be embedded in the narrative to increase online readability. </description>
      <pubDate>24 Mar 20 12:31 EDT</pubDate>
      <guid>https://blog.dominodatalab.com/deep-learning-illustrated-building-natural-language-processing-models/</guid>
    </item>
    <item>
      <title>Null References: The Billion Dollar Mistake</title>
      <link>https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/</link>
      <description>&lt;a href=&#34;https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; InfoQ Homepage Presentations Null References: The Billion Dollar Mistake Summary Tony Hoare introduced Null references in ALGOL W back in 1965 &#34;simply because it was so easy to implement&#34;, says Mr. Hoare. He talks about that decision considering it &#34;my billion-dollar mistake&#34;. Bio Sir Charles Antony Richard Hoare, commonly known as Tony Hoare, is a British computer scientist, probably best known for the development in 1960, at age 26, of Quicksort. He also developed Hoare logic, the formal language Communicating Sequential Processes (CSP), and inspired the Occam programming language. About the conference QCon is a conference that is organized by the community, for the community.The result is a high quality conference experience where a tremendous amount of attention and investment has gone into having the best content on the most important topics presented by the leaders in our community. QCon is designed with the technical depth and enterprise focus of interest to technical team leads, architects, and project managers. INFOQ EVENTS April 4-6, 2022 (In-person, London)Oct 24-28 (In-person, San Francisco)May 10-20, 2022 (Online) Learn how to solve complex software engineering and leadership challenges. Attend in-person at QCon London, (April 4-6) and QCon San Francisco (Oct 24-28) or attend online at QCon Plus (May 10-20). Save your spot now! Key Takeaways Null references have historically been a bad idea Early compilers provided opt-out switches for run-time checks, at the expense of correctness Programming language designers should be responsible for the errors in programs written in that language Customer requests and markets may not ask for what&#39;s good for them; they may need regulation to build the market If the billion dollar mistake was the null pointer, the C gets function is a multi-billion dollar mistake that created the opportunity for malware and viruses to thrive Show notes 00:45 Thesis: historically, null references have been a bad idea. 02:15 Null references were created in 1964 - how much have they cost? Less or more than a billion dollars? 03:20 Whilst we don&#39;t know, the amount is probably in the order of an (American) billion - more than a tenth of a billon, less than ten billion. History of programming languages 03:35 A little on the history of the idea. Tony started as a programmer with Elliot&#39;s [Ed: Elliot Brothers, London Ltd] in 1960, and was asked to design a new programming language. 04:10 In the library was a 23-page booklet entitled &#34;Report on the international language ALGOL60&#34;;, edited by Peter Naur. 04:30 Used as a basis for the new language, but left out the complicated parts such as &#34;if&#34;; and &#34;then&#34;;. 05:00 Most software was still written in machine code (including the complier). 05:25 Most assembly was simple enough to understand that when it went wrong, it could be diagnosed by following through to find out what the fault was. Towards a high level language 05:40 Using a high level language meant you couldn&#39;t step through the machine code. 05:50 The Elliot&#39;s machine had 4096 locations, with a length of 4 7/8 bytes long (39 bits), although other machines had different sizes (IBM&#39;s had 36 bits. 06:30 To shield customers from implementation details, customers were told the errors in terms of the high level programming language, instead of a hexadecimal core dump. 07:10 In order to implement error messages, an array had a check to verify whether its reference was in the bounds. 08:00 Adding checks to arrays added space and time to the program; on Tony&#39;s first machine it ran at less than 2k operations per second (500 micro seconds per operation, and two such tests for each array bounds). 08:40 No undetected array errors, and customers didn&#39;t know they could trade off safety for speed. 09:30 The Java language has, after 30 years, decided to replicate the decision to bounds checking arrays. [Ed: other languages, like Python, handle this as well]. Record oriented programming 10:20 Introduced the concept of an object, which could be referred to with a pointer. 10:30 With pointers, it is possible to wreak havoc with the program you are trying to test [Ed: this is the single biggest cause of security failures in modern day code]. 10:55 If a floating point value or integer is used as a pointer accidentally, and the value it is pointing to is updated, then it will just as likely update the program which may then crash or cause problems now or in the future. [Ed: these days, virtual memory and page mapping takes away some of the problems about editing program code, but these weren&#39;t present in the computers of that era.] 12:00 As a given, when invoking a function with a pointer required the type of the pointer to be declared. 13:30 The type of the program can be compile time checked from the static types. 13:45 Many years later Tony discovered that some of these ideas had been integrated for the first time, although previous examples came from both Doug Rossier&#39;s Plex and Simula. Records avoid subscript errors 14:35 The great thing about record handling is that you don&#39;t need to have a subscript error, because you cannot construct a pointer that points to something that doesn&#39;t exist, and a whole set of errors cannot occur and do not need to be checked at run-time. 15:50 Later, we asked the customers whether they wanted the option to be able turn off the type checking in production. It&#39;s a bit like wearing a life jacket when you are practicing drills, but then taking it off the ship was sinking. The customers decided to not switch off the type checking. 17:00 We produced a compiler that would translate Fortran programs to Algol programs. It was a disaster, and no Fortran user would use it. 18:00 The reason that they couldn&#39;t use it was because they couldn&#39;t use any of their programs. Within a few milliseconds of running it would come up with a subscript error. The error wasn&#39;t wanted as they just wanted the code to run. Type checking as standard 19:00 Things have changed a bit - mainstream programming languages like Java now have subscript checking as standard, type-checked object oriented programming. 19:30 And then I went and invented the null pointer. You either have to check every reference, or you risk disaster. 19:45 Fortran programmers preferred to risk disaster; in fact, experience disaster, rather than check subscripts. 20:00 I didn&#39;t know it a the time, but my friend Edsger Dijkstra thought the null reference was a bad idea. He said: 20:20 &#34;If you have a null reference, then every bachelor who you represent in your object structure will seem to be married polyamocursly to the same person Null&#34;. 20:55 It brings back the same question whether you want to run your code quickly (without checks) or safely (with checks). Disjoint unions and discrimination test 21:10 I did know there was a solution based on the idea of discrimination of objects belong to a disjoint union class; that is, two sets in which there are no members in common. For example a Vehicle class that has subtypes Car and Bus; the Car may have a luggage carrying capacity property while the Bus has a person carrying capacity. You would then have a discrimination test and do different operations based on whether it was a Bus or a Car. 23:40 The size of the program grows with the number of discrimination clauses and number of types. This allows null to be represented as a different class, which can then be passed in to functions. 24:30 The types of the pointer could then be implemented as a union of either a pointer to the null type, or a pointer to the type. 25:20 This leads to implementation problems; what happens if you assume that a pointer is a Bus but change that pointer to a Car instead? 25:55 One of the things you want is to be able to know in a high level language is that when it is created, all of its data structure is initialised. In this case, a null reference can be used to indicate that the data is missing or not known at this time. In fact, it&#39;s the only thing that can be assigned if you have a pointer to a particular type. 26:35 If you don&#39;t want to use null, you have to implement a sublanguage for representing how to initialise objects of the right type. If the data structure is a tree-based representation, this is achievable if you create the leaves first because they can be fully created. 27:10 It isn&#39;t possible to create a cyclic structure using this technique; if there&#39;s a cycle in the data structure you can start with a null pointer and then assign it once the rest of the cycle has been completed. Introducing null 27:40 This led me to suggest that the null value is a member of every type, and a null check is required on every use of that reference variable, and it may be perhaps a billion dollar mistake. 28:00 Modern languages such as C# or Spec# and even Java are introducing the idea of non-null reference parameters, and compile time checking which verifies that they cannot possibly have null values. 28:50 The issues of overloading and inheritance make it a lot more difficult to do these when null references were originally created. 29:20 The movement must have been made based on the fact that null references were an expensive mistake. Programming languages should be responsible for their users 30:20 A programming language designer should be responsible for the mistakes made by programmers using the language. It is a serious activity; not one that should be given to programmers with 9 months experience with assembly; they should have a strong scientific basis, a good deal of ingenuity and invention and control of detail, and a clear objective that the programs written by people using the language would be correct. free of obvious errors and free of syntactical traps. 31:40 This was the idea that led me to the idea of using proof and formal verification of programs as logical and mathematical models, is a method of conducting research into the design of good programming languages. I wasn&#39;t too optimistic in 1969 would actually be using proofs to guarantee correctness of programs. 32:20 By looking at the programming language and whether programs written would be possible to prove the programs written in the language gives an objective measure of how easy it would be to verify the program later. If the understanding of applying a rule locally has to depend on global knowledge of the program then you haven&#39;t done a good job in creating the programming language, and you don&#39;t need your customers to tell you that. 33:30 In fact customers don&#39;t tell you - it&#39;s very easy to persuade your customers that anything that goes wrong is their fault rather than yours. 33:40 I rejected that - programming language design is a serious scientific engineering activity, and we should begin to take responsibility for the mistakes that our users make. Designing for safety 33:55 It&#39;s beginning to happen again - the Java programming language and its successors have all used avoidance of error as one of the criteria in the detail ed design of new features of the language, and I&#39;m delighted to give them a great deal of credit for that - but it is only one criteria, and it is only one. 34:35 The most important criteria is backwards compatibility of everything that has gone before, with the millions or billions lines of code that have been written. 34:55 Every commercial language has to make concessions for commercial and historical reasons; but gradually, ideas change, programmers get more interested in provable correctness; production techniques, languages, checkers, analytic tools, test case generators and so on that are going to help them get their programs correct. Safe at any speed? 35:40 The analogy that I draw is with agricultural pollution and vehicle security. When Ralph Nader first started publishing &#34;Unsafe at any speed&#34;, what he was saying had no connection with the marketplace - customers were not asking for reliability or safety as one of their vehicles. 36:20 But gradually, customers started to demand reliability and safety, with the aid of law making and legal constraints requiring basic levels of safety to be included in every vehicle sold. 36:50 There is a possibility that the marketplace will move the reliability of programs and the language in which they&amp;&#39;re expressed. 37:15 For many professional engineers, they do have ideals and do pursue them in preference to not pursuing them whenever the opportunity arises. The commercial imperative that requires greater attention paid to the formal correctness of the programs is the virus. 37:50 The virus (or malware, or worm) does dreadful things by reaching the parts of the program that it doesn&#39;t usually reach. It is no longer applicable to test the cases that are likely to arise, the virus will attack the places that are not likely to arise, and so need just the same level of testing. 38:35 It forces you to get the while program correct, not just the ones that will be used by customers, the code that will be used by viruses needs to be checked too. 38:45 And that can&#39;t be done by testing, it has to be done by analysis. 38:55 Analysis of the source code, type-checking techniques are the simplest, but more sophisticated reasoning techniques are being used to high volume code to check that it doesn&#39;t contain any naughty things like null reference dereferencing. Introduction of the virus 39:30 So if I am responsible for a billion dollar mistake; and I bring it up because other designers are much more responsible. 39:40 The designers of C - one can definitely quantify. The buffer overflow is a direct result of the C language gets fnction that doesn&#39;t check the bounds of the string input. That allowed the early viruses to get in by overwriting the return values of the code. 40:10 These simple viruses taught the world how to write malware. Without this very simple entry point, it is quite possible that nobody would ever have thought to look for the more subtle kind of thing which are now being exploited every day by people who are now motivated, skilled, and whose profession and income it is to write botware, malware. 40:45 If it hadn&#39;t been for the gets routine in C, we might have had no malware. 40:55 Now one virus - the CodeRed virus - was estimated to have cost the world economy 4 billion dollars, because it brought down all the networks, and the interruption to business and all the ordinary banking, other business was estimated to cost that amount. There was another one later as well. 41:30 And that was more than the Millennium bug, which was estimated a little less than 4 billion dollars. Companies mentioned Elliot Brothers (London) Ltd People mentioned Peter Naur Doug Rossier Edsger Dijkstra Ralph Nader Languages mentioned Algol60 Occam Plex Simula Fortran C# Spec# C Products mentioned ACM Turing Award speech See more presentations with show notes Recorded at: Aug 25, 2009 </description>
      <pubDate>26 Jan 21 15:36 EST</pubDate>
      <guid>https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/</guid>
    </item>
    <item>
      <title></title>
      <link>http://www.cs.jhu.edu/~jason/advice/</link>
      <description>&lt;a href=&#34;http://www.cs.jhu.edu/~jason/advice/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Advice for Research Students Over the years, I&#39;ve written a lot of advice for students and others. Feel free to link to these pages. For prospective graduate students For undergrads who want to get involved in research How to ask for a recommendation letter How should I prepare myself for grad school? Should I go straight to grad school? How to think about grad school interviews   How to ask a question in class How to read a paper How to present a paper in reading group How far to go with double-blind review How to be a teaching assistant   How to organize your files How to evaluate an advisor How to deal with an unresponsive or angry advisor How to find research problems   Write the paper first (as an early step in the research) How to cite your own work without breaking anonymity How to prepare a talk How to meet with your dissertation committee How to write up a Ph.D. dissertation In defense of footnotes How to write an academic research statement (when applying for a faculty job) How to serve as program chair of a conference How to set up publication practices for a professional society: conferences/journals (2009, 2010), preprint policies (report 2017)   How to use the CLSP massage chair I&#39;ve also been answering questions on Quora lately. See also my technical tutorials. A couple of writings that I like to recommend are Michael Nielsen&#39;s essay Principles of Effective Research and Phil Agre&#39;s opus Netwokring on the Network. This give a high-level perspective on how to build a research career: Nielsen writes about cultivating your own thinking, and Agre writes about cultivating your relationships with other researchers. JHU students can also find a outdated collection of pointers to good advice (mostly compiled by me) in the CLSP FAQ, and a bigger collection here compiled by Tao Xie. And why not Google for career advice computer science graduate students? Use your undergraduate education: &#34;But you go to a great school, not for knowledge so much as for arts and habits; for the habit of attention, for the art of expression, for the art of assuming at a moment&#39;s notice a new intellectual posture, for the art of entering quickly into another person&#39;s thoughts, for the habit of submitting to censure and refutation, for the art of indicating assent or dissent in graduated terms, for the habit of regarding minute points of accuracy, for the habit of working out what is possible in a given time, for taste, for discrimination, for mental courage and mental soberness.&#34; -William Johnson Cory (1861) This page online: http://cs.jhu.edu/~jason/advice </description>
      <pubDate>04 Apr 20 10:15 EDT</pubDate>
      <guid>http://www.cs.jhu.edu/~jason/advice/</guid>
    </item>
    <item>
      <title></title>
      <link>https://console.dev/qa/rsync-john-kozubik/</link>
      <description>&lt;a href=&#34;https://console.dev/qa/rsync-john-kozubik/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Interview with John Kozubik CEO, rsync.net - Cloud storage in the form of a UNIX filesystem available over SSH. What is rsync.net? rsync.net is a cloud storage provider focused on offsite backup. I like to think of it as a safe-deposit box for data. What makes rsync.net special is that there is no app or API - we simply give you an empty UNIX filesystem accessible with any SSH tool. What problem does rsync.net solve? Originally - in 2005 - the problem was simply “cloud storage” and the ability to make offsite backups easy for people that didn’t have their own colocation or datacenter assets. Now, of course, there are lots of cloud storage providers - the problem we solve in 2021 is interfacing with cloud storage directly on the UNIX command line using UNIX tools and practices. Sometimes people just want to rsync or sftp a file somewhere without installing new tools. At the same time, we now support a number of very exciting tools (like borg, restic, rclone, git-annex, etc.) that add a lot of power to the UNIX command line environment. rsync.net is not built on a cloud platform - we own (and have built) all of our own platform. Our storage arrays, as we call them, are 2U “head units” with x16 2.5&#34; drive slots and one or more SAS HBAs. ZFS makes good use of fast cache storage so, after using up two drive slots for our boot mirror (which is always a mix of two totally different SSDs) we have room for up to 14 more SSDs for read (L2ARC) and write (SLOG) cache. ZFS is also RAM hungry so we choose a motherboard that can support up to 2TB of RAM. Attached to these head units, externally, are one or more JBOD chassis filled with hard drives. JBOD stands for “just a bunch of disks” and they are typically 4U chassis containing 45 or 60 drives that attach back to the head unit with SAS cables. rsync.net has no firewalls and no routers. In each location we connect to our IP provider with a dumb, unmanaged switch. This might seem odd, but consider: if an rsync.net storage array is a FreeBSD system running only OpenSSH, what would the firewall be ? It would be another FreeBSD system with only port 22 open. That would introduce more failure modes, fragility and complexity without gaining any security. What does a “day in the life” look like? I start the day with a short walk outdoors. I don’t want the first thing my eyes see to be print, and I don’t want the first thing my body does to be sitting. So I walk a bit. Then I sit down and read hackernews with some coffee. I try to maintain an “Hedonic Fast” Monday through Wednesday so, on those days, I am only looking for truly actionable headlines and comment threads that are relevant to my businesses. On other days I read (and comment) for pleasure - typically HN but also MetaFilter, Marginal Revolution, Crooked Timber, The Believer, and the excellent London Review of Books. I spend my work day in a terminal window and a browser window. The terminal window is typically open to email which I read with Alpine and the browser window is open to our home-grown CRM/management website. I do not typically handle technical support but I do answer the “info” mailbox where pre-sales and other informational requests end up. I usually spend about two hours with these tools before lunch - checking Key Performance Indicators (both corporate and computational) in our management system and having email conversations with potential and existing customers. At some point in the morning I will have a phonecall with my lead engineer, Dave Boodman, and we’ll have a very high bandwidth discussion about all aspects of operations. Dave then delegates the results of these conversations to everyone on our team at all of our locations. Finally, if I am lucky and all of the housekeeping and firefighting can be taken care of, I spend the afternoon working on longer term development projects and strategic initiatives, etc. What is the team structure around rsync.net? rsync.net has a handful of full-time employees - most notably myself, the CEO, and my lead engineer who I have worked with, across three startups, since 1998. In addition, we have contract employees that we regularly utilize in our different datacenter locations as well as a handful of contract coders around the world. How did you first get into software development? I got started in software by copying BASIC programs from magazines onto my commodore 64. Although I have written software, sporadically, throughout my career my life’s work is as a systems designer or systems engineer. If we’re not using fancy titles I am really just a UNIX sysadmin. I got started with UNIX in 1992 when my father loaded ESIX onto his 386 from 50 floppy disks. The next year I got a dial-up login to the umn.edu SunOS system(s) and promptly got myself banned from irc. At some point along the way I settled on FreeBSD as server OS of choice and have built every one of my startups entirely on that platform - including rsync.net which runs solely on FreeBSD. What is the most interesting development challenge you’ve faced working on rsync.net? When rsync.net began, in 2001, we were using UFS2 filesystems on top of hardware RAID arrays built from 80 and 120 GB hard drives. We ran this way using RAID6 arrays on 3ware controllers for many years and it was a very simple and elegant platform. In the past it has always pleased me, from the standpoint of simplicity, that the RAID controller presents a single device to the OS and the OS thinks it just has one big giant disk. Problems began to arise as early as 2008, however, as we began to push the boundaries of UFS2 - both in terms of filesystem size and number of inodes (files). Not only did we start to see prohibitively long fsck times but we started to run out of memory address space that fsck could track inodes with. At some point in 2011, while working with Kirk McKusick, the author of UFS, I wondered out loud what the real, long-term solution was to the problems we were having as we pushed UFS2 well beyond its design … and he simply said “use ZFS”. So we embarked on a two year testing and development project to stress test and deploy ZFS which culminated in our first production ZFS systems in 2013. It took another four years before we migrated the final UFS2 storage array to ZFS. This was a relatively large undertaking for us and was, in essence, a complete reworking of our cloud storage platform. It was well worth it as ZFS, as McKusick predicted, solved all of our problems. Further, it enabled new use-cases for us as the point-in-time snapshots that ZFS can create and maintain are now a critical feature for our users. What is the most interesting tech you are playing around with at the moment? As a hobby I have been slowly building my own personal phone company with Twilio as the back-end. I have nostalgia for telephony and phone phreaking and it pleases me greatly to send SMS messages, from my mobile number, from the command line (among other things). In fact, I have a small suite of little shell scripts that do telephony tasks and do almost all of my texting from the UNIX command line. Describe your computer hardware setup I have a early-2009 “octo” Mac Pro that I bought new 12 years ago. Other than adding a USB3 card it is in its original configuration and it feels smooth and performant. Even when I load VMWare Fusion I don’t notice that I am on an old machine … I have four monitors attached to this workstation - three of which are on my desktop in a familiar triple-monitor configuration and one of which is an NEC P461 commercial display in the next room. You’ve seen these monitors - they are used for arrival/departure boards in airports and they are “dumb” flat panels that last forever. It pleases me to drag a window off the edge of my desktop display and into the next room … Describe your computer software setup OS: macOS. I have no data stored, locally, on my workstation - everything is on a server I keep at a datacenter so I initiate my work in the terminal by port-knocking, logging in over SSH and attaching to my GNU screen session. When I login, one of those Twilio scripts sends an SMS alert notifying me of that fact. I believe very strongly in the superiority of tiling / non-overlapping window management and I accomplish this in macOS with Spectacle although I believe that is no longer maintained and there are better tools for that now. I load a utility called “ykeys” that allows me to set custom hotkeys for volume and pause/unpause, etc. I dislike interacting with my iPhone on an import/export basis and I refuse to use an mp3 player as my backup tool so I have purchased a license for Macroplant iExplorer which lets me browse the iPhone like a filesystem. Finally, I use Rich Somerfields TextBar app to put a few, scriptable, pieces of information into my macOS menu bar. Specifically, I like to see what wireless AP (SSID) I am connected to and what IP address I appear to be browsing from. I have specific expectations about those two and I want to see, immediately, if they are different than I expect them to be. When I bought my current workstation, 12 years ago, I felt that it was indeed a UNIX-like system. I do a lot of basic use and maintenance on the command line of macOS using commands like ‘airport’ and ‘drutil’ and ‘diskutil’, etc. I’m also quite happy with Homebrew for package management. However, I can see the shell environment becoming more and more restricted as macOS continues to lock down actions and disallows even root from controlling things. I suspect I am going to need to go back to FreeBSD as a desktop environment and that should be relatively easy for a workstation that does not use wireless networking and never prints, etc. It’s going to be a lot harder to make that switch with my laptop … Browser: Firefox. I have exactly one extension: uBlock Origin. Email: My email client is running on the remote system and I interact with email using Alpine in the terminal. This is very interesting because it means that when I send an email to another person at rsync.net, the email never traverses any network - it is just a local copy operation because they, also, use alpine. Chat: I used to always (as in, for 25 years) have an IRC client running in one of my screen windows but … that platform, at least among my friends, has died off. IDE: Inasmuch as I have an IDE it is vim with some very minor tab/folding customizations. I also use an interesting vi-related tool named ‘vimv’ which, if run in a directory, lets you manipulate the filenames in that directory in ‘vi’ as if you were editing a text file. If you have a number of hard-to-script files to rename, ‘vimv’ makes it easy. Why connect remotely rather than having a local dev environment? It’s a workflow I’ve been using since 1999 when I put my own machine into a datacenter for the first time. A text only workflow doesn’t suffer (much) from working over a WAN link and it pleases me greatly to know that there is nothing of value on my laptop or workstation - if they were lost or stolen it would be temporarily inconvenient but not disastrous. Describe your desk setup I work at a standing desk that is a slab of old bowling alley. The desk is height adjustable using Linak actuators but I very seldom lower it. If I don’t want to stand I will just lie on the couch with a laptop. When coding Daytime or nighttime? In terms of motivation and aesthetic preference I would much rather do actual development work late at night - perhaps 10pm to 2am. Historically, my best and most valuable work was done at that time. However, I have three children and many responsibilities at home so that time slot no longer exists for me. As mentioned above, if I have time for real development it is in the afternoon after all of the housekeeping and firefighting are over. Tea or coffee? Coffee. We have such tremendous, wonderful coffee in the bay area and I appreciate every bit of it. I especially like Equator Coffee which is based here in Marin County. Silence or music? It depends. If I need to concentrate and think through complex issues I prefer silence. If I am doing housekeeping activities or mindless, rote work, I like to listen to music. I find the soundtrack and film scoring that Trent Reznor has done in recent years (Social Network, Girl with Dragon Tattoo, Watchmen) to be very good. What non-tech activities do you like to do? I am a BJJ player and have been doing that on and off for 20 years. I also have a Kung Fu / Tai Chi practice and do a lot of running and biking here in Marin County where I live. I’m also a volunteer firefighter/medic with my small town fire department. Find out more rsync.net is cloud storage in the form of a UNIX filesystem available over SSH. It was featured as an “Interesting Tool” in the Console newsletter on 28 Jan 2021. This interview was conducted on 12 Mar 2021. Subscribe to the weekly Console newsletter An email digest of the best tools and beta releases for developers. Every Thursday. See the latest email. </description>
      <pubDate>18 Mar 21 16:37 EDT</pubDate>
      <guid>https://console.dev/qa/rsync-john-kozubik/</guid>
    </item>
    <item>
      <title>Reflections on Successful Meetings with Undergraduate Researchers</title>
      <link>https://medium.com/@jurgens_24580/reflections-on-strategies-for-successful-meetings-with-undergraduate-researchers-ae22306ecd8d</link>
      <description>&lt;a href=&#34;https://medium.com/@jurgens_24580/reflections-on-strategies-for-successful-meetings-with-undergraduate-researchers-ae22306ecd8d&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Or, how to have a good research meeting with your advisorThis summer I’ve had the good fortune to work with a large group of 13 new undergraduate researchers in my lab. They are talented Michigan students who want to learn how to do world-class research but are just getting started along the path. One aspect of learning to do good research is learning how to do research with others — and in particular, when starting out, how to do research with an advisor. I wanted to touch on one small part of this process: meetings with an advisor. Meetings are a critical part of the knowledge transfer process but not all of them are useful, let alone productive. What makes a good meeting and how should a new undergraduate researcher think about meetings?¹ Further, what kind of behavior should an advisor encourage to help their students grow as researchers?My goal with this post is to help students recognize how to maximize the effectiveness of their meetings and, overall, my goal as an advisor is always to help students turn into the best researchers they can be. To talk about meetings, I’ll contrast two hypothetical examples of students: One, I’ll call the Pine student (Michigan’s state tree²), and the other the Sapling student. Both students start from the ground level and are often learning their computational craft at the same time as learning how to do good research. The Pine students have made the most of their opportunities and grown in their abilities and flourish. In contrast, the Sapling students certainly put in effort but haven’t seen much growth or much success — yet. To illustrate how these hypothetical students differ in their behavior, I’ll describe several general scenarios for meetings that can help lead one student to grow substantially as a researcher into a towering pine, whereas the other remains just a sapling.Look at all those aspiring young researchers — so much potential! (Image credit Dleduc, shared under a Creative Commons Attribution 3.0 Unported license)For context, my lab is structured so that undergraduate students typically work in groups. We use a few different tools for communicating about projects. Project meetings happen on an at-least weekly basis and provide low-level guidance on the research, including goal setting, staring at figures, and discussing ideas. Project meetings are scheduled by the students themselves in one or more 30 minute blocks using an appointment calendar; there’s generally enough time for two 30 minute meetings for each group spaced throughout the week. Weekly all-hands lab meetings provide more dissemination and allow students to share details and highlights from that week’s work. Between meetings, we use Slack to communicate for quick updates, questions, and showing off upcoming results to discuss at the next meeting. For some projects, we have also adopted Asana for managing project to-dos and keeping organized on what’s coming down the project pipeline. I also keep my door open for the occasional drop-by question or chat.In different scenarios below, I outline hypothetical situations of how the Pine and the Sapling might differ in their meeting experiences. Before reading these, I want to be clear that I believe any Sapling student can become a Pine student. These are not permanent distinctions and in fact many of us (including me!) had these Sapling habits at one point. If you, as a student, recognize some part of yourself in a scenario, despair not! Recognizing a bad habit is an important step towards progress and improving that aspect. No student begins their research experience as an amazing PhD-level researcher; we all start somewhere close to ground level and work our way up from there. As an additional aside to the undergraduates in my lab (now and in the future), I am proud to have you as members and want to help you succeed; these scenarios are intended as guides to help you avoid potential pitfalls and shoot up into tall Pines as quickly as possible.The undergraduate researcher sits down to discuss research with their advisor (Image credit: Steven Severinghaus, shared under a CC BY-NC-SA 2.0 license)Scenario: a student has just arrived to a meeting they scheduled to discuss their project.The Sapling student will often arrive and ask what should we talk about or have nothing in particular to discuss. Sometimes this can be due to shyness (and not to being a Sapling student), so I may ask students to start by bringing me up to speed. In this case, a Sapling student may not have thought of how to summarize their progress and have trouble responding without more prompting. The meeting proceeds and we discuss as much as we can in the allotted time, which often runs out before we can wrap up.The Pine student arrives with an agenda of all the points they want to discuss. Often this list is organized around different topics and prioritized so that if we run out of time, the most pressing issues are addressed first. We go through each with a full understanding of how much we have to talk about for the rest of the meeting.Reflection: Having students create the meeting agenda provides an important sense of agency that they are driving the project and also leads to more efficient meetings. The Pine student’s effort in drafting an agenda is doubly worthwhile because it allows them to think big-picture about what they did and ask themselves which items need to be discussed first. When the agenda is created between students within the group, their discussions often help create a mental model of the project’s priorities that lead to more effective collaboration.The Sapling student’s meeting is more reactive to the advisor’s comments and generally leads to less productive meetings where the advisor drives everything or must use the socratic method to discover what has been done. The Sapling student’s meeting could turn into the Pine student’s meeting but it depends on the advisor knowing the agenda already and being able to quickly prioritize things. In projects where I’m very actively involved, I can typically do this, but only from the prospective of what I think has been accomplished. The new researcher is much closer to the day-to-day research and may have noticed or experience things that need to be on the agenda (unusual results, important questions, or new papers) that I won’t necessarily know about yet.One side-effect of the Sapling meeting style is meeting spill-over. I eventually have to cut the meeting off to start the meeting after, which is often not at an ideal point for deciding on next steps and leaves the student potentially hurt that I couldn’t spend more time. I don’t like cutting off these meetings but I also don’t want to shortchange the next students who are coming in.For some projects, I’ve borrowed the strategy from my colleague Libby Hemphill and use Asana to drive the meetings. The project is laid out in Asana using an ever-growing number of current tasks and backlog of tasks that will need to be done. Students are assigned (or self-assign) tasks for that week and each meeting’s agenda is determined by what’s currently assigned in Asana. I find this strategy has worked well for some teams, though it does come with the overhead of maintaining a task list and actually getting everyone involved to use Asana (Slack integration with Asana has helped this though).Finally, sometimes it can be nice to have the occasional informal meeting where there is no set agenda. These meetings might be more brainstorming or have other kinds of discussions not focused on research. However, if I’m collaborating with a new student, having a focused meeting is a much better use of both of our time to help give them the feedback and direction they need to grow into a Pine student.Suggestions on how to become a Pine:Come to a meeting with a prioritized agenda of what you want to discuss. This agenda can even be sent beforehand to everyone involved.If you’re in a team, collaboratively come up with the agenda based on what each person is doing.Be aware of meeting time and ensure everything you want to discuss gets brought up before the allotted time runs out.The last reported sighting of an undergraduate researcher who is thought to be still debugging their code in parts unknownScenario: the student has been working throughout the week but doesn’t feel like they have enough to discuss and hasn’t yet scheduled a weekly meeting with me.There are many reasons not to have scheduled a meeting, but sometimes I have seen Sapling students put off meetings out of a sense of shame for not doing enough or being successful enough. Occasionally, a Sapling student may also feel that weekly meetings are not a priority and that it is acceptable to slip to a later week.The Pine student messages me (on Slack or email) saying that they have been working but currently don’t think they have enough to discuss and would like to know if a meeting will be useful. Further, the Pine students adds what has been blocking them from making progress (e.g., server has crashed; GPUs have been in use) and what steps they have proactively taken to resolve them. Based on the situation, we may still meet to discuss these issues or postpone with the knowledge that things are being done.Reflection: Communication is the key here, along with an important acknowledgment that there is no shame in having your work not turn out like you thought — the process of research is never certain! Even when students don’t have results, we generally meet to discuss the process of research and how things might have been done differently (without blame) in light of what we learned from their effort. I sometimes find that a Sapling student has been working very hard on a problem that is difficult but either not central to our project’s goals or one that can easily be solved with a bit more experience to get them unblocked. Having a good communication channel can help such Sapling students reprioritize their effort and make progress again.Suggestions on how to become a Pine:Be open and honest with your advisor about your progress and raise attention for things blocking your progress.Meet weekly. If you find yourself without much to talk about, discuss why that is the case and aim to resolve it.Finding themself in unfamiliar territory, the undergraduate researcher has many new and exciting research questions — but realizes they have quite a bit of explaining to describe how they got there in the first place (Image credit Paul Farmer, shared under a CC BY-SA 2.0 license)Scenario: The meeting has just begun and the student is presenting the first results of the project.The Sapling student often starts with deep technical details with minimal context, e.g., “So I was trying to run the sklearn code on the data but when I set this parameter to 3, the performance improved.” The next several minutes of the meeting are spent unpacking such statements to determine why they were doing something and what effect it has on the project.The Pine student arrives and quickly summarizes the goals of the project as a whole and what research question they were trying to pursue within those goals, e.g., “we are trying to understand people’s reactions to memes on Twitter and this week I built a sentiment analysis classifier with sklearn to rate reactions.” This framing immediately sets expectations and helps me ensure that the student’s model of the project’s objectives are aligned with the project’s actual objectives.Reflection: Advisors are busy and often one research meetings is immediately followed by another on a completely different topic. As a result, any effort the student can put in to helping quickly familiarize the advisor with the project is incredibly useful. The Sapling student isn’t wrong for going into details; this kind of discussion can be incredibly useful. However, the issue is immediately starting out in the weeds without context. Starting by re-iterating the problem allows the student and advisor to quickly get on the same page and make it through more of the meeting by not having to ask clarifying questions.Suggestions on how to become a Pine:Start any discussion of an agenda item with what problem you’re trying to solve. Describe the problem from a high level that everyone understands and then become more specific until you reach the details of what you’re doing.If you need to spend all of the meeting on something very detailed, send an email the day before with the context and agenda so that the advisor is prepared to jump right into the details.“Huh, I could have sworn there was research paydirt around here just a second ago,” exclaimed one undergraduate researcher to another upon surveying their surroundings.Scenario: during the meeting, the student and advisor begin to look at the intermediate data produced over the previous few days.The Sapling student will pull up data they have just generated and possibly never looked at before. Often, there will be surprises in the data, e.g., blank entries, artifacts from encoding, or unexpected tab/newline insertions. The Sapling student will struggle to explain how these surprises came to be and pinpoint where they might come from during preprocessing. Often, results have been produced from this intermediate data (e.g., a classifier is trained on this data) without having looked at it, which raises additional questions about the validity of those results.The Pine student will have generated the data before the meeting with enough time to go over the data themselves ask whether anything is useful. If they spotted something unusual, the Pine student corrects the error (or finds an explanation) and regenerates the results. The Pine student can typically point to which programs, scripts, and methods are responsible for producing the data, and can fully replicate the setup they used to produce the data if we want to try something on hand. In the best cases, the Pine student has made things like Jupyter notebooks or bash scripts (pick your favorite language) that are easily sharable within the lab and outside to others.Reflection: Looking seriously at all the data in your experiments is a critical step for a young researcher. Even as an expert, having a healthy skepticism about your own data and methods is important to building intuition about what phenomena you’re looking at and what your software is actually doing. One of my old advisors, Roberto Navigli, had a sixth sense for finding surprises in data and would call out random line numbers for us to look at, only to find the one line in thousands that had something weird. This skill was terrifying at first as a student (no one wants to show bad data) and a great motivator for me and the other lab members to always double- and triple-check everything. You can only trust your results when you trust the data that led to them.Suggestions on how to become a Pine:Always, always look at your data before the meeting and allow enough time that if you spot something wrong, you can fix itLook random parts of your data, not just the first few lines. Be sure there are no surprisesMeeting time is precious; have your data ready to go in a format easily accessible to all meeting participants. Google docs/sheets work great for this. For large files on servers that everyone has access to, send out the link to the file to everyone before the meeting.The undergraduate researcher was close to results but hadn’t anticipated just how cold Michigan winters can be when it comes to getting into the lab for meetings.Scenario: the student has been hard at work on a project using a big dataset and wants to present their latest results at the meeting.The Sapling student will arrive and say they were hoping to present but ended up not having results, which often boils down to a few reasonsthere was a bug in their program and it crashed over nightthe data is too big and the program didn’t scalethey got the results and generated the plot but realized something was wrong in the plot and didn’t save the results to quickly remake the plotThe Pine student arrives with their results, though not always with the full extent of what they had planned. The Pine student has done the back of the envelope calculation to determine how much time they have before the meeting and generated results for as much of the data as they could, using appropriate subsampling.Reflection: Many of the projects in my lab involve large datasets, sometimes tens of terabytes of data. Even some smaller datasets have millions of data points that are impractical to study if analyzed naively. This scale can pose a substantial challenge to the budding scientist, who up to this point has not worked at scale in their coursework? The reality is that we generally don’t need to use all of that data to get our initial results. If we have a billion data points, using a random sample of one million will probably give us a fairly close result to the full data, just with larger error bars. Getting small results quickly is important for building intuition on your research questions. Learning how to generate these results quickly — or discover a bug preventing you from scaling — is equally important in developing the craft of software development.The critical skill here is planning to get results. Running a program is an important step, but the Pine student’s effort in estimating the time is an important earlier step. How long will the process take? Can we use less data now? If we need all the data, can we parallelize this somehow? Good communication with the advisor and peers in the lab can help significantly with planning as well — and possibly on how to strategize to speed things up.Suggestions on how to become a Pine:Run your analysis on the smallest amount of data you can at first (maybe the first 1000 examples) to debug everything and to get a sense of how long it might take. Then try to run as much data as you can before the meeting.Never launch a program to produce results on all the data without first checking that it works on a small batch of data first.As you can see from this figure, most of the lines go up and to the right, meaning our method is working (Image credit Cullen328, shared under Creative Commons Attribution-Share Alike 3.0 Unported)Scenario: the student has produced a new figure for us to look at about their projectThe Sapling student’s figures often have a few common issues:no labels on axes or other interpretable visualizations, or a tiny fontthe data is oddly grouped or organized so that similar things are not visually distinguishablethe data is heavily skewed but the figure isn’t drawn at log scale for the appropriate axes, making it difficult to readthe figure was generated in a way that makes it difficult for us to quickly fix these issuesThe Pine student’s figures are appropriately labeled and scaled. They have the code for generating the figure handy (e.g., in a Jupyter notebook) so that if we need to change anything, we can do it together. Pine students often have generated more figures than we have time to talk about by doing lots of little analyses, which lets them prioritize the ones we spend time on in our meetings.Reflection: Figure making is an important art in the process of communicating science. The Sapling student often thinks of making the figure as the goal but the Pine student will realize that the true goal is having someone other than them understand the figure as they themselves do. Learning how to generate high quality, interpretable figures can take time, so being able to quickly iterate is useful and saves time later when we need to generate the figures for the paper.Suggestions on how to become a Pine:Label your axesMake sure the code for generating your figure is readily available and easily runnableGet the easy figure first (e.g., use Seaborn, Matplotlib, or Gnuplot with their defaults) and then go about the business of customizing. Example figure galleries can help show you possible ways to adjust — with code too.The undergraduate researcher writes a note to maintain a positive attitude with all the ups and downs of research (Image credit Gamma Man, shared under a CC by 2.0 license)Scenario: a long technical discussion is wrapping up by discussing the next steps for the projectThe Sapling student will ask questions about the discussion but often not take notes; or, if the discussion has included drawing on the whiteboard, no record is taken of any diagrams. When asked if they understand some specific part of what we discussed, the Sapling student says yes (often to avoid admitting they didn’t understand).The Pine student takes notes in multiple formats. If anything is unclear, the Pine student will ask clarification questions. Often, pictures are taken of the white board discussion and posted to the appropriate Slack channel for the project for everyone to see. Following the advice from my colleague Walter Lasecki, I’ve been encouraging students to use their phone to record the whole audio of the meeting, which is then posted to Slack as well.Reflection: Notes are critical. I’ve been impressed with the recorded meetings as well. When taking hand-written notes, it becomes a trade-off between writing down what’s being said and being an active participant. Recording the whole audio lets students go back and listen to any part they might have missed or need additional context for understanding.Moreover, in mixed-gender groups, recording audio eliminates the undesirable trend of having the female researcher end up as the notetaker and not participate as much. The audio recording ensures everyone can be an equal participant without having to worry about writing down what’s being said.Some students have also taken to following up after each meeting with a meeting summary. These summaries are very useful, as they help drive the discussion at the next meeting and provide a moment of reflection for thinking about what is to be done next. I often have students in my lab send me brief weekly summaries of their work during the week and their plans for the next week. While the former part is useful for finding highlights for recommendation letters, the latter is actually the critical part in nudging students to think bigger picture on the project and what they want to accomplish. Pausing to reflect on what was done during the week is also useful for thinking about the process of research and what could be done different.Beyond taking notes, being honest about what you do and don’t understand is essential for healthy discussions. There is no shame in admitting you don’t understand some part of the discussion and wanting more clarification. I have found that some students want to preserve the illusion of knowledge and avoid speaking up so they seem competent. Not knowing something can be intimidating — especially at the start of a project when most parts are unknown. One self-actualizing strategy I’ve liked from a student was to always add “yet” to the end of any sentence about not knowing something, e.g., “I don’t know how to train a neural language model yet,” when asking a question. This positive mindset is critical going forward as a researcher; there will be much you won’t know and even more than you’ll never know but it is essential that you have faith in your ability to learn new things as they come up.After the meeting, if a Pine student realizes they didn’t understand something, they can potentially go back their notes or recording. For technical questions, I encourage students to ask their fellow labmates first for help and advice before reaching out to me. A main motivation for this is building lab knowledge; having to explain something or debug someone else’s code provides a valuable experience to both parties, who inevitably learn more. It also creates a culture of helping one another that lets problems get solved more quickly.Suggestions on how to become a Pine:Record the meeting audio and immediately post it to Slack so you and your teammates can go back over anything later.Take pictures of any diagrams on the whiteboard and upload these to Slack too for everyone in the group.If you don’t understand something ask for clarification. One useful way to ask is to try to restate what you think is happening and ask if that’s right.Undergraduate researchers taking their next steps towards achieving their research goalsScenario: the meeting is wrapping up on time and the advisor and student have a few minutes left for discussion on what are the next steps they will be working on until the next meeting.In general, the Sapling student will leave the meeting without solid understanding of the next steps. Often this vagueness is due to one or more of a few reasons:the Sapling student has not taken notes during the meeting and can’t recall the specifics of what needs to be done or how it needs to be donethe objectives are specified somewhere (e.g., Asana) and the Sapling student has said they understand but upon further reflection realizes that they have more questionsthe Sapling student needs something to make progress (e.g., needs to figure out how to set up a Jupyter notebook) but hasn’t made this clear to the advisor, which blocks the effort on the actual next steps.The Sapling student may also have a few ideas they have thought of that might be good next directions but won’t bring these up, usually assuming that because there are already several next-steps to be done, their ideas can wait.The Pine student concludes the meeting by suggesting what they think are the next steps and why. These suggestions include addressing things that are currently blocking the student. Then the advisor and the Pine student discuss the next steps together. The Pine student may also challenge the advisor’s next steps and suggest new directions for the project as a whole.Reflection: One of the most powerful skills a new researcher can learn is to predict what their advisor would say. Instead of asking what are the next steps, a Pine student suggests what they think the advisor would suggest as the next steps. This role assumption helps students get unstuck on their own later by asking themselves what their advisor would say in the current predicament. In the case of project planning, a discussion of next steps helps the Pine student understand the connection between their current work and the goals of the project. While not everything the Pine student suggests is accepted, the advisor’s role is to help the student understand why certain things are re-prioritized or not done so that the student can learn about the process of science. As students grow in their understanding of the project, I appreciate being challenged to justify why one of my suggestions should be taken — healthy skepticism can lead to fruitful conversations about project goals and potentially new avenues for research. That said, lots of challenging can be exhausting.Suggestions on how to become a Pine:At the end of the meeting, suggest what you think should be the next steps for the project.Several mature undergraduate researchers, standing proudly together in a field (Image Credit: U.S. Fish and Wildlife Service)More generally, a part of the transition from Sapling to Pine is their reimagining of their work as research rather than homework-like programming assignments. The latter leads to overly narrow questions and a mentality that all work on the project is driven by the advisor, e.g., determining next steps, setting meeting agendas, specifying how to make figures. Initially, this mentality might be necessary for the truly fresh students who is just learning what research means. However, the goal is to have the student take ownership of the research and drive both the research questions and goals in conjunction with the advisor.In nature, not all seeds, sprouts, and saplings wind up growing into full pines — those that do are the ones that have access to resources (water, light, space, freedom from predators/pests). As an advisor, my responsibility is to provide these as much and best as I can. By preparing for meetings, you can ensure that you, young Sapling, are well positioned to absorb the resources (guidance, feedback, advice) your advisor and labmates can offer.I hope this post has helped frame some of the opportunities for growth that I’ve seen in my own lab’s students and I welcome any discussion or suggestions from others!¹ I’ve written this post with the undergraduate researcher in mind, but I suspect it would also apply broadly to new masters students and even the occasional PhD student.² Specifically, it’s the Eastern white pine.</description>
      <pubDate>04 Apr 20 10:14 EDT</pubDate>
      <guid>https://medium.com/@jurgens_24580/reflections-on-strategies-for-successful-meetings-with-undergraduate-researchers-ae22306ecd8d</guid>
    </item>
    <item>
      <title>cron is dead, long live launchd!</title>
      <link>https://blog.jan-ahrens.eu/2017/01/13/cron-is-dead-long-live-launchd.html</link>
      <description>&lt;a href=&#34;https://blog.jan-ahrens.eu/2017/01/13/cron-is-dead-long-live-launchd.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 1 minute read Now that I finally created my tarsnap backup script, how do I execute it regularly? Oh, I know: My Mac is just an Unix system, I’ll use cron! At least that’s what I thought I’ll do. After a few attempts to get cron to do the job, I learned that there’s a better way on macOS: launchd. launchd does a lot more than executing scripts cron-style. Like systemd on Linux, launchd is a replacement for a lot of old school Unix tools, like cron, inetd, init, etc. At it’s core, launchd distincts daemons and agents. Dameons are processes that always run in the background, while agents describe regular jobs that are to be executed on certain events. There are a lot of different events to choose from. For example you can trigger an agent, when a device gets mounted, when a file gets created, or when a certain time arrives. What really helped me in learning how to write my first launchd agent was launchd.info. Unlike the Apple documentation, it contains useful snippets and concise explanations. I highly recommend that you also have a look at the launchd agents that some of your applications put into ~/Library/LaunchAgents. Below you can see the agent that I ended up creating. You can learn how to load/unload agents and about the meaning of the different options at launchd.info. If you’re testing your script and you don’t want to wait for the next hour to arrive, you can start it immediately with launchctl start eu.jan-ahrens.tarsnap. &lt;?xml version=&#34;1.0&#34; encoding=&#34;UTF-8&#34;?&gt; &lt;!DOCTYPE plist PUBLIC &#34;-//Apple//DTD PLIST 1.0//EN&#34; &#34;http://www.apple.com/DTDs/PropertyList-1.0.dtd&#34;&gt; &lt;plist version=&#34;1.0&#34;&gt; &lt;dict&gt; &lt;key&gt;Label&lt;/key&gt; &lt;string&gt;eu.jan-ahrens.tarsnap&lt;/string&gt; &lt;key&gt;EnvironmentVariables&lt;/key&gt; &lt;dict&gt; &lt;key&gt;PATH&lt;/key&gt; &lt;string&gt;/bin:/usr/bin:/usr/local/bin&lt;/string&gt; &lt;/dict&gt; &lt;key&gt;ProgramArguments&lt;/key&gt; &lt;array&gt; &lt;string&gt;/bin/bash&lt;/string&gt; &lt;string&gt;/Users/jan/bin/run-tarsnap-backup&lt;/string&gt; &lt;/array&gt; &lt;key&gt;StartInterval&lt;/key&gt; &lt;integer&gt;3600&lt;/integer&gt; &lt;key&gt;StandardOutPath&lt;/key&gt; &lt;string&gt;/Users/jan/.tarsnap.log&lt;/string&gt; &lt;key&gt;StandardErrorPath&lt;/key&gt; &lt;string&gt;/Users/jan/.tarsnap.log&lt;/string&gt; &lt;key&gt;KeepAlive&lt;/key&gt; &lt;dict&gt; &lt;key&gt;NetworkState&lt;/key&gt; &lt;true/&gt; &lt;/dict&gt; &lt;key&gt;ExitTimeout&lt;/key&gt; &lt;integer&gt;900&lt;/integer&gt; &lt;key&gt;Nice&lt;/key&gt; &lt;integer&gt;10&lt;/integer&gt; &lt;/dict&gt; &lt;/plist&gt; P.S.: cron itself is implemented as a launchd daemon. You can find it at /System/Library/LaunchDaemons/com.vix.cron.plist. </description>
      <pubDate>03 Feb 21 10:12 EST</pubDate>
      <guid>https://blog.jan-ahrens.eu/2017/01/13/cron-is-dead-long-live-launchd.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://jblevins.org/log/lofi</link>
      <description>&lt;a href=&#34;https://jblevins.org/log/lofi&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In Praise of Low-Fidelity February 5, 2006 I believe in simplification. I strongly support technology and innovation, but I believe more strongly in choosing the best tool for any task. Simplicity can of course result from the application of technology but many times, the best tool happens to be the “low-fidelity” one. Expensive high-tech gadgets that promise to make you more efficient will probably do the opposite. Most of the time a notebook and pen will do. For example, those nifty special effects in PowerPoint that took an hour of tweaking only serve to detract your audience from your content in the end. On the other hand, my razor, my iPod, and my compact, lightweight titanium-frame umbrella, all very simple, elegant, and useful, were all possible only because of technology. I think that technology has a tendency to overstimulate and overextend us, but I am by no means a Luddite. Innovation is essential, even if only to help us realize that some previous solution worked better. Word processors with lots of bells and whistles are great for certain tasks, but despite 40 years of advances in computer technology, the plain text file has survived. It is used for its own purposes, it constitutes the source code of complicated computer programs, and supports the publication of the most thought-provoking new books. Windows was a major catalyst to the computer revolution, but it’s complexity, bulkiness, and closed nature was in turn a catalyst to a Unix revolution in favor of openness, flexibility, and simplicity. I believe that if you’re not pushing your limits, you’re wasting time, but if you push through the present without enjoying what you’ve done, there will be no time left to waste. This is where the low-fidelity approach is useful. There is a certain pleasure in listening to the radio instead of watching television, in writing a letter instead of an email, or in taking a walk instead of driving, and it helps keep you grounded in the present while keeping your mind clear for looking ahead. There are also possible efficiency gains. I can listen to NPR while driving or making breakfast, and I spare myself from an onslaught of advertisements in the process. Thus, the low-fidelity approach is the selective application of the simplest, most efficient tool available, whether it involves the newest technology on the market or just some old-fashioned practicality. I wasted time, and now doth time waste me. William Shakespeare, Richard II </description>
      <pubDate>02 Feb 21 13:33 EST</pubDate>
      <guid>https://jblevins.org/log/lofi</guid>
    </item>
    <item>
      <title>The Algorithms That Make Instacart Roll</title>
      <link>https://spectrum.ieee.org/artificial-intelligence/machine-learning/the-algorithms-that-make-instacart-roll</link>
      <description>&lt;a href=&#34;https://spectrum.ieee.org/artificial-intelligence/machine-learning/the-algorithms-that-make-instacart-roll&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; It&#39;s Sunday morning, and, after your socially distanced morning hike, you look at your schedule for the next few days. You need to restock your refrigerator, but the weekend crowds at the supermarket don&#39;t excite you. Monday and Tuesday are jam-packed with Zoom meetings, and you&#39;ll also be supervising your children&#39;s remote learning. In short, you aren&#39;t going to make it to the grocery store anytime soon. So you pull out your phone, fire up the Instacart app, and select your favorite grocery store. You click through your list of previously purchased items, browse specials, search for a new key-lime sparkling water a friend recommended, then select a delivery window. About 2 hours later, you watch a shopper, wearing a face mask, place bags on your porch. The transaction seems simple. But this apparent simplicity depends on a complex web of carefully choreographed technologies working behind the scenes, powered by a host of apps, data science, machine-learning algorithms, and human shoppers. Grocery delivery isn&#39;t a new concept, of course. In our great-grandparents&#39; day, people could select items at a neighborhood store and then walk home empty-handed, the groceries to follow later, likely transported by a teenager on a bicycle. Customers often had basics like milk and eggs delivered weekly. But with the advent of the fully stocked supermarket, with its broad selection of staples, produce, and specialty foods, customers shifted to selecting goods from store shelves and toting them home themselves, though in some cities local stores still offered delivery services. Then in 1989, Peapod—followed in the mid-1990s by companies like Webvan and HomeGrocer—tried to revive grocery delivery for the Internet age. They invested heavily in sophisticated warehouses with automated inventory systems and fleets of delivery vehicles. While these services were adored by some for their high-quality products and short delivery windows, they never became profitable. Industry analysts concluded that the cost of building up delivery networks across dozens of large metro areas rapidly ate into the already thin margins of the grocery industry. In These COVID Times: Instacart orders surged during the pandemic, with the company adding some 300,000 shoppers and an option for customers to avoid human contact: “Leave at My Door Delivery.&#34; Photos, top and bottom: Michael Loccisano/Getty Images; middle: Cheney Orr/Reuters Timing, of course, is everything. Cloud computing and inexpensive smartphones emerged in the decade after the launch of the first-generation online grocery companies. By 2012, when Instacart began, these technologies had created an environment in which online grocery ordering could finally come into its own. Today, retailers like Target and Whole Foods (via Amazon) offer delivery and pickup services, using their existing brick-and-mortar facilities. Some of these retailers run their delivery businesses from warehouses, some pull from the stocked shelves of retail stores, and some fulfill from a mix of both. Small, online-only companies like Good Eggs, Imperfect Foods, and Thrive Market offer curated selections of groceries sourced from local farms and suppliers. Meanwhile, food and grocery delivery services emerged to bring brick-and-mortar restaurants and stores into the online economy. These businesses—which include DoorDash, Shipt, and Uber Eats in the United States, and Buymie, Deliveroo, and Grofers, based elsewhere—have built technology platforms and fulfillment networks that existing stores and restaurants can use to reach customers online. In this model, the retailer&#39;s or restaurant&#39;s physical location nearest the customer is the “warehouse,&#34; and a community of independent contractors handles fulfillment and delivery. Our employer, Instacart, is the North American leader in this type of online grocery service, with more than 500 grocers, including Aldi, Costco, Food Lion, Loblaws, Publix, Safeway, Sam&#39;s Club, Sprouts Farmers Market, and Wegmans, encompassing nearly 40,000 physical store locations in the United States and Canada. At the onset of the COVID-19 pandemic, as consumers heeded stay-at-home orders, we saw our order volume surge by as much as 500 percent, compared with the volume during those same weeks in 2019. The increase prompted us to more than double the number of shoppers who can access the platform from 200,000 in early March to 500,000 by year-end. Here&#39;s how Instacart works. From the customer&#39;s perspective, the ordering process is simple. Customers start by opening a mobile app or logging on to a website. They enter their delivery zip code to see available retailers. After choosing a store or retail chain, they can browse virtual aisles of produce, deli, or snacks and search for specific products, clicking to add items to an online shopping cart and specifying weights or quantities as appropriate. When finished, they see a list of available 2-hour delivery windows, from later the same day to a week or more in the future. Customers can adjust their orders up until the shoppers start picking their items off store shelves, usually an hour or two before the delivery window. They can enter preferred substitutions beforehand or chat with their shoppers in real time about what&#39;s available. Once the groceries are out of the store and on the move, customers get alerts when their orders are about to be delivered. That&#39;s Instacart from a customer&#39;s perspective. Behind the scenes, we face huge technical challenges to make this process work. We have to keep track of the products in nearly 40,000 grocery stores—billions of different data points. We have to predict how many of our 500,000-plus shoppers will be online at a given time in a given area and available to work. We have to group multiple orders from different customers together into batches, so that the designated shopper can efficiently pick, pack, and deliver them. When products run out, we suggest the best replacements. Finally, we dispatch shoppers to different delivery addresses along the most efficient route. We&#39;re crunching enormous volumes of data every day to keep the customer-facing Instacart app, our Shopper app, our business management tools, and other software all humming along. Let&#39;s start with how we keep track of products on the shelf. The average large supermarket has about 40,000 unique items. Our database includes the names of these products, plus images, descriptions, nutritional information, pricing, and close-to-real-time availability at every store. We process petabytes daily in order to keep these billions of data points current. Images: Instacart App to App: A customer selects groceries and chooses a delivery window using the Instacart app. Then the Shopper app takes over, guiding Instacart&#39;s contractors to and through stores as they fulfill grouped orders, suggesting replacements, and, finally, directing them to customers&#39; doors. Back in 2012, when Instacart started making its first deliveries in the San Francisco Bay Area, we relied on manual methods to get this product data into our system. To stock our first set of virtual shelves, our founders and a handful of employees went to a store and purchased one of every item, filling up cart after cart. They took everything back to the office and entered the product data into the system by hand, taking photos with their phones. It worked, but it obviously wasn&#39;t going to scale. Today, Instacart aggregates product data from a variety of sources, relying on automated rule-based systems to sort it all out. Many stores send us inventory data once a day, including pricing and item availability, while other retailers send updates every few minutes. Large consumer products companies, like General Mills and Procter &amp; Gamble, send us detailed product data, including images and descriptions. We also purchase specialized data from third-party companies, including nutrition and allergy information. One listing in our database could have information from dozens of sources that must be sorted out. Let&#39;s say a popular apple juice just underwent a rebranding, complete with new packaging. Our system has to decide whether to use the image provided by a third-party data provider last week, the image sent in by the local store last night, or the image submitted by the manufacturer earlier that morning. Our rules address this problem. Usually images and other data provided by the manufacturer on the morning of a rebrand will be more up-to-date than data provided by individual stores the night before. But what if a store and the manufacturer upload data at about the same time? In this case, our rules tell the system to trust the image provided by the manufacturer and trust the price and availability data provided by the store. Our catalog updates automatically and continuously around the clock to account for all sorts of incremental changes—more than a billion data points every 24 hours on average. Because Instacart doesn&#39;t own and operate its own stores or warehouses, we don&#39;t have a perfect picture of what is on the shelves of a particular store at any moment, much less what will be there later that day or several days in the future. Instead, we need to make well-informed predictions as we stock our virtual shelves. There&#39;s a lot to consider here. Stores in certain regions may get produce shipments on, say, Monday mornings and meat shipments on Thursday evenings. Some stores restock their shelves periodically throughout the day, while others just restock at night. We&#39;ve built two machine-learning models to help us understand what&#39;s on each store&#39;s shelves and manage our customers&#39; expectations about what they will actually receive in their grocery bags. Our Item Availability Model predicts the likelihood that popular items are in stock at any location at any given time. We trained this model using our own data set, which includes millions of anonymized orders from across North America. Some items—like a particular brand of organic eggs, chips, or seasoned salt, or niche items like fresh-made stroopwafels—are considered “active,&#34; meaning they&#39;re regularly ordered year-round from a particular store. “Non-active&#34; items include discontinued products as well as seasonal items like eggnog, Advent calendars, and Peeps marshmallows. The model looks at the history of how often our shoppers are able to purchase the items consumers order most. For each, it calculates an availability score ranging from 0.0 to 1.0; a score of 0.8 means the item has an 80 percent chance of being found in the store by a shopper. We can update this score in real time because our shoppers scan each item they pick up or else mark it as “not found.&#34; Having this score enables us to reduce the chances our customers will order items that won&#39;t be on store shelves when our shoppers look for them, whether that&#39;s a few hours away or days ahead. We do this in several ways. For example, if a customer&#39;s favorite type of peanut butter has a very low availability score, we will automatically bump that listing down in the search results and, in turn, bump up similar products that have a higher availability score. In these times of supply-chain shortages, we&#39;ll also add “out of stock&#34; labels to affected items and prevent customers from adding them to their carts. The COVID-19 pandemic pushed our Item Availability Model in a number of other ways and challenged our assumptions about customer behavior. In March 2020, at the start of the U.S. shelter-in-place orders, we saw massive spikes in demand for common household products like toilet paper and disinfecting wipes. Such items were flying off the shelves faster than retailers could stock them. Consumers behaved in new ways—instead of buying their preferred brand of toilet paper, they grabbed any kind of toilet paper they could find. In response, we broadened the number of products our availability model scores to include lesser-known products. We also tweaked the model to give less weight to historical data from several weeks earlier in favor of more recent data that might be more representative of the times. If a customer adds an item with a low availability score to her cart, a second machine-learning model—our Item Replacement Recommendation Model—gets to work, prompting the customer to select a replacement from a menu of automatically generated alternatives in case the first-choice item isn&#39;t in stock. Giving customers great replacements is a critical part of making them happy. If you&#39;re shopping in-store for yourself, our research suggests that you&#39;ll have to find replacements for about 7 percent of the items on your list. When Instacart shoppers are shopping for you, they can&#39;t just leave out an item—some items may be critical for you, and if you have to make your own trip to the store after unpacking an Instacart order, you might be less likely to use the service again. But our shoppers aren&#39;t mind readers. If the store is out of your preferred brand of creamy peanut butter, should a shopper replace it with crunchy peanut butter from the same brand? What about a creamy peanut butter from a different brand? We trained our Item Replacement Recommendation Model on a range of data inputs, including item name, product description, and five years of customer ratings of the success of our chosen replacements. When we present a menu of replacement choices, we rank them according to scores assigned by this model. If you select one of the replacements, we&#39;ll remember it for your future orders; if you don&#39;t, our shopper picks from products our model recommends. That&#39;s how machine learning helps us set expectations with our customers as they fill their shopping carts. Once an order is placed, another piece of technology enters the picture: the Shopper app. The vast majority of Instacart&#39;s shoppers are independent contractors who have signed up to shop for us, meeting requirements and passing a background check. They drive to the stores, select items off the shelves, check out, and deliver the orders. They can choose to work at any time by logging onto the Shopper app. In certain high-volume stores, we also directly employ part-time shoppers who pick and pack orders and then hand them off to contractors for delivery. The Shopper app includes a range of tools meant to make it easy to access new orders, address issues that shoppers encounter, and guide checkout and delivery. When shoppers are ready to work, they open up the app and select batches of orders. As they go through the store and fill orders, they can communicate with the customers via in-app chat. The Shopper app suggests an item-picking order to help the shopper navigate the store efficiently. Generally, this picking order puts refrigerated and frozen items, along with hot or fresh deli preparations, near the end of a shopping trip. Meanwhile, customers can watch their shopper&#39;s progress via the Instacart app, tracking each item as it&#39;s scanned into the shopper&#39;s cart, approving replacement items, and viewing yet-to-be-shopped items. When shoppers check out, they can charge the order to a physical card that Instacart mails to them or use a mobile payments system in the Shopper app. If they encounter a problem, they can communicate with our help team through the app. And when they complete a delivery, they can use the app to transfer their earnings to their bank accounts. We have many orders coming in at once to the same stores, slated to be delivered in the same general vicinity. In a major metropolitan area, we may get more than 50 orders a minute. So we typically group orders into batches to be picked off store shelves at the same time. Here, our Matching Algorithm comes into play. This technology applies rules of thumb and machine-learning models to try to balance the number of shoppers with customer demand in real time. The algorithm benefits from scale—the more orders we have in a given area, the more options we can give the algorithm and the better decisions it can make. It considers things like a shopper&#39;s age: If shoppers are not yet 21, they may not be eligible to deliver orders containing alcohol. We rerun the Matching Algorithm as often as every few minutes as we get new information about orders and delivery locations. Photos: Instacart The algorithm works hand in hand with our Capacity Model. This model calculates how much delivery capacity we have throughout the day as conditions on the ground change. We used machine learning to build this system; it takes demand predictions based on historical data and historical shopping speeds at individual stores and couples them with real-time data, including the number of shoppers completing orders and the number of orders waiting in a queue for each store. We rerun this model every 2 minutes to ensure that we&#39;re getting a close-to-real-time understanding of our capacity. That&#39;s why a customer may log on at 1:00 pm and see only one late-evening delivery slot remaining, but when they look again at 1:30 pm, they see a host of afternoon delivery slots pop up. While these models are critical to Instacart&#39;s operation, other tools are crucial for getting the groceries from the store to the customer smoothly and predictably. Our Drive Time Model uses historical transit times and real-time traffic data to estimate when a shopper will arrive at the store. Our Parking Model calculates how long it can take the shopper to get in and out of a particular store&#39;s parking lot. If a shopper is likely to spend 10 minutes cruising for a spot in a small, crowded parking lot, that needs to be built into delivery-time estimates for that store. Once the shopper is ready to make deliveries, our Routing Algorithm comes into play. This model is our take on the classic “traveling salesman&#34; problem. Given three customers at three different addresses in the same city, what&#39;s the most efficient route from the store to the first location and from there to the next two? That&#39;s tricky enough, but Instacart has to work with added complexity. For example, in highly dense areas like New York City, some shoppers may walk to their destinations. And we need to ensure that all three deliveries are made within their designated delivery windows—if a customer isn&#39;t home, too early can be just as bad as too late. So our algorithm considers the projected arrival time, using real-time traffic conditions, to create a delivery route. Our system also sends the projected arrival time to the customer and an alert when the shopper is just a few minutes away. All of our databases, machine-learning models, and geolocation technologies work in concert to build an efficient system. But no model is perfect. And the COVID-19 pandemic proved to be an unexpected stress test for our systems. As stay-at-home orders rippled across North America, with more data flowing into the platform than ever before, we had to repeatedly reconfigure our databases and tools to keep up with the new demand. At the peak, we found ourselves making upgrades multiple times a week. We also had to speed up the rollout of a new feature we had just started testing: Leave at My Door Delivery, which allows shoppers and customers to remain socially distant. Shoppers can drop groceries on the porch of a house or the reception or lobby area of an apartment building and send customers a photo of their completed orders at the site. We are continually looking at ways to optimize our technology and operations. Right now, we are exploring how to improve the suggested picking orders in the Shopper app. Today we rely on a set of rule-based formulas guided by human intuition—for example, that it&#39;s best to pick up fresh vegetables and fruit together, since they&#39;re usually in the same section of the store. But not all stores have the same layout, aisles in a given store can be rearranged, and items may get moved around the store seasonally. We&#39;re hoping we can use machine learning to develop an algorithm that determines such “rhythms&#34; in the way a location should be shopped, based on historical item-picking data along with seasonal additions to store shelves and regular changes in store layouts. As we add retailers and brands and serve more customers, our algorithms and technologies continue to evolve. We retrain all of our models over and over again to better reflect new activity on our platform. So the next time you click on the Instacart app and order groceries to get you through a busy week, know that anonymized data from your order and from your shopper will get fed into this feedback loop, informing the models we train and the technologies we build. We are proud that our system has been able to keep groceries flowing to people across North America who have been sheltering at home during the pandemic, especially those who are particularly vulnerable to the novel coronavirus. These are extraordinary times, and we&#39;ve taken our responsibility to serve our customers, shoppers, partners, and corporate employees very seriously, as well as to keep them safe. As the world continues to shop from home, we hope that our investments in machine learning will continue to make it easier for everyone to get access to the food they love and more time to enjoy it together.</description>
      <pubDate>26 Mar 21 07:20 EDT</pubDate>
      <guid>https://spectrum.ieee.org/artificial-intelligence/machine-learning/the-algorithms-that-make-instacart-roll</guid>
    </item>
    <item>
      <title></title>
      <link>https://setosa.io/blog/2014/07/26/markov-chains/index.html</link>
      <description>&lt;a href=&#34;https://setosa.io/blog/2014/07/26/markov-chains/index.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Markov chains, named after Andrey Markov, are mathematical systems that hop from one &#34;state&#34; (a situation or set of values) to another. For example, if you made a Markov chain model of a baby&#39;s behavior, you might include &#34;playing,&#34; &#34;eating&#34;, &#34;sleeping,&#34; and &#34;crying&#34; as states, which together with other behaviors could form a &#39;state space&#39;: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or &#34;transitioning,&#34; from one state to any other state---e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first. A simple, two-state Markov chain is shown below. With two states (A and B) in our state space, there are 4 possible transitions (not 2, because a state can transition back into itself). If we&#39;re at &#39;A&#39; we could transition to &#39;B&#39; or stay at &#39;A&#39;. If we&#39;re at &#39;B&#39; we could transition to &#39;A&#39; or stay at &#39;B&#39;. In this two state diagram, the probability of transitioning from any state to any other state is 0.5. Of course, real modelers don&#39;t always draw out Markov chain diagrams. Instead they use a &#34;transition matrix&#34; to tally the transition probabilities. Every state in the state space is included once as a row and again as a column, and each cell in the matrix tells you the probability of transitioning from its row&#39;s state to its column&#39;s state. So, in the matrix, the cells do the same job that the arrows do in the diagram. If the state space adds one state, we add one row and one column, adding one cell to every existing column and row. This means the number of cells grows quadratically as we add states to our Markov chain. Thus, a transition matrix comes in handy pretty quickly, unless you want to draw a jungle gym Markov chain diagram. One use of Markov chains is to include real-world phenomena in computer simulations. For example, we might want to check how frequently a new dam will overflow, which depends on the number of rainy days in a row. To build this model, we start out with the following pattern of rainy (R) and sunny (S) days: One way to simulate this weather would be to just say &#34;Half of the days are rainy. Therefore, every day in our simulation will have a fifty percent chance of rain.&#34; This rule would generate the following sequence in simulation: Did you notice how the above sequence doesn&#39;t look quite like the original? The second sequence seems to jump around, while the first one (the real data) seems to have a &#34;stickyness&#34;. In the real data, if it&#39;s sunny (S) one day, then the next day is also much more likely to be sunny. We can minic this &#34;stickyness&#34; with a two-state Markov chain. When the Markov chain is in state &#34;R&#34;, it has a 0.9 probability of staying put and a 0.1 chance of leaving for the &#34;S&#34; state. Likewise, &#34;S&#34; state has 0.9 probability of staying put and a 0.1 chance of transitioning to the &#34;R&#34; state. In the hands of metereologists, ecologists, computer scientists, financial engineers and other people who need to model big phenomena, Markov chains can get to be quite large and powerful. For example, the algorithm Google uses to determine the order of search results, called PageRank, is a type of Markov chain. Above, we&#39;ve included a Markov chain &#34;playground&#34;, where you can make your own Markov chains by messing around with a transition matrix. Here&#39;s a few to work from as an example: ex1, ex2, ex3 or generate one randomly. The transition matrix text will turn red if the provided matrix isn&#39;t a valid transition matrix. The rows of the transition matrix must total to 1. There also has to be the same number of rows as columns. You can also access a fullscreen version at setosa.io/markov </description>
      <pubDate>24 Mar 20 12:18 EDT</pubDate>
      <guid>https://setosa.io/blog/2014/07/26/markov-chains/index.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.bloomberg.com/news/features/2020-04-07/zuckerberg-s-jealousy-held-back-instagram-and-drove-off-founders</link>
      <description>&lt;a href=&#34;https://www.bloomberg.com/news/features/2020-04-07/zuckerberg-s-jealousy-held-back-instagram-and-drove-off-founders&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Why did this happen? Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. Need Help? For inquiries related to this message please contact our support team and provide the reference ID below. Block reference ID: </description>
      <pubDate>08 Apr 20 20:04 EDT</pubDate>
      <guid>https://www.bloomberg.com/news/features/2020-04-07/zuckerberg-s-jealousy-held-back-instagram-and-drove-off-founders</guid>
    </item>
    <item>
      <title></title>
      <link>https://overreacted.io/my-decade-in-review/</link>
      <description>&lt;a href=&#34;https://overreacted.io/my-decade-in-review/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I started this decade as a first-year college student fresh out of high school. I was 17, didn’t have a job, didn’t have any industry connections, and really didn’t know shit. And now you’re reading my blog! I would have been proud. I’ve told bits and pieces of my story on different podcasts. Now feels like an appropriate time to write down the parts that were most memorable to me. Every person’s story is unique and not directly reproducible. I’ve benefited immensely from the privilege of being born in an upper middle class family and looking like a typical coder stereotype. People took chances on me. Still, I hope that sharing my story can be helpful to compare our experiences. Even if our circumstances are too different, at least you might find some of it entertaining. 2010 I was born in Russia and I finished the high school there in 2009. In Russia, higher education is free if you do well enough at tests. I tried my chances with a few colleges. I was particularly hoping to get into one college whose students often won programming competitions (which I thought was cool at the time). However, it turned out my math exam scores weren’t good enough. So there were not many options I could choose from that had to do with programming. From the remaining options, I picked a college that gave Macbooks to students. (Remember the white plastic ones with GarageBand? They were the best.) By the summer of 2010, I had just finished my first year there. It turned out that there wasn’t going to be much programming in the curriculum for two more years. But there was a lot of linear algebra, physics, and other subjects I didn’t find particularly interesting. Everything was well in the beginning, but I started slacking off and skipping lectures that I had to wake up early for. My gaps in knowledge gradually snowballed, and most of what I remember from my first year in the university is the anxiety associated with feeling like a total failure. Even for subjects I knew well, things didn’t quite go as I planned. Our English classes were very rudimentary, and I got a verbal approval from the teacher to skip most of them. But when I came for the final test, I wasn’t allowed to turn it in unless I pay money for hours of “catch up training” with the same teacher. This experience left me resentful and suspicious of higher education. Aside from being a lousy student, I was also in my first serious relationship — and it wasn’t going very well either. I was unhappy, but I thought that you can solve this by continuing to be unhappy and “fixing” it. Unfortunately, I didn’t have the wisdom to get out of a non-working relationship for a few more years. Now onto the bright side. Professionally, 2010 was an exciting year for me. I got my first job as a software developer! Here’s how that happened. There was a small venue close to my college that hosted different events. This venue was a “business incubator” — mind you, not a Silicon Valley kind of business incubator — but a tiny Russian one. I have no idea what businesses they “incubated”. However, they hosted a talk about software development, and I decided to check it out because I was starving for that kind of content. I didn’t know any programmers in real life, and I didn’t know meetups existed! I don’t remember what the talk was about now. But I knew the person who gave it was an executive in a Russian-American outsourcing company. I’ve been programming since 12, so I approached him and asked if they’re hiring. He gave me an email, I went through their test exercises, and in a few weeks got the job. I started at my first job during the summer of 2010. My salary was $18k/year (yes that’s 18 and not 180). This is peanuts in the developed world, but again, this was Russia — so the rent was cheap too. I immediately moved out of my mom’s apartment and started renting a room for $150 a month. I was excited. With my first salary, I bought an iPhone and marvelled at how good the UI was. Summer came and went, and then the second college year started. But it started without me. Now that I was doing actual work and people payed me for it, I lost my last bits of motivation for sitting at lectures and doing exercises. I stopped going there and didn’t show up for the midterm exams. I returned my Macbook. The only time I went there again was five years later, to pick up my papers. A short digression. I’m not saying colleges are worthless, or that you should drop out. It was the right decision for me, but I knew I could fall back on my family (more on that later) when things are tough. I also already had a job. I had the privilege to be seen as knowledgeable by default due to my background (a guy who started coding early). People who don’t fit this stereotype often get a degree just to gain access to that assumed competence. So there’s that. 2011 Most of my job was fixing up poor code after cheaper outsourcing companies. Having no industry experience myself, I overengineered every project to try every cool technology I could get my hands on. I even put random Microsoft Research projects in production. Sorry about that. I did some good work too. My first interesting work project involved a trip. Our client was an investment group in New York. I still don’t know anything about investments, but basically they had an email system that received orders, and those orders needed to go through different levels of approval. They had a service that manages that, but the service was extremely flaky, and nobody could figure out how it works. My job was to go onsite, work from New York for a month, and fix the service. The service was written by a contractor from a cheaper outsourcing company. Nine years later, I still remember his name. The most memorable part of that code was a single thirty thousand line function. To figure out what it does, I had to print it on paper, lay out the sheets on my desk, and annotate them with a pencil. It turned out that it was the same block of code, repeated fifty times with different conditions. I guess someone was paid by the number of lines of code. I spent that month adding a shitton of logging to figure out what the service does in production, and then rebuilding it from scratch to be less flaky. Working with a non-tech company was a perplexing experience. For example, I couldn’t push a bugfix without writing a Word document describing my changes and getting the IT department head to sign off on it. Now that’s some code review. Close to the end of my trip, I went to see a concert at a bar late at night. The next morning I was supposed to present my month of work to the client. My meeting was scheduled for 9am. Unfortunately, I overslept and only woke up at 1pm that day. My manager apologized for me, and I went home bitterly embarrassed. There were no repercussions at work. The project was a success overall, and the client knew I’m some weird Russian dude who doesn’t know how to groom his hair. But I knew I made a fool of myself. I also wasn’t particularly looking forward to more “enterprise projects”. Work became a chore. I was back in Saint Petersburg in Russia. In the summer, the sky there doesn’t go dark. During one night of soul-searching, I hopped from a bar to another with a vague sense of unease. Around 7am, a lightbulb went off in my head. I ate a shawarma, took a subway to the office, waited for HR, and quit my job. My friend was planning a trip to Crimea (before it got annexed) and asked if I would like to join. I packed up a tent and an old Nokia phone that held battery for a week. We camped for two weeks, mostly naked, in a fog of alternative mind states. I barely remember anything from that trip except two episodes. Once, somebody threatened me with a knife. That person said he would kill me, but he was gone the next day, and everything went on as normal. Another time, I foolishly tried to swim around the cliff alone and almost drowned. I was saved by a rock in the sea that I climbed and passed out on for what felt like an hour. This trip acted like a hardware reset. My burnout was cured, and I was ready to write code again. (But don’t say I told you to almost die to cure a burnout.) The only problem was… my skills were irrelevant! Yikes. You see, I was mostly writing desktop software. Has anyone even heard of desktop software? That’s not a thing anymore. Either you do backend, or you do mobile, or you do front-end. I didn’t know anything about either of them. And I didn’t have a job. So I had to move back to live with my Mom. (Thanks, Mom!) Soon, I saw a post on a social network. It was written by a Russian guy who came back from the Silicon Valley to Russia. He was looking for people who would volunteer to work on his projects for free, in return for him teaching us web development for free. At the time, that sounded like a good deal to me. I joined this program. I found out quickly enough that there was no real teaching involved: we were given a few tutorials from the web, and we mostly learned from helping each other. Luckily, I could afford to do that for some time while I lived at my Mom’s place. I learned Git, basics of Python, Django, a bit of CSS and JavaScript, and some Bash to deploy my changes. Hello Web, here I go. Nine years later, I’m still unsure how I feel about this program. On the one hand, we were working on his projects for free. On the other hand, we were given full root access, and it was really exciting to be able to push our changes in production and learn from our mistakes. It gave us a structure for learning. It didn’t cost us anything, and you could drop out any time. The projects had some social utility due to being around education. This reminded me of open source. I still feel grateful to this person for setting up this ad hoc “bootcamp” and being my mentor. But I don’t want to imply that working for free is a good way to practice in general. I’m not offering advice here — I am only telling my story. I built a dashboard where we could track our own learning achievements. My mentor suggested that I pitch it as a product to companies that run courses. My brief foray into playing “startups” was awkward. I didn’t know what product I was building, and I pitched different things to different people. Essentially, I ended up making several completely different websites for different clients with a single engine, and earned about $200 in the process. I wasted months of my time on it, as well as the time of friends who offered to help. I was ashamed of it, and shut it down. The silver lining was that I became a web developer. But I still didn’t have a job. 2012 As a 20 year old web developer, there was only one place I wanted to work at. It was a Russian social media company. Everybody in Russia used their product. That product was very polished. And the team was considered cool. Almost elite. Their executives often posted about how well-paid their engineers are. The small team of engineers seemed happy with the technical challenges and how the company treated them. In the Russian tech circles, many knew their names. My mentor introduced me to their CTO, and I got a takehome exercise in JavaScript. It involved building a clone of their typeahead where you could select friends to message. I spent two weeks building it. It was pixel perfect in all browsers. I took care to replicate a similar caching and debouncing behavior. The onsite interview was a disaster. Clearly, I didn’t have the experience at their scale. However, they said they’re willing to give me a try if I “understand their product”. They gave me a take-home exercise of designing a logged out state for that social media website. They wanted it to show a picture of a feature phone — many people didn’t know the mobile website works on cheap phones. I spent a week designing that page. I did a lot of tiny details and even hid some Easter eggs in it. I was proud of myself. However, I couldn’t find any decent designs of a feature phone that wouldn’t look ugly. Instead, I put a beautiful iPhone design there. So aesthetically pleasing, I thought. Of course, I got rejected. I ignored literally the only hard requirement — why was I so dense? I cried for a few hours because I didn’t really want to work anywhere else. I was still living with my Mom and was making no money. At the time, I was seriously doubting my skills. Many things seemed “magic” to me. I started having doubts about whether dropping out was a good idea, after all. I signed up for an iOS development course on iTunes U. I also signed up for two courses on Coursera: Compilers and Machine Learning. Maybe they would make me a “real programmer”. It was lonely to go through these courses on my own. I organized a tiny meetup with a few people from our web development “bootcamp”. We would gather and watch different online courses together at my mentor’s coworking space. A month into it, I got an email. Someone was looking to hire a developer, and he heard about me from a person who went to my meetup. I was sick with mono and ignored the email, but this guy kept emailing me. He wanted to skype. Roman Mazurenko was not a typical startup founder. Roman was interested in DIY publishing. Together with a couple of friends, for a few years, he somehow made Moscow cool. He organized parties and posed for fashion magazines. I didn’t know what to expect. But Roman was very down-to-earth and fun to talk to. His dream was to build a DIY publishing platform like in this concept video. I would have to move to Moscow and learn iOS development on the go. (By the way, the video guy is not Roman but a friend, and the app in the video is a fake animation made with Flash. Roman was great at crafting smoke and mirrors.) I said yes. I didn’t finish my Compilers and Machine Learning courses. I learned enough to know these topics aren’t magic. After that, I lost most of my interest in them. 2013 By 2013, my salary was $30k/year — almost twice what I made at my previous job. While low by US standards, it was pretty decent in Russia. I also negotiated some stock at Stampsy (spoiler alert: it ended up completely worthless). Our team had five developers and two designers. We started by developing an iPad app, but neither of us had any real knowledge of iOS. I remember my relief when a teammate figured out how to implement the animation we needed for the first time. Until then, I thought we were doomed. For a few months, I literally lived in our office. Looking back at this period, I’m not proud of my life-work balance, and it was not healthy. However, I learned more during these months than in two years before, and I don’t regret them. Eventually, I moved out of the office. I’ve started to live in the same flat as Roman. My room cost me $1k/month. It was a spacious flat in the only area of Moscow that I enjoyed, and it was only five minutes of walk from the office. We thought that some of the code we wrote might be useful to others. So we started publishing those pieces on GitHub. We didn’t expect anything grand, and even getting a couple of contributions was really nice. The most popular project I worked on during that period has 30 stars. To us, that was a lot. A designer on our team introduced me to Bret Victor’s talks — particularly, to Inventing on Principle. I thought it was a very good talk. A very good one. In April, we released the iPad app we’ve been working on. Apple reached out to our team and asked for assets to feature it in the App Store. We were over the moon. It stayed featured for weeks, and people started using it. Our excitement quickly wore down as we realized there was no product market fit. The app was designed to create beautiful magazine-like layouts, but nobody had any beautiful content on an iPad. Also, the iPad camera had a horrible quality. The product didn’t make any sense. How could we not realize this? My personal relationship was falling apart too. We weren’t a good fit, and mostly clung to each other out of fear of being alone. We finally broke up. For a few months, I didn’t talk to people from our shared mutual circle and focused on work. But I realized that I missed one particular friend. I wrote to her, and she said she missed me too. I arranged a plan for a trip together. I caught a cold. As the day of our would-be trip got closer, I felt worse, but I was hoping that maybe I’d be okay. When my train from Moscow to St. Petersburg had arrived, I clearly had a temperature. She said to come to her place anyway. She made me some hot tea, gave me warm socks, and we kissed. I moved in. 2014 For me, 2014 was the year of React. After a brief existential crisis, we abandoned the iPad app and decided to pivot to a webapp. That meant I had to learn JavaScript, this time for reals. We built a decent prototype with Backbone, but interactive parts were a pain to code. My colleague saw React but initially dismissed it. But a few months later, he told me React wasn’t actually that bad. I decided to give React a try. Ironically, the first component I converted from Backbone to React was a Like button. And it worked well. Really well. It felt unlike anything I’ve seen. React wasn’t a hard sell for the team. Over the next year we gradually converted all of our UI to React while shipping new features. React and the principle of unidirectional data flow allowed us to develop faster and with fewer bugs. We started a private beta, and some photographers enjoyed creating visual stories with it. It was something between Medium, Pinterest, and Tumblr. There wasn’t a lot of traction, but it wasn’t a complete failure like the iPad app. The only problem with using React was that there was virtually no ecosystem. When we just started, there was only one router (which was not React Router), and we couldn’t figure out how to use it. So we made our own. After React Router came out, we adopted it and added a feature we needed for our product. There was no drag and drop solution for our use cases, so I ported my colleague’s library to React. I made a helper to manage document titles. Wrote another library to normalize API responses. Jing Chen from the React IRC channel suggested the core idea, and it worked! Little did I know that in a few years, Twitter would build their new website with this library and maintain it. I wanted to contribute to React itself, too. I reached out to Paul O’Shannessy and asked if there were any pull requests I could work on. I finished my first task in a few days, but it didn’t get merged until a few months later. Big project release cycles, and all that. I was frustrated by the slowness in response so I put effort into the ecosystem instead. In retrospect, that was a lot more impactful. In 2014, I did some of my first public speaking. I gave sort of a lecture about React at our office. It ended up going for two hours, and I’m still surprised that most of the people who showed up were polite enough to stay to the end. Later, I signed up to give a talk at the BerlinJS meetup. My topic was “React and Flux”. I didn’t practice my talk, and I only went through the first half when my time ran out. People rolled their eyes, and I finally learned my lesson. From that point on, I would rehearse every talk from three up to fifteen times. In 2014, I got my first email from a Facebook recruiter. I missed it in my inbox and only found it many months later. We still chatted eventually, but it turned out that hiring me in USA wouldn’t be easy because I didn’t have enough years of experience and I dropped out of college. Oops. There was one project I started in 2014 that was particularly dear to me. Like most important things in my life, it happened randomly. I was converting our app from require.js to webpack to enable code splitting. I read about a bizarre webpack feature called “hot module replacement” that allowed you to edit CSS without reloading the page. But in webpack, it could work for JavaScript too. I was really confused by this feature so I asked about it on StackOverflow. Webpack was still very new, and its creator noticed my question and left a response. It gave me enough information to see I could tie this feature with React, in the spirit of the first demo from Bret Victor’s talk I mentioned earlier. I wrote an extremely hacky proof of concept by editing React source code and adding a bunch of global variables. I decided I won’t go to sleep until it works, and by 7am I had a demo I could tweet. Nobody cared about my Twitter before that, but this received some likes and retweets, and those 20 retweets were hugely validating. I knew then I’m not the only one who thinks this is exciting. This proof of concept was a throwaway effort and I didn’t have time to keep working on it at my job. I took a vacation, and finished off the prototype there. Quick disclaimer: again, I’m not saying you “need to” work nights or vacations. I’m not glorifying hustle, and there are plenty of people with great careers who did none of that. In fact, if I were better at time management, I could probably find a way to squeeze those non-interrupted hours in my regular day, or to learn to make progress with interruptions. I am sharing this because I’m telling my story, and it would be a lie to pretend I did everything in a 40 hour week. 2015 Our product went out of a private beta. It was growing, but slowly and linearly. The company was running out of funding and struggled to raise more money. And I wanted to spend more and more time on my open source projects. I also wanted to give my first conference talk. Naturally, I wanted to talk about hot reloading, but I knew somebody already mentioned it at ReactConf, and I thought people wouldn’t be excited about it. I decided to add some spice to my talk proposal by adding “with time travel” — again, inspired by Bret’s demo. The proposal got accepted, and for a few months I didn’t think much about it. In April, my salary got delayed by several weeks. It went through eventually, but I realized it’s time to look for a new job. I found some company using one of my projects, and they agreed to sponsor my work on it for a few months. My girlfriend asked if I wanted to get married. I said I thought I’d get married late in my 30s. She asked: “Why?” I couldn’t really find any justification for waiting so we soon bought rings and got married. Our wedding has cost us $100. The deadline for my talk was approaching. But I had no idea how to implement “time travel”. I knew that Elm had something similar, but I was scared to look at it because I worried I’d find out time travel can’t be implemented well in JS. At the time, there were many Flux libraries. I’ve tried a few of those, notably Flummox by Andrew Clark, and I had a vague sense that making hot reloading work with Flux would also let me implement time travel. Sunil’s gist led me to an idea: a variant of Flux pattern with a reducer function instead of a store. I had a neat name in mind for it already. And I really needed it for my talk! I implemented Redux just in time for my demo of time travel. My first talk rehearsal was on Skype. I was sweating, mumbling, and running over it too fast. At the end of my rehearsal, I asked the organizer if my talk was any good. He said “well… people like you” which I took as an euphemism for horrible. I asked a designer friend from the startup I just quit to help make my slides pretty. I added animations and transitions. The more polished my talk looked, the calmer and more confident I felt. I practiced it a dozen times. I flew to Paris for my first technical conference. This was probably the happiest day of my life. For the first time, I put faces next to avatars. There were UI nerds and my personal heroes around me. It felt like going to Hogwarts. My talk almost didn’t happen. In the morning, I found that my laptop refused to connect to the projector. I only had a few hours left. Christopher Chedeau was kind enough to lend me his laptop, and I transferred my live demo setup to his computer (except for the Sublime license, as you may know if you watched it). At first, my demo didn’t run on Christopher’s laptop because we had different Node versions. The conference WiFi was so bad that downloading another Node version was a non-starter. Luckily, I found an npm command that rebuilds the binaries. It saved my demo. I gave my talk with his computer, and it went well. I met many people in the audience who I already knew from Twitter. One of them was Jing Chen. I remembered her from our IRC chat on the React channel, and came to say hi. She asked whether FB recruiters contacted me before, and I said I couldn’t get a US visa. Jing asked whether I’m interested in working at the London office, and I had no idea there even was a London office! I called my wife and asked if she’s up for moving to London. I thought she’d hate the idea, but she immediately said yes. So I said yes to an interview. There were four people from FB at the conference, so Jing arranged a full interview right at the conference hotel. It was a regular interview process, except it was in Paris and everyone was sweaty because it was so hot outside. Everything happened so spontaneously that I neither had time to prepare, nor to get nervous. At one point I freaked out and panicked because I couldn’t write three lines of code that swap two items in an array. I asked Jing to look away for a few seconds. She said “I know you can swap two items”, and that gave me the confidence to finish the answer and make it through the interview. I probably didn’t pass with flying colors, but I got the offer. My talk got really popular. Andrew Clark has already deprecated Flummox — the most popular Flux library — in favor of Redux, which he co-wrote with me. People started putting Redux in production. New learners were confused by the README, which was written for early adopters who had all the existing context. I didn’t have a job, and it would take me several more months to get a UK visa. I started a Patreon to sustain my projects for a few months — and specifically to write Redux documentation, create small example apps, and record some free videos about it. I raised about $5k/month on Patreon which was more than any salary I made by that point in my life. Folks from Egghead sent me some mic gear, and I recorded my “Getting Started with Redux” course. I can’t watch it without cringing today, but it has been very popular and made me good money (around $3k/month in royalties) for quite a while — even though it was free. FB took care of handling most of the immigration process. My wife and I only had to fill some papers, and I had to take an English exam and do some health checks. FB did most of the work to relocate us, including moving our cat from Russia to UK (which cost around $5k). I was hired at engineering level 4, with initial base salary of $100k/year and an initial restricted stock unit grant of $125k vesting over four years. I also had a signing bonus of $18k which helped a lot as we were settling in. (By the way, tech salaries are lower in UK than in US.) We arrived to London at the end of November 2015. We’ve never been to London before. We took a black cab from the airport. We couldn’t figure out how to turn off the heating in the cab for ten minutes so we got very sweaty and couldn’t see anything through the window. When we turned off the fan and the window cleared up, our eyes were wide like saucers. London was beautiful. The next day, Roman Mazurenko got hit to death by a careless driver. He just got his US visa approved, and he came to Moscow to pick up his documents. He once told me that there’s something devilish about Moscow. It doesn’t just let you go. I would not see my friend again. Not in 2015, not ever. Roman has had a sort of a digital afterlife, courtesy of his friends. I know for a fact he would’ve loved the irony of having a two point five star App Store rating. 2016 New job. New city. New country. Different language. Unfamiliar accent. Big company. Orientation. Meeting rooms. Projects. Teams. Documents. Forms. Shit. Fuck. Fuck. Fuckety Fuck, Shit, Oh Dear, Blimey! I barely remember the first few months. I was in a constant state of stress from trying to understand what people are saying without subtitles. What the hell is my manager telling me? Is it rude to ask to repeat again? Spell it for me? What, I need to call a lady in Scotland to get a national insurance number by phone? I don’t get a word she’s saying. What even is the national insurance? Why do I have a “zero” tax code and why is my salary less than I expected? Wait, people actually pay taxes here? What do I do when I’m sick? What’s NHS? During my first trip to US in 2016 (part of onboarding), I forgot to eat the whole day, drank a lot of coffee, and had a full-fledged panic attack trying to explain how hot reloading works to a colleague. We called an ambulance, and I got a $800 bill (thankfully, paid by FB — or at least I don’t recall paying it myself). Relocation was nerve-wracking even though the company handled most of the difficulties. I thought I did everything in the onboarding instructions, but I forgot to register with the police. (I confused this with registering at the post office, which we also had to do.) I only found out that we screwed up months later, and we were told it might affect our visas. Luckily, it didn’t so far. Originally, I was supposed to join the React Native team in London. Usually, we hire people to go through Bootcamp and pick a team, but I didn’t have that freedom. I was preallocated. However, I wasn’t very excited about React Native in particular. I talked to Tom Occhino (he managed the React team at that time), and he suggested that I can join the React Core team (based in US) as the only UK member. I was already used to remote work from open source, so I agreed. In 2016, there was a React boom, but everybody made their own “boilerplate” with a bundler, watcher, compiler, and so on. React became synonymous with modular JavaScript, ES6, and all the tooling complexity. Christopher Chedeau suggested to prototype a command-line interface for getting started with React. We made the first version in a few weeks and released Create React App. 2017 Egghead courses continued to bring steady side income with royalties. I didn’t think twice before spending them on food delivery or nice clothes. It’s only in 2017 that I came to realize that these royalties are taxable as foreign income, and that I owe Her Majesty something like $30k. Oops. Like an adult, I got an accountant. Fixing this mess depleted all my savings again. At work, we spent most of 2017 rewriting React from scratch. You know the result of it as React 16. Sophie tells the story of how we did it here. Aside from taxes, there wasn’t a lot happening in my personal life. I was still settling in. I got less shy dealing with bureaucracies. I could make and take phone calls without freaking out. I watched movies without subtitles. I learned how to deal with NHS and with private insurance. I stopped doing side projects. 2018–2019 The last two years have been a blur. I’m still too close to them to have a clear perspective on what was important. Professionally, our projects are as demanding as ever. If you follow React, you know about some things we have been working on. I’ve grown as an engineer, and still have much to learn. Our London team has grown — I’m not alone now. People occasionally recognize me. This is humbling. Someone once recognized me in a sauna and started complaining about React. Please don’t be that person. I got promoted. I started this blog as a side project. I have another side project coming, which I’ve been musing about for the better part of these two years. I met more people from the internet and put more faces to avatars. This is fun. I always knew that I liked building UIs. I got hooked on Visual Basic. I spent this decade building UIs, and then building a way to build UIs. And then talking about it and explaining it. But I realize now that my drive to explain things is just as important to me as my drive to build. Perhaps, even more important. I look forward to doing more of that in the next decade. Or, should I say, this decade? Welcome to the twenties.</description>
      <pubDate>24 Mar 20 22:14 EDT</pubDate>
      <guid>https://overreacted.io/my-decade-in-review/</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.samaltman.com/the-days-are-long-but-the-decades-are-short</link>
      <description>&lt;a href=&#34;https://blog.samaltman.com/the-days-are-long-but-the-decades-are-short&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;﻿I turned 30 last week and a friend asked me if I&#39;d figured out any life advice in the past decade worth passing on.  I&#39;m somewhat hesitant to publish this because I think these lists usually seem hollow, but here is a cleaned up version of my answer:1) Never put your family, friends, or significant other low on your priority list.  Prefer a handful of truly close friends to a hundred acquaintances.  Don’t lose touch with old friends.  Occasionally stay up until the sun rises talking to people.  Have parties.2) Life is not a dress rehearsal—this is probably it.  Make it count.  Time is extremely limited and goes by fast.  Do what makes you happy and fulfilled—few people get remembered hundreds of years after they die anyway.  Don’t do stuff that doesn’t make you happy (this happens most often when other people want you to do something).  Don’t spend time trying to maintain relationships with people you don’t like, and cut negative people out of your life.  Negativity is really bad.  Don’t let yourself make excuses for not doing the things you want to do.3) How to succeed: pick the right thing to do (this is critical and usually ignored), focus, believe in yourself (especially when others tell you it’s not going to work), develop personal connections with people that will help you, learn to identify talented people, and work hard.  It’s hard to identify what to work on because original thought is hard.4) On work: it’s difficult to do a great job on work you don’t care about.  And it’s hard to be totally happy/fulfilled in life if you don’t like what you do for your work.  Work very hard—a surprising number of people will be offended that you choose to work hard—but not so hard that the rest of your life passes you by.  Aim to be the best in the world at whatever you do professionally.  Even if you miss, you’ll probably end up in a pretty good place.  Figure out your own productivity system—don’t waste time being unorganized, working at suboptimal times, etc.  Don’t be afraid to take some career risks, especially early on.  Most people pick their career fairly randomly—really think hard about what you like, what fields are going to be successful, and try to talk to people in those fields.5) On money: Whether or not money can buy happiness, it can buy freedom, and that’s a big deal.  Also, lack of money is very stressful.  In almost all ways, having enough money so that you don’t stress about paying rent does more to change your wellbeing than having enough money to buy your own jet.  Making money is often more fun than spending it, though I personally have never regretted money I’ve spent on friends, new experiences, saving time, travel, and causes I believe in.6) Talk to people more.  Read more long content and less tweets.  Watch less TV.  Spend less time on the Internet.7) Don’t waste time.  Most people waste most of their time, especially in business.8) Don’t let yourself get pushed around.  As Paul Graham once said to me, “People can become formidable, but it’s hard to predict who”.  (There is a big difference between confident and arrogant.  Aim for the former, obviously.)9) Have clear goals for yourself every day, every year, and every decade. 10) However, as valuable as planning is, if a great opportunity comes along you should take it.  Don’t be afraid to do something slightly reckless.  One of the benefits of working hard is that good opportunities will come along, but it’s still up to you to jump on them when they do.11) Go out of your way to be around smart, interesting, ambitious people.  Work for them and hire them (in fact, one of the most satisfying parts of work is forging deep relationships with really good people).  Try to spend time with people who are either among the best in the world at what they do or extremely promising but totally unknown.  It really is true that you become an average of the people you spend the most time with.12) Minimize your own cognitive load from distracting things that don’t really matter.  It’s hard to overstate how important this is, and how bad most people are at it.  Get rid of distractions in your life.  Develop very strong ways to avoid letting crap you don’t like doing pile up and take your mental cycles, especially in your work life.13) Keep your personal burn rate low.  This alone will give you a lot of opportunities in life.14) Summers are the best.15) Don’t worry so much.  Things in life are rarely as risky as they seem.  Most people are too risk-averse, and so most advice is biased too much towards conservative paths.16) Ask for what you want.  17) If you think you’re going to regret not doing something, you should probably do it.  Regret is the worst, and most people regret far more things they didn’t do than things they did do.  When in doubt, kiss the boy/girl.18) Exercise.  Eat well.  Sleep.  Get out into nature with some regularity.19) Go out of your way to help people.  Few things in life are as satisfying.  Be nice to strangers.  Be nice even when it doesn’t matter.20) Youth is a really great thing.  Don’t waste it.  In fact, in your 20s, I think it’s ok to take a “Give me financial discipline, but not just yet” attitude.  All the money in the world will never get back time that passed you by.21) Tell your parents you love them more often.  Go home and visit as often as you can.22) This too shall pass.23) Learn voraciously. 24) Do new things often.  This seems to be really important.  Not only does doing new things seem to slow down the perception of time, increase happiness, and keep life interesting, but it seems to prevent people from calcifying in the ways that they think.  Aim to do something big, new, and risky every year in your personal and professional life.25) Remember how intensely you loved your boyfriend/girlfriend when you were a teenager?  Love him/her that intensely now.  Remember how excited and happy you got about stuff as a kid?  Get that excited and happy now.26) Don’t screw people and don’t burn bridges.  Pick your battles carefully.27) Forgive people. 28) Don’t chase status.  Status without substance doesn’t work for long and is unfulfilling.29) Most things are ok in moderation.  Almost nothing is ok in extreme amounts.30) Existential angst is part of life.  It is particularly noticeable around major life events or just after major career milestones.  It seems to particularly affect smart, ambitious people.  I think one of the reasons some people work so hard is so they don’t have to spend too much time thinking about this.  Nothing is wrong with you for feeling this way; you are not alone.31) Be grateful and keep problems in perspective.  Don’t complain too much.  Don’t hate other people’s success (but remember that some people will hate your success, and you have to learn to ignore it). 32) Be a doer, not a talker.33) Given enough time, it is possible to adjust to almost anything, good or bad.  Humans are remarkable at this.34) Think for a few seconds before you act.  Think for a few minutes if you’re angry.35) Don’t judge other people too quickly.  You never know their whole story and why they did or didn’t do something.  Be empathetic.36) The days are long but the decades are short.</description>
      <pubDate>08 Feb 21 12:13 EST</pubDate>
      <guid>https://blog.samaltman.com/the-days-are-long-but-the-decades-are-short</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner</link>
      <description>&lt;a href=&#34;https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; July 30, 2019 How I became a machine learning practitioner For the first three years of OpenAI, I dreamed of becoming a machine learning expert but made little progress towards that goal. Over the past nine months, I’ve finally made the transition to being a machine learning practitioner. It was hard but not impossible, and I think most people who are good programmers and know (or are willing to learn) the math can do it too. There are many online courses to self-study the technical side, and what turned out to be my biggest blocker was a mental barrier — getting ok with being a beginner again. Studying machine learning during the 2018 holiday season. Early days # A founding principle of OpenAI is that we value research and engineering equally — our goal is to build working systems that solve previously impossible tasks, so we need both. (In fact, our team is comprised of 25% people primarily using software skills, 25% primarily using machine learning skills, and 50% doing a hybrid of the two.) So from day one of OpenAI, my software skills were always in demand, and I kept procrastinating on picking up the machine learning skills I wanted. After helping build OpenAI Gym, I was called to work on Universe. And as Universe was winding down, we decided to start working on Dota — and we needed someone to turn the game into a reinforcement learning environment before any machine learning could begin. Dota # Turning such a complex game into a research environment without source code access was awesome work, and the team’s excitement every time I overcame a new obstacle was deeply validating. I figured out how to break out of the game’s Lua sandbox, LD_PRELOAD in a Go GRPC server to programmatically control the game, incrementally dump the whole game state into a Protobuf, and build a Python library and abstractions with future compatibility for the many different multiagent configurations we might want to use. But I felt half blind. At Stripe, though I gravitated towards infrastructure solutions, I could make changes anywhere in the stack since I knew the product code intimately. In Dota, I was constrained to looking at all problems through a software lens, which sometimes meant I tried to solve hard problems that could be avoided by just doing the machine learning slightly differently. I wanted to be like my teammates Jakub Pachocki and Szymon Sidor, who had made the core breakthrough that powered our Dota bot. They had questioned the common wisdom within OpenAI that reinforcement algorithms didn’t scale. They wrote a distributed reinforcement learning framework called Rapid and scaled it exponentially every two weeks or so, and we never hit a wall with it. I wanted to be able to make critical contributions like that which combined software and machine learning skills. Szymon on the left; Jakub on the right. In July 2017, it looked like I might have my chance. The software infrastructure was stable, and I began work on a machine learning project. My goal was to use behavioral cloning to teach a neural network from human training data. But I wasn’t quite prepared for just how much I would feel like a beginner. I kept being frustrated by small workflow details which made me uncertain if I was making progress, such as not being certain which code a given experiment had used or realizing I needed to compare against a result from last week that I hadn’t properly archived. To make things worse, I kept discovering small bugs that had been corrupting my results the whole time. I didn’t feel confident in my work, but to make it worse, other people did. People would mention how how hard behavioral cloning from human data is. I always made sure to correct them by pointing out that I was a newbie, and this probably said more about my abilities than the problem. It all briefly felt worth it when my code made it into the bot, as Jie Tang used it as the starting point for creep blocking which he then fine-tuned with reinforcement learning. But soon Jie figured out how to get better results without using my code, and I had nothing to show for my efforts. I never tried machine learning on the Dota project again. Time out # After we lost two games in The International in 2018, most observers thought we’d topped out what our approach could do. But we knew from our metrics that we were right on the edge of success and mostly needed more training. This meant the demands on my time had relented, and in November 2018, I felt I had an opening to take a gamble with three months of my time. Team members in high spirits after losing our first game at The International. I learn best when I have something specific in mind to build. I decided to try building a chatbot. I started self-studying the curriculum we developed for our Fellows program, selecting only the NLP-relevant modules. For example, I wrote and trained an LSTM language model and then a Transformer-based one. I also read up on topics like information theory and read many papers, poring over each line until I fully absorbed it. It was slow going, but this time I expected it. I didn’t experience flow state. I was reminded of how I’d felt when I just started programming, and I kept thinking of how many years it had taken to achieve a feeling of mastery. I honestly wasn’t confident that I would ever become good at machine learning. But I kept pushing because… well, honestly because I didn’t want to be constrained to only understanding one part of my projects. I wanted to see the whole picture clearly. My personal life was also an important factor in keeping me going. I’d begun a relationship with someone who made me feel it was ok if I failed. I spent our first holiday season together beating my head against the machine learning wall, but she was there with me no matter how many planned activities it meant skipping. One important conceptual step was overcoming a barrier I’d been too timid to do with Dota: make substantive changes to someone else’s machine learning code. I fine-tuned GPT-1 on chat datasets I’d found, and made a small change to add my own naive sampling code. But it became so painfully slow as I tried to generate longer messages that my frustration overwhelmed my fear, and I implemented GPU caching — a change which touched the entire model. I had to try a few times, throwing out my changes as they exceeded the complexity I could hold in my head. By the time I got it working a few days later, I realized I’d learned something that I would have previously thought impossible: I now understood how the whole model was put together, down to small stylistic details like how the codebase elegantly handles TensorFlow variable scopes. After three months of self-study, I felt ready to work on an actual project. This was also the first point where I felt I could benefit from the many experts we have at OpenAI, and I was delighted when Jakub and my co-founder Ilya Sutskever agreed to advise me. Ilya singing karaoke at our company offsite. We started to get very exciting results, and Jakub and Szymon joined the project full-time. I feel proud every time I see a commit from them in the machine learning codebase I’d started. I’m starting to feel competent, though I haven’t yet achieved mastery. I’m seeing this reflected in the number of hours I can motivate myself to spend focused on doing machine learning work — I’m now around 75% of the number of coding hours from where I’ve been historically. But for the first time, I feel that I’m on trajectory. At first, I was overwhelmed by the seemingly endless stream of new machine learning concepts. Within the first six months, I realized that I could make progress without constantly learning entirely new primitives. I still need to get more experience with many skills, such as initializing a network or setting a learning rate schedule, but now the work feels incremental rather than potentially impossible. From our Fellows and Scholars programs, I’d known that software engineers with solid fundamentals in linear algebra and probability can become machine learning engineers with just a few months of self study. But somehow I’d convinced myself that I was the exception and couldn’t learn. But I was wrong — even embedded in the middle of OpenAI, I couldn’t make the transition because I was unwilling to become a beginner again. You’re probably not an exception either. If you’d like to become a deep learning practitioner, you can. You need to give yourself the space and time to fail. If you learn from enough failures, you’ll succeed — and it’ll probably take much less time than you expect. At some point, it does become important to surround yourself by existing experts. And that is one place where I’m incredibly lucky. If you’re a great software engineer who reaches that point, keep in mind there’s a way you can be surrounded by the same people as I am — apply to OpenAI! 1,182 Kudos 1,182 Kudos </description>
      <pubDate>24 Mar 20 12:31 EDT</pubDate>
      <guid>https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.drmaciver.com/2007/12/no-seriously-why-scala/</link>
      <description>&lt;a href=&#34;https://www.drmaciver.com/2007/12/no-seriously-why-scala/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Recently an article called Why Scala? was posted on reddit. It’s an ok introduction to the language, but the very fair observation was made that it’s much more of a “What is Scala?” than a “Why Scala?”. I thought I’d share my thoughts on the subject. Mainly because I like hearing (reading) myself talk (write). :-) Quick background: I initially learned to program in standard ML while at university (self taught, mostly, with the help of some friends doing computer science. I was doing maths). On graduating I then switched tracks entirely and started doing Java web development in a small software firm in London (I’ve since switched again, but I’m still doing Java professionally). I’ve also dabbled and read a lot with computer science and programming languages in my spare time since then, filling in the gaps that not having done any real computer science at university left me. My train of thought on the language switch from ML to Java was basically: a) Wow, this is different. b) Ugh. Where are my higher order functions? c) Where are all these random exceptions coming from?? I’ve compiled the code successfully, isn’t it supposed to work now? d) Hmm. But there’s some useful stuff here too. Scala’s a nice way to scratch both itches, and adds some very interesting features and functionality of its own. It’s not my favourite language (I don’t really have one. All languages suck. It’s just that some of them suck less in interesting ways), but it has a lot I like. Here’s a brain dump of some of it. Things I like: Object oriented programming I know, it’s so entrenched it’s past even bothering with its buzzword status. But object oriented programming has a lot of advantages for medium to large scale composition. It has some disadvantages, and frankly sucks at small scale composition of functionality (which is where functional programming shines), but it allows for some very nice pluggability. Module oriented programming ML has higher order modules. I never really used them much when I was programming it more often (mostly because I was only writing enough code to do some simple maths projects. I never wrote anything large scale), but having looked into them in more details since they’re really powerful. They’re essentially a different take on the composition that object orientation provides. Where object orientation resolves everything dynamically, ML’s higher order modules resolve everything statically. This introduces some limitations in flexibility but makes up for them in power and type safety – they provide a much more flexible and interesting abstraction over a type than mere subclassing and interfaces can. Scala has both. Further, it has both and lo and behold they are the same thing. Objects are modules, and can declare their own types (note: This is much more than just declaring an inner class in Java is), imported, etc. Modules are objects and can be instantiated at runtime, extended, etc. You lose a bit of the static guarantees that ML modules but you gain a lot of flexibility from both sides. Static Typing I’ve written too much Java to not like static typing. Wait, I know that sounds like a non sequitur, but read on. I’ve written too much Java and seen flagrantly stupid and really subtle runtime errors that should never have made it past the compiler coming out of it to not like static typing. NullPointerException, ClassCastException, argh. If you’ve written enough code in a language like ML, OCaml or Haskell you will know that the compiler is your friend. And, like all good friends, it will yell at you if you do something stupid and then help you pick up the pieces. Scala doesn’t quite manage that. If you write code in just the right way you can achieve that level of guarantee (and in some cases, more. But that tends to be the result of abuse of the type system by deranged maniacs), but the combination of subtyping and some Java interoperability decisions mean that it’s not quite as good. It’s not bad though. So: I like object oriented programming, I like static typing. It logically follows that I must like statically typed object oriented languages, right? Well, in principle, yes. But Scala is the first one I’ve met with a type system that didn’t suck. Scala’s traits (a sort of mixin) are so much better to work with than interfaces, the generics work properly, provide variance annotations, etc. A reasonable subset of the types are inferred. Compared to the type systems of Java, C# and C++ it’s a dream (it’s not as nice as the type systems of the statically typed functional languages I know of. Subtyping seems to cause issues, with a lot of research still needed to make it work well, and Scala seems to have largely ignored what prior work there was Hindley-Milner style type systems with subtyping) Functional programming You’ve all been dreading this section. “Oh no. Now he’s going to enthuse about how marvelous functional programming is and how it’s going to cure cancer”. Nope. Can’t be bothered. Functional programming is nice. If you don’t believe that, I’m not going to try to convince you of it. Scala’s support for functional programming is ok. It has some warts, but it also has some nice points, and it generally works well and isn’t too verbose. I’m not going to get any more excited about its presence than I am about the fact that my bike has wheels (but I’d be pretty pissed off if my bike didn’t have wheels). Higher order functions, pattern matching, etc. It’s all there. It works. Moving on swiftly… Implicits Scala offers a bag of features under the keyword ‘implicit’. This is one of those things that makes you go “Oh, that’s cute” when you first see it and then go “Wow, that’s powerful” six months later. Essentially implicits give you statically guaranteed and provided dynamic scoping. You say “I need a Foo. I don’t care where it comes from”, the compiler says “Here you go” or “Sorry, no Foos today”. These can be objects, implicit conversions between types (You know the way Ints get implicitly converted to longs, double, etc in Java? Scala does that too, but it’s all programmer definable. They’re just library functions in scala.Predefined). If you remember what I said about Scala objects being modules and you’ve read this paper a little light might just have gone on in your brain. If you haven’t read it and don’t want to, here’s the summary version: Implicit function arguments + first class modules gives you something that looks and quacks very much like Haskell type classes (yes, I know this isn’t actually what the paper says, but it follows from it). Mmm. These are the big things to like about Scala. Here are a few little things: Sane constructor/class semantics. If you’ve written a lot of Java there’s a good chance you hate its constructor system. Scala’s is much nicer. Expression oriented code. Everything is an expression. You can form compound expressions trivially – { val foo = bar(); baz(foo, foo); } is an expression which evaluates to baz(foo, foo). Sanely uniform scope. Pretty much anything you can do inside a method you can do inside an object and vice versa. Things are for the most part lexically scoped in the right way. The primitive/object divide is much less irritating. Primitives get a few special treatments at the language level, but mostly they’re just objects. When things should compile to use primitives, they do. When the primitives need to be boxed, they will be. It’s almost entirely transparent. Performance. Scala generates very good (well. ‘good’. Java-like) bytecode, which means it gets to take advantage of most of the optimizations the JVM is willing to throw its way. Further it puts a reasonable amount of its own work into performing optimisations on the bytecode, etc so you get those nice juicy abstractions without much overhead. There’s essentiall y no performance penalty for choosing Scala over Java etc. Scala’s far from perfect. It has some syntactic weirdnesses, a few issues carried over from Java, a moderately buggy compiler and a host of little features and edge cases that are really hard to keep in your head. However, I find that these issues don’t actually do more than annoy you from time to time. The core language is powerful and very useful for just sitting down and writing good code in. </description>
      <pubDate>27 Apr 20 15:15 EDT</pubDate>
      <guid>https://www.drmaciver.com/2007/12/no-seriously-why-scala/</guid>
    </item>
    <item>
      <title></title>
      <link>https://osu.instructure.com/courses/71145/pages/week-15-class-31</link>
      <description>&lt;a href=&#34;https://osu.instructure.com/courses/71145/pages/week-15-class-31&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; NOTE: In addition to the cases below, a new possible source of this error is the use of Apple&#39;s iCloud Private Relay feature, available on newer iPhone, iPad, and Mac devices. This feature is not compatible with the university&#39;s Web Login Service at this time, so you must disable that feature while using services that require a login. You may be seeing this page because you waited an extended period of time to submit the login form. After a set period, the server abandons your original request to save resources, and you&#39;ll need to return where you started from and try and login again. You may also be seeing this page because you used the Back button while browsing a secure web site or application, causing the original web request for login to be &#34;replayed&#34;. Alternatively, you may have mistakenly bookmarked the web login form instead of the actual web site you wanted to bookmark, or used a link created by somebody else who made the same mistake. Left alone, this can cause errors on some browsers (such as Safari) or result in you returning to the web site you tried to leave, so this page is presented instead so that you can continue safely with your online activities. Note that you may need to hit the Back button multiple times to skip over all of the login-related pages in your history list. If the problem is a bookmark, you&#39;ll probably notice that the bookmarked location contains a long URL that starts with https://webauth.service.ohio-state.edu.... If so, you should delete and recreate (or edit) the bookmark to contain the web address of the site you want to access. For example, to bookmark the Carmen web site, you would create a bookmark for https://carmen.osu.edu If you&#39;re unable to avoid seeing this message, please contact the maintainer of the web site containing the link you&#39;re starting from so that they can investigate the problem. Be sure to provide the surrounding context and explain what you&#39;re trying to access so that they can find and correct the offending link. Remember to clear your browser history to avoid improper access to whatever web sites you&#39;ve been using if this is a shared machine, and never leave your own machine unlocked and unattended. </description>
      <pubDate>21 Apr 20 18:37 EDT</pubDate>
      <guid>https://osu.instructure.com/courses/71145/pages/week-15-class-31</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.bloomberg.com/news/features/2021-06-15/airbnb-spends-millions-making-nightmares-at-live-anywhere-rentals-go-away</link>
      <description>&lt;a href=&#34;https://www.bloomberg.com/news/features/2021-06-15/airbnb-spends-millions-making-nightmares-at-live-anywhere-rentals-go-away&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Why did this happen? Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. Need Help? For inquiries related to this message please contact our support team and provide the reference ID below. Block reference ID: </description>
      <pubDate>16 Jun 21 10:52 EDT</pubDate>
      <guid>https://www.bloomberg.com/news/features/2021-06-15/airbnb-spends-millions-making-nightmares-at-live-anywhere-rentals-go-away</guid>
    </item>
    <item>
      <title></title>
      <link>https://sahandsaba.com/understanding-sat-by-implementing-a-simple-sat-solver-in-python.html</link>
      <description>&lt;a href=&#34;https://sahandsaba.com/understanding-sat-by-implementing-a-simple-sat-solver-in-python.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Understanding SAT by Implementing a Simple SAT Solver in Python Introduction SAT is short for &#34;satisfiability&#34;. Chances are you have heard of it or one of its variants like 3-SAT in passing, especially in discussions of complexity and NP-completeness. In this post, we will go into details of what it is all about, why it is of such importance from both a theoretical and practical perspective, and how to approach solving it by developing a simple Python SAT solver. By the end of this post, we will have a working SAT solver with a command-line interface. The code for it is on GitHub: https://github.com/sahands/simple-sat. Feel free to fork and contribute improvements. Of course, our implementation will not be anywhere close to more complicated SAT solvers implemented in C or C++, such as miniSAT. The focus here is on simplicity since the code is to be an introduction to SAT and SAT solvers. Sections marked with * are more theoretical and not required for understanding the algorithm we will use. On the other hand, the rest of the introduction section below can be skipped if you already know the problem definition and relevant technical terms. Non-Technical Definitions &amp; Example Before we start with the definitions, you might be asking why SAT is written in all capitals if it is not an acronym. Well, great question. SAT happens to fall under what are called decision problems in computer science. What that means is that the answer to a particular instance of the problem is either &#34;yes&#34; or &#34;no&#34;. Decision problems are often simply identified with the set of inputs for which the answer is &#34;yes&#34;, and that set is given a capitalized name. For example, SAT is the set of all satisfiable CNF expressions, and PRIMES is the set of all prime numbers (the decision problem in the latter is that of primality; i.e. given the binary representation of number nn , decide if it is a prime or not). To go on a bit of a tangent, this is also the reason that the title of the paper that introduced the AKS primality test (&#34;PRIMES is in P&#34;) is not a silly grammar mistake; PRIMES is a set and the paper shows that it is in P, which is the set of decision problems solvable in polynomial-time. This naming style, as far as I know, is mainly due to Garey and Johnson&#39;s classic textbook on complexity theory. So, back to SAT. So far we mentioned that SAT is a decision problem, and something about mysterious sounding &#34;CNF expressions&#34;. Now, if you happen to know your Boolean logic and already know all about satisfiability and CNF expressions, then feel free to skip ahead to next section. The rest of this section assumes no prior knowledge of logic. Like many other interesting problems, there are a variety of ways of describing SAT, some more technical and some less. Here I will provide a very non-technical description of the problem that nonetheless is an accurate description. Assume you are in charge of elections in a society. Elections in this society work as follows: there are nn candidates, and any number of them, from 00 (nobody) to nn (everybody) can be elected as the result of the elections. Each voter provides a list of candidates they want elected and candidates they want not elected. For example, if we call the candidates A, B and C, then one vote might be &#34;A, B, not C&#34;. We say a voter will be satisfied with the results of the election if at least one of his/her preferences is met. For example, the voter with the &#34;A, B, not C&#34; vote will be satisfied if either A or B is elected, or if C is not elected. To be clear, that voter will be happy even if nobody is elected (anarchy!) because one of the preferences is &#34;not C&#34; which is met if we do not pick anyone. It&#39;s also possible to receive an empty vote. We take this to mean that the voter will not be satisfied regardless of who is elected. You are given all the votes, and your job is to determine if all the voters can be satisfied or not, and if yes, provide at least one possible pick of candidates that would satisfy everybody. We assume that each candidate is represented by a unique identifier that will be a string in the input. For the votes, we will write just the string representing candidate xx to indicate the voter wants the candidate elected, and ∼x\sim{}x to indicate the voter wants xx not elected. Let&#39;s look at an example. Assume the list of votes is given as follows, one per line: Then choosing to elect just candidates AA and CC but not BB will satisfy all the voters. Take a moment to convince yourself that no other choice of candidates (there are a total of 23=82^3 = 8 possibilities) can satisfy everyone. It is easy to see that in general the search-space is of size 2n2^n where nn is the number of candidates. Technical Terminology Now that the problem makes sense, let&#39;s define the technical vocabulary. First, what we called &#34;candidates&#34; are called variables. The variables in the above example are AA , BB and CC . A variable can be assigned true or false. A literal is a variable or its negation. For example AA and ∼A\sim A are literals. Literals without the ∼\sim are called positive, pure, or unnegated literals. Literals with ∼\sim are called negated literals. A set of literals is called a clause. An assignment is a mapping of variables to true or false. For example, the assignment that satisfied the clauses in the previous example was given by A=trueA = true , B=falseB = false and C=trueC = true . A clause is satisfied by an assignment if at least one of its unnegated literals is assigned true by the assignment, or one of its negated literals is assigned false in the assignment. It is customary, in logic notation, to separate the literals in a clause using the ∨\vee symbol, read &#34;or&#34;. For example, the first clause above is written as A∨B ∨∼CA \vee B ~ \vee \sim C in mathematical notation. So SAT can be summarized as follows: given a list of clauses, determine if there exists an assignment that satisfies all of them simultaneously. It is also worthy of mention that there is a variation of SAT called 3-SAT with the restriction that each clause consists of at most 3 (distinct) literals. It can be shown with relative ease that SAT is in fact reducible to 3-SAT. A Simple SAT Solver In Python Even though SAT is NP-complete and therefore no known polynomial-time algorithm for it is (yet) known, many improvements over the basic backtracking algorithms have been made over the last few decades. However, here we will look at one of the most basic yet relatively efficient algorithms for solving SAT. The encoding and the algorithm are based on Knuth&#39;s SAT0W program which you can download from his programs page. The algorithm is a watch-list based backtracking algorithm. What makes the watch-list based algorithms particularly simple, as we will see, is that very little (practically nothing) needs to be done to &#34;undo&#34; steps taken when we need to backtrack. Parsing &amp; Encoding The Input Before we can approach solving a SAT instance, we need to be able to represent the instance in memory. Let&#39;s remember that a SAT instance is a set of clauses, and each clause is a set of literals. Finally, a literal is a variable that is either negated or not. Of course, we can just store the instance as a list of clauses, with each clause being a list of strings that are the literals. The problem with this approach is that we will not be able to quickly look up variables, and checking to see if a literal is negated or not, and negating it if not, would be rather slow string operations. Instead, we will first assign a unique number, starting from 00 and counting up, to each variable as we encounter them, using a dictionary to keep track of the mapping. So variables will be encoded as numbers 00 to n−1n-1 where nn is the number of variables. Then for an unnegated literal with variable encoded as number xx we will encode the literal as 2x2x , and the negated one will be 2x+12x + 1 . Then a clause will simply be a list of numbers that are the encoded literals, and Let&#39;s look at an example first. For this, let&#39;s see how the code that we will look at in a minute behaves: &gt;&gt;&gt; from satinstance import SATInstance &gt;&gt;&gt; s = SATInstance() &gt;&gt;&gt; s.parse_and_add_clause(&#39;A B ~C&#39;) &gt;&gt;&gt; s.variables [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] &gt;&gt;&gt; s.variable_table {&#39;A&#39;: 0, &#39;C&#39;: 2, &#39;B&#39;: 1} &gt;&gt;&gt; s.clauses [(0, 2, 5)] So as you see, the clause A∨B∨∼CA \vee B \vee \sim C is encoded as the tuple (0, 2, 5) since variable AA is assigned number 00 , and hence literal AA is 2⋅0=02 \cdot 0 =0 . On the other hand, ∼C\sim C is encoded as 55 since CC is assigned 22 and hence ∼C\sim C is encoded as 2⋅2+1=52 \cdot 2 + 1 = 5 . Why the funny encoding, you ask? Because it has a few advantages: we can keep track of variables by keeping a list of length nn , and of literals by keeping a list of length 2n2n , checking to see if a literal is negated or not is simple: just do a bit-wise AND with 11 , that is x &amp; 1 == 0, looking up the variable in a literal is a matter of dividing by two, which is the same as a bit-wise shift to the right, that is v = x &gt;&gt; 1, switching a literal from negated to unnegated and back can be done by doing a bit-wise XOR with the number one, that is negate(x) = x ^ 1, and finally going from a variable to a literal can be done by doing a bit-wise shift to the right (and a bit-wise OR with 1 if negated), that is x = v &lt;&lt; 1 or x = v &lt;&lt; 1 | 1. Notice that all of the above can be done using bit-wise operations which are generally very fast to do. And since these operations will be happening an exponential number of times, we will take any performance boost we can get. With this, we are ready to write the code that takes care of reading an input file and encoding the clauses. Here it is: class SATInstance(object): def parse_and_add_clause(self, line): clause = [] for literal in line.split(): negated = 1 if literal.startswith(&#39;~&#39;) else 0 variable = literal[negated:] if variable not in self.variable_table: self.variable_table[variable] = len(self.variables) self.variables.append(variable) encoded_literal = self.variable_table[variable] &lt;&lt; 1 | negated clause.append(encoded_literal) self.clauses.append(tuple(set(clause))) def __init__(self): self.variables = [] self.variable_table = dict() self.clauses = [] @classmethod def from_file(cls, file): instance = cls() for line in file: line = line.strip() if len(line) &gt; 0 and not line.startswith(&#39;#&#39;): instance.parse_and_add_clause(line) return instance def literal_to_string(self, literal): s = &#39;~&#39; if literal &amp; 1 else &#39;&#39; return s + self.variables[literal &gt;&gt; 1] def clause_to_string(self, clause): return &#39; &#39;.join(self.literal_to_string(l) for l in clause) def assignment_to_string(self, assignment, brief=False, starting_with=&#39;&#39;): literals = [] for a, v in ((a, v) for a, v in zip(assignment, self.variables) if v.startswith(starting_with)): if a == 0 and not brief: literals.append(&#39;~&#39; + v) elif a: literals.append(v) return &#39; &#39;.join(literals) As you can see, we also include methods here to decode variables, literals, clauses, and assignments. These are used for outputting logging messages as well as the final solutions. Keeping Track Of The Assignment Our algorithm will be a backtracking algorithm, in which we will assign true or false to all the variables, starting from variable 00 and going in order to variable n−1n-1 . Of course, the basic search space is of size 2n2^n but by pruning, we will not explore the whole space (usually anyway). The assignment will be kept as a list of length nn , with item at index ii being None if neither true or false has been assigned variable ii , and 00 (false) or 11 (true) otherwise, depending on the assignment. When we backtrack, we set the corresponding item in the assignment list back to None to indicate it is no longer assigned. Watch-lists Now that we have the encoding in place, and know how to keep track of the assignment, let&#39;s look at the key idea of our algorithm. For each clause to be satisfied, it needs to have at least one of its literals satisfied. As such, we can make each clause watch one of its literals, and ensure that the following invariant is maintained throughout our algorithm: Invariant All watched literals are either not assigned yet, or they have been assigned true. We then proceed to assign true or false to variables, starting from 00 to n−1n-1 . If we successfully assign true or false to every variable while maintaining the above variant, then we have an assignment that satisfies every clause. To maintain this invariant, any time we assign true or false to a variable, we ensure to update the watch-list accordingly. To do this efficiently, we need to keep a list of clauses that are currently watching a given literal. This is done in the code below using a list of length 2n2n of double-ended queue (collections.deque), with each clause initially watching the first literal in it. The function below takes care of this setting up of the watch-list: def setup_watchlist(instance): watchlist = [deque() for __ in range(2 * len(instance.variables))] for clause in instance.clauses: # Make the clause watch its first literal watchlist[clause[0]].append(clause) return watchlist Why double-ended queues instead of just a list? Short answer is that after experimenting, I found out that double-ended queues provided the best performance. Back to the algorithm, whenever we assign true to a variable xx we must make clauses watching ∼x\sim x watch something else. And similarly, whenever we assign false to a variable xx we make clauses watching xx watch something else. If we can not make a clause watch something, which happens when all the other literals in a clause have already been assigned false, then we know that the current assignment contradicts the clause, and we stop and backtrack. We only need one clause to be contradicted to know not to go any further. As such, the heart of our algorithm will be where we update the watch-list after an assignment has been made. The Python function below, which is in (watchlist.py), implements this part of the algorithm: def update_watchlist(instance, watchlist, false_literal, assignment, verbose): &#34;&#34;&#34; Updates the watch list after literal &#39;false_literal&#39; was just assigned False, by making any clause watching false_literal watch something else. Returns False it is impossible to do so, meaning a clause is contradicted by the current assignment. &#34;&#34;&#34; while watchlist[false_literal]: clause = watchlist[false_literal][0] found_alternative = False for alternative in clause: v = alternative &gt;&gt; 1 a = alternative &amp; 1 if assignment[v] is None or assignment[v] == a ^ 1: found_alternative = True del watchlist[false_literal][0] watchlist[alternative].append(clause) break if not found_alternative: if verbose: dump_watchlist(instance, watchlist) print(&#39;Current assignment: {}&#39;.format( instance.assignment_to_string(assignment)), file=stderr) print(&#39;Clause {} contradicted.&#39;.format( instance.clause_to_string(clause)), file=stderr) return False return True So why the watch-list based approach? The main reason is the simplicity it affords us. Since during a backtracking step, assignments only go from 00 or 11 to None, the watch-list does not need to be updated at all to maintain the invariant. This means the backtracking step will simply be changing the assignment of a variable back to None and that&#39;s it. Putting It All Together We are now ready to put it all together to get a simple recursive algorithm for solving SAT. The steps are simple: try assigning 00 to variable dd , update the watch-list, if successful, move on to variable d+1d+1 . If not successful, try assigning 11 to variable dd and update the watch-list and continue to variable d+1d+1 . If neither succeed, assign None to variable dd and backtrack. Here is the code: def solve(instance, watchlist, assignment, d, verbose): &#34;&#34;&#34; Recursively solve SAT by assigning to variables d, d+1, ..., n-1. Assumes variables 0, ..., d-1 are assigned so far. A generator for all the satisfying assignments is returned. &#34;&#34;&#34; if d == len(instance.variables): yield assignment return for a in [0, 1]: if verbose: print(&#39;Trying {} = {}&#39;.format(instance.variables[d], a), file=stderr) assignment[d] = a if update_watchlist(instance, watchlist, (d &lt;&lt; 1) | a, assignment, verbose): for a in solve(instance, watchlist, assignment, d + 1, verbose): yield a assignment[d] = None Making It Iterative * For fun, let&#39;s see if we can implement the above algorithm without recursion. This is in fact how Knuth implements the algorithm. (He seems to dislike recursion, see for example this story on Quora.) The basic idea here is to manually keep track of the current state of the backtrack tree. When we use recursion, the state is kept implicitly using the stack and which instruction is executing in each of the function calls. In the iterative case, we will store the state using d which is the current depth of the backtrack tree we are currently in, and also the variable we are to assign to currently, and the state list which keeps track of which assignments for each variable have been tried so far. Here is the code: def solve(instance, watchlist, assignment, d, verbose): &#34;&#34;&#34; Iteratively solve SAT by assigning to variables d, d+1, ..., n-1. Assumes variables 0, ..., d-1 are assigned so far. A generator for all the satisfying assignments is returned. &#34;&#34;&#34; # The state list wil keep track of what values for which variables # we have tried so far. A value of 0 means nothing has been tried yet, # a value of 1 means False has been tried but not True, 2 means True but # not False, and 3 means both have been tried. n = len(instance.variables) state = [0] * n while True: if d == n: yield assignment d -= 1 continue # Let&#39;s try assigning a value to v. Here would be the place to insert # heuristics of which value to try first. tried_something = False for a in [0, 1]: if (state[d] &gt;&gt; a) &amp; 1 == 0: if verbose: print(&#39;Trying {} = {}&#39;.format(instance.variables[d], a), file=stderr) tried_something = True # Set the bit indicating a has been tried for d state[d] |= 1 &lt;&lt; a assignment[d] = a if not update_watchlist(instance, watchlist, d &lt;&lt; 1 | a, assignment, verbose): assignment[d] = None else: d += 1 break if not tried_something: if d == 0: # Can&#39;t backtrack further. No solutions. return else: # Backtrack state[d] = 0 assignment[d] = None d -= 1 Theoretical and Practical Significance * All right, so SAT is a cool problem, sure; possibly even useful. But why is it given so much importance? The short answer is that many other problems, often &#34;difficult&#34; problems, can be reduced to SAT. Let&#39;s consider an example first, and then look at Stephen Cook&#39;s result that established SAT as the first NP-complete problem, to get a sense of both practical applications of SAT, and its theoretical importance. Four Colouring * You might have heard of the &#34;four colour theorem&#34;. In simplest terms, it states that the regions in any map can be coloured using at most four colours such that no two neighbouring regions are coloured the same. See the Wikipedia page on it for more details. This lends itself to a simple decision problem: given a map, is it possible to colour it using 4 or less colours such that no two neighbouring regions are the same colour? The four colour theorem is then true if and only if the answer to this decision problem is always true (provided the input map meets the requirements of a planar graph, a detail we are not too concerned with here). As input, we will take the number of regions nn , and assume the regions are labelled using numbers 11 to nn , and a list of neighbouring regions of the form {i,j}\{i, j\} with i̸=ji \ne j , indicating regions ii and jj are neighbours. Let us use colours red (R), blue (B), green (G), and yellow (Y) to colour the regions. Our variables are going to be RiR_i , BiB_i , GiG_i and YiY_i , for 1≤i≤n1 \le i \le n , indicating that region ii is coloured red, blue, green, or yellow, respectively. Next, we need to construct the right set of clauses such that if all of them are satisfied, then we have a proper colouring of the map. Specifically, we need every region to be coloured, and we need no two neighbouring regions to be the same colour. First, let us construct the clauses that will make sure every region has one and only one colour assigned to it. For this, we need to make sure only one of RiR_i , BiB_i , GiG_i or YiY_i is picked for our assignment at a time. We can express this in terms of KK clauses for each region ii . First, we add Ri∨Bi∨Gi∨YiR_i \vee B_i \vee G_i \vee Y_i as a clause, which ensures that region ii gets at least one colour assigned to it. Then for pair of colours, say RR and BB , we add the clause ∼Ri∨∼Bi\sim R_i \vee \sim B_i which basically says &#34;not both of RiR_i and BiB_i can be picked at the same time&#34;, effectively making sure that exactly one colour is assigned to each region. Finally, for any two neighbouring regions, say ii and jj , and each colour, say RR , we add the clause ∼Ri∨∼Rj\sim R_i \vee \sim R_j which says not both of ii and jj can be coloured red. Let&#39;s look at a very simple example. Suppose our map has only two regions, regions 11 and 22 and that they are neighbours. Then our SAT input would be: # Assign at least one colour to region 1 R1 B1 G1 Y1 # But no more than one colour ~R1 ~B1 ~R1 ~G1 ~R1 ~Y1 ~B1 ~G1 ~B1 ~Y1 ~G1 ~Y1 # Similarly for region 2 R2 B2 G2 Y2 ~R2 ~B2 ~R2 ~G2 ~R2 ~Y2 ~B2 ~G2 ~B2 ~Y2 ~G2 ~Y2 # Make sure regions 1 and 2 are not coloured the same since they are neighbours ~R1 ~R2 ~B1 ~B2 ~G1 ~G2 ~Y1 ~Y2 Running this through our SAT solver gives: $ python sat.py --brief --all &lt; tests/colouring/01.in Y1 G2 Y1 B2 Y1 R2 G1 Y2 G1 B2 G1 R2 B1 Y2 B1 G2 B1 R2 R1 Y2 R1 G2 R1 B2 As you can see, there are many possible solutions, since in such a simple case we have a valid colouring as long as we assign a different colour to each region, which can be done in 4⋅3=124 \cdot 3 = 12 ways, corresponding precisely to the 1212 solutions given by our SAT solver. In the next section, we see that a much broader set of problems can be reduced to SAT. In general, the decision problem of the above example is known as graph colouring, or GT4 in Garey-Johnson&#39;s naming, where given a graph and a number kk the decision problem is to determine if a kk -colouring for the graph exists. In the above, we had k=4.k=4. In this more general definition, with nn regions, our reduction to SAT involves introducing k⋅nk\cdot n variables and 1+n⋅(k2)+k⋅e 1 + n \cdot \binom{k}{2} + k \cdot e clauses, where ee is the number of edges. Since e=O(n2)e = O(n^2) (in fact, e=O(n)e = O(n) for planar graphs), the number of variables and clauses in our construction above are polynomials in nn and kk . Hence we have a polynomial-time reduction to SAT. The significance of this is discussed further in the next section. NP-Completeness Of SAT * In previous section we saw how a problem regarding colouring of regions in a map can be reduced to SAT. This can be further generalized to much larger class of problems: any decision problem that can be decided in polynomial time using a non-deterministic Turing machine can be reduced in polynomial time to SAT. This was first proved in Stephen Cook&#39;s paper &#34;The Complexity of Theorem-Proving Procedures&#34;, which is the paper that introduced the famous P = NP question as well. Let&#39;s go over the basic idea in the paper very briefly here. If you are interested in more details, make sure you have a look at the paper, as it is rather short and a pleasure to read. But before we go into detail, let us take a moment to discuss why it is of such importance. First, nobody has yet come up with an efficient (polynomial time) algorithm to solve SAT in its generality. (SAT with some restrictions, e.g. 2-SAT, can be solved efficiently though.) Showing that a problem can be reduced to SAT means that if we find an efficient algorithm for SAT then we have found an efficient algorithm for that problem as well. For example, if we find a polynomial-time algorithm for SAT then we immediately have a polynomial-time algorithm for the graph colouring problem given above. Now, the class of decision problems that can be solved in polynomial-time using a non-deterministic Turing machine is known as NP (which stands for Non-deterministic Polynomial). This is a very large class of problems, since Turing machines are one of the most general computational models we have, and even though we are limited to polynomial-time Turing machines, the fact that the Turing machine does not have to be deterministic allows us much more freedom. Some examples of problems that are in NP are: all problems in P, e.g. determining if a number is prime or not (PRIMES), and decision versions of shortest path, network flow, etc., integer factorization, graph colouring, SAT, and all NP-complete problems (see here for a rather large list of examples). A problem is said to be NP-complete if it, in addition to being in NP, also has the property that any other problem in NP can be reduced to it in polynomial-time. Cook&#39;s paper proved SAT to be NP-complete. In fact, since that paper introduced the concept of NP-completeness, SAT was the first problem to be proved NP-complete. Since then, many other problems have been shown to be NP-complete, often by showing that SAT (or 3-SAT) can be reduced in polynomial-time to those problems (converse of what we proved earlier for graph colouring). Now, as promised, let&#39;s briefly look at why SAT is NP-complete. For this, we need to know more precisely what a Turing machine is. Unfortunately, this would involve a bit more detail than I want to include in this section. So instead, I am going to show that if a problem can be solved using a finite-state machine (FSM) then it be reduced in polynomial-time to SAT. The case for Turing machines, which are a generalizations of finite-state machines (Turing machines are basically FSM&#39;s with the addition of a tape that they can read from and write to), is quite similar, just more complicated. I encourage you to read Cook&#39;s original paper for details of the proof with Turing machines. First, let&#39;s define what an FSM is. In simplest terms, an FSM is a program that has a finite number of states, and that when fed an input character, moves to another state (or possibly stays in the same state) based on a fixed set of rules. Also, some states are taken as &#34;accepting&#34; states. Given an input string, we feed the string character by character into the FSM, and if at the end the FSM is in an accepting state, the answer to our decision problem is yes. If not, the answer is no. The below code shows how an FSM could can be implemented in Python. Note that in this implementation, we are forced to have a deterministic FSM. Let&#39;s ignore this detail for now though. This particular example implements an FSM that accepts input strings that contain an even number of ones. from __future__ import print_function def even_ones(s): # Two states: # - 0 (even number of ones seen so far) # - 1 (odd number of ones seen so far) rules = {(0, &#39;0&#39;): 0, (0, &#39;1&#39;): 1, (1, &#39;0&#39;): 1, (1, &#39;1&#39;): 0} # There are 0 (which is an even number) ones in the empty # string so we start with state = 0. state = 0 for c in s: state = rules[state, c] return state == 0 # Example usage: s = &#34;001100110&#34; print(&#39;Output for {} = {}&#39;.format(s, even_ones(s))) So the core of an FSM is a list of rules of the form (S,c)→T(S, c) \rightarrow T which says if the FSM is in state SS and receives input character cc then it goes to state TT . If for any unique pair of (S,c)(S, c) there is only one rule (S,c)→T(S, c) \rightarrow T then the FSM is said to be deterministic. This is because the FSM will never need to make a &#34;choice&#34; as to which of the rules to apply. With non-deterministic FSM&#39;s, the definition of acceptance needs to be modified a bit: if any set of choices of rules would get us to an accepting state given an input then the input is said to be accepted. It is a well-established result in Automata theory that deterministic and non-deterministic FSM&#39;s are computationally equally powerful, because any non-deterministic FSM can be translated to an equivalent deterministic one by the &#34;powerset construction&#34; method. The equivalent deterministic FSM might have an exponentially larger number of states compared to the non-deterministic one, however. It is also well-known that FSM&#39;s can solve a class of problems known as &#34;regular&#34; problems. What this means, in very simple terms, is that if you can write a regular expression that would accept the &#34;yes&#34; instances of your decision problem, then you can solve the problem using an FSM. In fact, regular expressions are often implemented using FSM-like structures. The &#34;compile&#34; phase of using regular expression is precisely when the regular expression engine builds the FSM-like structure from your regular expression. (Exercise: Find a regular expression that accepts the above language, namely binary strings with an even number of ones.) All right, so let&#39;s say a decision problem can be solved using an FSM with states numbered 11 to nn . For simplicity, let&#39;s assume that our input will be binary (character set is {0,1}\{0, 1\} ). Suppose the FSM has kk rules given by (Si,ci)→Ti(S_i, c_i) \rightarrow T_i , for 1≤i≤k1 \le i \le k . And assume the input characters are given by s1s_1 to sms_m . So our input is of length mm . Finally, assume that the initial state is 11 and accepting states are a1a_1 to aqa_q , where qq is the number of accepting states. Following Cook&#39;s footsteps, we will introduce the following variables for our SAT reduction: PtP_t which is true iff st=1s_t = 1 , and QtiQ^i_t which is true iff the FSM is in state ii after input character sts_t has been fed into the FSM, for 1≤i&lt;j≤n1 \le i &lt; j \le n and 0≤t≤m0 \le t \le m . We will take t=0t=0 to be the starting step, before anything has been fed into the FSM. With these definitions, we proceed to translate the question of whether the input is accepted by the FSM into an instance of SAT. The goal is to produce a set of clauses that are satisfiable iff the FSM ends in an accepting state given the particular input. The clauses that will accomplish this are: PtP_t for 0≤t≤m0 \le t \le m such that st=1s_t = 1 and ∼Pt\sim P_t for all other 1≤t≤m1 \le t \le m . These will be the first mm clauses, each consisting of a single literal. QmajQ^{a_j}_{m} for 1≤j≤q1 \le j \le q . This says that after the last character is fed into the FSM, we want to be in one of the accepting states. ∼Qti∨∼Qtj\sim Q^i_t \vee \sim Q^j_t for any 1≤i&lt;j≤n1 \le i &lt; j \le n and 1≤t≤m1 \le t \le m , which effectively says that the FSM can not be in both states ii and jj at step tt . Collectively, these clauses will ensure that the FSM is not in more than one state at a time. Qt1∨…∨QtnQ^1_t \vee \ldots \vee Q^n_t for 1≤t≤m1 \le t \le m . This says that the FSM needs to be in at least one state at any step. Together with the last set of clauses, we ensure that the FSM is in exactly one state at any step. ∼Qt−1Si∨Pt∨QtTi\sim Q^{S_i}_{t-1} \vee P_t \vee Q^{T_i}_{t} for all rules (Si,0)→Ti(S_i, 0) \rightarrow T_i and ∼Qt−1Si∨∼Pt∨QtTi\sim Q^{S_i}_{t-1} \vee \sim P_t \vee Q^{T_i}_{t} for all rules (Si,1)→Ti(S_i, 1) \rightarrow T_i , for 1≤t≤m1 \le t \le m . These clauses are logically equivalent to &#34;Qt−1SiQ^{S_i}_{t-1} and PtP_t implies QtTiQ^{T_i}_{t} &#34; which is equivalent to (Si,1)→Ti(S_i, 1) \rightarrow T_i . In other words, they ensure proper transition between states based on the input. Finally, we want to start in the initial state so we add the clause Q01Q^1_0 . Let&#39;s see this in action for the above FSM which accepts strings with an even number of ones in them. First, we have two states, so n=2n=2 . Let&#39;s build the SAT instance to handle inputs of length tt . Also note that we can leave out the first set of clauses (the PtP_t and ∼Pt\sim P_t ones), in which case any SAT assignment will give us some accepted input. Which means we can list all the strings accepted by the FSM by looking at all the satisfying assignments of the above set of clauses. Here is an example for t=3t=3 . In this input QtiQ^i_t is written as Qi-t. The states are also labelled 00 and 11 instead of 11 to nn in the above. # No more than one state at each step ~Q0-0 ~Q1-0 ~Q0-1 ~Q1-1 ~Q0-2 ~Q1-2 ~Q0-3 ~Q1-3 # At least one state in each step Q0-0 Q1-0 Q0-1 Q1-1 Q0-2 Q1-2 Q0-3 Q1-3 # Add the rules # (EVEN, 1) -&gt; ODD ~Q0-0 ~P1 Q1-1 ~Q0-1 ~P2 Q1-2 ~Q0-2 ~P3 Q1-3 # (ODD, 1) -&gt; EVEN ~Q1-0 ~P1 Q0-1 ~Q1-1 ~P2 Q0-2 ~Q1-2 ~P3 Q0-3 # (EVEN, 0) -&gt; EVEN ~Q0-0 P1 Q0-1 ~Q0-1 P2 Q0-2 ~Q0-2 P3 Q0-3 # (ODD, 0) -&gt; ODD ~Q1-0 P1 Q1-1 ~Q1-1 P2 Q1-2 ~Q1-2 P3 Q1-3 # Start in state 0 Q0-0 # End in an accepting state Q0-3 Let&#39;s see the output of running a SAT solver on this, and another file for t=3t=3 and t=4t=4 : $ python sat.py --all --starting_with P --brief &lt; tests/fsm/even-ones-3.in P1 P3 P1 P2 P2 P3 $ python sat.py --all --starting_with P --brief &lt; tests/fsm/even-ones-4.in P1 P4 P1 P3 P1 P2 P3 P4 P1 P2 P2 P4 P2 P3 P3 P4 As expected, all the possible ways of picking a subset of {P1,P2,P3}\{P1, P2, P3 \} with an even number of elements in them are listed above, and similarly for {P1,P2,P3,P4}\{P1, P2, P3, P4 \} , although not necessarily in any meaningful order. (Notice that the empty lines are the empty subsets, which also have even numbers of ones.) </description>
      <pubDate>24 Mar 20 12:22 EDT</pubDate>
      <guid>https://sahandsaba.com/understanding-sat-by-implementing-a-simple-sat-solver-in-python.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/pdf/2012.02943.pdf</link>
      <description>&lt;a href=&#34;https://arxiv.org/pdf/2012.02943.pdf&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;%PDF-1.5 %� 109 0 obj &lt;&lt; /Filter /FlateDecode /Length 4179 &gt;&gt; stream xڝZ[��ȱ~�_Ѳd�Z��� H��뵏��E;st���@WQ]h((������_ddR$�#�/UIf�y����d����ß�����?}�?����6��ç�C���Cكx�Y�P�,�T�����cRu��&#39;)Lb�_s�?�W?�</description>
      <pubDate>25 Jan 21 13:57 EST</pubDate>
      <guid>https://arxiv.org/pdf/2012.02943.pdf</guid>
    </item>
    <item>
      <title>Your configs suck? Try a real programming language. [see within blog graph]</title>
      <link>https://beepb00p.xyz/configs-suck.html</link>
      <description>&lt;a href=&#34;https://beepb00p.xyz/configs-suck.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Or yet another rant about YAML In this post, I&#39;ll try to explain why I find most config formats frustrating to use and suggest that using a real programming language (i.e. general purpose one, like Python) is often (but not always) a feasible and more pleasant alternative for writing configs. Table of Contents 1. Most modern config formats suck 2. Workarounds 3. Use a real programming language Downsides Why Python? Who else does it? 4. What if you don&#39;t have a choice? 5. Extra links 6. -- 7. [2020-04-11] P.S. ¶1 Most modern config formats suck In this section, I&#39;m mostly referring to JSON/YAML/TOML/ini files, which are the most common config formats I encounter. I&#39;ll refer to such configs as plain configs. Not sure if there is a better name for it, please let me know! An incomplete list of my frustrations: JSON doesn&#39;t have comments, by design 🤯 bits of configs can&#39;t be reused For example, while YAML, in theory, supports reusing/including bits of the config (they call it anchors), some software like Github Actions doesn&#39;t support it Usually, you just don&#39;t have any means of reusing parts of your config and have to copy-paste. .gitconfig uses a custom syntax for merging the configs can&#39;t contain any logic This is considered as a positive by many, but I would argue that when you can&#39;t define temporary variables, helper functions, substitute strings or concatenate lists, it&#39;s a bit fucked up. The workarounds (if present) are usually pretty horrible and impose cognitive overhead. Programming language constructs are reinvented from scratch: variables and string interpolation Ansible uses Jinja templates (!) for variable manipulations. Github Actions use a custom syntax for that In addition, they&#39;ve got their own set of functions to manipulate the variables. Have fun learning a new language you never wanted to! scoping I.e. there are several custom scopes for env directive in Github Actions. control flow for loop: build matrices and &#39;excludes&#39; always give me a headache if statement: e.g. when in CircleCI &#34;This is not structured data. This is programming masquerading as configuration.&#34; can&#39;t be validated You can validate the config syntax itself (i.e. check JSON for correctness), but if you want more sophisticated semantic checks, you need to spend extra effort. This is kind of a consequence of not having logic in the config files. Typically you&#39;ll have to write a supplementary program to check your configs and remember to call it before passing to a program. Very few programs bother with that. Usually, your program crashes because of something that would be trivial to catch with any simple type system. YAML simply stands out with its implicit conversions and portability issues (e.g. &#34;The NOrway Problem&#34;) There are enough rants about it, so I&#39;ll just leave a link to a good one: &#34;YAML: probably not so great after all&#34;. Summary: we spend time learning useless syntax and reinventing programming constructs, instead of productive work. ¶2 Workarounds So what happens when people encounter these problems? Often they end up using a &#39;real&#39; (i.e. general purpose, Turing complete) programming language anyway: you write a program to filter out custom comment syntax you write a program to merge configs or use a templating engine you write a program that &#39;evaluates&#39; the config Often, you end up reimplementing an interpreter for a simple functional language in the process. you write a program to validate the config For the most part, it&#39;s boilerplate for type checking. You&#39;re not only working on a solved problem but in addition, end up with mediocre error messages as a result. All this stuff is unpleasant and distracts you from your main objective. Perhaps you can see where I&#39;m coming with this. ¶3 Use a real programming language The idea is to write your config in your target programming language. I&#39;ll have Python in mind here, but the same idea can be applied to any dynamic enough language (e.g. Javascript/Ruby). Then, you simply import/evaluate your config file and voila – you&#39;re done. That&#39;s it. Toy example: config.py from typing import NamedTuple class Person(NamedTuple): name: str age: int PEOPLE = [ Person(&#39;Ann&#39; , 22), Person(&#39;Roger&#39;, 15), Person(&#39;Judy&#39; , 49), ] from pathlib import Path config = {} exec(Path(&#39;config.py&#39;).read_text(), config) people = config[&#39;PEOPLE&#39;] print(people) [Person(name=&#39;Ann&#39;, age=22), Person(name=&#39;Roger&#39;, age=15), Person(name=&#39;Judy&#39;, age=49)] I find it pretty neat. Let&#39;s see how it helps us with the problems I described: comments: duh includes: trivial, use imports You can even import the very package you&#39;re configuring. So you can define a DSL for configuration, which will be imported and used in the config file. logic You have your language&#39;s syntax and libraries available to use. For example, something like pathlib alone can save you massive amounts of config duplication. Of course, one could go crazy and make it incomprehensible. But personally I&#39;d rather accept potential for abusing the power of the language rather than being restricted. validation You can keep validation logic right in the config, so it would be checked at the time of loading. Mature static analysis tools (i.e. JS flow/eslint/pylint/mypy) can be used to aid you. ¶Downsides Are there any problems with that approach? Sure: interoperability Okay, maybe if your program is in Python it makes sense. But what if it isn&#39;t, or you&#39;ll rewrite it to another language (i.e. compiled, like c++) later. If you&#39;ll be running your software somewhere without an interpreter, then sure, good point. Modern FFI is tedious and linking against your config is going to be pretty tricky. In case of Python specifically, it&#39;s present in most modern OS distributions. So you might get away with the following: make your Python config executable in the main() function, build the config, convert to JSON and dump to the stdout This step is possible with no boilerplate due to Python&#39;s dynamic nature. in your c++ code, execute the Python config (i.e. use popen()), read the raw JSON and process Yep, you will still have to manually deserialize config in the c++ code. But I think that&#39;s at least not worse than only using JSON and editing it manually. Obviously that has a performance hit (i.e. milliseconds taken to run the Python interpreter). Make your own judgment whether it&#39;s acceptable for you. If the tool you&#39;re configuring is running for hours, you&#39;re probably going to be fine, or you can always generate the config in advance/cache. general-purpose programming languages are harder to reason about This is somewhat subjective. Personally, I&#39;d be more likely overwhelmed by an overly verbose plain config. I&#39;d always prefer a neat and compact DSL. A large factor here is code style: I&#39;m sure you can make your config file readable in almost any programming language, even for people not familiar with the language at all. However, I appreciate that my experience is different from other engineers (i.e. sysadmins) who would not trade off flexibility for the increase of configuration complexity. general-purpose languages are hard to modify programmatically To some extent it overlaps with the previous point. For example, git config commands manipulates the .git/config file. It&#39;s easy to modify an INI file, because it&#39;s basically a dictionary, so you only have to locate the key in the config file and change a single line. If the config is (say) a Python program, the model can be much more complicated than a dictionary, and it might be tricky to modify settings programmatically. Most likely, you&#39;ll have to resort to only appending new code to the config, which may not always be enough. To me, it&#39;s a very strong point against code as a config. As counter-points: not many programs have (or need) TUI/GUI for editing settings the settings that belong to the UI are usually very simple, and possible to adjust by appending only For example, Emacs customization interface is backed by an Elisp config. The most serious issues are probably security and termination checking: security I.e. if your config executes arbitrary code, then it may steal your passwords or format your hard drive. Whether security is actually something you need to think about depends on your threat model: if your configs are supplied by third parties you don&#39;t trust, then I agree that plain configs are safer. however, often, especially for end-user software, it&#39;s not the case Often the user controls their own config, and the program runs under the same permissions. In addition, this is something that can be potentially solved by sandboxing. Whether it&#39;s worth the effort depends on the nature of your project, but for something like CI executor you need a sandbox anyway. Also, note that using a plain config format doesn&#39;t necessarily save you from trouble. See &#34;YAML: insecure by default&#34;. termination checking Even if you don&#39;t care about security, you don&#39;t want your config to hang the program. Personally, I&#39;ve never run into such issues, but here are some potential workarounds for that: explicit timeout for loading the config using a subset of the language might help, for example, Skylark Anyone knows examples of /conservative/static analysis tools that check for termination in general purpose languages? Note this is not the same as the Halting problem. You don&#39;t want to determine whether any program terminates, you want to figure out a reasonable subset of the language that terminates. Even if your config language is Turing incomplete, you might have to resort to using timeouts anyway: your config can take very long time to evaluate, while taking finite time to complete in theory See &#34;Why Dhall advertises the absence of Turing-completeness&#34; While an Ackermann function is a contrived example, that means that if you truly care about malicious inputs, you want to sandbox anyway. If your configs support some form of including, you can very likely construct an input that will inflate it exponentially. Note that using a plain config doesn&#39;t mean it won&#39;t loop infinitely: See &#34;Accidentally Turing complete&#34; for an excellent overview ¶Why Python? Some reasons I find Python specifically enjoyable for writing config files: Python is present on almost all modern operating systems Python syntax is considered simple (not a bad thing!), so hopefully Python configs aren&#39;t much harder to understand than plain configs data classes, functions and generators form a basis for a compact DSL typing annotations serve as documentation and validation at the same time However, you can achieve a similarly pleasant experience in most modern programming languages (provided they are dynamic enough). ¶Who else does it? Some projects that allow for using code as configuration: Webpack, web asset bundler, uses a Javascript as a config setuptools, the standard way of installing Python packages Allows using both setup.cfg and setup.py files. That way if you can&#39;t achieve something solely with plain config, you can fix this in setup.py, which gives you a balance between declarative and flexible. Jupiter, interactive computing tool Uses a python file to configure the export. Emacs: famously uses Elisp for its configuration While I&#39;m not a fan of Elisp at all, it does make Emacs very flexible and it&#39;s possible to achieve any configuration you want. On the other hand, if you&#39;ve ever read other people&#39;s Emacs setups, you can see it also demonstrates how things can get out of hand when you allow a general purpose language for configuration. Surfingkeys browser extension: uses a Javascript DSL for configuration Gradle provides Groovy and Kotlin DSLs for writing build files Pyinfra automates configuration/infrastructure deployment and uses standard Python I&#39;m personally using it for some of my own infrastructure – and it&#39;s a massive relief after Ansible and its yaml crap. Here is an example of its DSL. Awesome Window Manager uses Lua for configuration Guix package manager: uses Guile Scheme for configuration Pelican static site generator: uses Python for configuration Some languages are designed specifically for configuration: Bazel Skylark uses a subset of Python for describing build rules While it&#39;s deliberately restricted to ensure termination checking and determinism, configuring Bazel is orders of magnitude more pleasant than any other build system I&#39;ve used. Meson build system: borrows the syntax from Python Nix: language designed specifically for the Nix package manager While a completely new language feels like an overkill, it&#39;s still nicer to work with than plain configs. Dhall: language designed specifically for config files Dhall advertises itself as &#34;JSON + functions + types + imports&#34;. And indeed, it looks great, and solves most of the issues I listed. Jsonnet: JSON + variables + control flow See comparison with other configuration languages Downsides of such languages is that they aren&#39;t widespread yet. If you don&#39;t have bindings for your target language, you&#39;d end up parsing JSON again. However, at least it makes writing configs pleasant. But again, if your program is written in Javascript and doesn&#39;t interact with other languages, why don&#39;t you just make the config Javascript? ¶4 What if you don&#39;t have a choice? Some ways I&#39;ve found to minimize the frustration while using plain configs: write as little in config files as possible This typically applies to CI pipeline configs (i.e. Gitlab/Circle/Github Actions) or Dockerfiles. Often such configs are bloated with shell commands, which makes it impossible to run locally without copying line by line. And yeah, there are ways to debug, but they have a pretty slow feedback loop. use tools that are better suited to set up local virtual environments, like tox-dev/tox prefer helper shell scripts and call them from your pipeline It is a bit frustrating since it introduces indirection and scatters code around. But, as an upside, you can lint (e.g. shellcheck) your pipeline scripts, and make it easier to run locally. Sometimes you can get away if your pipeline is short, so use your own judgment. Let the CI only handle setting up a VM/container for you, caching the dependencies, and publishing artifacts. generate the config instead of writing manually The downside is that the generated config may diverge if edited manually. You can add the warning comment that the config is autogenerated with the link to the generator, and make the config file read-only to discourage manual editing. In addition, if you&#39;re running CI, you can make the consistency check a part of the pipeline itself. ¶5 Extra links (commandline) flags are great for configuration Overall, I agree, but there are still cases when using flags isn&#39;t feasible. It&#39;s also prone to leaking secrets (keys/tokens/passwords) – both in your shell history and via ps. Xmonad: config is the executable Interesting approach, but not always feasible, e.g. you might not have the compiler installed. Mage: a tool for writing makefiles in Go Dhall wiki: Programmable configuration files Why are we templating YAML? (HN) Updates from the comments (thanks everyone!): The evolution of an extension language: a history of Lua: apparently Lua has started as a config language Cue: A language for defining, generating, and validating data I&#39;ve really struggled to find a code example on the website, so here you go. The configuration complexity clock: a case for hard-coding ¶6 -- A followup question, which I don&#39;t have an answer for: why is it that way? I&#39;m sure Ansible/CircleCI or Github Actions are developed by talented engineers who have considered pros and cons of using YAML. Do the pros really outweigh the cons? Open to all feedback, and feel free to share your config pain and how are you solving it! Updates: [2020-04-11] Added P.S. section ¶7 [2020-04-11] P.S. Thanks everyone for the discussions and comments! There were some polar opinions involved, so I&#39;d like to clarify the most common objections here: &#34;Programs as a config are a security nightmare&#34; I admit that I have a programmer&#39;s mindset (as opposed to sysadmin&#39;s), and very likely underestimate the security risks. But again, I agree that executable configs are not always a good idea. You can still have the best of both worlds by providing a DSL for generating a plain config and consuming the plain config. &#34;If your config is a program, it might end up arbitrarily complex and incomprehensible&#34; Sure, but again, it largely depends on the discipline. You can also make a plain config incomprehensible and hard to modify. The best compromise here is probably configuration languages like Dhall. &#34;What happens in 20 years, when there is no &lt;insert programming language&gt; around&#34; That&#39;s a good point, but languages don&#39;t disappear in an eye blink. There will be plenty of time to adapt. In addition, if your software and config are written in the same language, the software will need to be rewritten anyway, which is a bigger problem. Also even plain config formats come and go. 20 years ago XML was common for configuration; how many times you&#39;ve seen it lately? Does your programming language even include XML parser in the standard library? &#34;If your config is so complex you need a DSL, your design has gone wrong and your software sucks&#34; Frankly, I&#39;ve found many of such comments as very opinionated and not constructive, but I&#39;ll try to respond. Software comes in very different shapes and while having the simplest configuration possible is desirable (ideally, none!), sometimes it would change the very nature of the thing you&#39;re trying to develop. Sure, you can stop calling it &#39;software&#39; and start calling a &#39;library&#39; at this point, but I don&#39;t feel it changes the point of the discussion. Perhaps, my constructive takeaways from this argument would be: think how flexible your configuration might have to be, and whether you need to give up on plain configs early A good example of this would be some mail filtering systems, that started simple and ended as Turing complete. in the rapid development phase, resort to having a flexible config When/if your software matures, think about supporting plain configs or/and using a special configuration language. Discussion: hackernews lobsters </description>
      <pubDate>05 Apr 20 16:19 EDT</pubDate>
      <guid>https://beepb00p.xyz/configs-suck.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://evjang.com/2021/10/23/generalization.html</link>
      <description>&lt;a href=&#34;https://evjang.com/2021/10/23/generalization.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; This blog post outlines a key engineering principle I’ve come to believe strongly in for building general AI systems with deep learning. This principle guides my present-day research tastes and day-to-day design choices in building large-scale, general-purpose ML systems. Discoveries around Neural Scaling Laws, unsupervised pretraining on Internet-scale datasets, and other work on Foundation Models have pointed to a simple yet exciting narrative for making progress in Machine Learning: Large amounts of diverse data are more important to generalization than clever model biases. If you believe (1), then how much your model generalizes is directly proportional to how fast you can push diverse data into a sufficiently high-capacity model. To that end, Deep Neural nets trained with supervised learning are excellent data sponges - they can memorize vast amounts of data and can do this quickly by training with batch sizes in the tens of thousands. Modern architectures like ResNets and Transformers seem to have no trouble absorbing increasingly large datasets when trained via supervised learning. When a model has minimized training loss (a.k.a empirical risk), it can be said to have “memorized” the training set. Classically one would think that minimizing training loss to zero is shortly followed by overfitting, but overparameterized deep networks seem to generalize well even in this regime. Here is an illustration of the “double descent” phenomena from Patterns, Predictions, and Actions, which illustrates that in some problems, overparameterized models can continue to reduce test error (risk) even as training loss is fully minimized. A recent ICLR workshop paper investigates this phenomenon on synthetic datasets, showing that if you train long enough in this zero-training-loss regime, the model can suddenly have an epiphany and generalize much later on (the authors call this “Grokking”). Furthermore, the paper also presents evidence that increasing training data actually decreases the amount of optimization required to generalize. It’s as my colleague Chelsea Finn once told me: “Memorization is the first step towards generalization!” State-of-then-art neural networks trained this way can do really impressive things. Here is a DALL-E model that, when prompted with “A banana performing stand-up comedy”, draws the following picture: Here is another DALL-E output, prompted with “an illstration of a baby panda with headphones staring at its reflection in a mirror”. Note that there are no such images of “pandas looking into mirrors” or “banana comedians” in the training data (I think), so these results suggest that the DALL-E model has learned to interpret distinct concepts from text, render the corresponding visual parts in an image and have them interact with each other somewhat coherently. The ability to “just ask” language-conditioned deep learning models for what you want has led to “prompt engineering” as a viable space for improving our ML models. Here is a Tweet discussing how priming a VQGAN + CLIP model with the words “Unreal Engine” leads to drastically higher-quality images. What if we could extend this principle - just asking generalization - to other challenging problems that have eluded analytical algorithmic improvements? Reinforcement Learning: Not a Great Data Sponge In contrast to supervised learning, reinforcement learning algorithms are much less computationally efficient when it comes to absorbing vast quantities of diverse data needed for generalization. To see why this is the case, let’s consider a thought experiment where we train a general-purpose robot to do millions of tasks in unstructured environments. The standard Markov Decision Process is set up as follows: a policy is represented as a state-conditioned distribution over actions, \(p(a \vert s)\), and the environment as consisting of a reward function \(r(s_t, a_t)\) and transition dynamics \(p(s_{t+1} \vert s_t, a_t)\). Initial states and task objectives are encoded in the initial state \(s_0\), which is sampled from a distribution \(p(s_0)\). The goal is to maximize the sum of rewards across the episode, averaged across different starting states sampled from \(p(s_0)\): \[\DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \text{Solve}~\theta^*\ = \argmax_\theta~R(\theta)\] \[\text{where}~R(\theta)=E_{p(s_0)}[\sum_{t=1}^{T}{r(s_t, a_t)}]~\text{and}~a_t \sim p_\theta(\cdot|s_t)~\text{and}~s_{t+1} \sim p(\cdot|s_t, a_t)~\text{and}~s_0 \sim p(s_0)\] Let’s assume the existence of some optimal policy which we call \(p^\star(a \vert s)\), that achieves the maximum reward \(\max_\theta R(\theta)\). “Supremum” would be more accurate, but I use the \(\max\) operator for notational simplicity. We want to bring our model, \(p_\theta(a \vert s)\), as close as possible to \(p^\star(a \vert s)\). If we had access to the optimal policy \(p^\star(a \vert s)\) as an oracle, we could simply query the oracle action and use it like a supervised learning label. We could then train a feedforward policy that maps the states to the oracle actions, and benefit from all the nice properties that supervised learning methods enjoy: stable training, large batches, diverse offline datasets, no need to interact with the environment. while not converged: batch_states = replay_buffer.sample(batch_size) oracle_actions = [oracle_policy.sample_action(s) for s in batch_states] model.fit(batch_states, oracle_actions) However, in reinforcement learning we often don’t have an expert policy to query, so we must improve the policy from its own collected experience. To do this, estimating the gradient that takes the model policy closer to the optimal policy requires evaluating the average episodic return of the current policy in the environment, and then estimating a gradient of that return with respect to parameters. If you treat the environment returns as a black-box with respect to some parameter \(\theta\) you can use the log-derivative trick to estimate its gradients: \[\nabla_\theta E_{p(\theta)} [R(\theta)] = \int_\Theta d\theta \nabla_\theta p(\theta) R(\theta) \\ \\ = \int_\Theta d\theta p(\theta) \nabla_\theta \log p(\theta) R(\theta) = E_{p(\theta)} [\nabla_\theta \log p(\theta) R(\theta)]\] This gradient estimator contains two expectations that we need to numerically approximate. First is computing \(R(\theta)\) itself, which is an expectation over starting states \(p(s_0)\). In my previous blog post I mentioned that accurate evaluation of a Binomial variable (e.g. the success rate of a robot on a single task) could require thousands of trials in order to achieve statistical certainty within a couple percent. For our hypothetical generalist robot, \(p(s_0)\) could encompass millions of unique tasks and scenarios, which makes accurate evaluation prohibitively expensive. The second expectation is encountered in the estimation of the policy gradient, over \(p(\theta)\). Some algorithms like CMA-ES draw samples directly from the policy parameter distribution \(p(\theta)\), while other RL algorithms like PPO sample from the policy distribution \(p_\theta(a\vert s)\) and use the backpropagation rule to compute the gradient of the return with respect to the parameters: \(\frac{\partial R}{\partial \theta} = \frac{\partial R}{\partial \mu_a} \cdot \frac{\partial \mu_a}{\partial \theta}\). The latter is typically preferred because the search space on action parameters is thought to be smaller than the search space on policy parameters (and therefore requires fewer environment interactions to estimate a gradient for). If supervised behavior cloning on a single oracle label \(a \sim p^\star(a\vert s)\) gives you some gradient vector \(g^\star\), estimating the same gradient vector \(\bar{g} \approx g^\star\) with reinforcement learning requires something on the order of \(O(H(s_0) \cdot H(a))\) times as many episode rollouts to get a comparably low-variance estimate. This is a hand-wavy estimate that assumes that there is a multiplicative factor of the entropy of the initial state distribution \(O(H(s_0))\) for estimating \(R(\theta)\) and a multiplicative factor of the entropy of the action distribution \(O(H(a))\) for estimating \(\nabla_\theta R(\theta)\) itself. Consequently, online reinforcement learning on sparse rewards and diverse, possibly multi-task environments require enormous numbers of rollouts to estimate returns and their gradients accurately. You have to pay this cost on every minibatch update! When the environment requires handling a wide variety of scenarios and demands generalization to unseen situations, it further increases the number of minibatch elements needed. The OpenAI DOTA team found that having millions of examples in their minibatch was required to bring down gradient noise to an acceptable level. This intuitively makes sense: if your objective \(R(\theta)\) has a minimum minibatch size needed to generalize well across many \(s_0\) without excessive catastrophic forgetting, then switching from supervised learning to online reinforcement learning will probably require a larger batch size by some multiplicative factor. What about Offline RL? What about offline RL methods like Deep Q-Learning on large datasets of \((S,A,R,S)\) transitions? These methods work by bootstrapping, where the target values that we regress value functions to are computed using a copy of the same network’s best action-value estimate on the next state. The appeal of these offline reinforcement learning methods is that you can get optimal policies from diverse, off-policy data without having to interact with the environment. Modified versions of Q-learning like CQL work even better on offline datasets, and have shown promise on smaller-scale simulated control environments. Unfortunately, bootstrapping does not mix well with generalization. It is folk knowledge that the deadly triad of function approximation, bootstrapping, and off-policy data make training unstable. I think this problem will only get worse as we scale up models and expect to train them on increasingly general tasks. This work shows that repeated bootstrapping iteratively decreases the capacity of the neural network. If you believe the claim that overparameterization of deep neural networks is key to generalization, then it would appear that for the same neural net architecture, offline RL is not quite as “data absorbent” as supervised learning. In practice, even algorithms like CQL are still challenging to scale and debug on larger, real-world datasets; colleagues of mine tried several variations of AWAC and CQL on large-scale robotics problems and found them to be trickier to get them to work than naive methods like Behavior Cloning. Instead of going through all this trouble, what if we lean into what deep nets excel at - sponging up data quickly with supervised learning and generalizing to massive datasets? Can we accomplish what RL sets out to do using the tools of generalization, rather than direct optimization? Learn the Distribution instead of the Optimum What if we make generalization the first-class citizen in algorithmic design, and tailor everything else in service of it? What if we could simply learn all the policies with supervised learning, and “just ask nicely” for the best one? Consider the recent work on Decision Transformer (DT), whereby instead of modeling a single policy and iteratively improving it with reinforcement learning, the authors simply use supervised learning coupled with a sequential model to predict trajectories of many different policies. The model is conditioned with the Return-to-Go so that it may predict actions consistent with a policy that would achieve those returns. The DT simply models all policies - good and bad - with supervised learning, and then use the magic of deep learning generalization to infer from the expert-conditioned policy. This phenomenon has been observed and developed in several prior and concurrent works, such as Reward-Conditioned Policies, Upside Down Reinforcement Learning and Reinforcement Learning as One Big Sequence Modeling Problem. The AlphaStar team also found that conditioning a model on human player skill level (e.g. future units they ended up build order, MMR, ELO scores) and using it to imitate all player data was superior to only imitating expert-level build orders. This technique is also commonly used in the Autonomous Vehicle space to model both good drivers and bad drivers jointly, even though the autonomous policy is only ever deployed to imitate good driving behavior. Hindsight Language Relabeling At a high level, DTs condition the supervised learning objective on some high level description \(g\) that partitions what the policy will do in the future based on that value of \(g\). The return-to-go is an especially salient quantity for a reinforcement learning task, but you can also express the future outcomes via a goal state or StarCraft build order or even a natural language description of what was accomplished. In Learning Latent Plans from Play, the authors pair arbitrary trajectories with post-hoc natural language descriptions, and then train a model to clone those behaviors conditioned on language. At test time, they simply “ask” the policy to do a novel task in a zero-shot manner. The nice thing about these techniques is that they are indispensable for reaching sparse goals on RL tasks like Ant-Maze. This lends support to the claim that generalization and inference across goal-conditioning can do far better than brute force search for a single sparse goal in a long-horizon task. Language is a particularly nice choice for conditioning because it can be used to partition a trajectory not just on skill level, but also by task, by how much the policy explores, how “animal-like” it is, and any other observations a human might make about the trajectory. Clauses can be composed ad-hoc without developing a formal grammar for all outcomes that the robot might accomplish. Language is an ideal “fuzzy” representation for the diversity of real-world outcomes and behaviors, which will become increasingly important as we want to partition increasingly diverse datasets. Generalizing From Imperfect Demonstrations A recent work I am quite inspired is D-REX, which tackles the problem of inferring the environment’s reward function from the demonstrations of a suboptimal policy. Classically, one requires making an assumption that the demonstrator is the optimal policy, from which you can use off-policy algorithms (e.g. Q-learning) to estimate the value function. Offline value estimation with deep neural nets can suffer from poor generalization to state-action pairs not in the demonstrator trajectory, and thus requires careful algorithmic tuning to make sure that the value function converges. An algorithm with poor convergence properties makes the propsects of minimizing training loss - and therefore generalization - tenuous. D-REX proposes a really clever trick to get around not having any reward labels at all, even when the demonstrator is suboptimal: Given a suboptimal policy \(\pi_\theta\), generate trajectory rollouts \(\tau_1, \tau_2, ... \tau_N\) by having the policy interact with the environment. On each rollout, add variable amounts of noise \(\epsilon\) to its actions. Assume that adding noise to a suboptimal policy makes it even more suboptimal, i.e. \(R(\tau) \geq R(\tau + \epsilon)\). Train a ranking model \(f_\theta(\tau_i, \tau_j)\) to predict which of two trajectories \(\tau_i, \tau_j\) has a higher return. The ranking model magically extrapolates to trajectories that are better than what \(\pi_\theta\) can generate, even though the ranking model has never been trained on trajectories better than \(\pi_\theta\) itself. I like this approach because ranking models are stable to train (they are just classifiers), and this method is able to achieve better-than-demonstrator behavior not through the explicit construction of the Bellman inequality or implicit planning through a learned model, but rather via extrapolation on a family of perturbations. Do You Even Need RL to Improve from Experience? In the above sections I’ve described how you can “generalize and infer” to get around exploration and even inverse reinforcement learning from sparse rewards. But what about “improving from a policy’s own experience, tabular rasa”? This is the main reason why people put up with the pain of implementing RL algorithms. Can we replace this with supervised learning algorithms and a bit of generalization as well? The goal of RL is to go from the current set of parameters \(\theta^{n}\) and some collected policy experience \(\tau\) to a new set of parameters \(\theta^{n+1}\) that achieves a higher episode return. Instead of using a “proper” RL algorithm to update the agent, could we just learn this mapping \(f: (\theta^{n}, \tau) \to \theta^{n+1}\) via supervised deep learning? This idea is sometimes referred to as “meta-reinforcement learning”, because it involves learning a better reinforcement learning function than off-the-shelf RL algorithms. My colleagues and I applied this idea to a project where we trained a neural network to predict “improved policy behavior” from a video of a lesser policy’s experience. I could imagine this idea being combined with ranking and trajectory augmentation ideas from D-REX to further generalize the “policy improvement behavior”. Even if we never train on optimal policy trajectories, perhaps sufficient data augmentation can also lead to a general improvement operator that extrapolates to the optimal policy regime of parameters. People often conflate this policy improvement behavior with “reinforcement learning algorithms” like DQN and PPO, but behavior is distinct from implementation. The “policy improvement operator” \(f: (\theta^{n}, \tau) \to \theta^{n+1}\) can be learned via your choice of reinforcement learning or supervised learning, but is deployed in a RL-like manner for interacting with the environment. The “Just-Ask-Generalization” Recipe Here is a table summarizing the previously mentioned RL problems, and comparing how each of them can be tackled with a “generalize-and-infer” approach instead of direct optimization. Goal “Direct Optimization” Approach “Generalize + Inference” Approach Reinforcement Learning with Sparse Rewards Find \(p^\star(a_t\vert s_t)\) s.t. \(R_t=1\), brute force exploration DT: Learn \(p(a_t\vert s_t,R_t)\) from many policies, infer \(p(a_t\vert s_t, R_t=1)\). H.E.R - Infer tasks for which gathered trajectories are optimal, then learn \(p(\text{trajectory}\vert \text{task})\). Then infer optimal trajectory for desired task. Learn a Reward Function from Suboptimal Trajectories Offline Inverse RL D-REX: Trajectory augmentation + Extrapolate to better trajectories. Improve the policy from experience Q-Learning, Policy Gradient Watch-Try-Learn: Learn \(p(\theta^{n+1} \vert \theta^n , \tau, \text{task})\) Fine-tune a simulated policy in a real-world environment Sample-efficient RL fine-tuning Domain Randomization: train on a distribution of simulators, and the policy “infers which world” it is in at test time. The high-level recipe is simple. If you want to find the solution \(y_i\) for a problem \(x_i\), consider setting up a dataset of paired problems and solutions \((x_1, y_1), ..., (x_N, y_N)\) and then training a deep network \(y = f_\theta(x)\) that “simply maps your problems to solutions”. Then substitute your desired \(x_i\) and have the deep network infer the solution \(y_i\) via generalization. “Problem” is meant in the most abstract of terms and can refer to a RL environment, a dataset, or even a single example. “Solutions” could be represented as the optimal parameters of a policy or a neural network, or a single prediction. Techniques like goal relabeling help generate post-hoc problems from solutions, but building such a dataset can also be achieved via data augmentation techniques. At its core, we are transforming a difficult optimization problem into an inference problem, and training a supervised learning model on a distribution of problems for which it’s comparatively cheap to obtain solutions. To summarize the recommendations in a three-step recipe: Choose a method capable of minimizing training loss on massive datasets, i.e. supervised learning with maximum likelihood. This will facilitate scaling to complex, diverse datasets and getting the most generalization mileage out of your compute budget. If you want to learn \(p(y\vert x, \text{task}=g^\star)\) for some prediction task \(g^\star\), try learning \(p(y\vert x, \text{task})\) for many related but different tasks \(g \sim p(g), g \neq g^\star\) Then at test time just condition on \(g^\star\). Formulate conditioning variables that help partition the data distribution while still admitting generalization on held-out samples from \(p(g)\). Natural language encoding is a good choice. The insight that we can cast optimization problems into inference problems is not new. For example, the SGD optimizer can be cast as approximate Bayesian inference and so can optimal control via AICO. These works present a theoretical justification as to why inference can be a suitable replacement for optimization, since the problems and algorithms can be translated back and forth. I’m suggesting something slightly different here. Instead of casting a sequential decision making problem into an equivalent sequential inference problem, we construct the “meta-problem”: a distribution of similar problems for which it’s easy to obtain the solutions. We then solve the meta-problem with supervised learning by mapping problems directly to solutions. Don’t overthink it, just train the deep net in the simplest way possible and ask it for generalization! Perhaps in the near future we will be able to prompt-engineer such language-conditioned models with the hint “Generalize to unseen …”. Just ask for … Consciousness? How far can we stretch the principle of “generalize-and-infer” as an alternative to direct optimization? Here is a “recipe for consciousness” which would probably be better pondered over some strong drinks: Train a language-conditioned multi-policy model \(p_\theta(a\vert s, g)\) (implemented via a Decision Transformer or equivalent) to imitate a variety of policies \(\pi_1, ..., \pi_N\) conditioned on natural language descriptions \(g\) of those agents. At test time, some default policy \(p(a\vert s, g=\text{Behave as myself})\) interacts with another agent \(\pi_\text{test}\) for a number of steps, after which we instruct the model to “behave as if you were \(\pi_\text{test}\).” The model would require a sort of “meta-cognition of others” capability, since it would have to infer what policy \(\pi_\text{test}\) would do in a particular situation. We make a copy of the multi-policy model \(p_\phi \sim p_\theta\), and embed multiple test-time iterations of step (1) within a single episode, with dozens of agents. Two of these agents are initially conditioned as \(p_\theta(a\vert s, g=\text{Behave as myself})\) and \(p_\phi(a\vert s, g=\text{Behave as myself})\). This generates episodes where some agents imitate other agents, and all agents observe this behavior. Then we ask \(p_\phi\) to emit actions with the conditioning context “behave as if you were \(\pi_\theta\) pretending to be you”. This would require \(\pi_\phi\) to model \(\pi_\theta\)’s imitation capabilities, as well as what information \(\pi_\theta\) knows about \(\pi_\phi\), on the fly. Researchers like Jürgen Schmidhuber have previously discussed how dynamics models (aka World Models) of embodied agents are already “conscious”, because successful modeling the dynamics of the environment around oneself necessitates a representation of the self as an embodied participant in the environment. While I think that “self-representation” is a necessity in planning and dynamics prediction problems, I think the framework is too vacuous to be of use in reproducing a convincing imitation of consciousness. After all, any planning algorithm that represents “the self” explicitly within each imagined trajectory rollout would be conscious under this definition. An A* maze-planner would satisfy this definition of consciousness. What I’m proposing is implementing a “more convincing” form of consciousness, not based on a “necessary representation of the self for planning”, but rather an understanding of the self that can be transmitted through language and behavior unrelated to any particular objective. For instance, the model needs to not only understand not only how a given policy regards itself, but how a variety of other policies might interpret the behavior of a that policy, much like funhouse mirrors that distort one’s reflection. The hypothesis is that through demonstrating this understanding of “distorted self-reflection”, the policy will learn to recognize itself and model the internal motivations and beliefs of other agents in agent-agent interactions. There are some important implementation details that I haven’t fleshed out yet, but at high level, I do think that supervised learning and natural language conditioning with enormous agent-interaction datasets are sufficiently powerful tools to learn interesting behaviors. Imbuing agents with some kind of meta-cogition ability of the self and other agents is an important step towards a convincing imitation of consciousness. Acknowledgements Thanks to Daniel Freeman, David Ha, Karol Hausman, Irwan Bello, Igor Mordatch, and Vincent Vanhoucke for feedback and discussion on earlier drafts of this work. References Generalization and scaling: Scaling Laws for Neural Language Models Self-supervised Pretraining of Visual Features in the Wild On the Opportunities and Risks of Foundation Models Understanding deep learning requires rethinking generalization A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes Patterns, Predictions, Actions: Generalization Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets DALL·E: Creating Images from Text RL challenges: Robots Must Be Ephemeralized An Empirical Model of Large-Batch Training Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning Deep Reinforcement Learning and the Deadly Triad Conservative Q-Learning AW-Opt: Learning Robotic Skills with Imitation andReinforcement at Scale Hindsight Imitation Decision Transformer: Reinforcement Learning via Sequence Modeling Reward-Conditioned Policies Upside Down Reinforcement Learning Reinforcement Learning as One Big Sequence Modeling Problem Grandmaster level in Starcraft II via multi-agent reinforcement learning Hindsight Experience Replay Learning Latent Plans from Play Replacing RL with Supervised Learning Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards Distribution Augmentation for Generative Modeling Stochastic Gradient Descent as Approximate Bayesian Inference Robot Trajectory Optimization using Approximate Inference Q/A Igor Mordatch supplied interesting questions and comments in reviewing this blog post. I have paraphrased his questions here and added responses in this section. 1. You discussed Supervised Learning and Reinforcement Learning. What do you think about Unsupervised Learning and “The Cake Analogy”? I consider unsupervised learning to be simply supervised learning for a different task, with comparable gradient variance, since targets are not usually noisly estimated beyond augmentation. Maximum likelihood estimation and contrastive algorithms like InfoNCE seem to be both useful for facilitating generalization in large models. 2. For the first difficulty of RL (evaluating success), aren’t there parallels to current generative models too? Success evaluation is hard for language models, as evidenced by dissatisfaction with BLEU scores and difficulty of evaluating likelihoods with non-likelihood based generative image models. There are parallels to likelihood-free generative models which require extensive compute for either training or sampling or likelihood evaluation. In practice, however, I think the burdens of evaluation are not directly comparable, since the computational expense of marginalization over observations for such models is dwarfed by the marginalization of success rate estimation in RL. In RL, you have to roll out the environment over O(coin flips) x O(initial state distribution) x O(action distribution) in order to get a low-variance policy gradient for “improved success across all states and tasks”. O(coin flips) is O(1000) samples for local improvement of a couple percent with statistical certainty, wheras I think that typically the marginalization costs of implicit likelihood tends to be cheaper with tricks like Langevin sampling O(minibatch=32). Also, the backprop passes used in Langevin dynamics are usually cheaper than running full environment simulations with a forward pass of the neural net on every step. 3. One of the findings of current language model work is that proxy objectives for what you really want are good enough. Simple next-token prediction induces generalization. But alignment to what you really want is still a hard problem in large model field and we don’t have good answers there yet (and ironically many attempts so far relied on incorporation of RL algorithms). Alignment objectives may lack a per-example surrogate loss. But under the “generalize-then-infer” school of thought, I would simply recommend learning \(p(y\vert x, \text{alignment objective})\) with max likelihood over numerous hindsight alignment objectives, and then simply condition on the desired alignment object at test time. One could obtain a distribution of alignment descriptions by simply running the model live, and then hindsight labeling with the corresponding alignment realized by the model. Then we simply invoke this meme by Connor Leahy: Just asking the AI to be nice sounds flippant, but after seeing DALL-E and other large-scale multi-modal models that seem to generalize better as they get bigger, I think we should take these simple, borderline-naive ideas more seriously. 4. For the second difficulty of RL (gradient estimation), we know that for settings where you can backprop through environment dynamics to get exact policy gradient, doing so often leads to worse results. This reminds me of an old FB comment by Yann Lecun that a better way to estimate Hessian-vector products with ReLU activations is to use a stochastic estimator rather than computing the analytical hessian, since the 2nd-order curvature of ReLU is 0 and what you actually want is the Hessian-vector product of the smoothed version of the function. If you need to relax the dynamics or use an unbiased stochastic estimator to train through a differentiable simulator, then I think you’re back to where you’re starting with expensive evaluation, since presumably you need many rollouts to smooth out the simulator function and reduce variance. However, maybe the number of samples you need to estimate a smoothed policy gradient is a reasonable tradeoff here and this is a nice way to obtain gradients. 5. Why hasn’t something as simple as what you propose (generalize-then-infer) been done already? Some researchers out there are probably pursuing this already. My guess is that the research community tends to reward narratives that increase intellectual complexity and argue that “we need better algorithms”. People pay lip service to “simple ideas” but few are willing to truly pursue simplicity to its limit and simply scale up existing ideas. Another reason would be that researchers often don’t take generalization for granted, so it’s often quicker to think about adding explicit inductive biases rather than thinking about generalization as a first-class citizen and then tailoring all other design decisions in support of it. 6. How does your consciousness proposal relate to ideas from Schmidhuber’s “consciousness in world models” ideas, Friston’s Free Energy Principle, and Hawkin’s “memory of thoughts”? I consider Schmidhuber and Friston’s unified theories as more or less stating “optimal control requires good future prediction and future prediction with me in it requires self-representation”. If we draw an analogy to next-word prediction in large language models, maybe optimizing next state prediction perfectly is sufficient for subsuming all consciousness-type behaviors like theory-of-mind and the funhouse self-reflections I mentioned above. However, this would require an environment where predicting such dynamics accurately has an outsized impact on observation likelihoods. One critique I have about Schmidhuber and Friston’s frameworks is that they are too general, and can be universally applied to sea slugs and humans. If a certain environmental complexity is needed for future prediction to give rise to something humans would accept as conscious, then the main challenge is declaring what the minimum complexity would be. Hawkin’s “consciousness as memory of perception” seems to be more related to the subjective qualia aspect of consciousness rather than theory of mind. Note that most people do not consider a program that concatenates numpy arrays to be capable of “experiencing qualia” in the way humans do. Perhaps what is missing is the meta-cognition aspect - the policy needs to exhibit behaviors suggesting that it contemplates the fact that it experiences things. Again, this requires a carefully designed environment that demands such meta-cognition behavior. I think this could emerge from training for the theory-of-mind imitation problems I described above, since the agent would need to access a consistent representation about how it perceives things and transform it through a variety of “other agent’s lenses”. The flexibility of being able to project one’s own representation of sensory observations through one’s representation of other agents’ sensory capabilities is what would convince me that the agent understands that it can do sufficient meta-cognition about qualia. 7. Your formulation of consciousness only concerns itself with theory-of-mind behavior. What about attention behavior? See the second paragraph of the response to #6. Update 20211025: Updated with a paraphrased question from Alexander Terenin 8. In Rich Sutton’s Bitter Lesson Essay, he argues that search and learning are both important. Do you really think that search can be completely replaced by a learned approach? I agree that having a bit of light search in your program can be immensely helpful to learning and overall performance. It’s a bit of a chicken/egg though. Does AlphaGo work because MCTS uses a learned value function to make search tractable? Or does the policy distillation only work because of search? I’m suggesting that when search becomes too hard (most RL tasks), it’s time to use more learning. You’re still doing search when performing supervised learning - you just get a lot more gradient signal per flop of computation. </description>
      <pubDate>26 Oct 21 11:05 EDT</pubDate>
      <guid>https://evjang.com/2021/10/23/generalization.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.ycombinator.com/science-startups/</link>
      <description>&lt;a href=&#34;https://blog.ycombinator.com/science-startups/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;When Dave Messina entered a graduate program for genomics in 1998, he was elated. He was taking the first steps of what could be a career at a top-tier university, working on scientific research that could impact millions of lives. But while his chosen field had so much promise, he realized that for him, the academic environment might not. “I looked around, and saw that some of the smartest scientists I’d ever met were having trouble getting funded, and moving forward in their research,” Messina says. “The life of an academic scientist is really hard. You’ve got to really, really want it. And even then, for many people it just doesn’t work out.” Transcriptic Messina quickly realized that the best way for him to succeed in the field of genomics might be to work outside of academia altogether. At that time, though, there wasn’t a clear path to follow. “I thought, ‘Maybe I can get into a company, and move my way up so that I can dig into important research there.’ So I talked to a lot of people,” he says. “They all knew of someone who had tried it at a larger biotech corporation, but it definitely was the exception.” So Messina created the path for himself. He eventually pulled back from the PhD track in genetics at Washington University in St. Louis, and instead obtained a Masters Degree. He went on to earn his PhD in computational biology, opting to concentrate on the software that does the heavy lifting in genome analysis. And in 2012, he joined RNA sequencing software startup Cofactor Genomics, where he now serves as COO. Today, Messina says that startups are an increasingly viable place for the kinds of research careers that used to be found only in the ivory tower and big corporations. He’s not alone in that belief. Y Combinator has funded an increasing number of startups with biotech and life sciences applications, many of which are led by founders who have crossed over from years in the academic realm. Since 2014 alone, more than 24 startups with a biotech or life sciences focus have launched out of the YC program. The wider field of Silicon Valley investors has also developed a heightened interest in the space, with firms such as Andreessen Horowitz launching new funds dedicated entirely to investing in biotech and life sciences startups. Below are some insights and lessons learned from several founders from this new breed of science-oriented startups on how to navigate the path between academia and Silicon Valley. Analyzing risk The first step for many people leaving the realm of traditional academic research and entering the startup space is assessing the risks of making such a change. The odds may not be as daunting as they may expect. “Coming from academia, there’s a myth that it’s really risky to join a startup, and in my experience that’s not true at all,” Messina says. “It depends on the field, but look at the Genome Institute at Washington University in St. Louis, which is a premiere organization. Just a few weeks ago, they didn’t get a big grant that they were expecting to get, and they had to lay off 20 percent of their workforce. The good news now is that there are companies and startups around who are hiring those people.” Cofactor Genomics That said, there are a number of options for scientists to weigh. “You don’t have to jump in all the way and say you’re going to start a company immediately. There’s a whole continuum,” Messina says. He recommends that current PhD and postdoctoral candidates take on part-time roles at startups through programs such as the St. Louis-based BALSA Group. For those who have decided to launch a science-oriented startup, it’s also important to consider the inherent risks of your product and approach. “If you’re someone coming from a PhD or postdoc background with an idea for a startup, it’s so important that you reduce your technical risk,” says Max Hodak, the co-founder of robotic bio lab startup Transcriptic. “Otherwise, you might spend a lot of time and money trying to find some novel pathway, and then find it’s not there.” Not being clear-eyed about your startup’s technical risks could lead to a crunch in funding, Hodak says. “Venture capitalists say they want to invest in hard tech, but VCs hate technical risk. They’re comfortable with market risk, but technical risk is really difficult for them to reconcile. In biotech, a lot of founders have more technical risk than they think they do.” Adjust your time scales Thanks to advances in both computing and equipment, it’s now affordable for startups to perform the kinds of lab research that would not have been possible outside of large institutions even five years ago, says Matt De Silva, the founder of personalized brain cancer treatment startup Notable Labs. But, he says, there are key differences in how a lab should operate at a startup versus a traditional research environment. “There are certain aspects of academia that are very useful in a startup: the creativity, the looking at things from first principles, the open-mindedness,” De Silva says. “But the big challenges are the time scales and the quality expectations. For a startup, work needs to be both faster and more rigorous than an academic lab. In academia, in order to publish a paper, often you just have to get it to work one time out of ten — so you think, OK, I’ll just keep doing the experiment until it works. We need it to work nine or ten times out of ten. If something from a paper doesn’t bear out, the impact can be mitigated. If you have a product you’re developing, it can be a disaster.” uBiome Jessica Richman, the co-founder and CEO of human microbiome testing startup uBiome, agrees. “In academia, it’s not uncommon to get a different result than expected and say ‘Okay, let’s come back to it in a month, and let’s discuss it then.’ At a startup, that really doesn’t work. You can wait and come back to it that afternoon, at the latest,” she says. “Think about what a ‘hacker’ was to traditional computer scientists. Startups need the life sciences equivalent of that.” Communicate your idea “The cultural differences between academia and startups cannot be overstated,” uBiome’s Richman says. “I tell scientists to approach the startup world like an anthropologist would: Read all the books you can, learn about the culture, observe the people, figure out how they do things. A lot of the books you’ll read will probably have bad startup advice, but you’ll get a picture of the thought processes and the language.” Coming from academia, where projects are funded by grants, it can be a big change to go into investor-funded startups. “It’s important to learn how to tell the story of a business, as opposed to the story of a grant,” Richman says. Often, people successfully secure a grant by describing things that have been proven in existing scientific literature, and highlighting how their project fills a gap in the research that’s been done. Startup investors aren’t typically impressed by that narrative, she says. “The story of a business is about vision and progress: What are you going to do, how far have you gone so far, and how are you going to make money.” She says that the mores around raising funding are different from grants, too. “The whole funding environment for startups can be hard to navigate. You’re used to applying to grants — there’s no having lunch with someone, they turn you down, but then they talk to a friend, then you hear that their friend wants to invest,” Richman says. “In academia, there’s no wheeling and dealing around the grant process. It’s important to get used to that. I’ll talk to founders who are coming from academia about their fundraising, and they’ll say, ‘I talked to 3 people and they all said no.’ That’s not how it works. You need to talk to 300 people. It’s a sales process. Getting startup funding isn’t about applying for something and sitting back and waiting.” Nailing communication is also key for hiring. Transcriptic’s Hodak says that if you can quickly and clearly communicate what your startup does to someone without a life sciences or biotech background, you may find that it’s easier to hire engineers than you might expect. “Sam Altman told me once that it’s paradoxically easier to do a ‘hard’ startup than an easy one, because people want to help you. I’ve found that to be really true,” he says. “I can’t imagine in this environment trying to recruit for a social mobile app. That would blow my mind. But it’s easier to get really good people when you can tell them you’re building robots that are furthering the field of science.” Cofactor Genomics Notable Labs’ De Silva says that his company’s first full stack developer hire actually reached out to them, after reading about Notable’s mission of helping find a cure for brain cancer in the press. “There’s a growing group of software engineers who are tired of using their skills to optimize ads,” De Silva says. “If you can communicate that if your company is successful, it will be directly impacting people’s lives, great people will want to help you achieve that.” What shouldn’t change For all the differences between the two sectors, there are some things that should not change as scientists enter the startup environment. “As the tech world is blending into the biotech world, founders and investors need to be conscious that the burden of proof in science is very high, and that won’t change,” Notable Labs’ De Silva says. “If you’re a startup in this space, you need to be transparent, you need to run trials, you need to show your science works in all the same ways that traditional institutions have. Talking a big game, being secretive, and raising a huge amount of money sometimes works in software and web tech. But it’s an inverse playbook in biotech. The stakes are higher, and people will hold you to that, as they should.” According to Transcriptic’s Hodak, maintaining a healthy respect for the complexity of science and instilling it into the non-scientific team members and investors is crucial. “For people not coming from a science background, it’s easy to underestimate how difficult these things are,” Hodak says. “There are a lot of people from the tech sector seeing the opportunities here but underestimating the technical complexity. Yes, there’s a lot of opportunity, and a lot of improvement to be had, but it’s very complicated in non-obvious ways.” </description>
      <pubDate>27 Mar 20 18:25 EDT</pubDate>
      <guid>https://blog.ycombinator.com/science-startups/</guid>
    </item>
    <item>
      <title>Why you should migrate everything from Linux to BSD</title>
      <link>https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html</link>
      <description>&lt;a href=&#34;https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Published on 2020-01-18. Modified on 2021-02-17. As an operating system GNU/Linux has become a mess because of the fragmented nature of the project, the bloatware in the kernel, but mainly because of the manipulation by corporate interests. There exist several technical reasons for when a migration from GNU/Linux to BSD make sense, but this article isn&#39;t about that, it&#39;s an &#34;analyzes&#34; of the current status in Linux-land, and it is an opinionated rant, more than anything else. Table of Contents Introduction Linux is fragmented Linux is being heavily influenced by corporate interests BSD is the place to be License problems Time to migrate everything to BSD Relevant links Introduction In the past I have always been a favorite of choosing operating system and tools based upon technical merit. However, in today&#39;s world of companies like Microsoft, Apple, Google, and many others, compromising user privacy, and conducting controversial activities, I don&#39;t believe that to be the right cause of action. Proprietary operating systems like Microsoft Windows 10, Apple MacOS, and Google Android have become famous for their ill conduct, and even companies like Lenovo is using UEFI boot to inject custom Windows components, so that the system can phone home to Lenovo. I have been a proponent for the open source alternatives, like GNU/Linux and BSD, for a very long time. Not only that, I also believe that the open source alternatives are much better in many technical areas. I have also always been very much against The typical discussions about BSD vs Linux, and as I wrote in my article back then, I have always believed that the different open source projects can help each other and cooperate, and that end-users should only debate such issues from a technical stand point rather than personal preference. Whenever it has been possible, I have proposed people, both private and in the industry, to change the operating systems they use to open source alternatives, and when people have been receptive to my advocacy I have helped them migrate from Microsoft Windows on their workstations to BSD or Linux. And likewise on the server side. This has been a truly successful endeavor and I have honestly never experienced a dissatisfied person or company. However, things are beginning to change in the GNU/Linux world as more and more corporations want to control the direction of Linux as an operating system. Due to the structure and organization of GNU/Linux as an operating system, it is unfortunately susceptible to these influences, and while it is still open source, and still not anywhere near the bad things that is going on with the proprietary alternatives, some opt-out features have slowly been introduced into both the kernel and systemd. You can still choose to opt-out of these features and go your merry way, but as an open source enthusiast and proponent, and as a privacy concerned individual, perhaps the better approach is to migrate systems to something where you don&#39;t have to concern yourself with &#34;creepware&#34;. As a system administrator I don&#39;t want to worry about whether I am going to be surprised the next time I upgrade a system, and I don&#39;t want to keep a list of spyware I have to remember to opt-out of whenever I run one of these systems. Several Linux distributions have decided (not only because of privacy opt-out issues, but other issues as well) to implement other init solutions than systemd, but with the situation going on in the kernel development, and with many third party applications becoming more and dependent upon systemd, the problems are spreading to other parts of the operating system and I believe this is becoming an uphill battle. From a community perspective, and from a security perspective, I don&#39;t believe the future of GNU/Linux looks very bright, and as a possible alternative solution I suggest migrating everything (when possible) to something a bit more sane, like one of the BSD projects. Linux is fragmented In 1983 Richard Stallman announced his intent to start coding the GNU Project in a Usenet message. By June 1987, the project had accumulated and developed free and open source software for an assembler, an almost finished portable optimizing C compiler (GCC), an editor (GNU Emacs), and various Unix utilities, such as ls, grep, awk, make and ld. In 1991, the Linux kernel appeared, developed outside the GNU project by Linus Torvalds, and in December 1992 it was made available under version 2 of the GNU General Public License. Combined with the operating system utilities already developed by the GNU project, it became the GNU/Linux operating system, better known as just &#34;Linux&#34;. Then came the Linux distributions. Different projects took the Linux kernel, the GNU tools and libraries, additional third party software, documentation, the X Window System, a window manager, and a desktop environment, and combined those components into the distributions. Different distributions focused on different goals, some put focus on the desktop while others put their main focus on servers, and again others tried to provide a multi-purpose operating system. In the past all these different components and projects where developed by open source enthusiasts and communities and the passion for programming and open source was the driving force. This is no longer the case! Please see The real motivation behind systemd. Linus Torvalds has many times made it very clear that he doesn&#39;t care about what goes on in the &#34;Linux world&#34;, all he cares about is the kernel development, and on January 6, 2020 in the &#34;Moderated Discussions&#34; forum at realworldtech.com, Linus Torvalds answered a user&#39;s question, with an absolute jaw-dropping comment, about a year-old kernel maintenance controversy that heavily impacted the ZFS on Linux project. After answering the user&#39;s actual question, Torvalds went on to make very wrong and damaging claims about the ZFS filesystem. Torvalds said: It (ZFS) was always more of a buzzword than anything else. By that statements Linus Torvalds has just reduced more that 15 years of development of one of the most robust and popular filesystems in the world into a &#34;buzzword&#34;! ZFS is described as &#34;The last word in filesystems&#34;. It is a combined filesystem and logical volume manager originally designed by Sun Microsystems. ZFS is a stable, fast, secure, and future-proof filesystem. It is scalable, and includes extensive protection against data corruption, support for high storage capacities, a maximum 16 Exabyte file size, and a maximum 256 Quadrillion Zettabytes storage with no limit on number of filesystems (datasets) or files, efficient data compression, snapshots and copy-on-write clones, continuous integrity checking and automatic repair, RAID-Z, native NFSv4 ACLs, and can be very precisely configured. The two main implementations, by Oracle and by the OpenZFS project, are extremely similar, making ZFS widely available within Unix-like systems. As mentioned in the Wikipedia article, OpenZFS is an umbrella project aimed at bringing together individuals and companies that use the ZFS file system and work on its improvements, aiming as well at making ZFS more widely used and developed in an open-source manner. OpenZFS brings together developers from the illumos, Linux, FreeBSD, and macOS platforms, and a wide range of companies. High-level goals of the project include raising awareness of the quality, utility and availability of open-source implementations of ZFS, encouraging open communication about ongoing efforts toward improving open-source variants of ZFS, and ensuring consistent reliability, functionality and performance of all distributions of ZFS. OpenZFS on Linux, which is the Linux part of the project, has currently 345 active contributors with more that 5.600 commits, and commits are being made on an almost daily basis! Some of the worlds biggest CDN and data storage services runs ZFS on either FreeBSD or Linux! In another situation Linus Torvalds gave an interview on TFiR: open source and Emerging Tech YouTube channel about Linux on the desktop in which he makes another amazing statement saying that Linux still isn&#39;t ready for the desktop and that perhaps Chrome OS is the solution to that problem. These and many other statements by Linus Torvalds show that Linux as an operating system has no real direction and no clear management because the kernel development is performed in isolation from the rest of the Linux world. Linus Torvalds is generally also very open to the rapid influence by corporate interests and his perspective on security is also worrying. In 2009 Linus Torvalds admitted that the kernel development is getting out of control. We&#39;re getting bloated and huge. Yes, it&#39;s a problem ... Uh, I&#39;d love to say we have a plan ... I mean, sometimes it&#39;s a bit sad that we are definitely not the streamlined, small, hyper-efficient kernel that I envisioned 15 years ago ... The kernel is huge and bloated, and our icache footprint is scary. I mean, there is no question about that. And whenever we add a new feature, it only gets worse. At LinuxCon 2014, he said that he thinks the bloat situation is better because modern PCs are a lot faster! We&#39;ve been bloating the kernel over the last 20 years, but hardware has grown faster. This is a very problematic attitude. When software gets bloated it not only becomes more insecure and more error prone, but it also becomes much slower. Thinking that the problem goes away because hardware becomes faster is an immature attitude. In this day and age we need to optimize software so that less power is required, we need to save power and limit pollution. In a 2007 interview &#34;Why I quit&#34;: kernel developer Con Kolivas he stated: If there is any one big problem with kernel development and Linux it is the complete disconnection of the development process from normal users. You know, the ones who constitute 99.9% of the Linux user base. The Linux kernel mailing list is the way to communicate with the kernel developers. To put it mildly, the Linux kernel mailing list (lkml) is about as scary a communication forum as they come. Most people are absolutely terrified of mailing the list lest they get flamed for their inexperience, an inappropriate bug report, being stupid or whatever. ... I think the kernel developers at large haven&#39;t got the faintest idea just how big the problems in userspace are. Besides from the above mentioned problems, the fact of the matter is that Linux as an operating system is put together by different applications from different projects that has absolutely nothing to do with each other. If you don&#39;t know anything about this you should take a look at how you build Linux From Scratch. Another good read that demonstrates some of these problems is the article Linux maintains bugs: The real reason ifconfig on Linux is deprecated. This is very different from the BSDs (meaning FreeBSD, OpenBSD, NetBSD and DragonFly BSD) as each are independent projects that put together their systems &#34;in-house&#34;, so to speak. The kernel, the standard C library, the user land tools, etc., are all part of the base system of the operating system, not something put together from a bunch of different outside sources. In a 2005 interview Theo de Raadt, the founder of OpenBSD, makes the following remarks: I am sure by now you all know that Linux is just a kernel, while OpenBSD is a complete Unix system: kernel, device drivers, libraries, userland, development environment, documentation, and all the tools you need to continue doing development. That said, based just on completeness of functionality, it is not handled like a Linux distribution, not at all. When we find that a change must be made to the system (security or otherwise) we can therefore force such a change into the system by changing it all the way from userland through the libraries down to the kernel. We can change interfaces as we want to. We can move quickly. Sometimes changes are even made which break previous executables; but if we need to, we can choose to make such decisions. This gives us great flexibility to move forward fast. If something is designed wrong, and the fix depends on changes in more than just the kernel, we can fix it by. We change all the required pieces in the right places. We don’t need hacks in the wrong place to fix a problem. Linux is being heavily influenced by corporate interests A Linux distribution is a collection of tools written by different groups of people, often with conflicting interests and priorities, and because of this fragmented structure of the GNU/Linux operating system, the project as a whole is rapidly spinning out of control as it gets pushed around by commercial interests. Even the best GNU/Linux distributions, such as Debian GNU/Linux and Arch Linux, that are still mainly driven by open source communities, are not immune to this problem because they not only still depend heavily on the fragmented tools, but several developers has been hired by some of these major commercial companies. In my article The real motivation behind systemd I have written about how the primary reason for developing systemd is Red Hats financial interests in embedded devices, primarily at the U.S. Military and the automobile industry. Initially systemd was released as a new init system, but it has slowly grown into what Poettering describes as &#34;a suite of software that provides fundamental building blocks for a Linux operating system.&#34; In an interview with Red Had CEO Jim Whitehurst he states: We partner with the largest embedded vendors in the world, particularly in the telecom and automotive industries where stability and reliability is the number one concern. They easily adapted to systemd. I have nothing against the &#34;init&#34; part of systemd, but systemd is no longer just an init system, and the main problem with systemd is that its continued development is motivated by a company&#39;s financial interests and not the open source community interests. As such, the adoption of systemd by the major Linux distributions, such as Debian GNU/Linux and Arch Linux, was a big mistake in my humble opinion. They have made themselves heavily dependent upon systemd and Red Hat. This is pure speculation, but I must also admit that I suspect systemd to be a platform for introducing security holes into the Linux operating system. These will of course look like normal &#34;programming mistakes&#34;, however some of these bugs resembles what happened with the OpenSSL Heartbleed bug quite a lot. And it is a well know strategy used in the open source community, to use &#34;programming mistakes&#34; to create backdoors and other issues. systemd has a long line of standing and open bugs (more than 1.400 open bugs as of writing) that still haven&#39;t been fixed since 2015, yet instead of fixed all of these bugs, the systemd developers keep adding more and more clutter to systemd! Another heavy influence on the Linux world is Google. Google has developed both Android and Chrome OS, both Linux kernel-based operating systems. Chrome OS is derived from Chromium OS and uses the Google Chrome web browser as its principal user interface. Chrome OS is viewed as a competitor to Microsoft, both directly to Microsoft Windows and indirectly the company&#39;s word processing and spreadsheet applications, the latter through Chrome OS&#39;s reliance on cloud computing. And this is one of the core problems with Chrome OS, it is built with great reliance of Googles cloud infrastructure. Google has become one of the most controversial companies. Google is in its essence an advertisement company and it has become famous for their manipulation of search results and extreme user tracking capabilities, mainly thanks to the stupidity of web developers adding Google Analytics to their websites. In a YouTube video from August 2019 by Linus Tech Tips, Linus Sebastian demonstrates how tracking on the Internet works and how it affects the prices you get offered when you search for products. Please note: The video was sponsored by Private Internet Access, a company that has since been bought by Kape Technologies which is known for sending malware through their software and for being really scummy in general. Don&#39;t use Private Internet Access! Cloudflare is another American web infrastructure and website company that is affecting different areas of the development. The company provides services that actually sit between a website&#39;s visitor and the Cloudflare user&#39;s hosting provider, acting as a reverse proxy for websites. As such Cloudflare has become one of the greatest cancers of the Internet. The systemd developers has managed to integrate Cloudflare, Quad9 and Google into the core of systemd-resolved, which is now something you have to manually disable (opt-out). With the growing influence of Red Hat (IBM) through systemd, they have managed to steer the direction of most of the major Linux distributions in a direction that contradicts what many system administrators and users would like to see. BSD is the place to be Contrary to the Linux distributions the Berkeley Software Distribution (BSD) is not a fragmented project. The BSD projects maintain the entire operating system, not only the kernel. BSD was an operating system based on Research Unix, developed and distributed by the Computer Systems Research Group (CSRG) at the University of California, Berkeley. Today, &#34;BSD&#34; refers to its descendants, such as FreeBSD, OpenBSD, NetBSD and DragonFly BSD. These projects are real operating systems not just kernels and they are not &#34;distributions&#34;. Linux distributions, such as Debian GNU/Linux and Arch Linux have to do the work of bringing together all the software required to create a complete Linux operating system. They need the Linux kernel, the GNU tools and libraries, an init system, and some amount of third party applications in order to end up with a functioning operating system. In contrast the BSDs are both a kernel and a complete operating system. For example, FreeBSD provides both the FreeBSD kernel and the FreeBSD operating system. It is maintained as a single project. No one person or corporation owns BSD. It is created and distributed by a community of highly technical and committed contributors all over the world. Companies also use and contribute to BSD, but contrary to Linux, a company cannot &#34;hijack&#34; BSD. A company can make their own version of BSD, such as Sony Computer Entertainment has done for their PlayStation 3, PlayStation 4 and PlayStation Vita gaming consoles, but because the BSDs are complete operating systems, and because each BSD project is maintained and developed by open source enthusiasts and communities, not companies such as Red Hat, the BSD projects are really and truly independent. The result of this organization of the BSDs you wont find crazy opt-out spyware settings in your basic installation, no matter what BSD project you choose, and you don&#39;t find privacy compromising solutions integrated into the operating system core components. On the contrary, because the BSD projects are developed and driven by skillful and enthusiastic people who care much about operating system design, security, and privacy, you will often find that even the third party software that are available for installation using a package manager gets patched so that these problems are removed, such as the disabling of DNS over HTTPS by OpenBSD in Firefox. Another great benefit from all of this is that the communities that surround the BSD projects consist of experienced, helpful, and (for the most part) kind people. License problems The GPL license is more strict on developers and it is an open source anti-pattern as it forces a release of all modified source code and prevents other open source projects from being integrated, for example, the GPLv2 is preventing the integration of DTrace and ZFS in Linux. BSD developers on the other hand have no such restrictions. Manufacturers may opt for BSD as their operating system of choice when creating new devices instead of Linux. This would allow them to keep the code modifications to themselves if they wanted to. With Linux the license force the release of the source code to the public. The GPL license may sound better, because why should we allow companies to simply &#34;steal&#34; our open source code and produce proprietary products without even giving anything back. But it&#39;s not that simple. By forcing companies to release source code to the public via the GPL license, companies quickly become more manipulative. The tactics deployed by Red Hat with the release of systemd was to try to get as many &#34;important&#34; third party projects to cooperate very tightly with systemd, or even depend upon systemd. This way other Linux distributions are more easily persuaded into adopting systemd because of the easy integration of these third party projects. The systemd developers addressed several third party projects and tried to convince them to make their projects either depend upon systemd, such as the attempts made by Lennart Poettering on the Gnome mailing list, and the attempt made by Red Hat developer &#34;keszybz&#34; on the tmux project. Most of these attempts were originally &#34;disguised&#34; as technical issues, however when you read the long email correspondence on the Gnome mailing list and elsewhere, the real intent becomes quite clear. Such manipulation isn&#39;t needed in BSD. Companies are free to do whatever they want with BSD and as such they don&#39;t need to try to affect the way things are going. If that wasn&#39;t the case we would possible see, for example, Sony trying very hard to influence the development of FreeBSD because they use that in their PlayStation products. The different GNU/Linux distributions, such as Debian GNU/Linux, Arch Linux, and even Red Hat Linux back in the days, were really great projects. When projects are driven by passion and not profit they tend to become much better quality. However, when projects are no longer driven by passion, but rather by profit, they often decline in quality. This is natural because motivation by profit is very different from motivation by passion. This is one of the reasons why Microsoft Windows has always been such a crappy OS. The reason for the success of Microsoft Windows on the desktop isn&#39;t because people believe that Windows is a great operating system, no sane and experienced system administrator or IT supporter believes that, rather it is because of the aggressive marketing strategy deployed by Microsoft. While the BSD projects do get both code and occasional financial support from companies, they are driven by passion and not by profit. This mainly means well thought out decisions. There are no compromises with privacy or security for the benefit of profit such as what we may find in Linux. Please see my article The problems with the GPL for further information. Time to migrate everything to BSD Back in about 1998-2000 I started migrating every server and desktop operating system from Microsoft Windows, both at home and at my company, to GNU/Linux, initially Red Hat Linux and then later Debian GNU/Linux. I did that because I had spent about a decade doing Microsoft Windows support and wasted so much time on this absolutely horrible operating system. When I was recommended GNU/Linux by a good friend of mine I was amazed at how well it performed, how amazing the open source communities were, and how all the usual problems related to Windows just vanished. Whenever I replaced a Windows setup with a Linux setup at a customer, a family member, or a friend, the support hours rapidly declined. Of course this meant less customer support work, but this was great because now we could concentrate our time on more important matters. A little later I discovered the BSD world and eventually also began deploying both FreeBSD and OpenBSD on servers and on desktops too. Back in the days Linux often had better hardware support than BSD and as a result I generally used Linux more than BSD. Hardware was expensive and it was not always possible to purchase hardware based upon which operating system you wanted to run on the system. This is different today where the BSDs generally have great support for modern hardware. I still like GNU/Linux, but I don&#39;t want to worry about possibly privacy breaking crap in systemd, or whatever creepware Lennart Poettering comes up with next, I also don&#39;t want to worry about all the bloatware that goes into the kernel, such as the kernel forcing adaption of DRM. I generally don&#39;t want to worry about whatever the next problematic thing is going to be. Everything needs to be sane options and decisions by default! Not opt-out! You can read about FreeBSD in my article FreeBSD is an amazing operating system and about OpenBSD in my article OpenBSD is fantastic. Other relevant links Linux maintains bugs: The real reason ifconfig on Linux is deprecated FreeBSD Committer Allan Jude Discusses the Advantages of FreeBSD and His Role in Keeping Millions of Servers Running. Note: FreeBSD has grown considerably since the interview with Allan Jude back in 2016. Why choose FreeBSD? FreeBSD is not a Linux distribution FreeBSD The FreeBSD Journal OpenBSD The OpenBSD Journal The BSD family tree EuroBSDcon playlists on YouTube </description>
      <pubDate>07 Feb 21 23:22 EST</pubDate>
      <guid>https://www.unixsheikh.com/articles/why-you-should-migrate-everything-from-linux-to-bsd.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://mattorb.com/level-up-shortcuts-hammerspoon-home-row/</link>
      <description>&lt;a href=&#34;https://mattorb.com/level-up-shortcuts-hammerspoon-home-row/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; After my initial experiments using Karabiner to bind a keyboard shortcut to an image-based cheatsheet and leverage a hyper key to avoid application level key binding conflicts and finger twister, I was super excited.  I had combed through many blogs and git repositories.Having an extra meta key with a whole slew of nonconflicting keybinding slots seemed awesome.   I like keyboard shortcuts bound to keys based on a pnemonic, so binding my global spectacle keyboard shortcut cheatsheet to hyper+k [&#39;k&#39; for keys] seemed like a good, quick win that would help me memorize those Spectacle shortcuts.Incremental and Safe . . . time to test it out!  👍The Home RowThen, mid-way through patting myself on the back, I discovered global vi mode with a hyper key and Karabiner.  With this Karabiner recipe enabled, you press a hyper key and then h/j/k/l to move the cursor left/down/up/right.  Additionally, tab can be rigged up to act as a modifier in conjunction with the hyper key to enable quick access to home/end/pgdn/pgup – which is especially awesome when you are on a laptop keyboard.   In order to to try out global vi mode w/Karabiner, I had to sacrifice hyper+k from  the Spectacle cheatsheet and switch that to be right-option+k.  Hey, another key I never use.  right-option.  hyper2? Fast forward a bit.  I dug deeper and deeper into the Hammerspoon ecosystem, looked at more custom Karabiner and Hammerspoon settings, and heard some feedback from my first post.Hammerspoon is an application you run in the background that loads custom Lua scripts to interact with your system, allowing you to script behaviors to react to system events.   One kind of system event is a key press.  For our purposes here, we&#39;re exploring Hammerspoon primarily in the context of using it to react to keyboard shortcuts and trigger something in response.  It has many other uses though.   Keep in mind we&#39;re talking about system-wide key bindings here – not something isolated to a single application.  🤯A unit of re-usable scripting in Hammerspoon is called a &#39;spoon&#39;.   After trying out many different spoons and customizations, two projects really caught my eye:FryJay&#39;s MenuHammer - &#34;A Spacemacs inspired menu system&#34;.   A quick, customizable, nestable, menu system you can access via a system-wide keyboard shortcut.Jason Rudolph&#39;s Karabiner+Hammerspoon setup which focuses on home-row centric shortcuts for cursor navigation and window managment (ht: @gregvaughn)Thinking about these in terms of the capabilities they enable relative to the caps-lock hyper key and Spectacle cheat sheet via a shortcut that we put together in the previous post, it seems there is a lot of potential. Let&#39;s explore. Key ChordsHammerspoon can enable key chords*.Karabiner can enable key chords too, but I found the nuances of getting it implemented well in Karabiner, for alpha keys, with the current version, to be painful. **  Wait wait . . . What is a key chord you say?  So, the aforementioned JR&#39;s setup has a &#39;super duper&#39; behavior where pressing the &#39;s&#39; and &#39;d&#39; keys at the same time acts almost like another new modifier key.  waaaaaaaat?  That new modifier is then leveraged for some home row centric key bindings that can substitute for moving your right hand or stretching the right pinky down to the arrow keys.Here&#39;s how it works: while &#39;s&#39; and &#39;d&#39; are pressed and held with the left hand, you use the right hand to press h to move the cursor left.   &#39;h&#39; for left, &#39;j&#39; for down, &#39;k&#39; for up, &#39;l&#39; for right.   You may recognize these commonly used vi cursor movement keys.  With hand in place, pressing down the &#39;s&#39; and &#39;d&#39;, you can press &#39;a&#39; to add the option/alt modifier, &#39;f&#39; to add cmd modifier, or spacebar to add shift modifier.  Those keys are all lying right under the fingertips of where the left hand is already positioned, minimizing the needed movement. I thought it would be nice to form the muscle memory for those h/j/k/l vi cursor movement keys as a side benefit for when I might occasionally run vi on a remote server.  Mostly though, I&#39;m thinking about not having to pinky stretch down to those tiny cursor keys or reposition the whole right hand each time I want to navigate the cursor from a laptop keyboard.What really pushed me over the line of I have to try this now was seeing those same home-row keys used for cursor movement -and- window positioning in a consistent way.Modes and MenusWhere key chords are generally more temporary states, enabling a behavior only if all the right keys are pressed in close proximity spatially and temporally, a &#39;mode&#39; let&#39;s you toggle into a state where keys react differently until it is dismissed.   Menus are similar and dismiss automatically upon the selection of an action.So in the context of JR&#39;s setup, when you hit control-S, you enable window layout mode.  Then, you press one of the h/j/k/l/i/o/,/. keys while focused on a particular window, and it will resize&amp;move the window to left/down/up/right/top-left/top-right/bottom-left/bottom-right.  This re-uses some familar home row mappings (i.e. - &#39;h&#39; = left whether it is for cursor movement or shifting a window to the left side of the screen).  A few other window resize/move keys are added (i/o/,/.) in a place that makes sense spatially for shifting windows to corners.  For example, out of the i/o/,/. keys, the &#39;i&#39; is the top left key on the keyboard and it arranges the window to the top left position.  After you make a selection the window layout mode dismisses itself.  There is a &#39;showHelp&#39; flag built in to the lua script that drives window layout mode.  When you set that flag to true, a built in cheatsheet displays when in window layout mode.   Note: I re-bound this mode to hyper-W to preserve ctrl-S for application-level bindings.  The built-in &#39;cheatsheet&#39; for window layout mode via JR&#39;s hammerspoon setup (set showHelp=true)It&#39;s worth noting that since I have had this working, I&#39;ve been using Spectacle less and less.  At the moment, it does not have feature parity with Spectacle, but it does seem possible to get there. 🤔Another variation of the idea of a &#39;mode&#39; is a shortcut driven menu.   Once activated, you get a list of options, along with the keys to quickly navigate and select from those options.  This is where the Menuhammer project comes in.  Menuhammer allows you to set up a totally customizable menu you can access system-wide via shortcut.  From that menu, you can trigger any action hammerspoon can initiate – launching applications, macros, running scripts, laying out multiple windows, etc.   My current Menuhammer system-wide menu (bound to Hyper-Space)It looks like a great place to house things that are launched less frequently like occasional use macros, or toggling Wi-fi on and off.  I&#39;m thinking this a good landing spot for things which either (1) are not used frequently enough to give up an immediate action keybinding slot or (2) are not used frequently enough that it is worth memorizing.  I bound it to hyper-space to try it out.  After using it for a while, I found it useful to bind the application submenu to Hyper-A and the Finder submenu to Hyper-F to jump directly to those as needed.   Application submenu, bound to hyper-AFinder submenu, bound to hyper-FNote the really cool feature of those hyper-F menu items:  Several of them launch Finder -and- send specific key to it, immediately selecting the &#39;Download&#39;s directory for instance.I&#39;m still experimenting with what feels most comfortable for applications I use regularly vs less often . . . especially in regards to putting applications on the shortlist in MenuHammer vs binding a key to launch them directly via hammerspoon. Hyper key -and- Super Duper key chord?Given what we did in part 1 of this series, where caps lock became a hyper key, with tab acting as a modifier when held, how does how does super duper mode compare to that and can these two worlds live together?  If not, which one is more comfortable and effective?With very few changes, I was able to fully enable Super Duper mode, and keep in place many of the caps-lock based hyper key things I already had.  This is for both cursor movement and window management (Spectacle vs Hammerspoon scripted).  This allowed me to compare the comfort level of each to see which one feels right.  After using super duper mode for a bit, I&#39;m finding it a lot more comfortable, and have swapped out caps lock to be a ctrl key on hold instead of being my hyper key which maps to ctrl-alt-shift-cmd.   I have moved that hyper key down to the right-command key position, which I&#39;m finding convenient.   I&#39;m also experimenting with an &#39;a&#39;h &#39;f&#39;udge 😜  mode which maps  home/pgdn/pgup/end keys to h/j/k/l when the a+f keychord is held.  As always, all my curent setup can be found in my dotfiles repo.  * This is what I like to call them at the moment.** Karabiners key chord pains: dupe keys, missed key presses, missed dropping and enabling of chord state, tried many solutions including stuff currently marked as &#39;working&#39; in Karabiner recipes.  All had problems with either messing up my normal typing of words or not working consistently enough. </description>
      <pubDate>07 Feb 21 17:12 EST</pubDate>
      <guid>https://mattorb.com/level-up-shortcuts-hammerspoon-home-row/</guid>
    </item>
    <item>
      <title>The Real Benefits Of Staying Off Social Media</title>
      <link>https://durmonski.com/life-advice/benefits-of-staying-off-social-media/</link>
      <description>&lt;a href=&#34;https://durmonski.com/life-advice/benefits-of-staying-off-social-media/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; “Don’t forget to like. Subscribe. Share. Hit – no! – smash that bell icon. Of course, follow me on all of my social media accounts so I can tell you how my life is better than yours.” These days, no one wants you – your physical company. People are aiming towards your attention and your credit card. If you are here reading, you probably acknowledge the huge amount of time you spend each day inside the soul-sucking algorithmic online playground that now represents a large part of our overall existence. And though you realize your dependence on social media, you still can’t quit. I get it. Letting go of something that consumes a good part of your life is a tough nut to crack. Worry not! In this post. I’m combining my personal experience with scientific research and adding a flavor of fictional characters from a famous book to help you break from your social media addiction. The goal? Carve out “me time” and find something worthy to do. An activity, that will help you reconnect with the curious, adventurous, creative, and brave side of your identity. A task that will serve as a source of sanity and an escape from the fractured anxiety caused by mindless scrolling. And before you say it, let me interrupt you… I know! There are quite a few articles explaining the benefits of staying off social media or such that talk about digital detox. I’m, personally, contributing to this topic for plenty of reasons – the ones just mentioned: help you regain your sanity, find more time, stop obsessing over others. But most of all, because, I believe that these days you should focus more on doing, not on consuming. Apart from what everyone else is sharing, though. I believe that most publications miss an important point. Yes, quitting social media or doing a social media detox will calm your senses and increase your well-being. Plus, you keep your personal data for a little longer. But there is something else. Something I don’t see others acknowledging. The main benefit of taking a break from social media is that you stop living in a fictional world and finally start dealing with your own reality. With the recent rebranding of the main player in the field – Facebook rebranding to Meta.1 I think that in the future, visiting social media will look more and more like the world described by Ernest Cline in his masterpiece Ready Player One.2 A futuristic dystopia. A place where the real world sucks. Where people consider talking to someone a chore. Connection, closeness, ambition are replaced by detachment and dogmatic slumber. Society is satisfied with shallow thoughts and the pursuit of artificially created stimuli in an imaginary world. If you still think that life is impossible without impulsive scrolling. That social connection only happens through a plastic screen – soon a VR set. Hopefully, in this post, I’ll convince you otherwise… That social media is to be avoided and that it’s about time to consider doing something other than doomscrolling. We need to get this out of the way, first. Before you realize that living inside your phone is not healthy. You need to hear about the initial angst that will emerge from disconnecting from the virtual world. A recent study about social media detoxication proves that taking a brief hiatus from social media won’t contribute much to your well-being. The researchers didn’t find compelling evidence to support the argument that abstaining from social media has a substantive and measurable positive impact on the ways people feel. The keyword here is brief. As you read through the study, you find that the practice of digital detoxication was done only for a day from the participants. I mean, they only stayed away from Facebook for around 24 hours. Insufficient time to understand the positives from social media abstention. They also found a direct correlation between negative effects and satisfaction for the given day. It seems that the more we use social media, both our satisfaction increases along with our dissatisfaction. Exactly like cigarettes and alcohol. If you smoke, you can feel both good and bad. Good because of the chemical released from the black tar (nicotine) you smoke. Bad because of the negative consequences – bad breath, cough, difficulty breathing, impatience provoked from when you’re not smoking. Therefore, if I know that I’m only going to quit for a day, I can’t get all the positives. Internally, I’m going to think about the next day. The day where I can smoke again. I’m saying this from experience. I’ve quit cigarettes several times in the past until one day I decided that I’ll no longer smoke. And only after around 2 to 3 months of not smoking – 3 months of internal dialogues where I almost convinced myself to start again – I was able to find joy in not holding a cigarette. The same happened when I decided to quit social media a couple of years ago.3 Initially, I got this itchy feeling. The desire to plug my brain back into the infinite feed and get my dose of memes and pictures that makes us feel strangely good. Only after a couple of weeks, I was able to spot the real benefits. So, to answer the question, is disconnecting from social media healthy? Yes, but it will feel quite strange at first. It’s quite normal to feel stressed and “worried” when you attempt to quit social media. Every withdrawal from something you’ve done for years will trigger physical and emotional responses. You’d want to get back but persisting and staying away for good reportedly – as mentioned in another study – leads to increased life satisfaction and more positive emotions. This is another common question from people who are considering leaving social media. The answer here is like pretty much everything important in life: It depends. In yet another study I’ve read about quitting social media I found that responders faced many problems during a brief hiatus from the said networks. They felt bad about not being able to connect with their loved ones, difficulty dealing with boredom, and also dull because their usual entertainment channel was missing. Why is that? Shouldn’t it be the other way around? We are here talking about staying away from social media after all? Correct. But I fully get why the participants in the study didn’t find any positive effects when abstaining from the evil platforms. If you’re using social media with the sole purpose to have fun, and this is no longer an option. I’m sure you’ll be pissed. It’s like throwing away your TV if you’re hooked to a particular show. Plainly, you can’t find joy in not using social media if you don’t internally believe that you don’t need it. After all, if you ask a smoker who doesn’t think that smoking is bad to quit for a day he won’t find any joy in this. He will resent you. People are mostly using social media to escape their current reality. A place they turn to when the real world is simply making them feel “blah”. When you use social media for these reasons, you will never see the benefits. Leaving social media will only make you happier when you stop idolizing it. When you find other activities that bring you joy and realize that it’s destroying your real life. For this to happen, as you can imagine, you need to be aware of all the problems caused by intensive social media use. Here’s what will happen if you decide to take the uncommon path. A life without social media. A life where you proudly confront the dullness. The dissatisfaction emerging from having nothing to do. You Finally Face Your Problems In the book Ready Player One. The OASIS – the virtual world described by Ernest Cline – serves as an escape from reality. In the book, real life is hard. Ugly. Almost destroyed. That’s why, The OASIS is such a visited place. More popular than the real world. Inside this meta-universe, there are no rules. You can be whoever you want to be. It’s literally an oasis. A place that relieves you from the harsh and monotonous desert called life. Does this remind you of something? Aren’t we all entering the current popular social media channels to do the same – i.e., escape our current problems? I think so. We never log out of Facebook and Instagram because we follow the mantra, “The best way to solve my problems is to avoid them.” The truth is, that this leads to a compounding effect – but on the one leading to net positives. The more you avoid your problems, the bigger they become. You Stop Consuming And Start Doing It’s always much easier to do some more research on a particular topic and stall taking action. The bad thing is that when you have unlimited access – the world’s information at your fingertips – it feels like there is always something more you can find. Something else that will help you make the best decision. Twitter, the platform that’s now famous for infinite-like threads, is the best example here. Since it’s trendy to leave your job and start your own thing. The demand for more posts by famous avatars is at its peak. People are crazily obsessed with adding more and more tweets to their collection – unrolling Twitter threads. Is this helpful, though? Well, no. Spending every waking moment browsing for more insightful information about doing something actually prevents you from doing the thing you want. You are spinning your wheels. Thinking and talking about doing something but never actually doing it. There are no amount of Twitter threads – or Instagram carousels – that will make you a better coder, manager, writer, or startup founder. You can only improve if you put the online tips you so passionately save into practice. Focusing on the saving part will delay your progress – if you ever start at all. You Break Free From Your Self-Imposed Prison Cell To enter The OASIS, Wade, and every other player in the book Ready Player One use rigs, gloves, and something like spacesuits. Hardware that’s making the virtual world feel more real to the body and mind. At some point in the story, Wade, the main character in the book describes his equipment as follows: “I’d come to see my rig for what it was: an elaborate contraption for deceiving my senses, to allow me to live in a world that didn’t exist. Each component of my rig was a bar in the cell where I had willingly imprisoned myself.” Similarly, as proud as you might be for the beautiful Instagram collection you’ve created. Realistically, these look more like bars added to your prison cell. The more you obsess with buying things – clothes, gear, etc. – that helps you make more perfect photos. The more dependent you become on the platform. At some point, it becomes your prison and you grow more and more disconnected from reality. Happiness is associated with getting likes, not having meaningful conversations with real people. Sure, some people succeed at the online game by reaching the status of an influencer but that, too, comes at a cost. You no longer experience real life. Your actions and thoughts are solely directed towards making your virtual life look better. It’s a rat-race lifestyle that feels inescapable. You Stop Obsessing Over Others As I wrote in one of my recent posts, you become what you consume. When you see someone purchasing something online, your mind immediately starts to want this product. Even if you don’t need it, you consider this as an option. The more posts you see, the more products you don’t have will line up for consideration from your brain. You visualize yourself owning this and you also imagine that the item will solve all of your problems – we shift the burden. Not only that. Online, you are constantly presented with a life that is absurdly impossible to sustain. The resorts. The vacations. The perfect homes. But that’s just decor. A set to sell you something. A lifestyle. A way of thinking. And, of course, some sort of products. A study exploring the relationship between frequency of Instagram use and psychological well-being revealed some scary details. The results showed that obsessive use of Instagram leads to lowered self-esteem and physical anxiety. The more you see perfect bodies, the more flaws you find in your body. Quite normally, you feel worse about yourself and your self-perceived attractiveness goes down to zero. Social media becomes a dangerous loop where you’re constantly reminded of what you don’t have and of the person you’re not. Since people post only their best images online. And since perfect is everything you see when “connected” with others. Perfect becomes the expected norm. You desperately want the same level of perfection in your life but when not achieved, you feel inferior. Only when you detach yourself from this perfection, you can find calmness and joy in the imperfection that’s a big part of everyone’s life. You Realize That You Will Never Get Everything You Want But That’s OK Imagine having everything you ever wanted. Would you be finally relieved or you will go crazy because you won’t have enough space to store all of the crap? Happiness is quite subjective. For different people, it means different things. What I found out though is that happiness doesn’t come from pursuing more things to buy. It comes from practicing a set of activities that bring you joy. For me, it’s writing. For you, it’s probably something different. Sure, we all feel good when we buy things. But this quickly fades. A new feeling always emerges and replaces the satisfaction that came from the previous purchase. A new object will enter your view and destroy all the previous positive sensations and replace them with negative ones that say, “your life is still not perfect, you are still not in the possession of X.” What really counts is having a life where you practice a set of routines that make your life meaningful. The author Erich Fromm describes this as having versus being mentality.4 When we use social media, we are obsessed with having things. We see what others have, and we think that we’ll reach their level of happiness only when possessing the same objects. This never comes. Being, the contrasting venue is what should be on our aim. We understand that we can’t feel lasting happiness by accumulating things. It comes from doing meaningful things. The reason you feel good about realizing that you will never get everything you want is because wants emerge all the time in your mind. There is always something you want. But that doesn’t mean that you actually need it. That’s the mentality of being. When you focus on being. You feel good about practicing activities that bring you joy and meaning – painting, drawing, writing, building something. Thus, you move away from wanting to have more to wanting to do and experience more. You Uncover What’s Terrifying In Your Real Life “As painful as reality can be, it’s also the only place where you can find true happiness. Because reality is real. Do you understand?” These lines are from the movie Ready Player One. They explain why James Halliday created The OASIS.5 He did it because he wasn’t sure how to connect with others. He was looking for an alternative way, a safer way, to build relationships with others. The above is the equivalent of using modern dating apps. It’s emotionally painful to get rejected in real life. That’s why apps like Tinder are so popular. A drink in your face hurts but you can live with a swipe. How this habit affects your life though? You never learn vital skills. A person who is only using social media to talk to others never learns how to actually talk to others. Besides, he breaks when there’s even the slightest obstacle in the way. We secretly avoid real interactions because we’re afraid. Afraid of rejections. If this is the case – and it commonly is – my question to you is to observe why you’re so addicted to the platform. What are you avoiding? As stated, as terrifying reality is, it’s still the only place that’s real. Online, things are safer. You can’t get injured. But if we do this all day, how to we mature? We don’t! We need to expose ourselves to more obstacles. The more we fall, the more we learn to pick ourselves up. Thus, we get stronger. You Will Be Amazed How Nuanced Life Is I still remember life without social media. I’m a kid of the ’90s. Born in 1988, my childhood was spent mostly outdoors. We didn’t have phones. Our parents had to roam the neighborhood, find us, and physically drag us home so we can eat together. Amazing times. Nowadays, I think that our whole physical life is a constant effort to create this flawless online persona. We’re always refreshing our feed to find more things that we can later acquire. Once we have them, we’re eager to share our newest possessions to evoke envy in others. It’s like the more others envy us, the better we feel. We don’t ever stop to look around us. Media have convinced us that what’s around us is unimportant. Only what’s online has value. And only if it’s with the right amount of followers. People look at you and then they quickly gaze back to their phones. Searching for a higher status. You don’t count. You’re not on the screen. You’re not famous enough. Only when you disengage from this madness you can finally feel and appreciate what’s right here, next to you. It might not be shiny enough. But it’s surely more real than what’s out there. Some Closing Thoughts People will do anything than consider their dreadful reality. The greatest source of suffering for the modern man is the most banal: Boredom. We can’t survive even a minute without doing something. That’s why social media websites are so popular and so hard to quit. They offer endless streams of positive sensations with almost no effort – you just have to move your finger. Even if life throws a curveball and we need to fight in order to survive. Resting after a wrestle proves intolerable because of the boredom it produces. That’s why I personally think that the best way to leave social media. The most adequate antidote is finding something else to do when you have nothing to do. Something that’s aligned with your long-term goals. Doing. Not consuming. Practicing something that can help you learn something new. Or, as strange as it might sound, even consider talking to the person sitting right next to you. Dare To Act: Join The Study Newsletter. A biweekly bookish newsletter inspiring you to commit to continuous growth. Great for lifelong learners, creators, and wanderers alike. The Verge. Mark Zuckerberg on why Facebook is rebranding to Meta. Heath, AlexA science fiction book turned after that into a movie.I unfollowed everyone online 3 years ago. The best decision in my life.You can read about this comparison in more detail here and here.James Donovan Halliday is the side character and tech industry businessman in the backstory of Ready Player One. </description>
      <pubDate>08 Nov 21 12:14 EST</pubDate>
      <guid>https://durmonski.com/life-advice/benefits-of-staying-off-social-media/</guid>
    </item>
    <item>
      <title>How to lose $1B in 10 seconds</title>
      <link>https://thehustle.co/gerald-ratners-billion-dollar-speech</link>
      <description>&lt;a href=&#34;https://thehustle.co/gerald-ratners-billion-dollar-speech&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; When Gerald Ratner took the stage before 6k high-powered businesspeople, journalists, and dignitaries at London’s Royal Albert Hall in April 1991, he had no idea his speech would be a professional death sentence. His incredible success had led him to this moment. He’d inherited a struggling chain of jewelry stores and turned it into a £1B enterprise in less than a decade. He’d flipped an elitist industry on its head by making earrings and rings for the working class. And in the process, he’d built his company, Ratners Group, into the UK’s biggest, and most recognizable, jewelry chain. But in a matter of seconds on that fateful night in April, a few jokes would destroy it all. The rise of a populist jeweler Gerald Ratner joined his father’s small, fledgling jewelry business in 1965, at the age of 15, after being expelled from grammar school for “being too stupid.” He spent his youth cleaning up the shop, running errands, and getting to know the “grass roots of the business.” When he inherited the company, Ratners Group (AKA “Ratners”), in 1984, it consisted of 120 bland, traditional storefronts, and was posting annual losses of £350k (US$459k). A Ratners jewelry store in 1991, complete with garish advertisements (via Getty Images) In his youth, Ratner had learned a valuable lesson by observing London’s street shops: the vendors who “yelled the loudest and had the most garish, eye-catching displays” landed the most sales. He decided to employ the same strategy at Ratners. Within months, all Ratners locations were plastered with vibrant orange and red posters with all-caps pitches like “LAST CHANCE — MEGA RED STAR SALE!” and “SALE SALE SALE: HALF PRICE!” Everything in the window was clearly marked with a price tag. Prior to the 1980s, jewelry had largely been elitist. The average item cost over £300 (about US$950 in today’s dollars) — and jewelers thrived on an aura of exclusivity and prestige. Ratner made a decision to market his chain toward a wider working class demographic, offering earrings, bracelets, and rings for an average price of just £20, and as low as £1. “I put the earrings and chains in the front of the window and diamond rings at the back, and played pop music,” he later told the Financial Times. Enjoying this article? Get the Hustle’s 5-minute weekday roundup that keeps you hip to happenings in tech, business, and internet… things. Delivered weekdays plus a bonus Sunday feature. Unsubscribe whenever. This approach came at a cost: other jewelers (and the press) constantly berated Ratners for selling “cheap” and “tacky” products. Top: Ratner surveying his goods; Bottom: Ratner proudly displaying his wares shortly after he took over as CEO of Ratners Group (via The Telegraph) But Ratner’s strategy paid off: by 1990, he grew Ratners from 120 to more than 2k stores, captured 50% of the UK’s jewelry market, and had annual sales of £1.2B (US$1.57B) — £125m of which was profit. They bought up competing chains like Jared and Kay Jewelers. In quick order, Ratners became a household name and the great democratizer of a previously stuffy industry. “[I] just felt [I] could not fail,” said Ratner. Until, of course, he did. The speech that broke the businessman’s back In 1991, Ratner’s success earned him an invitation to speak at the prestigious Institute of Directors annual convention. Leading up to his speech, Ratner passed a draft by a public speaking consultant, and was given some advice: “I think you should put in a couple of jokes,” the man told him. “People like your jokes.” Unfortunately, Ratner took it to an extreme. On the night of April 23, 1991, Ratner began his speech (in full here) innocently enough, harping on the event’s thematic values of quality, choice, and prosperity. Then, about 3 minutes in, he dropped several brutally honest jokes. “Ratners doesn’t represent prosperity — and come to think of it, it has very little to do with quality as well,” he began. “We do cut-glass sherry decanters complete with six glasses on a silver-plated tray that your butler can serve you drinks on, all for £4.95. People say, ‘How can you sell this for such a low price?’ I say, because it’s total crap.” Then, several minutes later, just for good measure: “We even sell a pair of [gold] earrings for under £1,” he said. “Some people say, ‘That’s cheaper than a prawn sandwich!’…I have to say, the sandwich will probably last longer than the earrings.” Gerald Ratner’s ill-fated jokes (The Hustle) The next morning, Ratner awoke to terrifying news: his comments had made national headlines to the effect of: “JEWELRY CEO CALLS HIS OWN PRODUCTS ‘CRAP.’” The Sunday Times dubbed him “Gerald Crapner” — a nickname that caught on with disgruntled ex-customers. Initially, Ratner tried to play it off by featuring special in-store promotions that put a “humorous twist” on his remarks — but within a few weeks, it was clear that what he’d said had taken an irreparable toll on his business. The downfall Within a few days of the speech, Ratners Group shares dropped by £500 million (US$1.8B today); by the end of 1991, its stock was down 80%. One-time enthusiastic customers boycotted the brand and Ratners, which was quickly losing sales volume, was forced to close down hundreds of stores and lay off a hefty percentage of its 25k-person workforce. The company claimed there had been a “shift in consumer spending habits,” and that the ongoing recession had finally caught up to its bottom line. But stock charts show the company suffered clear consequences from Ratner’s speech. Ratners stock declined by as much as 80% within 10 months of Ratner’s comments (Zachary Crockett/The Hustle) In November of 1992, Ratner was let go as CEO of Ratners Group. The day he left, he sold his shares for “pittance” to pay off the £1B (US$1.3B) he owed the bank, and walked away with nothing. The Ratner effect In an age where tweet-happy CEOs are empowered with large digital audiences they can instantly broadcast to, Ratner’s story is a worthy parable. Today, the phrase “Doing a Ratner” is British parlance for any time someone says something stupid that undermines his or her own product or customers — something that tends to play out more often than it should. We’ve seen many similarly high-profile stumbles in recent years: Helen Mirren, actress and paid brand ambassador of L’Oreal, said that using the company’s products “probably does f*ck all.” Matt Barrett, ex-CEO of Barclays, insinuated that customers shouldn’t use the bank’s credit card products because they could “pile up debts.” John Pluthero, then-CEO of the telecom giant Cable &amp; Wireless, sent out a memo calling his company an “underperforming business in a crappy industry.” Michael O’Leary, CEO of Ryanair, called his passengers “idiots;” on another occasion, he said customers who ask for a refund should “f**k off.” Chip Wilson, founder of lululemon, told customers his products “don’t work for certain women’s bodies.” A Harvard Business School study that analyzed instances of CEO misbehavior between 200 and 2015 found that the average comment (or action) resulted in 250 negative news stories (some of which were cited up to 5 years later), and a 3.1% decline in company stock price. Though not true “Doing a Ratner” situations, we’ve also seen a number of business leaders say incredibly stupid things in a very public way, with very real financial consequences: In 2018, Tesla stock fell 4-5% after Elon Musk spoke — once in April, when he joked about going “bankrupt,” and again in July, when he called a Thai cave rescuer a “pedo guy.” Elon Musk (right) has drawn comparisons to Gerald Ratner in recent years, in part for his constant barrage of controversial tweets that impact Tesla’s stock (via Daily Express, SpaceX) As for Ratner? After losing everything, he toiled in misery for years — but he eventually made an improbable comeback. In 1997, he took out a £155k (US$203k) loan on his house, built up a health club business, and sold it for £3.9m (US$5.1m). He then used the profits to start an online jewelry company. (The Ratners Group rebranded as Signet in 1993; today, it is the largest diamond retailer in the world.) But Ratner is unlikely to ever live down his ill-timed remarks nearly 3 decades ago. ‘It doesn’t seem to matter that in the ‘80s I was Britain’s largest jeweler, with over 50% of the UK market,” he told This Is Money. “My obituary will be all about being a disaster.” Get the 5-minute roundup you’ll actually read in your inbox​ Business and tech news in 5 minutes or less​ 100% free. We don’t spam. Unsubscribe whenever. </description>
      <pubDate>24 Mar 20 12:28 EDT</pubDate>
      <guid>https://thehustle.co/gerald-ratners-billion-dollar-speech</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.intel.com/content/dam/www/program/education/us/en/documents/project-design/strategies/dep-question-socratic.pdf</link>
      <description>&lt;a href=&#34;https://www.intel.com/content/dam/www/program/education/us/en/documents/project-design/strategies/dep-question-socratic.pdf&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;%PDF-1.4 %���� 1 0 obj &lt;�&#39;���n��d!B��)/Creator(q[�4�f�ȯZ1\)d&gt;N�����й���MF��)/Producer(q[�4�f�Ȼw��l9G�ɹ����W2�tMꃄ[�q���$/ ,}�R�\b ��������mp��0W�������0q��H�w�9����I�v7.%�&gt;x)/Project#20Phase(u@�7�uٜ�q�)/Title(dP�{%hۚ�j��%�^����������NL朙��$�)/ModDate(t��kG5���*FU0l�Đ�����X)/SourceModified(t��kF0���&#39;EW4`��)/CreationDate(t��kF0���&#39;F\\6`�ɖ�����X)/Author(}Y�&lt;�uݜ�_�\n%�Y���)&gt;&gt; endobj 2 0 obj &lt;&gt;stream 5��������G;Y���2+�%Kd��۝�I�����u�����=� �D[����ݭ����wI���f�m����D����w�f���L���G���y���T��^Pvl�^&#34;)��̖�Ӗ�:��k���� y� ����Ai��������nzښ�0X%����(��+I�s��r���W$ \cR7mcFs���|������wy���v�Ά��GK6at� �dK!|��</description>
      <pubDate>21 Apr 20 14:48 EDT</pubDate>
      <guid>https://www.intel.com/content/dam/www/program/education/us/en/documents/project-design/strategies/dep-question-socratic.pdf</guid>
    </item>
    <item>
      <title>The Trail Leading Back to the Wuhan Labs</title>
      <link>https://www.nationalreview.com/2020/04/coronavirus-china-trail-leading-back-to-wuhan-labs/amp/</link>
      <description>&lt;a href=&#34;https://www.nationalreview.com/2020/04/coronavirus-china-trail-leading-back-to-wuhan-labs/amp/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; There’s no proof the coronavirus accidentally escaped from a laboratory, but we can’t take the Chinese government’s denials at face value. NRPLUS MEMBER ARTICLE I t is understandable that many would be wary of the notion that the origin of the coronavirus could be discovered by some documentary filmmaker who used to live in China. Matthew Tye, who creates YouTube videos, contends he has identified the source of the coronavirus — and a great deal of the information that he presents, obtained from public records posted on the Internet, checks out. The Wuhan Institute of Virology in China indeed posted a job opening on November 18, 2019, “asking for scientists to come research the relationship between the coronavirus and bats.” The Google translation of the job posting is: “Taking bats as the research object, I will answer the molecular mechanism that can coexist with Ebola and SARS- associated coronavirus for a long time without disease, and its relationship with flight and longevity. Virology, immunology, cell biology, and multiple omics are used to compare the differences between humans and other mammals.” (“Omics” is a term for a subfield within biology, such as genomics or glycomics.) On December 24, 2019, the Wuhan Institute of Virology posted a second job posting. The translation of that posting includes the declaration, “long-term research on the pathogenic biology of bats carrying important viruses has confirmed the origin of bats of major new human and livestock infectious diseases such as SARS and SADS, and a large number of new bat and rodent new viruses have been discovered and identified.” Tye contends that that posting meant, “we’ve discovered a new and terrible virus, and would like to recruit people to come deal with it.” He also contends that “news didn’t come out about coronavirus until ages after that.” Doctors in Wuhan knew that they were dealing with a cluster of pneumonia cases as December progressed, but it is accurate to say that a very limited number of people knew about this particular strain of coronavirus and its severity at the time of that job posting. By December 31, about three weeks after doctors first noticed the cases, the Chinese government notified the World Health Organization and the first media reports about a “mystery pneumonia” appeared outside China. Scientific American verifies much of the information Tye mentions about Shi Zhengli, the Chinese virologist nicknamed “Bat Woman” for her work with that species. Shi — a virologist who is often called China’s “bat woman” by her colleagues because of her virus-hunting expeditions in bat caves over the past 16 years — walked out of the conference she was attending in Shanghai and hopped on the next train back to Wuhan. “I wondered if [the municipal health authority] got it wrong,” she says. “I had never expected this kind of thing to happen in Wuhan, in central China.” Her studies had shown that the southern, subtropical areas of Guangdong, Guangxi and Yunnan have the greatest risk of coronaviruses jumping to humans from animals — particularly bats, a known reservoir for many viruses. If coronaviruses were the culprit, she remembers thinking, “could they have come from our lab?” . . . By January 7 the Wuhan team determined that the new virus had indeed caused the disease those patients suffered — a conclusion based on results from polymerase chain reaction analysis, full genome sequencing, antibody tests of blood samples and the virus’s ability to infect human lung cells in a petri dish. The genomic sequence of the virus — now officially called SARS-CoV-2 because it is related to the SARS pathogen — was 96 percent identical to that of a coronavirus the researchers had identified in horseshoe bats in Yunnan, they reported in a paper published last month in Nature. “It’s crystal clear that bats, once again, are the natural reservoir,” says Daszak, who was not involved in the study. Some scientists aren’t convinced that the virus jumped straight from bats to human beings, but there are a few problems with the theory that some other animal was an intermediate transmitter of COVID-19 from bats to humans: Analyses of the SARS-CoV-2 genome indicate a single spillover event, meaning the virus jumped only once from an animal to a person, which makes it likely that the virus was circulating among people before December. Unless more information about the animals at the Wuhan market is released, the transmission chain may never be clear. There are, however, numerous possibilities. A bat hunter or a wildlife trafficker might have brought the virus to the market. Pangolins happen to carry a coronavirus, which they might have picked up from bats years ago, and which is, in one crucial part of its genome, virtually identical to SARS-CoV-2. But no one has yet found evidence that pangolins were at the Wuhan market, or even that venders there trafficked pangolins. On February 4 — one week before the World Health Organization decided to officially name this virus “COVID-19” — the journal Cell Research posted a notice written by scientists at the Wuhan Institute of Virology about the virus, concluding, “our findings reveal that remdesivir and chloroquine are highly effective in the control of 2019-nCoV infection in vitro. Since these compounds have been used in human patients with a safety track record and shown to be effective against various ailments, we suggest that they should be assessed in human patients suffering from the novel coronavirus disease.” One of the authors of that notice was the “bat woman,” Shi Zhengli. In his YouTube video, Tye focuses his attention on a researcher at the Wuhan Institute of Virology named Huang Yanling: “Most people believe her to be patient zero, and most people believe she is dead.” There was enough discussion of rumors about Huang Yanling online in China to spur an official denial. On February 16, the Wuhan Institute of Virology denied that patient zero was one of their employees, and interestingly named her specifically: “Recently there has been fake information about Huang Yanling, a graduate from our institute, claiming that she was patient zero in the novel coronavirus.” Press accounts quote the institute as saying, “Huang was a graduate student at the institute until 2015, when she left the province and had not returned since. Huang was in good health and had not been diagnosed with disease, it added.” None of her publicly available research papers are dated after 2015. The web page for the Wuhan Institute of Virology’s Lab of Diagnostic Microbiology does indeed still have “Huang Yanling” listed as a 2012 graduate student, and her picture and biography appear to have been recently removed — as have those of two other graduate students from 2013, Wang Mengyue and Wei Cuihua. Her name still has a hyperlink, but the linked page is blank. The pages for Wang Mengyue and Wei Cuihua are blank as well. (For what it is worth, the South China Morning Post — a newspaper seen as being generally pro-Beijing — reported on March 13 that “according to the government data seen by the Post, a 55 year-old from Hubei province could have been the first person to have contracted Covid-19 on November 17.”) On February 17, Zhen Shuji, a Hong Kong correspondent from the French public-radio service Radio France Internationale, reported: “when a reporter from the Beijing News of the Mainland asked the institute for rumors about patient zero, the institute first denied that there was a researcher Huang Yanling, but after learning that the name of the person on the Internet did exist, acknowledged that the person had worked at the firm but has now left the office and is unaccounted for.” Tye says, “everyone on the Chinese internet is searching for [Huang Yanling] but most believe that her body was quickly cremated and the people working at the crematorium were perhaps infected as they were not given any information about the virus.” (The U.S. Centers for Disease Control and Prevention says that handling the body of someone who has died of coronavirus is safe — including embalming and cremation — as long as the standard safety protocols for handing a decedent are used. It’s anyone’s guess as to whether those safety protocols were sufficiently used in China before the outbreak’s scope was known.) As Tye observes, a public appearance by Huang Yanling would dispel a lot of the public rumors, and is the sort of thing the Chinese government would quickly arrange in normal circumstances — presuming that Huang Yanling was still alive. Several officials at the Wuhan Institute of Virology issued public statements that Huang was in good health and that no one at the institute has been infected with COVID-19. In any case, the mystery around Huang Yanling may be moot, but it does point to the lab covering up something about her. China Global Television Network, a state-owned television broadcaster, illuminated another rumor while attempting to dispel it in a February 23 report entitled “Rumors Stop With the Wise”: On February 17, a Weibo user who claimed herself to be Chen Quanjiao, a researcher at the Wuhan Institute of Virology, reported to the public that the Director of the Institute was responsible for leaking the novel coronavirus. The Weibo post threw a bomb in the cyberspace and the public was shocked. Soon Chen herself stepped out and declared that she had never released any report information and expressed great indignation at such identity fraud on Weibo. It has been confirmed that that particular Weibo account had been shut down several times due to the spread of misinformation about COVID-19. That Radio France Internationale report on February 17 also mentioned the next key part of the Tye’s YouTube video. “Xiaobo Tao, a scholar from South China University of Technology, recently published a report that researchers at Wuhan Virus Laboratory were splashed with bat blood and urine, and then quarantined for 14 days.” HK01, another Hong Kong-based news site, reported the same claim. This doctor’s name is spelled in English as both “Xiaobo Tao” and “Botao Xiao.” From 2011 to 2013, Botao Xiao was a postdoctoral research fellow at Harvard Medical School and Boston Children’s Hospital, and his biography is still on the web site of the South China University of Technology. At some point in February, Botao Xiao posted a research paper onto ResearchGate.net, “The Possible Origins of 2019-nCoV coronavirus.” He is listed as one author, along with Lei Xiao from Tian You Hospital, which is affiliated with the Wuhan University of Science and Technology. The paper was removed a short time after it was posted, but archived images of its pages can be found here and here. The first conclusion of Botao Xiao’s paper is that the bats suspected of carrying the virus are extremely unlikely to be found naturally in the city, and despite the stories of “bat soup,” they conclude that bats were not sold at the market and were unlikely to be deliberately ingested. The bats carrying CoV ZC45 were originally found in Yunnan or Zhejiang province, both of which were more than 900 kilometers away from the seafood market. Bats were normally found to live in caves and trees. But the seafood market is in a densely-populated district of Wuhan, a metropolitan [area] of ~15 million people. The probability was very low for the bats to fly to the market. According to municipal reports and the testimonies of 31 residents and 28 visitors, the bat was never a food source in the city, and no bat was traded in the market. The U.S. Centers for Disease Control and Prevention and the World Health Organization could not confirm if bats were present at the market. Botao Xiao’s paper theorizes that the coronavirus originated from bats being used for research at either one of two research laboratories in Wuhan. We screened the area around the seafood market and identified two laboratories conducting research on bat coronavirus. Within ~ 280 meters from the market, there was the Wuhan Center for Disease Control &amp; Prevention. WHCDC hosted animals in laboratories for research purpose, one of which was specialized in pathogens collection and identification. In one of their studies, 155 bats including Rhinolophus affinis were captured in Hubei province, and other 450 bats were captured in Zhejiang province. The expert in Collection was noted in the Author Contributions (JHT). Moreover, he was broadcasted for collecting viruses on nation-wide newspapers and websites in 2017 and 2019. He described that he was once by attacked by bats and the blood of a bat shot on his skin. He knew the extreme danger of the infection so he quarantined himself for 14 days. In another accident, he quarantined himself again because bats peed on him. Surgery was performed on the caged animals and the tissue samples were collected for DNA and RNA extraction and sequencing. The tissue samples and contaminated trashes were source of pathogens. They were only ~280 meters from the seafood market. The WHCDC was also adjacent to the Union Hospital (Figure 1, bottom) where the first group of doctors were infected during this epidemic. It is plausible that the virus leaked around and some of them contaminated the initial patients in this epidemic, though solid proofs are needed in future study. The second laboratory was ~12 kilometers from the seafood market and belonged to Wuhan Institute of Virology, Chinese Academy of Sciences . . . In summary, somebody was entangled with the evolution of 2019-nCoV coronavirus. In addition to origins of natural recombination and intermediate host, the killer coronavirus probably originated from a laboratory in Wuhan. Safety level may need to be reinforced in high risk biohazardous laboratories. Regulations may be taken to relocate these laboratories far away from city center and other densely populated places. However, Xiao has told the Wall Street Journal that he has withdrawn his paper. “The speculation about the possible origins in the post was based on published papers and media, and was not supported by direct proofs,” he said in a brief email on February 26. The bat researcher that Xiao’s report refers to is virologist Tian Junhua, who works at the Wuhan Centre for Disease Control. In 2004, the World Health Organization determined that an outbreak of the SARS virus had been caused by two separate leaks at the Chinese Institute of Virology in Beijing. The Chinese government said that the leaks were a result of “negligence” and the responsible officials had been punished. In 2017, the Chinese state-owned Shanghai Media Group made a seven-minute documentary about Tian Junhua, entitled “Youth in the Wild: Invisible Defender.” Videographers followed Tian Junhua as he traveled deep into caves to collect bats. “Among all known creatures, the bats are rich with various viruses inside,” he says in Chinese. “You can find most viruses responsible for human diseases, like rabies virus, SARS, and Ebola. Accordingly, the caves frequented by bats became our main battlefields.” He emphasizes, “bats usually live in caves humans can hardly reach. Only in these places can we find the most ideal virus vector samples.” One of his last statements on the video is: “In the past ten-plus years, we have visited every corner of Hubei Province. We explored dozens of undeveloped caves and studied more than 300 types of virus vectors. But I do hope these virus samples will only be preserved for scientific research and will never be used in real life. Because humans need not only the vaccines, but also the protection from the nature.” The description of Tian Junhua’s self-isolation came from a May 2017 report by Xinhua News Agency, repeated by the Chinese news site JQKNews.com: The environment for collecting bat samples is extremely bad. There is a stench in the bat cave. Bats carry a large number of viruses in their bodies. If they are not careful, they are at risk of infection. But Tian Junhua is not afraid to go to the mountain with his wife to catch Batman. Tian Junhua summed up the experience that the most bats can be caught by using the sky cannon and pulling the net. But in the process of operation, Tian Junhua forgot to take protective measures. Bat urine dripped on him like raindrops from the top. If he was infected, he could not find any medicine. It was written in the report. The wings of bats carry sharp claws. When the big bats are caught by bat tools, they can easily spray blood. Several times bat blood was sprayed directly on Tians skin, but he didn’t flinch at all. After returning home, Tian Junhua took the initiative to isolate for half a month. As long as the incubation period of 14 days does not occur, he will be lucky to escape, the report said. Bat urine and blood can carry viruses. How likely is it that bat urine or blood got onto a researcher at either Wuhan Center for Disease Control &amp; Prevention or the Wuhan Institute of Virology? Alternatively, what are the odds that some sort of medical waste or other material from the bats was not properly disposed of, and that was the initial transmission vector to a human being? Virologists have been vehemently skeptical of the theory that COVID-19 was engineered or deliberately constructed in a laboratory; the director of the National Institutes of Health has written that recent genomic research “debunks such claims by providing scientific evidence that this novel coronavirus arose naturally.” And none of the above is definitive proof that COVID-19 originated from a bat at either the Wuhan Center for Disease Control &amp; Prevention or the Wuhan Institute of Virology. Definitive proof would require much broader access to information about what happened in those facilities in the time period before the epidemic in the city. But it is a remarkable coincidence that the Wuhan Institute of Virology was researching Ebola and SARS-associated coronaviruses in bats before the pandemic outbreak, and that in the month when Wuhan doctors were treating the first patients of COVID-19, the institute announced in a hiring notice that “a large number of new bat and rodent new viruses have been discovered and identified.” And the fact that the Chinese government spent six weeks insisting that COVID-19 could not be spread from person to person means that its denials about Wuhan laboratories cannot be accepted without independent verification. </description>
      <pubDate>12 Apr 20 13:34 EDT</pubDate>
      <guid>https://www.nationalreview.com/2020/04/coronavirus-china-trail-leading-back-to-wuhan-labs/amp/</guid>
    </item>
    <item>
      <title>Interactive Storytelling Tools for Writers</title>
      <link>https://www.erasmatazz.com/personal/self/i-really-blew-it.html</link>
      <description>&lt;a href=&#34;https://www.erasmatazz.com/personal/self/i-really-blew-it.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I recently realized that I have been wasting my time on a dumb effort for the last 35 years. It all started back in 1983. My game for the Atari, Eastern Front (1941), had been a big hit, and Dale Yocum of the Atari Program Exchange suggested that I release the source code for it. This was an exciting idea, as nobody had ever release source code for a commercial game before then, and I readily agreed. However, I insisted that the source code be thoroughly explained, so I spent some months writing up a long explanation of all the algorithms used in the game. The entire package came to some 150 pages. We charged a lot of money for it, so it didn’t sell many copies. Still, I hoped that the buyers would use the ideas in the package. I was disappointed, because only one game appeared using any of the ideas we published in the source code: Ghost Ship, by Kellyn Beeck. Perhaps somebody else used some of the ideas without my knowing, but I rather doubt it.A few years later, I had another hit game: Balance of Power. In 1986, Microsoft Press invited me to write a book explaining how it worked, and again I agreed that this would be a good idea. We didn’t publish source code, because it was too specific to the Macintosh, written in Pascal, using 16-bit integer arithmetic. Instead, I explained the geopolitical considerations behind the game in a series of chapters that broke it down by topic, and then provided the equations and algorithms, explaining in great detail exactly what each term meant. It was a manual on how to build a geopolitical game. Again, I had high hopes that other people would use some of the ideas.Again, I was disappointed; I am under the impression that nobody ever applied the concepts used in Balance of Power. Indeed, as far as I can tell, in the 34 years since I published that book, there has never been a geopolitical game as sophisticated as Balance of Power. Perhaps I missed something, but most of my friends have been on the lookout and I have had a number of games referred to me; none of them came close to Balance of Power in algorithmic sophistication.A Different Perception of RealityIt’s only recently that I have realized just how different is my perception of reality. It is best represented by this classic image from the movie The Matrix: Neo has been revived and looks down the hall at the agents and sees the reality of the Matrix: that it is numbers. I see the same thing when I look at the real world: it is not a collection of objects, but rather a system of processes, and those processes are best understood through mathematics. I don’t see all the numbers, but I know that they’re there, and I can put together primitive mathematical representations of simpler parts of the world. I can look at a car traveling down a road and know that the distance it will cover is equal to its speed multiplied by the time interval. With Eastern Front (1941) I could write a simple equation for the damage an attacking military unit could inflict upon the defender:Casualties = 3 * (strength of attacker / strength of defender)With Balance of Power, I could write a simple equation for the number of fighters who would join an insurgency against the government:Fighters = Political Immaturity * Population * Previous Success of InsurgencyHere, Political Immaturity is a constant I defined for each country based on my estimate of how much people respected the rule of law. The Previous Success of the Insurgency was simply how successful the insurgency had been in the previous year in its battle against the government. It’s this kind of thinking that has driven all of my design approaches. Indeed, the central focus of all my design efforts has been designing interesting algorithms that capture the truth of some system, be it military, political, physical, environmental, emotional, or narrative. That’s the entire point and purpose of my design career, and I have seen my work as more demonstrative than expository. That is, I have always expected that the ideas I incorporate into my work would serve as working examples to others. In this, I have failed completely.I have devoted as much time and energy to teaching people about my design philosophy as implementing it. I have written six books, academic papers, articles for magazines, and 1500 pages of text on this website. I have delivered hundreds of lectures all over the world. I have prepared videos explaining my work and given classes over the Internet to small groups. I have even invited a few special students to my home to spend days in intensive tuition, trying to imbue them with the style of thinking I use. And yet, after all this work, I don’t think I can point to a single person who is truly following in my footsteps. There are a handful of people who have made some goodly progress in this direction. After all this time and all this effort, I remain a solitary figure walking across a desert landscape. Why Failure?Why have I failed after so much effort? The simple answer is that, as my wife says, I’m too far ahead of my time. Perhaps I’m a misunderstood genius. There’s a grain of truth in these answers, but I don’t think that they capture the bulk of the truth, because I am dead certain that most other people have the native intelligence to understand the ideas I’ve been peddling. The main problem, I think, is that the world doesn’t yet perceive a need for the ideas I peddle.This concept was first enunciated by Thomas Kuhn in his milestone book, The Structure of Scientific Revolutions. In it, he observed that scientific revolutions were not initiated by geniuses; they happened only when scientists perceived a need for a new idea. For example, had Albert Einstein proposed his special theory of relativity just twenty years earlier, it would have been ignored and Einstein would today be another unknown scientist. That’s because in 1885, physicists were fairly confident that they understood how light propagates. Then in 1887 two American scientists performed the Michelson-Morley experiment that shattered the existing understanding of how light works. At first, physicists were incredulous at their results, so they repeated the experiment, expecting to find that the American yokels had screwed it up. When they got exactly the same results, they tried to find flaws in the analysis. Then they tried a variety of patches to their theory—but those didn’t work. All through the 1890s, physicists struggled with the conundrum created by Michelson-Morley. They failed. Thus, by 1905, when Einstein proposed his special theory of relativity, physicists were desperate enough that they were willing to consider a theory that, on its face, seemed insane. Space contracting? Time stretching out? Mass increasing? It was crazy! But it successfully explained Michelson-Morley, and it was successful everywhere it was applied. Lots of physicists tried to come up with experiments that would disprove Einstein’s theory, and they all failed. In 1885, physicists didn’t perceive a need for special relativity, and they would have rejected it out of hand. In 1905, physicists were trapped in a bad hole, and special relativity offered a way out, so they grabbed it.Another good example from science is provided by Gregor Mendel. This humble Augustinian friar experimented with the peas he grew in the garden at his monastery. He cross-bred them and kept track of various traits across generations. His paper presenting his results was published in 1866 and was ignored, being cited only three times in other scientific papers in the next 35 years. He knew that he was onto something important, but was disappointed that nobody recognized its significance. “My time will come”, he told a friend. He died in 1884, unnoticed and unrecognized. Then in 1900, biologists were struggling with some problems in evolutionary theory, because their existing theories of inheritance didn’t explain what they observed. Some biologists started carrying out research that replicated Mendel’s work, and it was at about this time that somebody stumbled upon Mendel’s paper. It was with great embarrassment that biologists realized that Mendel had founded the science of genetics and nobody had noticed. Oops.Here’s another example: Charles Babbage. This fellow invented the computer—in 1840! It wasn’t electronic, it was mechanical, using gears and cams and levers. It was an immensely complicated design and, since it relied on mechanical components, would have been huge and horridly expensive had anybody ever built it—which never happened. It was simply too big, too expensive, and too complicated. While nobody really believed that it would work, the real problem was that nobody saw any use for it. After all, what would you use a computer for in 1840? Spreadsheets? Airline reservations? Word processing? It simply didn’t fit into the world of the nineteenth century. It wasn’t until 1990 that somebody built a working version of his simpler difference engine, a kind of limited-use calculator. I think that my own experience is a faint shadow of the stories of Gregor Mendel and Charles Babbage. I do not rank my own contribution as being in the same ball park with theirs, or even in the same league, but I think it fair to say that it belongs in the same sport. I’m right, I’m sure of that, but my ideas aren’t useful just yet. The goal I seek (making the computer a medium of artistic expression) is as yet of little interest to society. Sure, people agree that this would be a good thing, but as yet there’s no sense of desperation, no realization that we have failed using existing styles of thinking. People are still bewitched by the progress that has been made and still believe that getting true artistic expression is just a matter of fiddling around with existing techniques for a little longer.  </description>
      <pubDate>15 Feb 21 09:14 EST</pubDate>
      <guid>https://www.erasmatazz.com/personal/self/i-really-blew-it.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://mostly-adequate.gitbooks.io/mostly-adequate-guide/</link>
      <description>&lt;a href=&#34;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;About this bookThis is a book on the functional paradigm in general. We&#39;ll use the world&#39;s most popular functional programming language: JavaScript. Some may feel this is a poor choice as it&#39;s against the grain of the current culture which, at the moment, feels predominately imperative. However, I believe it is the best way to learn FP for several reasons:You likely use it every day at work.This makes it possible to practice and apply your acquired knowledge each day on real world programs rather than pet projects on nights and weekends in an esoteric FP language.We don&#39;t have to learn everything up front to start writing programs.In a pure functional language, you cannot log a variable or read a DOM node without using monads. Here we can cheat a little as we learn to purify our codebase. It&#39;s also easier to get started in this language since it&#39;s mixed paradigm and you can fall back on your current practices while there are gaps in your knowledge.The language is fully capable of writing top notch functional code.We have all the features we need to mimic a language like Scala or Haskell with the help of a tiny library or two. Object-oriented programming currently dominates the industry, but it&#39;s clearly awkward in JavaScript. It&#39;s akin to camping off of a highway or tap dancing in galoshes. We have to bind all over the place lest this change out from under us, we have various work arounds for the quirky behavior when the new keyword is forgotten, private members are only available via closures. To a lot of us, FP feels more natural anyways.That said, typed functional languages will, without a doubt, be the best place to code in the style presented by this book. JavaScript will be our means of learning a paradigm, where you apply it is up to you. Luckily, the interfaces are mathematical and, as such, ubiquitous. You&#39;ll find yourself at home with Swiftz, Scalaz, Haskell, PureScript, and other mathematically inclined environments.Read it OnlinePlay Around with CodeTo make the training efficient and not get too bored while I am telling you another story, make sure to play around with the concepts introduced in this book. Some can be tricky to catch at first and are better understood by getting your hands dirty. All functions and algebraic data-structures presented in the book are gathered in the appendixes. The corresponding code is also available as an npm module:$ npm i @mostly-adequate/supportAlternatively, exercises of each chapter are runnable and can be completed in your editor! For example, complete the exercise_*.js in exercises/ch04 and then run:Download itDo it yourselfgit clone https://github.com/MostlyAdequate/mostly-adequate-guide.gitcd mostly-adequate-guide/Table of ContentsContributingTranslationsFAQPlans for the futurePart 1 (chapters 1-7) is a guide to the basics. I&#39;m updating as I find errors since this is the initial draft. Feel free to help!Part 2 (chapters 8-13) address type classes like functors and monads all the way through to traversable. I hope to squeeze in transformers and a pure application.Part 3 (chapters 14+) will start to dance the fine line between practical programming and academic absurdity. We&#39;ll look at comonads, f-algebras, free monads, yoneda, and other categorical constructs.</description>
      <pubDate>24 Mar 20 22:13 EDT</pubDate>
      <guid>https://mostly-adequate.gitbooks.io/mostly-adequate-guide/</guid>
    </item>
    <item>
      <title></title>
      <link>http://okmij.org/ftp/Computation/lightweight-static-guarantees.html</link>
      <description>&lt;a href=&#34;http://okmij.org/ftp/Computation/lightweight-static-guarantees.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Safe and Efficient, Now We have identified (more than a decade ago, in fact) a disciplined programming style that uses existing type systems in practical, mature languages (such as OCaml, Scala, Haskell, etc., and to an extent, Java and C++) to statically assure a wide range of safety properties: never dereferencing a null pointer or taking the head of an empty list; always sanitizing user input; using only in-bounds indices to access (dynamically allocated) arrays of the statically unknown size. The technique is compatible with modular development, separate compilation, imperative code, native mutable arrays, indirect indexing, and general recursion. The resulting program is not just more reliable but also more efficient, because fewer run-time checks are needed. The technique does not obviate the foundational, formal methods approach; rather, it complements and structures it. Formal methods may still be used to prove the (typically small and simple) security kernel correct. Our technique extends the static guarantees from the kernel through the whole program. There are two surprises: (i) what we enumerate is even possible; (ii) that it is so old (tracing back to Morris&#39; 1973 paper), so simple, and yet so rarely used. Introduction User-input sanitation and SQL injection Abstract Types -- the language protection layer Eliminating array bound checks Eliminating array bound checking in multiple arrays Knuth-Morris-Pratt string search with safe array access Rights amplification Lightweight static capabilities Formalization of the abstract type protection The question of verification Lightweight guarantees and static analyses Lightweight guarantees and dependent types Conclusions Introduction The memory of Heartbleed, a flaw in the OpenSSL library, is beginning to fade. After all, exploitable security flaws are regular occurrence. Heartbleed is still remarkable in how much effort was spent to patch it, and how trivial it was. Like many other flaws of that nature, it was introduced without much ado. As the developer of the Heartbeat SSL feature explained, ``I was working on improving OpenSSL and submitted numerous bug fixes and added new features. In one of the new features, unfortunately, I missed validating a variable containing a length.&#39;&#39; A reviewer ``apparently also didn’t notice the missing validation,&#39;&#39; the developer said, ``so the error made its way from the development branch into the released version.&#39;&#39; The error remained (publicly) undetected for two years. The OpenSSL patch that finally fixes the error shows how indeed trivial it was: just one statement memcpy(bp, pl, payload); that copies the payload data of the size payload from the input packet (starting at the pointer pl) to the output packet buffer, starting at the location bp. The value of payload has been previously read from the input packet. The problem comes when the attacker sends a packet with the maximum value of the payload size, but with no actual payload data. In that case memcpy, instead of the data from the received packet (which is already ended), copies the large amount of garbage from the openSSL input buffer. That `garbage&#39; is actually left-over data, with often sensitive information including passwords. What is also disturbing about Heartbleed is how trivially the error could have been avoided, if the low-level functions like memcpy could not be invoked directly, but only through wrappers, which, for example, sanity check that pl + payload is still within the input packet (the boundaries of input packet are readily available). The invocation restriction could be effected in any language with module/namespace abstraction facilities (C++ and beyond) -- or even in C, when test-compiling against appropriately set up .h files that omit restricted functions. The performance need not be sacrificed: the wrapper could be an inline function (or a C macro), and the length sanity check has to be done anyway. Abstraction is the key word: Abstraction over internal data and functions forces the end programmer to use public APIs with safety checks. By preventing tampering with the internal state, abstraction also ensures that whatever invariant was verified by the safety checks remains valid. Therefore, the safety checks do not have to be done over and over again -- or even at all, if the needed invariant follows from the checks the algorithm had to do anyway. Hence our slogan: Safe and Efficient, Now. The basic idea behind our programming style is ubiquitous, going back to the dawn of computing: the hardware-enforced access restriction to memory and devices. This hardware protection layer separates the computing system into the (trusted) kernel (which operates in the privileged mode and may execute low-level operations) and `user-level&#39; programs, which may access devices only through public kernel APIs (system calls), which do sanity and access checks. A user program may not write onto a disk at will. Rather, it has to do an `open&#39; system call, which, after authorization and other checks, returns an opaque token, the file descriptor. The descriptor acts as a capability to do the prescribed set of operations. It also represents the fact of the successful authorization, so further operations do not have to repeat it. James Morris&#39; 1973 paper `Protection in Programming Languages&#39; was the first to apply to software the ideas from operating systems such as memory protection, authentication, capabilities, scope control. The paper demonstrated how the software/language protection layer helps us reason about programs locally, modularly. The abstraction facilities of programming languages became more extensive since 1973. It is high time we took the full advantage of Morris&#39; insights. We can do it right now, in existing, mature programming languages. Making sure that we: dereference only non-null pointers; divide by only non-zero denominators; take the head or tail of only non-empty linked lists; index into a static buffer with only in-range indices; and execute SQL commands containing no unsanitized strings from external input. can be done already in C++ and Java. We show examples below. We can also do more. The facilities for generic programming (parametric polymorphism) or advanced module systems in modern languages let us assure more safety properties, such as safe indexing into an array whose size is known only at run-rime -- without imposing a bound check on each access. We even show the assuredly buffer-overflow--free KMP string search, which is a rather complex imperative algorithm with indirect indexing. Morris&#39; old ideas do indeed work. Why don&#39;t we use them? References ``Costly error -- Heartbleed developer explains OpenSSL mistake that put Web at risk. `Trivial&#39; coding error in open source project wasn&#39;t intentional, report says&#39;&#39; Jon Brodkin. Ars Technica, Apr 11, 2014 2:45 pm UTC &lt;http://arstechnica.com/information-technology/2014/04/heartbleed-developer-explains-openssl-mistake-that-put-web-at-risk/&gt; (Surprisingly many of the `explanations&#39; of the problem one may find on the internet are wrong, falsely blaming OPENSSL_malloc) The openSSL patch that fixes the Heartbleed (moving from OpenSSL revision 1.0.1f to 1.0.1g) &lt;https://github.com/openssl/openssl/commit/96db9023b881d7cd9f379b0c154650d6c108e9a3#diff-2&gt; &lt;https://xkcd.com/1354/&gt; How the Heartbleed Bug Works User-input sanitation and SQL injection Heartbleed was just one instance of neglecting to validate/sanitize data received from the user or network. Another, unfortunately all-too-common instance is the input injection attacks -- in particular, SQL injection, `Bobby Tables&#39;--like attacks. I have implemented the static SQL injection prevention in a web application server used in production. The implementation turned out straightforward and instructive. I have written a web application server, which has been constantly running in production for at least eight years. The server makes SQL queries using the data from user-submitted requests. SQL injection and input validation had to be taken seriously. About half-way through the development I decided to try the `software protection layer&#39;: make the type-checker alert me to a missing input validation. The idea was easy to try: the simple library below is essentially it. (The shown code is OCaml; it could have been C++, Java, Scala, or many other languages.) module DirtyString : sig (* interface *) type dirty_string (* abstract *) val dirty : string -&gt; dirty_string val escape : dirty_string -&gt; string val read_int : dirty_string -&gt; int option val read_ncname : dirty_string -&gt; (string * dirty_string) option end = struct (* implementation *) type dirty_string = string let dirty = %identity let read_int = int_of_string_opt ... end The method dirty taints a given string, turning it into a value of the abstract dirty_string type. Once the string is dirty, there is not much one can do with it: only apply escape, read_int, and read_ncname methods. The latter attempts to read a sequence of alphanumeric, `safe&#39; characters, returning them as a string, paired with the remainder of the input dirty string. An ncname string can be used in building SQL queries worry-free, and hence is returned as the ordinary, `clean&#39; string. In the DirtyString implementation, within the `kernel&#39;, dirty_string is just the ordinary string. Likewise, the read_int method is nothing but the corresponding function from the standard library. When the module is inlined, there is really no overhead. There is still protection: to the users of the library, dirty_string and string are different, not interchangeable types. After I wrote the DirtyString library, I dirty-ed the result of network reading functions, and started the recompilation watching for type errors. Every place in the code that was using user input was flagged by the type checker: a dirty_string cannot be used as an ordinary string. Indeed, some sort of validation/sanitation is required. Fixing the errors was simple, because some sort of validation was already in place. I merely had to rename the extant read_ncname into DirtyString.read_ncname, etc. I clearly remember one place, however, which was flagged by the type checker but did not have any validation checking. After thinking for about ten minutes, I decided that there should have been a validation check at that place after all. The experience proved quite encouraging. I spent in total less than an hour implementing the DirtyString and adjusting the code correspondingly, and I found a genuine problem. There was also no run-time overhead as I could see. References &lt;https://www.xkcd.com/327/&gt; Exploits of a Mom (aka, `Bobby Tables&#39;) Non-empty lists The similar `tainting&#39; technique ensures the absence of `head/tail of empty list&#39; errors (the equivalent of `NullPointerException&#39;) reverse.ml [2K] List reversal with safe list access: The OCaml version of the Non-empty list code Abstract Types -- the language protection layer Let us explain, on a series of simple but already useful examples, how exactly abstraction acts as a protection layer, which invariants it enforces, and what good it does us. Suppose we have several (assumed constant) integer arrays a1...an, to be repeatedly searched for occurrences of some elements. It may make sense to create the sorted versions of the arrays a1&#39;...an&#39; and search those, with the efficient binary search. There are a couple of problems however. First of all, how to ensure that an ai&#39; is still the sorted version of the original ai? Arrays are mutable by nature. Whenever we pass ai&#39; as an argument to some function, we cannot be sure it remains unmodified (unless we carefully examine the code of that function, if it is even available). Mainly, how does the binary search function make certain its argument array is actually sorted? Examining the array for sortedness is out of the question since it takes more time than the actual search. Protection -- access restriction -- provides the sortedness guarantee. Consider the following simple library realized as an OCaml module. module ArraySearch : sig (* the interface *) type t (* abstract *) val sort : int array -&gt; t val search : int -&gt; t -&gt; bool end = struct (* the implementation: the `kernel&#39; *) type t = int array let sort : int array -&gt; t = fun a -&gt; let a&#39; = Array.copy a in Array.sort compare a&#39;; a&#39; (* binary search in the sorted array arr *) let search x arr = ... end One can write such code in almost any language with a module system. (Later on we show another implementation, using closures/objects.) The library provides only two operations: sort and search. The former takes an integer array and returns the value of type t. Nothing is known about the type t to the users of library. In particular, none of the array access and mutation operations apply to t values: if we try, the type checker will complain that t is not an array. In fact, t is not equal to any other type known to the type checker. Hence we may give a t value only to polymorphic functions (which merely pass it around), or to the operation search of the library. Here is a usage example (for the case of two arrays a1 and a2 in our original set-up): let a1&#39; = ArraySearch.sort a1 (* First, sort the arrays *) let a2&#39; = ArraySearch.sort a2 ... (* some code *) let v1 = ArraySearch.search 1 a1&#39; &amp;&amp; ArraySearch.search 2 a2&#39; ... (* more code *) let f x = ArraySearch.search x a1&#39; in f 1 &amp;&amp; f 2 The prefix ArraySearch may be omitted if we open ArraySearch first. Looking into the implementation of the library (the ArraySearch structure) shows that the type t is actually int array -- but this is known only within the implementation. The implementation may, therefore, treat t values as arrays, with no complaints from the type checker. The operation sort makes the copy of its input array and sorts the result; search does not modify the argument array. We thus may conclude that a value of type t represents a sorted array. It is made sorted by sort, it is not aliased to outside, and the operations on t preserve sortedness. We made this determination by looking only at the implementation of the module, rather than through any of the code that may use it. The users of the library cannot know what t is, and cannot apply any operations to it except for ArraySearch.search. Hence the sortedness -- the property, or the invariant that may be associated with the type t -- indeed holds. Thus the merit of type abstraction, of type-checker--reinforced access restrictions: invariants established by local reasoning are preserved globally. In our case, the sortedness invariant entails that the search operation does not need to check if its argument is a sorted array. It always is. The type abstraction hence let us use the faster algorithm and be sure it is correctly applied. Before we turn to the second example, let us see another realization of the sorted-array--search library, using a different language protection mechanism. let sort : int array -&gt; (int -&gt; bool) = fun a -&gt; let a&#39; = Array.copy a in let () = Array.sort compare a&#39; in (* binary search in the sorted array arr *) let search x arr = ... in fun x -&gt; search x a&#39; Here, sort takes an int array and returns an operation to (efficiently) search in that array. The abstraction mechanism now is first-class functions: returning functions as results. Crucially, the returned function is actually a closure fun x -&gt; search x a&#39; that contains a reference to the internal array a&#39;. Since closures are opaque and cannot be deconstructed, the embedded a&#39; reference cannot be used, or seen, outside the closure. We notice no other reference to a&#39; available to library users; therefore, once sorted, a&#39; remains constant and sorted. Again we see local reasoning (examining only the implementation but not the uses of the library) and the guarantee due to the protection that local invariants remain valid globally. Incidentally, the closure implementation is similar to the one presented already in Morris 1973 paper (in the context of a different example -- although Morris has also mentioned the array sortedness and the binary search). With closures, there is an overhead of creating and using them. With the ArraySearch module, there is no run-time overhead at all: internally a value of the type t is an int array, with no wrappers or indirections. As the next example, to be used later, consider the following module (library) with four operations: module Interv : sig (* the interface *) type t (* abstract *) val lwb : t val upb : t val mid : t -&gt; t -&gt; t val to_int : t -&gt; int end = struct (* the implementation: the `kernel&#39; *) type t = int let lwb = 0 let upb = 5 let mid x y = (x + y) / 2 (* no overflow, for our range *) let to_int x = x end The library is presented in the form of an OCaml module. The similar code can also be easily written in any Object-Oriented language, as a class with a protected field. Examining the implementation lets us conclude that we may attribute to the abstract type Interv.t the invariant that it represents an integer from 0 through 5, inclusive. Indeed, lwb returns 0, which respects the invariant. Likewise, upb returns 5, the number within the range. The operation mid is both the consumer of t values and the producer of them. As a consumer, it can assume t&#39;s invariant: it is a number from 0 through 5. Upon this assumption, the result of mid is also a number within the same range. Hence the invariant is established. As the consequence, the result of to_int operation will surely be an integer from 0 though 5, incl. Such an assurance is behind safe array indexing, described next. To conclude, the language protection layer helps ensure that the invariants established by local reasoning are preserved globally. Specifically, type abstraction lets us attribute an invariant (a proposition) to the values of an abstract type: e.g., the value represents a number within certain range, or a sorted array. The idea of abstract data type&#39;s statically representing and enforcing sophisticated properties of run-time values was Robin Milner&#39;s main insight behind the design of the Edinburgh LCF theorem prover back in the early 1970s. The local reasoning conducted so far has been informal. It could be made formal, to any desired degree. In fact, the protection layer helps formal reasoning by making it local and modular. It is much easier to formally analyze the implementation of the, say, Interv interface, than the whole program using that interface. It should be pointed out that guarantees of abstract types come from the lack of reflection, extension, and introspection of abstract data types. OCaml, incidentally, is weaker in this respect because polymorphic equality can break some of the invariants. So, reflection, extension and introspection facilities of a language can be viewed as vices rather than virtues. References James H. Morris Jr.: Protection in Programming Languages Comm. of the ACM, 1973, V16, N1, pp. 15-21 Abstract: Linguistic mechanisms which can be used to protect one subprogram from another&#39;s malfunctioning are described. Function-producing functions and various type-tagging schemes are considered. An attempt is made to distinguish between access limitation and authentication. Formalization of the abstract type protection Lightweight static capabilities Eliminating array bound checks We now describe in detail how the language protection layer helps ensure that all array indexing operations are within array bounds. Not only do we get the confidence that an out-of-bound access (aka, buffer overflow) never occurs in any run of the program. We also eliminate dynamic array bound checks and hence improve the program performance. Let us see how the slogan -- safe and efficient, now -- works for realistic, interesting programs. This article describes binary search in a sorted array -- the example in the famous Xi and Pfenning&#39;s PLDI 1998 paper. More involved examples appear later on this web page. This article uses OCaml, although our binary search code was first presented (in 2004) in Haskell. It is easy to reimplement it in Scala or even Java; the simpler versions have been coded in C++ (see the references at the end). Let&#39;s consider an extended version of the Interv interface from our earlier example: module type Index = sig type index = private int (* i:index implies lwb &lt;= i &lt;= upb *) type indexL = private int (* i:indexL implies lwb &lt;= i *) type indexH = private int (* i:indexH implies i &lt;= upb *) val lwb : indexL val upb : indexH val bsucc : index -&gt; indexL val bpred : index -&gt; indexH val bmid : index -&gt; index -&gt; index val bcmp : indexL -&gt; indexH -&gt; (index * index) option end The Index interface shows off another abstraction facility of OCaml: so-called private types. A value of the type private int can always be cast into int and later used as an ordinary integer. In fact, at run-time a private int is an int -- moreover, the optimizer can see that and optimize accordingly. On the other hand, int cannot be cast to private int; using an int value where a private int is expected is a type error. The only way to create private int values is to use the operations of the interface. Index has three abstract types, all distinct: for example, bsucc lwb is a type error. The code comments show the propositions we would like to attribute to the values of these types. That is, if i is a value of the type index, it is an integer within the closed range [lwb,upb] (the value of the type index hence also witnesses the fact that lwb &lt;= upb). In contrast, an indexL value is only bounded from below, by lwb, and an indexH value is only bounded from above. The operation bsucc is meant to be the index increment. Its result may exceed upb but surely stays above the lower bound; therefore the result type is indexL rather than index. The operation pred is the predecessor (which certainly preserves the upper bound); bmid averages two indices, leaving the result in range -- which is reflected in its type. Finally, bcmp compares an integer i:indexL bounded from below by lwb, with an integer j:indexH bounded from above by upb. If i&lt;=j, then both i and j are in fact within [lwb,upb]; they should be returned as the values of the type index this time. The comparison result is hence not just a mere true or false: a successful comparison improves our knowledge of values and, correspondingly, entitles to use more precise types. The following is the straightforward implementation, parameterized by upb, which could be an arbitrary integer. The lower bound is assumed zero. It is easy to see the implementation respects the invariants of the interface we have just described. module IndexF(S:sig val upb:int end) : Index = struct type index = int type indexL = int type indexH = int let lwb : indexL = 0 let upb : indexH = S.upb let bsucc : index -&gt; indexL = Stdlib.succ let bpred : index -&gt; indexH = Stdlib.pred let bmid : index -&gt; index -&gt; index = fun x y -&gt; x + (y - x)/2 let[@inline] bcmp : indexL -&gt; indexH -&gt; (index * index) option = fun i j -&gt; if i &lt;= j then Some (i,j) else None end Within the implementation, index, indexL and indexH types are no longer private, so we may create their values. Ascribing the signature Index adds the private qualification, and the corresponding access restrictions. The curious [@inline] is an inlining annotation similar to the inline keyword in C/C++. The invariant that a value of the index type is an integer within [lwb,upb] guarantees safe access to an array whose index range is the same [lwb,upb]. Because the safety is assured, the run-time bounds check can be elided: module BArray(S: sig type el val arr : el array end) = struct include IndexF(struct let upb = Array.length S.arr - 1 end) let[@inline] bget : index -&gt; S.el = fun i -&gt; Array.unsafe_get S.arr (i :&gt; int) end If a1 is an integer array then BArray(struct type el = int let arr = a1 end) is a module that implements the Index interface with one extra operation: bget for indexing within the array a1. (The operation (i :&gt; int) is the coercion of an index to an int, which is always possible and is, operationally, the identity.) The instantiation of IndexF makes lwb be zero and upb be the length of a1 minus one -- which is the index range of a1, if it is not empty, thus justifying the use of unsafe_get. (If a1 is an empty array, unsafe_get is still justified, because the index type has no values.) We have implemented the trusted `security kernel&#39;, providing the Index API with the extra bget method. We can now write the binary search itself. It takes the comparison operation cmp, a key and an array and returns either None if the key does not occur in the array, or Some (i,key) where i is the index at which the key does occur. The array is assumed sorted, according to cmp. We took this bsearch interface from Xi and Pfenning&#39;s PLDI 1998 paper. let bsearch (type a) : (a*a -&gt; int) -&gt; a -&gt; a array -&gt; (int * a) option = fun cmp key arr -&gt; let module M = BArray(struct type el = a let arr = arr end) in let open M in let rec look (lo:indexL) (hi:indexH) = (* type annotations are for clarity*) match bcmp lo hi with | None -&gt; None | Some (lo&#39;,hi&#39;) -&gt; (* lo&#39; and hi&#39; are of the type index now *) let m = bmid lo&#39; hi&#39; in let x = bget m in let cmp_r = cmp (key,x) in if cmp_r &lt; 0 then look lo (bpred m) else if cmp_r = 0 then Some ((m :&gt; int), x) else look (bsucc m) hi in look lwb upb The implementation is textbook, and also closely matches Xi and Pfenning&#39;s code: only theirs was in Dependent ML and ours is ordinary OCaml. The key part is bcmp lo hi that compares lo (bounded from below by zero) with hi (bounded from above by upb, the largest index value within the array). If lo does not exceed hi, then [lo,hi] interval is non-empty, and is contained within [0,upb], the safe index range. The bsearch code is not part of a trusted security kernel: rather, it is `user-level&#39;, so to speak. It is written using the interface of the (instantiated) BArray module and benefits from its invariants: the guaranteed safe indexing within the input array. If we compile the bsearch code with ocamlopt -O3 and look at the generated assembly, we see all array access and index calculations inlined, with no bounds checks and no calls to error functions. Safe and efficient, indeed. There are many variations of the bsearch code; the references below show several. It is worth noting one variation point, related to the feature of the BArray module that we did not stress. BArray assures safe indexing within an array whose length is not known until run-time. The more one thinks about it, the harder it gets to believe. How is it possible to use types -- which are checked statically, before the program runs -- to ensure in-bounds indexing when the bounds are not known until the run-time? The following example explains. After the set-up: let a1 = [|1;3|] and a2 = [|1;3|] module M1 = BArray(struct type el=int let arr=a1 end) module M2 = BArray(struct type el=int let arr=a2 end) Evaluating (M1.lwb :&gt;int);; - : M1.indexL = 0 M1.bget M1.lwb;; ^^^^^^ Error: This expression has type M1.indexL but an expression was expected of type M1.index gives a type error. One may be puzzled: how come we cannot index an array at index 0? Because it is not always safe: the array may be empty. Therefore, in the Index interface, M1.lwb has the type indexL, rather than index expected by bget. The only way to get the index zero value is through the comparison lwb with upb, which amounts to the non-emptiness check. The types force us to do this check: match (M1.bcmp M1.lwb M1.upb, M2.bcmp M2.lwb M2.upb) with (Some (l1,u1), Some (l2,u2)) -&gt; (M1.bget l1, M2.bget l2);; - : int * int = (1, 1) However, evaluating match (M1.bcmp M1.lwb M1.upb, M2.bcmp M2.lwb M2.upb) with (Some (l1,u1), Some (l2,u2)) -&gt; ((l1:&gt;int),(l2:&gt;int));; - : int * int = (0, 0) match (M1.bcmp M1.lwb M1.upb, M2.bcmp M2.lwb M2.upb) with (Some (l1,u1), Some (l2,u2)) -&gt; M1.bget l2;; ^^ Error: This expression has type M2.index but an expression was expected of type M1.index is again the type error -- even though we checked that both arrays are non-empty and l1 may indeed index within M1&#39;s array. Although both l1 and l2 are of the type index and both represent the integer 0, only l1 may index within M1&#39;s array. This is because the types of l1 and l2 are actually different: M1.index and M2.index, resp. Each instantiation of BArray with a different array creates a new, fresh `version&#39; of the index type -- to be used for indexing only within that instance of BArray. One may say, a BArray instance makes a distinct `brand&#39; of its abstract types, usable only with the operations of the same brand. (The code of the operations need not be duplicated since the optimizer knows that index of any brand is a mere integer.) Technically, M1.index and M2.index are considered distinct by the type checker because they have different `paths&#39;, or the provenance, which can be easily checked statically. This facility of OCaml is akin to path-dependent types in Scala. The branding, albeit not a `simple type&#39; facility, is not exotic: it can be accomplished in any language with existential (or, `abstract package&#39; types), or universal types. OCaml has both existential and universal types, which gives two other ways to write the safe bsearch. The code referenced below shows one such alternative, using universal types, in OCaml and Haskell. Version The current version is -- original (Haskell): August 2004; improved and explained OCaml: August 2019 References Hongwei Xi and Frank Pfenning: Eliminating Array Bound Checking Through Dependent Types. PLDI&#39;98 The famous paper introducing a practical dependent type system as a dialect of SML. We faithfully re-implement the bsearch example from that paper, in several languages. eliminating-array-bound-checks.ml [15K] OCaml code explained in this message, plus the older implementation of branding based on universal types eliminating-array-bound-check-literally.hs [8K] The Haskell version of bsearch. The code is written in Haskell98 with the sole extension for higher-ranked types. eliminating-array-bound-check.lhs [9K] The literate Haskell code with explanations and the examples The first version of the code was originally posted as Eliminating Array Bound Checking through Non-dependent types on the Haskell mailing list on Thu, 5 Aug 2004 19:31:36 -0700. The current version corrects the problem pointed out by Conor T. McBride in the discussion thread. bsearch-static.cc [2K] bsearch-template.cc [2K] bsearch-static.s [3K] bsearch-template.s [2K] Binary search in an array with the statically known bounds: C++ code (with and without templates) and the generated assembly code (gcc -O2). The assembly code shows no run-time overhead of the abstractions used to ensure the safety of array access. Lightweight guarantees and static analyses Another explanation of the branding technique, on a simpler example Lightweight static capabilities We describe a modular programming style that harnesses modern type systems to verify safety conditions in practical systems. This style has three ingredients: (i) A compact kernel of trust that is specific to the problem domain; (ii) Unique names, capabilities, that confer rights and certify properties, so as to extend the trust from the kernel to the rest of the application; (iii) Static (type) proxies for dynamic values. We illustrate our approach using examples from the dependent-type literature, but our programs are written in Haskell and OCaml today, so our techniques are compatible with imperative code, native mutable arrays, and general recursion. The three ingredients of this programming style call for (1) an expressive core language, (2) higher-rank polymorphism, and (3) phantom types. This paper demonstrates a lightweight notion of static capabilities that brings together increasingly expressive type systems and increasingly accessible program verification. Like many programmers, we want to assure safety conditions: array indices remain within bounds; modular arithmetic operates on numbers with the same modulus; a file or database handle is used only while open; and so on. The safety conditions protect objects such as arrays, modular numbers, and files. Our overarching view is that a static capability authorizes access to a protected object and simultaneously certifies that a safety condition holds. Rather than proposing a new language or system, our contribution is to substantiate the slogan that types are capabilities, today: we use concrete and straightforward code in Haskell and OCaml to illustrate that a programming language with an appropriately expressive type system is a static capability language. This is a joint work with Chung-chieh Shan. References lightweight-static-capabilities.pdf [334K] The paper published in Electr. Notes Theor. Comput. Sci, 174(7), pp. 79-104, 2007 lightweight-guarantees-u07-poster.pdf [81K] Poster `Lightweight Static Guarantees&#39; presented at the Poster Session of the 2007 USENIX Annual Technical Conference. June 20, 2007. Santa Clara, CA Lightweight static resources, for safe embedded and systems programming This follow-up paper describes further applications, for safe embedded and systems programming, ensuring, among other properties, proper alignment when accessing raw memory areas. The paper also introduces kind-level static capabilities to enforce invariants of type-level programming. Formalization of the abstract type protection How do we hope to prove that the protection layer indeed enforces invariants and extends locally established properties through the whole program? The Lightweight static capabilities paper (joint work with Chung-chieh Shan) introduced the so-called `strict&#39; type system with the dependent-type flavor. The safety invariants are enforced and propagated because they are part of types. The paper then introduced a relaxation of the strict system to the Hindley-Milner system with type abstraction (higher-ranked types), and demonstrated how the safety invariants are still maintained. safety.elf [17K] Twelf code with proofs of soundness of the `strict&#39; type system for the language based on System F with lists and non-empty lists. The progress theorem assures that a well-typed program shall not attempt to take the head or tail of an empty list. safety-array.elf [32K] Twelf code verifying manual proofs of soundness of the `strict&#39; type system for the language made of System F plus `arrays&#39; whose type reflects their size. The progress theorem states the safety property: in a well-typed program array access is always in-bounds. Eliminating array bound checking in multiple arrays This message gives another non-trivial example of writing code with non-trivial static guarantees in present-day functional languages. The example involves native Haskell arrays, index computations, and general recursion. All array indexing operations are statically guaranteed to be safe -- hence we can safely use the efficient unsafeAt primitive. Furthermore, the static assurances in the main loop cost us no run-time overhead. The example uses only Haskell98 + higher-ranked types. No new type classes are introduced. The safety is based on: Haskell type system, quantified type variables, and a compact general-purpose trusted kernel. I thank Daniel Yokomizo for the challenge. Our example is folding over multiple, variously-sized arrays. This is like a fold over an array -- generalized to an arbitrary number of arrays, whose index ranges do not have to be the same (and do not have to overlap). Typing this example in a genuinely dependent type system is probably going to be quite challenging. Our goal is to implement a function marray_fold :: (Ix i, Integral i) =&gt; (seed -&gt; [e] -&gt; seed) -&gt; seed -&gt; [Array i e] -&gt; seed Its third argument is a list of arrays; the arrays have all the same element and index types; the actual sizes (that is, the lower and upper bounds) may differ. Some arrays in the list may even be empty (with the lower bound higher than the upper one). The function marray_fold, like left fold, applies its left argument to the values extracted from the corresponding arrays. Because arrays may have different sizes and bounds, marray_fold operates over the range of indices that is the intersection of the ranges of the argument arrays. For example: dot a1 a2 = marray_fold (\seed l -&gt; product l + seed) 0 [a1,a2] computes the dot products of two arrays. Version The current version is February 2006 References eliminating-mult-array-bound-check.lhs [13K] The literate Haskell98 plus higher-order types The code was posted as Eliminating Multiple-Array Bound Checking through Non-dependent types on the Haskell mailing list on Fri, 10 Feb 2006 22:05:04 -0800 (PST) Knuth-Morris-Pratt string search with safe array access The largest example in the Xi and Pfenning&#39;s PLDI&#39;98 paper is Knuth-Morris-Pratt (KMP) string search. It is an imperative algorithm with complicated control flow, mutable arrays and indirect indexing within the pattern string. It also uses a deliberately out-of-bounds index value (-1) as a special indicator in the indirect indexing. The goal is to statically assure safety of all array and string access operations, and so to eliminate run-time array-bound check without introducing other overhead into the main loops of the algorithm. We re-implement Xi&#39;s KMP code in Haskell and OCaml, maintaining the same safety guarantees and efficiency. Version The current version is May 2006 References KMP-DML.ml [4K] The KMP code in Dependent ML, written by Hongwei Xi and published in Xi and Pfenning, PLDI&#39;98 KMP-deptype.hs [14K] The complete Haskell code: Haskell98 with higher-ranked types KMP.ml [11K] The corresponding OCaml code The question of verification The lightweight approaches depend on a trusted kernel. How to make sure the trusted library deserves our trust? The same question exists for any other dependent-type system: how we make sure that our Oracle is correct, the typing rules are correct, and, mainly, that the implementation of those rules in a compiler is correct? I have heard a similar question asked of J. Strother Moore and J. Harrison. J. Strother Moore said that most of ACL2 is built by bootstrapping, from lemmas and strategies that ACL2 itself has proven. However, the core of ACL2 just has to be trusted. ACL2 has been used for quite a while and so there is a confidence in its soundness. NSA and NIST found this argument persuasive when they accepted proofs by ACL2 as evidence of high assurance, in awarding Orange book A1 and IFIPS 140-1 ratings -- the highest security ratings -- to some products. In general, verification is a rather complex issue, far beyond the mere checking of the derivations of formal propositions: see the references below. Even in Mathematics, it is not at all resolved what exactly constitutes a mathematical proof and how much trust, including personal trust, is involved. References Randy Pollack: How to believe a machine-checked proof Toby Murray, P.C. van Oorschot. BP: Formal Proofs, the Fine Print and Side Effects Awarded `the best paper&#39; at IEEE SecDev 2018 ``We consider what value proofs about software systems deliver to end-users (e.g., in terms of net assurance benefits), and at what cost in terms of side effects (such as changes made to software to facilitate the proofs, and assumption-related deployment restrictions imposed on software if these proofs are to remain valid in operation).&#39;&#39; In short, this is the paper on how to believe in a formal (security) proof and what value it actually offers. Arthur Jaffe, Frank Quinn: ``Theoretical mathematics&#39;&#39;: Toward a cultural synthesis of mathematics and theoretical physics Bull.Am.Math.Soc. 29 (1993) 1-13 &lt;http://arxiv.org/abs/math.HO/9307227&gt; William P. Thurston: On Proof And Progress In Mathematics Bull.Am.Math.Soc. 30 (1994) 167-177 &lt;http://arxiv.org/abs/math.HO/9404236&gt; Michael Atiyah, Armand Borel, G. J. Chaitin, Daniel Friedan, James Glimm, Jeremy J. Gray, Morris W. Hirsch, Saunder MacLane, Benoit B. Mandelbrot, David Ruelle, Albert Schwarz, Karen Uhlenbeck, Rene&#39; Thom, Edward Witten, Christopher Zeeman: Responses to ``Theoretical Mathematics: Toward a cultural synthesis of mathematics and theoretical physics&#39;&#39;, by A. Jaffe and F. Quinn Bull.Am.Math.Soc. 30 (1994) 178-207. &lt;http://arxiv.org/abs/math.HO/9404229&gt; Arthur Jaffe, Frank Quinn: Response To Comments On ``Theoretical Mathematics&#39;&#39; Bull.Am.Math.Soc. 30 (1994) 208-211. &lt;http://arxiv.org/abs/math/9404231&gt; Lightweight guarantees and static analyses When the code for the safe and efficient binary search was first posted on the Haskell mailing list in August 2004, a lively discussion followed. In particular, Bjoern Lisper asked about the relation to classical range analyses known for a long time for imperative languages. First of all, within the divide ``making sense of already written programs v. writing only those programs that make sense&#39;&#39;, range analysis, as other static verification, belongs to the first group -- whereas types, model-driven development, refinement and lightweight guarantees belong to the second. Mainly, lightweight guarantees, or language protection layer, lets us implement the classical range analyses in a program itself -- and be sure of their outcome. In contrast, analyses in the compiler cannot be easily seen or controlled, and their outcome is often hard to detect and explain. Let&#39;s take an example: locating the first element of an array satisfying a given predicate and returning its index. let findarr : type a. (a -&gt; bool) -&gt; a array -&gt; int option = fun p arr -&gt; let n = Array.length arr in let rec loop i = if i &lt; n then if p (arr.(i)) then Some i else loop (succ i) else None in loop 0 The classical range analysis will see that i starts at the lower bound of arr, i.e., zero, and is incremented afterwards. When the analysis sees the test i&lt;n it infers that in the `then&#39; branch of that test i does not exceed the upper bound of the array. Therefore, the indexing operation arr.(i) is safe and the run-time range check may be elided. In the lightweight guarantees framework the code looks as follows (see the reference to the complete code below): let findarr&#39; : type a. (a -&gt; bool) -&gt; a array -&gt; int option = fun p arr -&gt; let module M = LenF(struct type el=a let arr=arr end) in let open M in let rec loop i = match cmp i length with | Some i&#39; -&gt; if p (get M.arr i&#39;) then Some (i&#39; :&gt; int) else loop (Nat.succ i) | _ -&gt; None in loop Nat.zero The programmer gives the array, array length and i more precise types: M.arr has the type a array M.len, length has the type int M.len, and i:nat. Here M.len is an `annotation&#39; (erased at run-time) that an object has the length len. We should stress that the comparison of i with the array length no longer returns a mere boolean. The type of cmp is nat -&gt; int M.len -&gt; (nat M.len) option If the comparison cmp i length succeeds, the result is M.len-annotated i, which is in bounds of the array M.arr. The successful comparison `improves i&#39;s type&#39;, so to speak, making it more precise. Thus the logical implication that was implicit in the range checker is made explicit to the type checker here. Bjoern Lisper further wrote ``A program analysis like range analysis is not exact, of course: it must make safe approximations sometimes and will sometimes say that an array index might be out of bounds when it actually won&#39;t. In your framework, this seems to correspond to the fact that you must verify your propositions about index expressions.&#39;&#39; True, just as the range analysis must verify the rules of the analysis. The difference is that the conventional range analyzer is part of a compiler, typically hidden from view (of a regular programmer). Here, the analyzer is part of a library. The need for approximations may also arise in our framework. Suppose that in the original findarr code, the line if p (arr.(i)) then Some i ... had been replaced with let j = very_complex_function i in if p (arr.(j)) then Some j ... Although the analysis may know that i is within array bounds, it may be very difficult to ascertain if j is. The classical analysis may give up and insert a run-time check (often without any indications it is done so). In our framework, we have to let j = very_complex_function (i&#39;:&gt;int) in match range_check j with | Some j&#39; -&gt; if p (arr.(j&#39;)) then Some (j&#39;:&gt;int) ... | None -&gt; on_out_of_range That is, we intentionally forget the more precise typing of i&#39;, do the complex index transformation, followed by a run-time witnessing to recover the precise typing. We now have to handle the situation if the result of very_complex_function (i&#39;:&gt;int) is out of range. If we somehow know that the very_complex_function keeps the result within the range, but do not have the time to verify or prove it, we can replace on_out_of_range with assert false. In any case, the fact that we gave up on the precise analysis is very explicit, and so is the dynamic check we had to insert. Incidentally, if we can prove that very_complex_function leaves the index in range, then we can give the function a more precise type: nat M.len -&gt; nat M.len and put into the trusted kernel, after the appropriate rigorous verification. References range-check-expl.ml [3K] The complete OCaml code for the example Thread Re: [Haskell] Eliminating Array Bound Checking through Non-dependent types on the Haskell mailing, August 4-8 2004 Lightweight guarantees and dependent types We have seen that type abstraction lets attribute an invariant (a proposition) to the values of an abstract type -- e.g., all values of the type represent integers within a certain range. There is a clear similarity with dependent types. A dependent type expresses the proposition about the values of the type directly (using the language of logic), whereas an abstract type merely refers to the proposition. The difference is akin to that between a golden coin and paper money. The coin has its worth in the gold it carries. It is relatively easy to compare and exchange coins of different coinages, based on the weight and purity of their gold. Paper money merely refers to worth, and requires trusted institutions (state, banks) to operate. Comparing paper money of different issuers is non-trivial. Still, we know from history that an economy based on paper money is viable. In the discussion thread following the first presentation of the lightweight guarantees approach in August 2004, Conor McBride has made an excellent summary of this approach and its relation to genuine dependent types: ``The abstract brand is just a type-level proxy for the bounding interval, and the library of operations provides interval-respecting operations on indices. This is a very neat solution in Haskell, but it goes round an extra corner which isn&#39;t necessary with dependent types, where you can just talk about the interval directly. The library-writer would develop and verify the same convenient operations for working with intervals and indices; the proofs would be independently recheckable terms in type theory.&#39;&#39; References Extensive discussion with Conor McBride, with his many insights and explanations of dependent-type programming. Haskell mailing list, August 6-9, 2004. Thread: Re: Eliminating Array Bound Checking through Non-dependent types Rights amplification A capability, mentioned several times earlier, is a token that authorizes an access to a resource or service -- something like a train ticket, which gives the right to board a train. Capabilities originated in secure operating systems; see the ode-capabilities article referenced below for history. Continuing the ticket analogy, to board a plane we also need a passport. Passport is an authorizing token as well, giving us, e.g., the right to enter and remain in the country. It does not, by itself, give the right to board a plane -- and neither does the ticket. We need both these tokens, and they have to match (have the same name). The right conferred by the matching combination of the passport and the ticket is more that the sum of the rights of these tokens by themselves. This is rights amplification. We have already met rights amplification in the Eliminating array bound checks example: to access an array value we have to possess the branded array as well as the branded index of the same `brand&#39;, which stands for array bounds. This article describes another classical example of rights amplification: `simple money&#39;, from the ode-capabilities: ``The function, makeMint, makes mints. Each mint defines a separate currency that isn&#39;t directly convertible with other currencies -- although, of course, money changers could trade one for the other, providing indirect convertibility. A mint can make purses that hold new units of its currency, thereby inflating that currency. A purse can report its balance and make [`sprout&#39;] a new empty purse of the same currency. Given two purses of the same currency, you can deposit money into one from the other.&#39;&#39; Thus a purse, by itself, supports only balance reporting and sprouting. It is only when holding two purses of the same currency that an additional operation becomes possible: deposit. ``It is a wonderful small example of the directness and simplicity with which capabilities allow the expression of arrangements in which mutually suspicious parties can cooperate safely,&#39;&#39; that article says. The example also demonstrates local reasoning to verify global security properties: nobody can double-spend or steal money. The ode-capabilities article implements the simple money in the `capability-oriented&#39; language E, using so-called `sealer-unsealer pairs&#39; provided by E as primitive. We re-implement the example in plain OCaml, with no special primitives. We also provide money changers, which were only hinted at in the original example. The OCaml implementation maintains the same security guarantees about money, and can ascertain them via static local reasoning. Before we start, let us make a special type Balance.t for money balances, ensured to be non-negative by construction. module Balance : sig type t = private int (* At run-time, identical to int *) val zero : t val (+) : t -&gt; t -&gt; t (* NO run-time checks or overhead *) val of_int : int -&gt; t option (* run-time positivity check *) val (-) : t -&gt; t -&gt; t option end = struct type t = int let zero = 0 let (+) = Stdlib.(+) (* no run-time checks or overhead *) let of_int : int -&gt; t option = fun x -&gt; if x &gt;= 0 then Some x else None let (-) : t -&gt; t -&gt; t option = fun x y -&gt; if x &gt;= y then Some (x-y) else None end This module is almost the same as Index in Eliminating array bound checks, which see for more explanations. The simple money interface is described by the following signature. module type simple_money = sig type &#39;currency mint (* heterogeneous equality for mints *) val mint_eq : &#39;c1 mint -&gt; &#39;c2 mint -&gt; (&#39;c1,&#39;c2) eq option type mint_holder = Mint : &#39;currency mint -&gt; mint_holder (* existential *) val make_mint : string -&gt; mint_holder val name : _ mint -&gt; string type &#39;currency purse val make_purse : &#39;currency mint -&gt; Balance.t -&gt; &#39;currency purse val string_of_purse : _ purse -&gt; string (* for documentation/debugging *) val balance : _ purse -&gt; Balance.t val sprout : &#39;currency purse -&gt; &#39;currency purse val deposit : Balance.t -&gt; from:&#39;currency purse -&gt; into:&#39;currency purse -&gt; unit (* could also throw an exception *) (* check if two purses are of the same currency *) val purse_eq : &#39;c1 purse -&gt; &#39;c2 purse -&gt; (&#39;c1,&#39;c2) eq option end We see the already introduced operations to make and query mints and purses and perform deposits. The operations name and string_of_purse are added for the sake of presentation. The heterogeneous equality comparison of mints and purses will be explained when we come to money changers. There are already static assurances that follow from the signature itself. First of all, the types mint and purse are abstract. Therefore, the only way to create mints and purses is by using the operations of the interface. The types are parameterized by the type of currency. A mint, therefore, inflates (that is, builds purses of) its own currency only: see the type of make_purse. A mint can only be created by invoking make_mint and then `opening&#39; the returned existential -- which gives a currency mint value with the fresh type currency distinct from any other type. Therefore, two purses whose types have the same &#39;currency parameter must be `siblings&#39;: must have eventually come from the same mint. Deposit is only possible among sibling purses: see the type of deposit. We thus realize rights amplification by so-called `sibling communication&#39;. The communication is enforced solely by types: it has no run-time overhead. We may already write the main example from ode-capabilities, of Alice paying Bob 10 coins: ``First, playing Alice, we would sprout a new purse from our main purse, and then transfer 10 coins into it. Then, we send a ... request to Bob, providing the purse containing 10 coins as payment&#39;&#39;: let play_alice alice_purse bob = let paymentForBob = sprout alice_purse in deposit tencoins ~from:alice_purse ~into:paymentForBob; bob paymentForBob; Printf.printf &#34;Alice: paid bob; the balance %s\n&#34; (string_of_purse alice_purse) val play_alice : &#39;a purse -&gt; (&#39;a purse -&gt; &#39;b) -&gt; unit = &lt;fun&gt; ``Playing Bob, deposit the payment into Bob&#39;s purse&#39;&#39;: let play_bob bob_purse payment_recd = deposit tencoins ~into:bob_purse ~from:payment_recd; Printf.printf &#34;Bob: got paid; %s\n&#34; (string_of_purse bob_purse) val play_bob : &#39;a purse -&gt; &#39;a purse -&gt; unit = &lt;fun&gt; The point of the example is to assure Bob that if deposit succeeded, then 10 coins will indeed be transferred from Alice&#39;s payment into his purse -- even though Bob cannot see how exactly Alice made the payment_rcd purse. What if it is `fake&#39;? What is there to prevent Alice from double-spending? Bob gets the assurances by examining the signature simple_money and its implementation, to be shown next. Bob does not need to see Alice&#39;s code. Just for completeness, here is how we run the example: mint the main purses for Alice and Bob, and let them perform the transaction: let _ = let Mint carol_mint = make_mint &#34;Carol&#34; in let aliceMainPurse = make_purse carol_mint (bal 1000) in let bobMainPurse = make_purse carol_mint Balance.zero in Printf.printf &#34;Alice&#39;s purse: %s\n&#34; (string_of_purse aliceMainPurse); Printf.printf &#34;Bob&#39;s purse: %s\n&#34; (string_of_purse bobMainPurse); let alice = play_alice aliceMainPurse in let bob = play_bob bobMainPurse in alice bob Here is an implementation of simple_money (abbreviated; see the full code for all details): module Money : simple_money = struct module type mint = sig type curr val name : string (* ... elided *) end type &#39;currency mint = (module mint with type curr = &#39;currency) type mint_holder = Mint : &#39;currency mint -&gt; mint_holder (* existential *) let make_mint : string -&gt; mint_holder = fun name -&gt; let module NewMint = struct type curr (* fresh type *) let name = name (* ... elided *) end in Mint (module NewMint) type &#39;currency purse = {mint: &#39;currency mint; bal: Balance.t ref} let make_purse : &#39;currency mint -&gt; Balance.t -&gt; &#39;currency purse = fun mint b -&gt; let bal = ref b in {bal;mint} let balance : _ purse -&gt; Balance.t = fun {bal} -&gt; !bal let sprout : &#39;currency purse -&gt; &#39;currency purse = fun {mint} -&gt; make_purse mint Balance.zero (* could also throw an exception *) let deposit : Balance.t -&gt; from:&#39;currency purse -&gt; into:&#39;currency purse -&gt; unit = fun b ~from ~into -&gt; let rem = Balance.(!(from.bal) - b) |&gt; Option.get in from.bal := rem; into.bal := Balance.(!(into.bal) + b) end It has the same security properties as the E code in the original example: Only someone with the mint of a given currency can violate conservation of that currency (that is, created new non-empty purses of that currency). The mint can only inflate its own currency. No one can affect the balance of a purse they don&#39;t have. With two purses of the same currency, one can transfer money between them. Balances are always non-negative integers. A reported successful deposit can be trusted as much as one trusts the purse one is depositing into. We already verified items 2 and 4, when looking at the signature simple_money. Item 5 is guaranteed by the Balance module. To see that property 1 holds, we examine all operations that touch the field bal of a purse. With the exception of make_purse, all such operations preserve currency. Also, the field bal is not aliased to a global reference. In fact, there are no global parameters. The absence of aliasing also assures property 3: if one does not have a reference to a purse, one cannot possibly affect the bal of that purse. For item 6 we check the implementation of deposit. Let&#39;s return to the main question, of assuring Bob that the purse with the payment from Alice is not fake, even if he does not have any way of knowing how Alice has made it. How can Bob be certain that if the deposit succeeded, 10 coins are transferred into his purse and Alice cannot spend them again. In the original code E code, the answer relies on examining the implementation for sealer/unsealer pairs and reasoning from the properties of such pairs. In our case, the reasoning is simple: the bal field of a purse is not exposed (purse is the abstract type in the simple_money signature). It is also not aliased, which is clear from examining the two functions that return purses. Therefore, purses cannot be faked, and the invariants of their implementation (whenever balance of one purse is decremented, another purse is credited) hold globally. One may say that the type-based enforcement of sibling communication is too rigid. Let&#39;s look again at the inferred type of play_bob : &#39;a purse -&gt; &#39;a purse -&gt; unit. Attempting to give Bob a purse whose currency cannot be ascertained the same as Bob&#39;s main purse raises a type error. This is often the desirable outcome. It could be the case however that the payment purse is received from a different host or read from a file, and its currency cannot be statically known. Also, Bob could be willing to accept payments in several currencies. Our simple_money interface and OCaml do permit such `dynamic&#39; currency processing. It uses a form of sealing and does involve a run-time check, which is necessary if the type of currency is not statically known and may vary. As an illustration, we implement a money changer, mentioned in passing in ode-capabilities. First we introduce type sealed_purse = Sealed : &#39;currency Money.purse -&gt; sealed_purse which hides the currency type in an existential. A money changer can then be given the type type money_changer = sealed_purse -&gt; sealed_purse -&gt; unit A money changer hence takes two purses of arbitrary currencies and attempts to transform all money from the former to latter, at some exchange rate. From the look at the simple_money interface, such an operation is impossible, one might think. However, if one has a reserve of appropriate currencies -- a bank -- money exchange is doable. The enclosed code implements it, outside the Money module and hence maintaining all the security invariants of Money. The key operation is the heterogeneous purse equality type (&#39;a,&#39;b) eq = Refl : (&#39;a,&#39;a) eq (* equality type *) val purse_eq : &#39;c1 purse -&gt; &#39;c2 purse -&gt; (&#39;c1,&#39;c2) eq option which compares two purses (actually, their mints) of possibly different currencies to see if the currencies are actually the same. If they are, the operation returns the evidence of the equality. Thus, if we have two sealed purses sealed1 and sealed2, we can write match (sealed1,sealed2) with (Sealed s1,Sealed s2) -&gt; match purse_eq s1 s2 with | Some Refl -&gt; deposit (bal 10) s1 s2 | None -&gt; (* s1 and s2 are of different currencies *) Within the Some Refl branch of the match statement, purses s1 and s2 can be treated as having the same currency type. One may therefore deposit money between them. Using the lightweight static capabilities, the simple money example was first implemented (without the money changer) in 2006, as an answer to a challenge by Mark S. Miller. I thank him for the challenge and insightful explanations. References &lt;http://www.erights.org/elib/capability/ode/ode-capabilities.html&gt; Explanation of rights amplification on the simple-money example, using the E language mint.ml [17K] The re-implementation of this example and the money changer in plain OCaml, with many comments, explanations and tests Conclusions We have demonstrated the programming style that ensures safety without sacrificing efficiency. The key idea is a language protection layer (specifically, type abstraction), which lets us restrict access to some operations on data. Operations that preserve a desired invariant may be accessed `publicly&#39;; the other, potentially invariant-destroying operations, may only be invoked within a (usually small) `trusted kernel&#39;, where the invariant is assured by careful code inspection or in some formal way. All in all, the invariants established by local reasoning are preserved globally. The globally-valid invariants (such as sortedness, range limit, etc) then guarantee safe execution of the program and let us elide run-time safety checks. Safe and Efficient can be practiced right now. In fact, the main idea was proposed by Milner and Morris in the mid-1970s, and was shown to work already then. I personally have been using it successfully, in production and in answering challenges. Further challenges and suggestions are welcome. </description>
      <pubDate>09 Feb 21 09:27 EST</pubDate>
      <guid>http://okmij.org/ftp/Computation/lightweight-static-guarantees.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://waitbutwhy.com/2014/06/taming-mammoth-let-peoples-opinions-run-life.html</link>
      <description>&lt;a href=&#34;https://waitbutwhy.com/2014/06/taming-mammoth-let-peoples-opinions-run-life.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; We made a fancy PDF of this post for printing and offline viewing. Buy it here. Part 1: Meet Your Mammoth The first day I was in second grade, I came to school and noticed that there was a new, very pretty girl in the class—someone who hadn’t been there the previous two years. Her name was Alana and within an hour, she was everything to me. When you’re seven, there aren’t really any actionable steps you can take when you’re in love with someone. You’re not even sure what you want from the situation. There’s just this amorphous yearning that’s a part of your life, and that’s that. But for me, it became suddenly relevant a few months later, when during recess one day, one of the girls in the class started asking each of the boys, “Who do youuu want to marry?” When she asked me, it was a no-brainer. “Alana.” Disaster. I was still new to being a human and didn’t realize that the only socially acceptable answer was, “No one.” The second I answered, the heinous girl ran toward other students, telling each one, “Tim said he wants to marry Alana!” Each person she told covered their mouth with uncontrollable laughter. I was finished. Life was over. The news quickly got back to Alana herself, who stayed as far away from me as possible for days after. If she knew what a restraining order was, she’d have taken one out. This horrifying experience taught me a critical life lesson—it can be mortally dangerous to be yourself, and you should exercise extreme social caution at all times. Now this sounds like something only a traumatized second grader would think, but the weird thing, and the topic of this post, is that this lesson isn’t just limited to me and my debacle of a childhood—it’s a defining paranoia of the human species. We share a collective insanity that pervades human cultures throughout the world: An irrational and unproductive obsession with what other people think of us. Evolution does everything for a reason, and to understand the origin of this particular insanity, let’s back up for a minute to 50,000BC in Ethiopia, where your Great2,000 Grandfather lived as part of a small tribe. Back then, being part of a tribe was critical to survival. A tribe meant food and protection in a time when neither was easy to come by. So for your Great2,000 Grandfather, almost nothing in the world was more important than being accepted by his fellow tribe members, especially those in positions of authority. Fitting in with those around him and pleasing those above him meant he could stay in the tribe, and about the worst nightmare he could imagine would be people in the tribe starting to whisper about how annoying or unproductive or weird he was—because if enough people disapproved of him, his ranking within the tribe would drop, and if it got really bad, he’d be kicked out altogether and left for dead. He also knew that if he ever embarrassed himself by pursuing a girl in the tribe and being rejected, she’d tell the other girls about it—not only would he have blown his chance with that girl, but he might never have a mate at all now because every girl that would ever be in his life knew about his lame, failed attempt. Being socially accepted was everything. Because of this, humans evolved an over-the-top obsession with what others thought of them—a craving for social approval and admiration, and a paralyzing fear of being disliked. Let’s call that obsession a human’s Social Survival Mammoth. It looks something like this: Your Great2,000 Grandfather’s Social Survival Mammoth was central to his ability to endure and thrive. It was simple—keep the mammoth well fed with social approval and pay close attention to its overwhelming fears of nonacceptance, and you’ll be fine. And that was all well and fine in 50,000BC. And 30,000BC. And 10,000BC. But something funny has happened for humans in the last 10,000 years—their civilization has dramatically changed. Sudden, quick change is something civilization has the ability to do, and the reason that can be awkward is that our evolutionary biology can’t move nearly as fast. So while for most of history, both our social structure and our biology evolved and adjusted at a snail’s pace together, civilization has recently developed the speed capabilities of a hare while our biology has continued snailing along. Our bodies and minds are built to live in a tribe in 50,000BC, which leaves modern humans with a number of unfortunate traits, one of which is a fixation with tribal-style social survival in a world where social survival is no longer a real concept. We’re all here in 2014, accompanied by a large, hungry, and easily freaked-out woolly mammoth who still thinks it’s 50,000BC. Why else would you try on four outfits and still not be sure what to wear before going out? The mammoth’s nightmares about romantic rejection made your ancestors cautious and savvy, but in today’s world, it just makes you a coward: And don’t even get the mammoth started on the terror of artistic risks: The mammoth’s hurricane of fear of social disapproval plays a factor in most parts of most people’s lives. It’s what makes you feel weird about going to a restaurant or a movie alone; it’s what makes parents care a little too much about where their child goes to college; it’s what makes you pass up a career you’d love in favor of a more lucrative career you’re lukewarm about; it’s what makes you get married before you’re ready to a person you’re not in love with. And while keeping your highly insecure Social Survival Mammoth feeling calm and safe takes a lot of work, that’s only one half of your responsibilities. The mammoth also needs to be fed regularly and robustly—with praise, approval, and the feeling of being on the right side of any social or moral dichotomy. Why else would you be such an image-crafting douchebag on Facebook? Or brag when you’re out with friends even though you always regret it later? Society has evolved to accommodate this mammoth-feeding frenzy, inventing things like accolades and titles and the concept of prestige in order to keep our mammoths satisfied—and often to incentivize people to do meaningless jobs and live unfulfilling lives they wouldn’t otherwise consider taking part in. Above all, mammoths want to fit in—that’s what tribespeople had always needed to do so that’s how they’re programmed. Mammoths look around at society to figure out what they’re supposed to do, and when it becomes clear, they jump right in. Just look at any two college fraternity pictures taken ten years apart: Or all those subcultures where every single person has one of the same three socially-acceptable advanced degrees: Sometimes, a mammoth’s focus isn’t on wider society as much as it’s on winning the approval of a Puppet Master in your life. A Puppet Master is a person or group of people whose opinion matters so much to you that they’re essentially running your life. A Puppet Master is often a parent, or maybe your significant other, or sometimes an alpha member of your group of friends. A Puppet Master can be a person you look up to who you don’t know very well—maybe even a celebrity you’ve never met—or a group of people you hold in especially high regard. We crave the Puppet Master’s approval more than anyone’s, and we’re so horrified at the thought of upsetting the Puppet Master or feeling their nonacceptance or ridicule that we’ll do anything to avoid it. When we get to this toxic state in our relationship with a Puppet Master, that person’s presence hangs over our entire decision-making process and pulls the strings of our opinions and our moral voice. With so much thought and energy dedicated to the mammoth’s needs, you often end up neglecting someone else in your brain, someone all the way at the center—your Authentic Voice. Your Authentic Voice, somewhere in there, knows all about you. In contrast to the black-and-white simplicity of the Social Survival Mammoth, your Authentic Voice is complex, sometimes hazy, constantly evolving, and unafraid. Your AV has its own, nuanced moral code, formed by experience, reflection, and its own personal take on compassion and integrity. It knows how you feel deep down about things like money and family and marriage, and it knows which kinds of people, topics of interest, and types of activities you truly enjoy, and which you don’t. Your AV knows that it doesn’t know how your life will or should play out, but it tends to have a strong hunch about the right step to take next. And while the mammoth looks only to the outside world in its decision-making process, your Authentic Voice uses the outside world to learn and gather information, but when it’s time for a decision, it has all the tools it needs right there in the core of your brain. Your AV is also someone the mammoth tends to ignore entirely. A strong opinion from a confident person in the outside world? The mammoth is all ears. But a passionate plea from your AV is largely dismissed until someone else validates it. And since our 50,000-year-old brains are wired to give the mammoth a whole lot of sway in things, your Authentic Voice starts to feel like it’s irrelevant. Which makes it shrink and fade and lose motivation. Eventually, a mammoth-run person can lose touch with their AV entirely. In tribal times, AVs often spent their lives in quiet obscurity, and this was largely okay. Life was simple, and conformity was the goal—and the mammoth had conformity covered just fine. But in today’s large, complex world of varying cultures and personalities and opportunities and options, losing touch with your AV is dangerous. When you don’t know who you are, the only decision-making mechanism you’re left with is the crude and outdated needs and emotions of your mammoth. When it comes to the most personal questions, instead of digging deep into the foggy center of what you really believe in to find clarity, you’ll look to others for the answers. Who you are becomes some blend of the strongest opinions around you. Losing touch with your AV also makes you fragile, because when your identity is built on the approval of others, being criticized or rejected by others really hurts. A bad break-up is painful for everyone, but it stings in a much deeper place for a mammoth-run person than for a person with a strong AV. A strong AV makes a stable core, and after a break-up, that core is still holding firm—but since the acceptance of others is all a mammoth-run person has, being dumped by a person who knows you well is a far more shattering experience. Likewise, you know those people who react to being criticized by coming back with a nasty low-blow? Those tend to be severely mammoth-run people, and criticism makes them so mad because mammoths cannot handle criticism. At this point, the mission should be clear—we need to figure out a way to override the wiring of our brain and tame the mammoth. That’s the only way to take our lives back. Part 2: Taming the Mammoth Some people are born with a reasonably tame mammoth or raised with parenting that helps keep the mammoth in check. Others die without ever reining their mammoth in at all, spending their whole lives at its whim. Most of us are somewhere in the middle—we’ve got control of our mammoth in certain areas of our lives while it wreaks havoc in others. Being run by your mammoth doesn’t make you a bad or weak person—it just means you haven’t yet figured out how to get a grip on it. You might not even be aware that you have a mammoth at all or of the extent to which your Authentic Voice has been silenced. Whatever your situation, there are three steps to getting your mammoth under your control: Step 1: Examine Yourself The first step to improving things is a clear and honest assessment of what’s going on in your head, and there are three parts of this: 1) Get to know your Authentic Voice This doesn’t sound that hard, but it is. It takes some serious reflection to sift through the webs of other people’s thoughts and opinions and figure out who the real you actually is. You spend time with a lot of people—which of them do you actually like the most? How do you spend your leisure time, and do you truly enjoy all parts of it? Is there anything you regularly spend money on that you don’t feel that comfortable with? How does your gut really feel about your job and relationship status? What’s your true political opinion? Do you even care? Do you pretend to care about things you don’t just to have an opinion? Do you secretly have an opinion on a political or moral issue you don’t ever voice because people you know will be outraged? There are cliché phrases for this process—”soul-searching” or “finding yourself”—but that’s exactly what needs to happen. Maybe you can reflect on this from whatever chair you’re sitting in right now or from some other part of your normal life—or maybe you need to go somewhere far away, by yourself, and step out of your life in order to effectively examine it. Either way, you’ve got to figure out what actually matters to you and start being proud of whoever your Authentic Voice is. 2) Figure out where the mammoth is hiding Most of the time a mammoth is in control of a person, the person’s not really aware of it. But you can’t make progress if you’re not crystal clear about where the biggest problem areas are. The most obvious way to find the mammoth is to figure out where your fear is—where are you most susceptible to shame or embarrassment? What parts of your life do you think about and a dreadful, sinking feeling washes over you? Where does the prospect of failure seem like a nightmare? What are you too timid to publicly try even though you know you’re good at it? If you were giving advice to yourself, which parts of your life would clearly need a change that you’re avoiding acting on right now? The second place a mammoth hides is in the way-too-good feelings you get from feeling accepted or on a pedestal over other people. Are you a serious pleaser at work or in your relationship? Are you terrified of disappointing your parents and do you choose making them proud over aiming to gratify yourself? Do you get too excited about being associated with prestigious things or care too much about status? Do you brag more than you should? A third area the mammoth is present is anywhere you don’t feel comfortable making a decision without “permission” or approval from others. Do you have opinions you’re regurgitating from someone else’s mouth, which you’re comfortable having now that you know that person has them? When you introduce your new girlfriend or boyfriend to your friends or family for the first time, can those people’s reaction to your new person fundamentally change your feelings for him/her? Is there a Puppet Master in your life? If so, who, and why? 3) Decide where the mammoth needs to be ousted It’s not realistic to kick the mammoth entirely out of your head—you’re a human and humans have mammoths in their head, period. The thing we all need to do is carve out certain sacred areas of our lives that must be in the hands of the AV and free of mammoth influence. There are obvious areas that need to be made part of the AV’s domain like your choice of life partner, your career path, and the way you raise your kids. Others are personal—it comes down to the question, “In which parts of your life must you be entirely true to yourself?” Step 2: Gather Courage by Internalizing That the Mammoth Has a Low IQ Real Woolly Mammoths were unimpressive enough to go extinct, and Social Survival Mammoths aren’t any better. Despite the fact that they haunt us so, our mammoths are dumb, primitive creatures who have no understanding of the modern world. Deeply understanding this—and internalizing it—is a key step to taming yours. There are two major reasons not to take your mammoth seriously: 1) The mammoth’s fears are totally irrational. 5 things the Mammoth is incorrect about: → Everyone is talking about me and my life and just think how much everyone will be talking about it if I do this risky or weird thing. Here’s how the mammoth thinks things are: Here’s how things actually are: No one really cares that much about what you’re doing. People are highly self-absorbed. → If I try really hard, I can please everyone. Yes, maybe in a 40-person tribe with a unified culture. But in today’s world, no matter who you are, a bunch of people will like you and a bunch of other people won’t. Being approved of by one type of person means turning another off. So obsessing over fitting in with any one group is illogical, especially if that group isn’t really who you are. You’ll do all that work, and meanwhile, your actual favorite people are off being friends with each other somewhere else. → Being disapproved of or looked down upon or shit-talked about has real consequences in my life. Anyone who disapproves of who you’re being or what you’re doing isn’t even in the same room with you 99.7% of the time. It’s a classic mammoth mistake to fabricate a vision of future social consequences that is way worse than what actually ends up happening—which is usually nothing at all. → Really judgy people matter. Here’s how judgy people function: They’re highly mammoth-controlled and become good friends with and date other judgy people who are also highly mammoth-controlled. One of the primary activities they do together is talk shit about whoever’s not with them—maybe they feel some jealousy, and eye-rolling disapproval helps them flip the script and feel less jealous, or maybe they’re not jealous and use someone as a vehicle for bathing in schadenfreude—but whatever the underlying feeling, the judging serves to feed their hungry mammoth. When people shit-talk, they set up a category division of which they’re always on the right side. They do this to prop themselves up on a pedestal that their mammoth can chomp away on. Being the material a judgy person uses to feel good about themselves is a fairly infuriating thought—but it has no actual consequences and it’s clearly all much more about the judgy person and their mammoth problem than it is about you. If you find yourself making decisions partially based on not being talked badly about by a judgy person, think hard about what’s actually going on and stop. → I’m a bad person if I disappoint or offend the person/people who love me and have invested so much in me. No. You’re not a bad person for being whoever your Authentic Voice is in your one life. This is one of those simple things—if they truly selflessly love you, they will for sure come around and accept everything once they see that you’re happy. If you’re happy and they still don’t come around, here’s what’s happening: their strong feelings about who you should be or what you should do are their mammoth talking, and their main motivation is worrying about how it’ll “look” to other people who know them. They’re allowing their mammoth to override their love for you, and they should be adamantly ignored. Two other reasons why the mammoth’s fearful obsession with social approval makes no sense: A) You live here: So who gives a fuck about anything? B) You and everyone you know are going to die. Kind of soon. So like…yeah. The mammoth’s fears being irrational is one reason the mammoth has a low IQ. Here’s the second: 2) The mammoth’s efforts are counterproductive. The irony of the whole thing is that the obsessive lumbering mammoth isn’t even good at his job. His methods of winning approval may have been effective in simpler times, but today, they’re transparent and off-putting. The modern world is an AV’s world, and if the mammoth wants to thrive socially, he should do the thing that scares him most—let the AV take over. Here’s why: AVs are interesting. Mammoths are boring. Every AV is unique and complex, which is inherently interesting. Mammoths are all the same—they copy and conform, and their motives aren’t based on anything authentic or real, just on doing what they think they’re supposed to do. That’s supremely boring. AVs lead. Mammoths follow. Leadership is natural for most AVs, because they draw their thoughts and opinions from an original place, which gives them an original angle. And if they’re smart and innovative enough, they can change things in the world and invent things that disrupt the status quo. If you give someone a paintbrush and an empty canvas, they might not paint something good—but they’ll change the canvas in one way or another. Mammoths, on the other hand, follow—by definition. That’s what they were built to do—blend in and follow the leader. The last thing a mammoth is going to do is change the status quo because it’s trying so hard to be the status quo. When you give someone a paintbrush and canvas, but the paint is the same exact color as the canvas, they can paint all they want, but they won’t change anything. People gravitate toward AVs, not mammoths. The only time a mammoth-crazed person is appealing on a first date is when they’re on the date with another mammoth-crazed person. People with a strong AV see through mammoth-controlled people and aren’t attracted to them. A friend of mine was dating a great on-paper guy awhile back but broke things off because she couldn’t quite fall for him. She tried to articulate why, saying he wasn’t weird or special enough—he seemed like “just one of the guys.” In other words, he was being run too much by a mammoth. This also holds among friends or colleagues, where AV-run people are more respected and more magnetic—not because there’s necessarily anything extraordinary about them, but because people respect someone with the strength of character to have tamed their mammoth. Step 3: Start Being Yourself This post was all fun and games until “start being yourself” came into the picture. Up to now, this has been an interesting reflection into why humans care so much what other people think, why that’s bad, how it’s a problem in your life, and why there’s no good reason it should continue to plague you. But actually doing something after you finish reading this article is a whole different thing. That takes more than reflection—it takes some courage. But courage against what, exactly? As we’ve discussed, there’s no actual danger involved in being yourself—more than anything, it just takes an Emperor Has No Clothes epiphany, which is as simple as this: Almost nothing you’re socially scared of is actually scary. Absorbing this thought will diminish the fear that you feel, and without fear, the mammoth loses some power. With a weakened mammoth, it becomes possible to begin standing up for who you are and even making some bold changes—and when you watch those changes turn out well for you with few negative consequences and no regrets, it reinforces the epiphany and an empowered AV becomes a habit. Your mammoth has now lost its ability to pull the strings, and it’s tamed. The mammoth is still with you—it’ll always be with you—but you’ll have an easier time ignoring or overruling it when it speaks up or acts out, because the AV is the alpha dog now. You can start to relish the feeling of being viewed as weird or inappropriate or confusing to people, and society becomes your playground and blank canvas, not something to grovel before and hope for acceptance from. Making this shift isn’t easy for anyone, but it’s worth obsessing over. Your Authentic Voice has been given one life—and it’s your job to make sure it gets the opportunity to live it. _________ If you’re into Wait But Why, sign up for the Wait But Why email list and we’ll send you the new posts right when they come out. It’s a very unannoying list, don’t worry. If you’d like to support Wait But Why, here’s our Patreon. You can buy the PDF of this post for offline sharing, or get your own Social Survival Mammoth here: _________ More on life and happiness from Wait But Why A different struggle going on in another part of your brain – Why Procrastinators Procrastinate The thing I learned from Elon Musk that changed the way I think about my life: The Cook and the Chef: Musk’s Secret Sauce A deeper look at the deal with the mammoth and the other animals in your brain. A post that ties it all together – A Religion for the Nonreligious To be happy, you have to know where happiness lives – Life is a Picture but You Live in a Pixel Mammoths are not good at picking life partners – How to Pick Your Life Partner You don’t have that many weeks, unfortunately. Make them count. Your Life in Weeks </description>
      <pubDate>07 Feb 21 23:42 EST</pubDate>
      <guid>https://waitbutwhy.com/2014/06/taming-mammoth-let-peoples-opinions-run-life.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://kevinyien.com/blog/swagger.html</link>
      <description>&lt;a href=&#34;https://kevinyien.com/blog/swagger.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;« Home / Posts Finding Your Swagger 2021-11-06 This is a longer post than usual. I&#39;ll start with the punchline so casual readers can get the gist. But the rest will be a long winding path into the details of one of the most difficult professional experiences I&#39;ve had, all the emotions I felt, and how they eventually led to the following lesson. I have never been a particularly confident person. I&#39;m not necessarily unconfident, but it takes an unusual amount of effort to build up my confidence. I don&#39;t think I&#39;m alone in this. Thus, I never quite realized the importance of swagger. What is swagger? For me, it&#39;s knowing what you&#39;re good at and acting accordingly — earned confidence. But it&#39;s not arrogance. Your behavior and ability are in lock step. When you lose your swagger, you lose your self. When you lose your self, you need to spend time alone. When you spend time alone, you are left with nothing but yourself. And only from there can you find yourself, and your swagger, again. Time for the long and winding part. This is a story about a time I lost my swagger. I joined a fast growing startup in 2015. I got the job by cold emailing the head of product. They brought me in for a day of interviews — all typical stuff. Until the end. My last conversation was with two of the co-founders — CEO and COO. They asked what kind of person I was. We talked for an hour. I thought they would thank me for coming in and get back to me in a couple days. They offered the job on the spot [1]. The COO left the room, printed the offer letter, and came back with a pen. I was stunned, but exhilarated. I would be the second product hire and hundredth employee. My first six months were a whirlwind. The company was growing fast. And it showed. Sales had ambitious quotas. Engineering had more bugs than they could fix. Customers had more feature requests than we could track. I felt right at home. I jumped in and got my hands dirty. The first problem to tackle was process. We needed a better way to prioritize work. I asked around, got input from all departments, made some changes, and communicated them out to the company. Bugs started going down. Development velocity went up. But we still didn&#39;t have a long term roadmap. Next was focusing on my favorite thing — customer discovery. We set up a research program, combed through customer feedback over the past year (e.g. notes in Salesforce, customer support tickets, interview notes), and aligned on the areas we needed to invest over the next year [2]. Everything seems pretty great, right? We split the engineering team into two squads. One would focus on the existing product (which was responsible for all our revenue) while another would build a new product to take us to the next level [3]. Then I had a surge of responsibility and attention. The Content team started (temporarily) reporting to me. I was presenting at the weekly all hands. And I was invited to the executive leadership meetings. They were usually held in the CEO&#39;s office. There were two couches arranged in an L-shape, an Eames lounge chair in one corner, and his desk chair in the other . I chose to sit on the air conditioning unit by the window behind the couches [4]. During the first meeting I attended, the CEO asked for my opinion in the middle of a debate. He liked it, and pointed to my answer as an exemplar of critical thinking. It felt good. But little did I know it put a target on my back. The direction I was nudging the company did not align with others. As the months went by, I started noticing a change. I stopped being invited to those meetings. Instead I would be told the outcome and what to do as a result of them. As a product manager, I didn&#39;t see it as my place to question those decisions (even if I disagreed with them). So I went along. This was the first step in losing my swagger. I lost the ability to question a decision. I let my title, rather than my beliefs, dictate my actions. Slowly, more and more product decisions were being made behind closed doors. At first they seemed innocent. Over time they became more substantial, diverging from what I was hearing from customers. But instead of challenging them, I stayed silent. Until one blew up in our faces. I wrote a lengthy email to the CEO laying out my thoughts on the situation. He called me that evening at my personal number and asked why the hell I didn&#39;t say something earlier. I described how I was feeling (left out) and he cut right to the core of it. You lost yourself. What happened to the Kevin I hired? To the spunky kid I took a bet on? You need to get that back, and fast. Or you&#39;re not gonna make it man. I was so confused. How could he say that to me when he was the one who excluded me from those meetings? Notice how my instinct was to blame him (someone else) instead of asking what I might have done to get into this situation. That sort of mentality means you&#39;re already working from a defensive position. But this was a pivotal moment. I could choose to hear his words, re-center myself, and start solving problems. Or let the gravitational pull of my downward spiral take over. Despite my best efforts, the latter happened [5]. All of a sudden, I started spending a lot of time with the COO. He told me he wanted to get more involved with product. I thought it was great. Then one day, I stayed late to whiteboard some new features with him. I needed to get home but he said how exciting this was and insisted I give a complete download of everything I was thinking. Since I hadn&#39;t been performing recently, this made it feel like things were turning around, so I obliged. Warning, it&#39;s about to get real sappy in here. I won&#39;t blame you for stopping at this point. The next day I got to the office early because I was so energized from that interaction. I went to put my stuff down at my desk, and the COO called me into his office. I walked in with a big smile on my face. He cut straight to the point. Kevin, effective immediately, you are no longer an employee at this company. I&#39;m pretty sure he said more, but my brain completely shut off at that point. I just sat there, motionless, with tears rolling down my face. I didn&#39;t actually feel an emotion. No anger. No sadness. Just a pure physiological reaction. You know those movie scenes where things get blurry and the sound muddles out? Yeah, that happened. Plus tears. Once he was finished, my legs carried me back to my desk, I said goodbye to my teammates who had since arrived, packed my stuff in a box, and left. I exited the building onto the corner of 34th and Park Ave, and just started walking. I didn&#39;t know what to do. But I knew I had to get away from that physical location. I knew I should call my wife immediately but couldn&#39;t bring myself to do it. She was 7 months pregnant with our first daughter and I was too scared to face that reality [6]. Instead, I walked the Highline for the first time. Despite working in the city for over 2 years, I had never taken the time to visit. I reached the southern end and saw an Ample Hills Creamery. I grabbed myself a cup of Ooey Gooey Buttercake ice cream and sat on a park bench eating it by myself — unemployed with dried tears on my face [7]. You can&#39;t make this stuff up people. I decided to call my parents. They did the natural thing parents do — tell you it&#39;s going to be okay, it&#39;s their loss, you&#39;ll find something better, everything happens for a reason. I heard all the words but none of it mattered. I felt like a complete failure. Then I finally called my wife. She told me to meet her at the train station so we could go home together. Once we got home, she knew exactly what to say. It just so happened to be exactly what the CEO told me a few months prior (but in her own words). You lost yourself. And you need to spend the time to find yourself before you look for a new job. What did they first see in you? What are your inherent strengths? What are you better at than anyone else? Why are those valuable to a company? Where did you succeed? What created doubt? How can you protect yourself from those? I wish I could say I went through a day of rigorous reflection and found myself on the other end. The truth is I really struggled. It required a lot of uncomfortable conversations (mostly with myself). And ultimately came down to writing...a lot [8]. I thought about my best moments. I thought about my worst moments. I derived the behaviors and attributes that led to each. I wrote a list of personal values [9]. I described the best version of myself (professionally) in narrative form. Typically I wouldn&#39;t share something like this — and I won&#39;t be sharing the full version — but I always find it useful to see real artifacts. So I&#39;m including an excerpt of that narrative here in hopes it resonates with someone and helps them perform a similar exercise. Start pushing. Live into that dream. Be the one that gets people excited. Be the one that can give the feedback that needs to be given, no matter who they are in the company. Be the one that aligns the company. Be the one that thinks more strategically than anyone else in the company, regardless of titles — they aren&#39;t important. Be the one that the company knows in its bones is responsible for the direction of the company. Be the navigation system for the company. The CEO sets the destination. You get the company there. You plan a route and know the next two directions by memory. You are reassessing the plan every minute to see if you need to re-route. Be assertive and opinionated even when you aren&#39;t as sure. Go back to having strong beliefs. Know your beliefs. Hold onto them. Especially when it *feels* right. Especially when you can point to historical events. No one should be able to shake you of your beliefs. No one in the company is above you for ensuring the success of the product. Don’t be an asshole about it, but push for what you need and don’t give in to every request. It is critically important to remember that the above was written by me, for me — not you. You need to write your own version. You need to learn enough about yourself to write it. Then and only then can you live into it. Then and only then will you know how to find your swagger. If this resonated with you or you ever feel like you lost your swagger and could use someone to talk to, my inbox is always open. NOTES [1] I felt an incredible pressure. It was a powerful move. It felt like an exploding offer. It felt like I had to take it. While I don&#39;t regret accepting the offer, I do wish I would have been more thoughtful in evaluating it. Let this be a cautionary note for anyone looking to join a startup. [2] I&#39;m oversimplifying what happened. It was way more winding. The executive team did not agree on where we should invest. It took a lot of effort to get partial alignment. And even more effort to retain that amount of alignment over time. [3] Dividing your engineering team this way isn&#39;t an inherently bad choice. But the way we did was. You can&#39;t put two engineers oncall 24/7 to maintain the existing product while everyone else works on something new. It misplaced the risk tolerance the company could afford from several dimensions. This was a clear showing of my lack of experience (and understanding of the current engineering team). [4] This is a weird but recurring behavior of mine. When sitting with a (professional) group for the first time, I will always put myself at the edge. It&#39;s not because I&#39;m avoiding the &#34;power position&#34; or anything. It&#39;s that I prefer to observe everyone first. And the best place to do that, is from the edge. [5] There are many stories I could tell at this point. But they would be very one-sided and unfair. The truth is, I don&#39;t really know what happened behind closed doors. So I&#39;ve chosen to focus on my own actions. Suffice to say, startups can be a coliseum. [6] Fortunately, the company agreed to provide COBRA which would cover our insurance costs for the delivery. This was a massive relief (more than I even realized in the moment). For anyone who ever has to fire someone who is expecting, please consider COBRA. It makes a difference. [7] I went back to that Ample Hills Creamery exactly two years after that day. I had launched a new product at my dream job. I took the day off to explore the city with my now 2-year-old daughter. We walked the Highline together. We got Ooey Gooey Buttercake ice cream. And it tasted so damn good. [8] It was all handwritten. I don&#39;t know if it made a difference. But I couldn&#39;t bring myself to sit at a computer. It felt like work. I still have all the pages of rambling thoughts. I didn&#39;t try to be very eloquent. It was all about getting things out. It didn&#39;t matter how they came out, just that they got out. Then I could sit with the words — like literally sit with the pages around me — and come to terms with them. [9] I have shared this list with people before, so it&#39;s not like some secret. But it felt odd to post it here. My concern is that people would treat it like a new productivity app — something they could quickly adopt that would solve their problems. But that&#39;s not how it works. Values are very personal. And need to be built up over time. I started with 3 and have added to that list each year. If you don&#39;t have a list of values yet, start small. Start with something you know in your bones about yourself. Then revisit it over time. </description>
      <pubDate>18 Nov 21 09:47 EST</pubDate>
      <guid>https://kevinyien.com/blog/swagger.html</guid>
    </item>
    <item>
      <title>The Coming Automation of Propaganda</title>
      <link>https://warontherocks.com/2019/08/the-coming-automation-of-propaganda/</link>
      <description>&lt;a href=&#34;https://warontherocks.com/2019/08/the-coming-automation-of-propaganda/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; If you want a vision of the future, imagine a thousand bots screaming from a human face – forever (apologies to George Orwell). As U.S. policymakers remain indecisive over how to prevent a repeat of the 2016 election interference, the threat is looming ever more ominous on the horizon. The public has unfortunately settled on the term “bots” to describe the social media manipulation activities of foreign actors, invoking an image of neat rows of metal automatons hunched over keyboards, when in reality live humans are methodically at work. While the 2016 election mythologized the power of these influence-actors, such work is slow, costly, and labor-intensive. Humans must manually create and manage accounts, hand-write posts and comments, and spend countless hours reading content online to signal-boost particular narratives. However, recent advances in artificial intelligence (AI) may soon enable the automation of much of this work, massively amplifying the disruptive potential of online influence operations. This emerging threat draws its power from vulnerabilities in our society: an unaware public, an underprepared legal system, and social media companies not sufficiently concerned with their exploitability by malign actors. Addressing these vulnerabilities requires immediate attention from lawmakers to inform the public, address legal blind spots, and hold social media companies to account. Characterizing the Threat What the American public has called AI, for lack of a better term, is better thought of as a cluster of emerging technologies capable of constructing convincing false realities. In line with the terms policymakers use, we will refer to the falsified media (pictures, audio, and video) these technologies generate as “deepfakes,” though we also suggest a new term, “machine persona,” to refer to AI that mimics the behavior of live users in the service of driving narratives. Improvements in AI bots, up to this point, have mostly manifested in relatively harmless areas like customer service. But these thus far modest improvements build upon breakthroughs in speech recognition and generation that are nothing short of profound. OpenAI, a project Elon Musk founded, made headlines this year for its GPT-2, a text generation language model the organization deemed “too dangerous to release.” This framing was perhaps an exaggeration, but OpenAI’s work was impressive nonetheless. Testers gave the algorithm 40GB of seed text from links aggregated across the Internet, which it studied with the aid of a supercomputer, producing a lightweight output that a regular desktop could run. OpenAI released a toned-down version of the algorithm to the public, but the products the organization revealed of the full version were remarkable. Though OpenAI admits to taking a few tries to get a good sample, given the first line of Orwell’s 1984, “It was a bright cold day in April, and the clocks were striking thirteen,” it eventually produced a coherent opening to a near-future novel set in Seattle. With an opening line about the discovery of unicorns in the Andes, an article GPT-2 produced wouldn’t look at all out of place in a pop-science website. That is, apart from the subject. The “fake news” applications require little imagination. One study explored this exact scenario, showing that GPT-2 was able to generate foreign policy news that subjects rated on average only marginally less credible than the New York Times seed text. These developments aren’t mere science projects either, but beneficiaries of market forces. Companies have used natural language processing (NLP) and generalized text generation to automate a growing share of the customer service and information technology workforce, cutting labor costs and freeing skilled labor from menial tasks. Advances in text generation have greatly benefited journalism in particular, driving media companies to invest in generating ever more believable content. However, NLP is just one facet of the AI revolution. Advancements in image recognition and generation can now produce faces that are almost entirely indistinguishable from those of real humans. Intelligence organizations have already used this technology to solicit unwitting contacts through social media. The same underlying technologies have also led to a recent spike in deepfake videos, now letting anyone with at-home software blend real footage almost seamlessly with generated content. And you don’t have to take our word for it, trust former president Barack Obama. AI technology also has less flashy, but no less substantial applications in influencing what users see online. Social media platforms work by identifying trending content and boosting it into the feeds of other users. While the case varies from platform to platform, these trend algorithms tend to be a function of ‘likes,’ ‘retweets,’ or ‘upvotes’ over time, but they weight early interaction most strongly. This means that a small, concentrated burst of interaction at the birth of new content is often all that is necessary to send it trending, pushing it into the feeds of thousands of legitimate users. Understanding the Impact To appreciate the threat at the intersection of deepfakes and machine personas, consider your daily diet of online information. You probably know enough to avoid following small, suspicious accounts on Twitter or browsing links to sites of which you’ve never heard. You probably don’t accept Facebook friend requests from people you’ve never met and generally stay out of the seedier parts of Reddit. However, the Internet is an ecosystem built for virality. A disruption somewhere can have impacts almost anywhere. Imagine an influence-actor posts a deepfake video of the NYPD beating a young minority man to death in an alley. The alley in the background is real. The faces of the police doing the beating are real. The face of the man beaten to death is real, taken from a list of missing persons. The video need only be dropped in a forum somewhere for the Internet to do the rest. Machine personas can then set about controlling the dialogue, goading opposition, reinforcing extremists, and generally shaping the conversation in the most confrontational direction possible. In popular forums such as Reddit, they automatically identify and signal-boost comments about the incident that threaten violence against the police and government. Users claiming the footage is deepfaked are targeted by these same machine personas with downvotes and accusations of supporting a cover up. The omnipresent machine personas fake a public consensus and make dissenters feel they are an unwelcome minority. Posts about the incident reach the front page of Reddit where real users pick up and spread the “news” across Facebook and Twitter, reaching an audience of millions in just a few hours. As the NYPD struggles to evaluate the video and debunk it, an operative assuming the identity of a concerned NYPD officer sends a deepfake audio file to a major U.S. news publication. The file contains the supervisor of the framed officers engaging in a racial epithet-laden rant about the alleged cover-up. The deceived news organization vouches for its credibility, lending its authority to the outrage. Machine personas automatically identify and signal-boost tweets and comments advocating for protest marches. In the following days, a video surfaces on 4chan of immigrants kidnapping and sexually assaulting a young girl, though nobody in the video actually exists to undermine its authenticity. Machine personas then begin advocating on 4chan and 8chan for acts of revenge against the planned protesters, who the machine personas label politically responsible for advocating pro-immigration policies. Whether a malicious group would engage in such an overtly provocative act or merely patiently stoke the same resentments is debatable. The danger is that these technologies exist now. Though they may only be prototypes, it is ill-advised to bet against technological progress. In a world where the above scenario is possible, curating your social media contacts is insufficient to insulate yourself from the effects of malign actors. If you are American, you also may have imagined Russia behind this fictional attack, but we ask you to think more broadly. While the resources of a state actor made possible the interference into the 2016 U.S. presidential campaign, AI technologies could put this power into the hands of minor state or even non-state actors. In fact, nothing about this vignette couldn’t be done by a talented lone wolf lacking any intelligence footprint whatsoever. Countering the Effects There are no easy solutions to the informational challenges AI presents. Each challenge warrants a deeper discussion than we can deliver here, and many of these challenges will have consequences that will require considerable reflection. Rather than proposing solutions in a vacuum, this conversation is best framed in terms of the vulnerabilities that any solution would need to address. The first vulnerability is a lack of public awareness or skepticism towards content that users view online. A concerted effort should be made by U.S. legislators and Silicon Valley to bring public attention to AI-enabled disinformation. The June 13 congressional hearing on AI-enabled influence operations was an important first step, and it is encouraging to hear bipartisan consensus on the threat. However, awareness has limitations. The public is fortunate that deepfakes and text generators still have a somewhat identifiable “off” quality to them. Yet the technology is unlikely to plateau here. Technical solutions also exist in identifying machine-generated content. However, there is no inherent quality of “realness” to a real image that more sophisticated software couldn’t eventually recreate. In a world where malign actors can generate pixel-by-pixel accurate content in the comfort of their own basements, distinguishing fake content from a grainy cell phone video could conceivably become impossible. Everyone should already have an untrusting eye turned towards what they see online, but we ourselves can’t claim to always abide by this virtue. Still, the public should be continuously confronted with the ease with which machine personas will soon be manufacturing provocative and disgusting content. To mitigate this vulnerability, our first impulse when we see members of a different political persuasion engaging in outrageous behavior should not be to share it, but to question its veracity. The second vulnerability is a legal system with numerous blind spots that lawmakers should close. Reddit is the third-most-popular social media site on the Internet, surpassing Facebook among American Internet users. It is also shockingly vulnerable, requiring only an e-mail address to register an account. Consequently, anyone could theoretically register an unlimited number of accounts and, being careful not to stand out to system administrators, effectively control conversations on whatever topics they want. This is no hypothetical — you can pay for this service right now. (Please don’t.) It’s hard to imagine a real-life analogy, but would Americans defend the right of the local felon to march on city hall with a thousand androids masquerading as fellow citizens? This drives to the heart of an ongoing legal debate on what exactly social media is and how regulators should treat it. However, to accept the status quo is to accept that such behavior is no more serious than a terms of service violation. This unsettled status that regards social media as no more than the footprint of its company is insufficient to capture the scope and impact that users who abuse social platforms have on American society. The third vulnerability is online anonymity. While we absolutely do not advocate for de-anonymizing the Internet, it is now so influential over American society that legislators should not leave regulation to social media platforms alone. Congress should put more pressure on social media companies to ensure their users are, at a minimum, who they say they are. That said, it is still important to remember that anonymity is both a bug and a feature, and not something regulators should crush out of hand. The Internet’s capacity to act as a platform for dissidents is not something lawmakers should root out, though any attempts are likely to fail anyway. Still, wherever a microphone appears before a crowd online, there should be no question that malicious actors will seek to place themselves before it. When they can do so with anonymity, tracing the origins of deepfaked media and rooting them out becomes a nearly impossible task. There is a middle ground between anarchy and government-issued Facebook accounts. That middle ground likely involves a far better vetting process for account creation at major sites. A modified pseudonymity system is one possibility, whereby a third party cryptographically verifies an individual can hold an account, then ties the account to that identity without disclosing the name of the holder. This is also not without its faults, both technical and otherwise, not least of which is who the third party should be. Though the government is one clear answer, for a public so enamored with conspiracy theories, Americans shouldn’t expect federally managed Internet identity tracking to be popular with any demographic except federal officials. Platforms also come and go, meaning that companies and regulators would have to continuously renegotiate such a solution. The Internet is also international — forcing sites to navigate the myriad of requirements various states impose on them. However, no solution needs to be 100% effective. Nor could it, in the face of well-resourced state actors. It should only make reaching critical mass in public spaces prohibitively expensive. Every solution will be painful. Consequently, we don’t expect that regulators will take any significant steps in the directions outlined above until the effects of machine personas become undeniable. It is the responsibility of both the U.S. government and Silicon Valley to ensure that the American public is aware of this threat so that policymakers have the necessary political capital to take action. The public should also be prepared for the possibility that malign actors will put their thumbs on the scale to the benefit of one political entity over another, so they should have a united voice in rejecting anti-democratic interference. The Internet may already be past the era of speculation about the problem and in the age of persistent machine interference. To protect both it and democracy, the American public needs to begin these conversations in earnest. Capt. Frank Adkins and Capt. Shawn Hibbard are both active duty Air Force cyber officers and graduates of the U.S. Air Force Academy. They’ve worked at Cyber Command in various positions as operators, red teamers, and planners on the leading edge of the U.S. cyber mission. Capt. Adkins received his Master’s in computer science from Northeastern University with a focus in cyber vulnerability assessment, and Capt. Hibbard received his in strategic intelligence from the National Intelligence University, studying the strategic implications of next-generation supercomputing technology. The views expressed are those of the authors and do not necessarily reflect the official policy or position of the U.S. Air Force, Cyber Command, or the U.S. government. Image: Daniel Carlbom and Johnny Lindner, adapted by War on the Rocks </description>
      <pubDate>24 Mar 20 12:31 EDT</pubDate>
      <guid>https://warontherocks.com/2019/08/the-coming-automation-of-propaganda/</guid>
    </item>
    <item>
      <title>On navigating a large codebase</title>
      <link>https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/</link>
      <description>&lt;a href=&#34;https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; A while ago, I’ve been working on a very large codebase that consisted of a few million lines of code. Large systems are usually a big mess and this one was no exception. Since this is a rather common problem in software engineering, I thought the internet would be littered with stories about this topic. There is a lot of talk about software carpentry, while software maintenance is rarely debated. Either large programs are being maintained by dark matter developers or nobody thinks that writing stories about large systems are interesting enough. In the past I’ve encountered a few of those large monsters and they seem to have a lot in common. This article will try to present some problems and tricks that I am using when I have to deal with them. Hopefully this will inspire others to write similar posts and share tips from their own bag of tricks. Large codebase problems The main problem of any large codebase is the extreme complexity that stems from the fact that we live in a messy world of details that are very hard to describe and put into words. The programming languages that we are using nowadays are still too primitive for that task, and it takes a lot of lines and various layers of abstractions before we are able to convey the rules of our world to the all mighty computer 1. The following sections will present common problems which I’ve discovered during my big system adventures. A common trait of a large codebases is that at some point they become so large and bloated that one person alone is no longer capable of understanding all its pieces. It seems to me that after 100&#39;000 lines of code, the maintenance related problems start to appear as the complexity of the code simply dwarfs the capabilities of the human brain. Such large systems are commonly maintained by more than one person, but with a large group of people also come large organizational problems. Within a large group of people the number of possible communication paths between them go bananas and so it often happens that the ass no longer knows what the head is doing. This misunderstanding in turn cause them to build the wrong thing that doesn’t fit into the rest of the system. You might also know this situation under the term of “those people had no idea what they were doing, and we will do it right this time” which is quite often floating around in the latest maintenance team. That rarely happens though, because it’s likely the Tower of Babel situation all over again. Loss of knowledge Large systems are usually maintained by the ones who did not build them. Initial developers often leave the company or move up in the pecking order to work on other projects and are therefore no longer familiar with the system. Sometimes the bright minds outsourced the initial development of the project in the name of lowering the costs, just to pay tenfold in the later stages once they realize the outsourcers developed the wrong thing. Even worse is the fact that the in house developers didn’t gain the internal domain knowledge that is necessary for further maintenance of the system. This presents a big problem for the new maintainers, as they can’t just go around the company and ask the original developers about the initial design decisions. Learning this tribal knowledge usually takes a lot of time, because the code is harder to read and understand than it is to write. These days most developers seem to switch jobs every 2 to 3 years, therefore the learning process has to be constantly going on, otherwise you might end up with a large and expensive monster that nobody knows anything about 2. For most of the past large projects on which I’ve been working on, the team has usually changed by the end of the first version. Rigorously documenting every step is not the cure for this problem, because at some point all that junk will become outdated and nobody will have the time to spend a year just reading the documentation and figuring out how the pieces fit together 3. Lack of knowledge Large systems become large, because they are usually trying to solve every problem under the sun. Often the organization that is embarking on such journey does not have enough experienced employees on board to actually pull it off. Some like to say that pressure makes diamonds, but sometimes it also crushes the things that are under. It’s fine to have less experienced people working on a large system as long as they have the elders overseeing their work. In the world where senior titles are handed left and right, that is often not the case and it’s how you end up with a very fragile system that is suitable for a replacement as soon as it was built. Most of the larger projects that I was working on and were considered successes, had the core parts of the system written by experienced developers. A significant chunks were also built by greenhorns, but they were usually guided and their blast radius was limited to the less complex parts of the system. The astronauts Big projects tend to attract the data modelers and other cultists who like to get in the way of getting shit done. These architecture astronauts will endlessly discuss the finer points of their UML data models and multithreaded layers of abstraction, that will one day allow them to be the heroes of their own story by writing some well encapsulated and “SOLID” code. Why IBM sales reps don’t have children? Because all they do is sit on the bed telling their spouses how great it’s going to be. Meanwhile, the for loopers have to fight this creeping metadata bureaucracy madness on a daily basis. The tools handed down to them from the ivory tower usually don’t stand the heat of the battle, but that doesn’t bother the modelers who will try to fix the problems with more obfuscation patterns. It’s how you end with a homebrewed middleware monstrosity, because the 100 existing ones out there are obviously not up to the task of powering our little CRUD app. Documentation problems I like to keep documentation separated from the code. Who am I? A fool, with an out of sync document. The documentation of any large system is almost always outdated. The code is usually changing faster due to the endless edge cases of the system that were not being thought of early on. The discovered edge case problems are usually fixed by bolting additional functionality right on the spot. The average code change of such patch is usually quite small, but a few tweaks here and there accumulate over time until the original design no longer matches with the reality. Tweaking the code is usually simple as most people are familiar with the process. You pull the code from the version control, you make your tweaks and then you push it back. On the other hand updating the documentation is way more convoluted and usually involves the whole ceremony, because the term documentation is actually a spaghetti of Word documents, pdfs, spreadsheets, emails, wiki pages and some text files on some dude’s hard drive. The corporate world still loves to use MS Word for writing technical documents, even though it’s entirely unusable for this use case. The Word doesn’t support syntax highlighting for code snippets and you get to play the game of “moving one image for 5 pixels to the left will mess with your headings and right align all text.” It also makes it very hard to have multiple people collaborating on the same document. The version control still treats Word documents in the same way as binary blobs, which makes merging changes and fixing merge conflicts far harder than it should be. I still remember how people collaborated by working each on their own copy of the document and having a documentation officer merging all the copies together manually to avoid any merge conflicts. Fun times. If you are lucky, you might be writing documentation in plain text, but then you may have to get familiar with all kinds of weird Lovecraftian toolchains that are relying on all sorts of ancient operating system specifics in order to produce a nicer looking document. After all these years of progress, writing documentation is still an unpleasant process due to all the pain surrounding the tools that we have to deal with on a daily basis. Large projects ensure that not only is the documentation hard to write, it’s also impossible to find and read due to the sheer number of documents 4. Tackling the beast In this section I will describe my ways of tackling the problems of an unknown large codebase that I often encounter in the wild. As mentioned before, the main problem of large systems is that nobody can understand them entirely and often you will be left wondering how the damn thing even works. When you are trying to understand a specific part of a large system, it’s worth taking the time to talk to the current maintainers. They usually know it well enough to guide you through the jungle, so you can avoid the traps and get up to speed faster. Sometimes you will encounter a situation where you will just have to figure it out on your own, because nobody will have the answers to your questions. Hopefully the following sections might give you some ideas on how to tackle such situations. Read the documentation The easiest way to get familiar with a large system, is by going through its documentation and actually reading it. Large systems usually contain large swaths of outdated documentation, but even a slightly outdated document is often better than not having it at all. Ask the elders about the current state of documentation, so you don’t completely waste your time with deciphering the irrelevant documents. Either way, the documentation will only give you an overview of the system. The details behind design decisions are almost never mentioned and you will have to find another way. Check the tests When I am trying to decipher how a specific part of the system is supposed to behave, I usually check for tests. If they exist, you might want to scroll through them and hopefully you will get another piece of the puzzle. Sometimes, when I am trying to figure out how to use some obscure unknown library, I try to write some simple learning tests that are using some methods from the library. If the tests are nowhere to be found, you can try to play with the debugger and step through the actual implementation code. The common advice on the internet regarding this topic is to always write the missing tests before modifying the unknown code, but that is usually easier said than done. On a large project you often won’t really know how the software is supposed to behave, but you will still be able to deduce the problem to a specific section that has to be changed. Writing the missing tests when you touch a section of code is a good idea, but more often than not, it’s not really feasible. Limit the blast radius When you are trying to tweak the existing functionality of the system, you can probably track it down to just a few places in the code where that tweak is necessary. I usually study the code in those places until I figure out exactly which part should be modified and I ignore the rest of the system. Resist the temptation of fixing the parts that you find horrifying, because first you can’t fix it all and second you will get crushed by the complexity of the system. Mark those places down as a horrifying place to be and keep them in mind when it’s time to refactor. If you don’t know the code well enough, you might also break an otherwise working system. Sometimes obvious bugs in the code become an expected behavior that should stay that way even if it’s wrong. At some point somebody might have started to rely on the broken behavior and if you decide to “fix” the broken part, you might actually break an otherwise working system. Running the tests is a good way to ensure that your changes did not break anything, but make sure the tests are actually reliable. Far too often you will encounter unit tests with some shady mocks written by the unit test zealots who sleep well at night because they know their mocks are working. Check the version control logs All large systems will have parts where a certain design decisions will not be documented and nobody will now why they were necessary or done that way. Version control usually contains a history of commit messages which may give you some hints for understanding the reasoning behind those decisions. This is why you can find so many blog posts advertising the importance of writing good commit messages. On smaller projects or when you are working alone, a good commit messages are not going make much difference. One person can only write so much code in one day of work and you can mostly figure out the intentions just by going through the source. If all else fails, you can still rewrite a small project in a reasonable time. On the other hand, large projects are unwieldy and rewrite is normally not economically viable. Taking the time to immortalize the intents of your changes in the commit logs might save your own ass six months down the road when you won’t remember a thing about the code that you have written. Check the bug reports Sometimes, the reasons behind a certain design decisions are stored in the past bug reports. Large projects will probably have some kind of a bug tracker with various discussions surrounding the reported bug. These bug reports might be accompanied with the hash of the commit that fixes the bug so you can go deeper into the forest in search for the truth. This is a bit more annoying process than going through the commit logs, as the bug trackers are normally not integrated with your editor of choice, but sometimes it’s the only way to obtain the missing piece of the puzzle. Visualize When I am struggling to understand how the pieces of system fit together, it usually helps me to visualize things. You don’t have to create a detailed UML diagram; in fact I don’t think I have ever seen an UML diagram that wasn’t a glorious cryptographic mess. Simple boxes and arrows will do just fine in most cases. For navigating through the unfamiliar code you may also use the tools that visualize the structure of the code (like SourceTrail). If necessary, you can write your own tools for drawing such visualizations. For example, if you are trying to visualize a mesh of microservices you can write a script that will automatically generate a graph of service connections by parsing the configuration files of those services. I personally find such connection diagrams much easier to follow and understand than figuring it out through the source code alone. Commenting the code is one of the hot topics on which everybody will want to comment on. People will claim that a well written code doesn’t need comments, because its structure and naming conventions will tell you the whole story. Afterwards they will come up with a trivial hundred line example which will show you how much better the non commented code is in comparison to the nasty commented one. It’s a baloney that is perpetuated by the book sellers and consultants that no longer work in the trenches. It’s easy to preach and stick to the principles when you don’t have to shovel the dirt on the large system for years. You can rewrite any trivial code into something that doesn’t need comments. After all, most of these silly examples easily fit into your brain just by reading the source code once. The problems of non commented code only start to appear at scale, when you have a revolving door of variously skilled developers working on the same code for multiple years. In such case, no amount of cleaning your code and naming variables in this or that way will help you. A project of 10&#39;000 lines behaves completely different in comparison to the project of 100&#39;000 lines or the project of 1 million lines. Since the internal domain knowledge and the design decisions are getting lost over time, I like to make my life easier by documenting my decisions and other “trivia” that are not obvious from the code alone. A well placed comment right where the action is will save you a lot of time, because you won’t have to search through the mess of design documents which usually won’t contain the detail that you are looking for. You won’t be able to document all your design decisions just by carefully naming variables and neither will your coworkers and other clean code enthusiasts. When I am trying to add a functionality to the system, and I realize that I am in an unfamiliar hard to understand territory, I like to put a trail of comments as I read through the code. I find such marked code much easier to understand and next time I have to go through that part, I can simply rely on the guiding comments as opposed to reading and understanding the entire source again. I hear you saying: “But the comments might be outdated or misleading, how can you claim to rely on the comments when in my entire career I have never seen one helpful comment?” If that’s the case, you can use the same strategy that you use for dealing with documentation. Finders changers. Revise and update the parts that are wrong, but the real question is: “How did those comments go wrong? You do have code reviews, don’t you?” Learn to grep I often want to know where a certain variable is used and how it is used. Sometimes the developers were to smart for their own good and will come up with an ingenious solutions that will trick your IDE in believing that the code is not used anywhere. This is particularly common in Java, where you will find ridiculous solutions glued together with a bunch of xml files that are spread throughout the entire project. Finding such documents manually is pretty much a hopeless task, but with grep this is a trivial thing to do. It’s worth spending some time learning the grep or similar tools that can quickly find the files containing the relevant keywords you are looking for. Often you will want to look for a certain keyword across the entire documentation. If you are new to the project, you won’t really know which document is relevant for you. This is actually a much harder problem than you might think, as searching through non plain text files is a world of pain (see also The power of text files). Don’t give up though. Word documents are just zip folders of xml files. If you extract them into plain xml files you can easily grep through that mess of content and layout. You might get fancy and use antiword tool instead. For searching through pdf documents you can use utilities like pdfgrep. Use the IDE (C++ programmers, feel free to skip this section) Sometimes you will encounter an old and undocumented codebase with nobody around to ask on how to approach your task. If you want a first hand experience you can try to write a Jenkins plugin. Jenkins is a really flexible continuous integration software that allows you to do everything, but at the same time it also fails to do anything and requires tons of plugins for even the most basic tasks. At some point I had to manage a large build pipeline that consisted of multiple job dependencies and somehow that plugin didn’t exist, so I’ve decided to write it (see DepBuilder - dependency builder for Jenkins). After spending some time reading the provided documentation for plugin development and poring over the code, I’ve realized it’s one big undocumented mess and the only way to figure out how it works is by trial and error and “reverse engineering” the actual functionality from other plugins. In a situations like that, an IDE with a decent autocomplete might help you decipher an otherwise impenetrable codebase. Press a dot and let the editor suggest you the possible options. Far too often I see people noodling around with some half assed vim plugins, as if struggling to get the task done makes you a real developer with chest hair and everything. There are people out there who are really productive with plain Emacs and nothing else (names like Jon Blow come to my mind), but unfortunately there are very few that are at that level of skill working in this industry. I’ve spent a lot of time maintaining my dotfiles until I’ve realized I was wasting so much time on the irrelevant nerd turf wars and that espoused productivity never really came around. A modern IDE with some custom key bindings will get you there way faster. It takes time Regardless of how you tackle the problems of an unknown large system, keep in mind that large systems did not appear overnight. A lot of people spent a lot of time building them and there are hundreds of hidden edge cases bolted on top that are only there due to the problems that were discovered in production. If it takes a lot of time to build a large system, it also takes a lot of time to understand it. Discussion Hacker News Notes [Extra] You might be interested in reading the Out of the Tar Pit article (532kB pdf) that thoroughly tackles problems of large-scale software systems. As you move through the ranks in the company, the higher you are, the more powerful language you are able to wield. In the beginning you are stuck working with primitive languages in which you have to specify every single detail. For example, if you are trying to read the contents of a file you have to specify exactly how you want that file to be read; either reading the entire file at once or iteratively line by line or character by character. As you move up into the higher levels of the foodchain, at some point you gain the access to the power of spoken language. At this level you no longer have to worry about every little detail, as you can simply blurt ambiguous things (like read this file) and it’s up to the grunts below to figure out the necessary details. ↩︎ People usually don’t leave the company when everything is fine. ↩︎ This loss of knowledge situation is not tied only to the programming world, because it happens everywhere and we as the society haven’t really figured it out how to pass the knowledge through generations. For more on this discussion, you might be interested in Preventing the Collapse of Civilization (Jon Blow) talk. ↩︎ In 1980’s Tim Berners-Lee realized that the documents are hard to find at CERN, so he started imagining a system of interconnected documents that would supposedly solve this thorny problem for good. Nowadays we know this invention as the internet. Despite 40 years of improvements and the internet becoming a part of our daily life, we still face the same problems. You can talk to another person half way across the world while watching a funny cat videos, but somehow we still struggle with finding the important project documents. ↩︎ </description>
      <pubDate>13 Feb 21 23:18 EST</pubDate>
      <guid>https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/</guid>
    </item>
    <item>
      <title></title>
      <link>https://davidthorpe.dev/kick-the-shit-out-of-procrastination/</link>
      <description>&lt;a href=&#34;https://davidthorpe.dev/kick-the-shit-out-of-procrastination/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Kick The Shit Out Of Procrastination I think I’ve spent my whole career trying out the next cool productivity software or system. It almost always fails. I tried as many of these things as I could but it just falls by the wayside. Basecamp, Reminders, Todoist, Trello, Bullet Journal, Post-It Notes… Jira. I’m joking on that last one; who would voluntarily use that… Constantly checking feeds, Twitter, Hackernews, Reddit, BBC. It becomes a feedback loop where you get that hit of dopamine every once in a while when you visit a site. Holy shit a new notification on Twitter. This is the shit I live for! Before I know it, my energy is sapped. The chance of me getting any meaningful work done is as slim as that fucking guy across the road who is really into fitness and makes me feel bad about how I’m not. Anyway, I digress… It’s Not About Routines I thought for a long time my occasional forays into procrastination filled days were a fundamental problem with my mind. I read articles, saw people exclaim I must have ADHD (although I think I probably have a touch of it), I listened to podcasts and I wondered why I’m such a terrible person. Why can’t I get as much shit done as that super-productive developer I see on Twitter always knocking stuff out? Why am I feeling like I have to keep treading water just to keep my clients/boss happy? I’ve tried every fucking morning routine going and I’m still feeling like a dick. Only when we have learned about the weaknesses of our own procrastination can we kick the shit out of it. You can’t beat an opponent without knowing it intimately. We’ll get to know your procrastination intimately, and just when it begins to trust you we’ll take it out back like a loyal aged dog and pop it right in the back of the head. Step 1: Stop Using Your Phone Let’s start reducing the time you spend on screens. To do that, let’s remove anything from your phone that doesn’t provide real value to you in your real life. This should include (all may not apply to you specifically of course): Social media apps Newsfeeds Slack Email (if you can) Games Notifications If you find yourself picking up your phone to “check-in” on something. Delete it straight away. It’s not a good use of your time and is probably part of an addiction to checking things rather than something that you really need to do. Browser Woes Your smartphone probably has a browser that can help you re-access all of the above if you really wanted to. This was a problem for me. I use Screen Time to enable content restrictions that block every single website except Google (since this provides me real value for looking up local businesses etc). My wife is the only one with the 4 digit passcode that will disable it. No impulsive disabling / checking anymore. Hide The Apps On my iPhone I move every single application into a folder named “.”. I then move that onto the second screen. My phone dock has four apps: Phone, Headspace, Things and SMS: Your Phone Now Works For You You’ll find your phone should be something that now reflects that things you truly want to spend your time on. In my case, this is mostly, but not limited to: Taking photos Listening to music Listening to audiobooks Headspace meditation app Banking apps Parking apps Maps Guitar tuner What you’ve done now is to ensure that your phone is reflective of your true needs and genuine interests. You can go monk-level crazy with this stuff and I may go into detail here about what I do in a video one day. For now you get the point I guess. What Does This Have To Do With Procrastination? I want you to have as clear a mind as possible that isn’t cluttered with potential distractions from devices that can be controlled. You probably already have many other distractions in your life already, so keep the ones you can control to a bare minimum. Step 2: Block Distractions On The Computer I will keep this one short. Try your utmost best to block distracting websites on your computer using something like freedom.to or by just manually editing your /etc/hosts file (if you’re on a proper computer). This is gonna suck. It’s going to hurt and you will quickly realise and be able to recognise the feeling that led you to want to visit that attention destroying content source. One of the biggest things that can get you drawn into procrastinating is to go into a crazy website checking loop where you loop through Twitter, Hackernews, Reddit, BBC, etc in the hope for a new bit of information that probably has no real relevance to your life. If we can stop this feedback loop we can start to become more aware of when it’s happening and ultimately why it’s happening. That’s the end goal. It’s that understanding of procrastination that allows us to kick the shit out of it. That mother-fucker! Step 3: Love Your Brain I’m going to list two things here that I’ve found to have a profound impact on my ability to stay more on-task and to notice when I’m drifting. I of course recommend paying due attention to both: Sleep: When I don’t have a good amount of sleep, my brain just can’t muster the energy to focus on heavy work. This is really important and now that you’ve blocked access to time-wasting things on your phone, this should be a bit easier. Meditation: This is where the magic happens. It’s not a quick fix, but learning to meditate and sticking to it as-daily-as-possible will help you notice your mind wandering off-task and will help you associate the emotion you’ve got that caused that mind wandering. For meditation, I use the app “Headspace”. It’s truly incredible although I might be biased because the CEO has a British accent and I’m a bit a little bit patriotic 🇬🇧. I’m hoping with enough meditation I’ll be as spiritual as this guy: For sleep, I use alcohol. Just kidding. I think. Sometimes. Don’t judge me. With these three steps in place you can now start to devise your battle plan against procrastination. Step 4: Get To Work Now go about your daily working life like normal. Except this time when you instinctively go to grab your phone to check a feed or to visit a distracting website try to right there and then stop yourself and think: How do I feel? For me it seems to have been a combination of the two emotions “Fear” and “Boredom”. The latter is now getting much easier to manage since by not having access to junk food content, I have started reading more and my brain is getting more attuned to staying focused for long amounts of time. For the former, fear seems to crop up when I approach a task that is daunting or that I don’t really know enough about. Whenever I used to read about that and people recommended to “break it down to small chunks”, I’d always think “Fuck off (wo)man, you just don’t get it”. In a sense this is still a problem I face although in a lesser capacity, but I’m now aware of that. Procrastination is actually my mind trying to tell me something that I’m not attuned enough to realise in the first place. Maybe that means I may actually need to break things down a bit more. Maybe it means I need to understand the problem a bit more. At least I now have something a bit more actionable in my arsenal in order to battle this psychologically interesting phenomena. Note: I think procrastination is quite normal and I don’t think you should see this article as one of those “always being productive” things because I think they’re really damaging. We all need downtime, if you’re trying these things but still just can’t make progress, maybe you need some time off to rebuild your energy and enthusiasm for your work. What Are Your Tips? If anyone reading this has some tips of their own, I’d be really interested in hearing them since I want to focus on this topic a considerable amount through audio / video content that you can ironically consume when you should be working. Feel free to email me your thoughts on the subject since I would very much appreciate it: david@davidthorpe.dev </description>
      <pubDate>20 Apr 20 10:45 EDT</pubDate>
      <guid>https://davidthorpe.dev/kick-the-shit-out-of-procrastination/</guid>
    </item>
    <item>
      <title>The Teenagers Getting Six Figures to Leave Their High Schools for Basketball</title>
      <link>https://www.nytimes.com/2021/11/30/magazine/overtime-elite-basketball-nba.html</link>
      <description>&lt;a href=&#34;https://www.nytimes.com/2021/11/30/magazine/overtime-elite-basketball-nba.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Credit...Victor Llorente for The New York TimesThe new pro league Overtime Elite is luring young phenoms with hefty salaries, viral success and — perhaps — a better path to the N.B.A.Players for the Overtime Elite league warming up before their first game, on Oct. 29.Credit...Victor Llorente for The New York TimesNov. 30, 2021Jalen Lewis liked high school, and why not? At 6-foot-9, with a bird’s nest of hair on top, he was instantly recognizable in the hallways of Bishop O’Dowd, in Oakland, Calif. Students he had never met would call out his name on the mornings after basketball games, raising a triumphant fist or extending a palm for a hand slap. In his freshman season, 2019-20, Lewis helped his team to the brink of a state title, until the pandemic came and shut down the tournament.Beyond the basketball, Lewis also enjoyed his classes. “Obviously, I’m tall, and I can play,” he told me recently. “Everyone knew that’s why I came to the school. But I also liked showing people in class that I could answer the tough questions you wouldn’t usually see an athlete raise his hand to answer.” Lewis has a knack for math and science. In those subjects, especially, he was determined to show his classmates that he was more than a jock. “Knowing they knew I was smart made me feel good,” he said.Lewis’s mother, Tiffany Massimino, died of breast cancer when he was 2 months old. His father, Ahlee Lewis, dedicated himself to raising his son. He played him classical music and Baby Einstein videos. A recruiter for a medical-device company, he used his salary (plus a chunk of financial aid) to enroll Lewis at Bentley, one of the East Bay’s best elementary and middle schools. He shuttled him around the region for practices and games.By third grade, Lewis had expressed a desire to play in the N.B.A. Ahlee, whose own basketball career ended after three seasons at U.C. Davis, promised to help, but only if Lewis studied as hard as he played. Bishop O’Dowd had a strong academic reputation and had sent several players to the pros. It felt like an ideal fit. Last May, following his sophomore year there, ESPN’s rankings placed Lewis second nationally among the class of 2023. His success on the court and in the classroom hadn’t gone unnoticed; the list of colleges recruiting him hard included Michigan, Georgetown, Vanderbilt, Stanford and U.C.L.A. Offers from Duke, North Carolina and U.C. Berkeley seemed sure to follow.But Lewis won’t be playing basketball at any of those schools. In July, he signed a contract with Overtime Elite, a fledgling league for teenagers with N.B.A. aspirations. Instead of studying for the SAT on the last Friday in October, he was inside a new 1,200-seat arena in midtown Atlanta, where Overtime Elite is based, with eight teammates from around the United States and overseas. As rap music pulsed and video screens flashed on all four walls, he burst through a curtain of smoke. The din was disorienting. The scene was like a video game come to life.While warming up on the court, Lewis briefly scanned the seats for celebrities who had promised to be there, including the rapper 2 Chainz and the N.B.A. legend Julius Erving. Neither was in the building, but the plush couches that served as V.I.P. seating under each basket were filled with local prep and college basketball players, familiar faces from reality TV series and assorted influencers. “There was a lot going on,” Lewis would tell me later. “You didn’t know whether to be excited, or try to lock in.” Then he stepped up to take the opening jump ball. At 16, he was the youngest professional basketball player in U.S. history.ImageCredit...Victor Llorente for The New York TimesFive years ago, Dan Porter and Zack Weiner started a basketball business called Overtime. Actually, it was a content business. It used deftly packaged highlights from high school games and other amateur competitions to attract 55 million followers on social media. Then it found ways to monetize that following.As Porter and Weiner immersed themselves in the world of teenage basketball, they found themselves bewildered by the process through which the most talented adolescents became N.B.A. players. It seemed to work well enough for everyone but the athletes and their families. Weekly travel to tournaments run by the Amateur Athletic Union, the A.A.U., was subsidized by parents who often couldn’t afford it. That was followed by a year or two of these aspiring pros playing basically without pay on a college campus. And the half dozen of them who did manage to land in the N.B.A. at 19 or 20 often had little notion of how to run their own lives. That led to truncated careers, financial distress and regret about lost opportunities. “I’ve seen a lot of talented kids who weren’t ready — physically, mentally, socially,” says Avery Johnson, the former N.B.A. and college coach, who is an Overtime Elite investor. “When they show up in the N.B.A., they don’t even know how to write a check.”Porter, 55, is a great-nephew of the economist Milton Friedman. A digital entrepreneur, he formerly ran the gaming studio that became Omgpop. Before that, he spent a decade in education, including a stint as president of Teach for America. Weiner, now 29, comes from a different generation. A three-time Ivy League chess champion at Penn, he was barely past graduation when he and Porter started Overtime. The idea of creating an alternate pathway to the N.B.A. appealed to their vision of themselves as disruptive outsiders. It also, not incidentally, promised to be another lucrative business.The ongoing rupture of amateur basketball’s traditional order has played out quite publicly. On July 1, following a Supreme Court decision, the N.C.A.A. finally allowed its athletes to be remunerated for the use of their names, images and likenesses. Still, a vast majority of them end up earning only the basic contours of an education, even as sponsors, television networks and sneaker companies reap profits from the multibillion-dollar business the sport has become. But the dysfunction starts earlier: Games held between individual high schools, once the centerpiece of teenage competition, have become almost irrelevant. College recruiters prefer the A.A.U. tournaments, where they appraise hundreds of prospects in a weekend. A.A.U. teams, organized and run by entrepreneurs with varying motives who may or may not have coaching experience, crisscross America from March to October. “It’s totally unhealthy,” Ahlee Lewis says.Amid the signs that the system was starting to unravel, Porter and Weiner saw an opportunity. They weren’t the only ones. In 2017, LaVar Ball, the father of two N.B.A. guards, created the play-for-pay Junior Basketball Association, a league for disaffected high schoolers that featured eight franchises nationwide. (All of them were nicknamed the Ballers.) That folded after one season. The Professional Collegiate League, founded by a group that included a former associate athletic director at Stanford, a Cleveland lawyer and the N.B.A. veteran David West, was supposed to start play this year as a salary-earning alternative to N.C.A.A. basketball, but its debut was postponed to 2022; it will require that players be enrolled in college to participate. And because players don’t become eligible for the N.B.A.’s draft until the year after their high school class graduates — a 15-year-old rule that may be changed after the current collective bargaining agreement with the players’ union expires in 2024 — the developmental G League now accepts prospects who have finished high school but don’t want to play in college.‘They kept telling us, “You won’t be able to get the high-level players.” With every one that we were able to secure, it crushed that argument.’But Porter and Weiner have something that those leagues do not: the 1.6 billion views their content gets every month. Their new venture is a professional league for teenagers that will take the place of A.A.U., high school and college competition. When they explained the concept to Carmelo Anthony, an Overtime investor who is playing in his 19th N.B.A. season, Anthony took to it immediately. “He literally interrupted us in the middle of our pitch and finished it for us,” Weiner says. “When we started talking to other people about it, many of them said, ‘I’ve been waiting for something like this.’”Many of those people asked to buy a piece of it. Overtime is backed by the venture-capital firm Andreessen Horowitz and a roster of investors that includes Jeff Bezos, Drake, Reddit’s Alexis Ohanian and four owners of N.B.A. franchises. The most recent round of financing, in April, raised more than $80 million. Kevin Durant, Trae Young, Devin Booker and more than two dozen other current pros have joined Anthony in signing on. For its first season, the league has grouped 27 players, ranging in age from 16 to 20, into three teams of nine. They compete against one another and against high school and international teams that agree to play them. In the coming years, the league hopes to grow to six or eight teams that will face opponents from the G League, the best college programs and — “you never know,” Porter says — eventually the Knicks and Lakers.Overtime Elite’s coaching staff is run by Kevin Ollie, who coached UConn to a national championship in 2014. The players are given personalized nutrition plans and training programs. They are marketed across Overtime’s social media network. (So far, sponsors include Gatorade and State Farm, which signed multiyear, eight-figure contracts with the league. Topps has a licensing deal.) And in the most obviously radical departure, each player gets a small share of the company and earns a salary of at least $100,000 annually, plus bonuses, depending on the contract he has negotiated. Jalen Lewis and some others make more than $500,000. (“There is a marketplace,” says Aaron Ryan, a former N.B.A. executive who has been hired as the league’s commissioner, “and players have varied value.”) In return, they have agreed to forgo their remaining years of high school and any chance of playing in college. That means no state titles or prom dates, no strolls on leafy campuses, no March Madness or Final Four. They also allow Overtime to use their names, images and likenesses, the same assets that college athletes have just earned the right to monetize for themselves, though the Overtime Elite players are permitted to strike their own deals with sponsors in noncompetitive categories.To ease the transition to N.B.A. life, Overtime Elite requires its players to spend as much as 20 hours a week in an academic setting, a mash-up of online classes, face-to-face instruction and guest lectures. Players are taught how to give news conferences and use social media. They learn how agents and sponsors operate. They also take basketball-focused versions of conventional subjects, math and history and English, so they will have fulfilled the necessary requirements if they ever want to apply to college. If basketball doesn’t work out, Overtime Elite promises to pay $100,000 toward a degree to any player who wants to get one.But if someone never reaches the N.B.A., will losing the opportunity to play in high school and college have been worth a few sure years of substantial income? When I put the question to Porter, he dismissed it. He described the connections made with Overtime Elite’s sponsors, investors and affiliated celebrities as yet another form of compensation, as if a shooting guard who turns out to be a step too slow could simply go to work for Drake instead. “We’re a family,” he insists. “We’re not going to forget about these guys.” If an Overtime Elite alum is struggling at some point in the future, Porter promised to volunteer his own services. “He can call me,” he says. “I’ll help him find a job.”ImageCredit...Victor Llorente for The New York TimesOne afternoon in September, a rented black van pulled up at Core4, a basketball facility in northeast Atlanta. This was where the Overtime Elite teams were practicing while their arena near downtown was being finished. Overtime staff members held up cameras and smartphones to record the players as they stepped off the bus. Once on the court, the players stretched. A few jogged in place. Then they split into six groups and started shooting. The cameras and smartphones roamed among them, capturing bits of dialogue and game play.Overtime’s videographers are charged with collecting footage for use on various platforms. Some of it, the attention-worthy dunks and no-look passes, will be sent out as clips on Instagram, TikTok and Twitter. Other interactions, including conversations among players and motivational speeches by Ollie, and the footage of classes and down time that offers a glimpse into the players’ daily routines, will show up in documentary-style pieces on its YouTube channel. If players go shopping for sneakers, a crew is likely to come along. If they’re relaxing in the living room of their apartments, watching a movie or playing Xbox, someone might stop in and record that too. Though players are told they will not be filmed without their consent, part of the bonus they get at the end of the season is based on their willingness to participate in the content generation.During practice, two of Overtime’s social media producers sat with their laptops open, organizing the material that was coming in. Occasionally, they posted an image accompanied by a comment in the vernacular of their target audience, a 13-to-35 demographic. One recent example: “Yo real talk T JASS been having that thing on a STRING,” referring to a former prep basketball player, now 21, who became an Instagram celebrity with videos of trick shots. “It’s not that young people aren’t sports fans,” Weiner says. “It’s that they don’t want to necessarily consume sports in the way that is traditional. It’s not always about the final score of the game. Or even about who won or who lost.”Beginning in 2016, Overtime started building its following by recording highlights of entertaining plays in high school and A.A.U. games. It paid $25 for someone to stand on the baseline in an Overtime T-shirt and hold up an iPhone. Every alley-oop or windmill dunk was uploaded to its servers with the press of a button. When Zion Williamson, who played at a small private school in Spartanburg, S.C., and for the South Carolina Hornets A.A.U. team, emerged as the next great prep standout, Overtime was just getting started. The company sent three videographers to each of his games. “Every time Zion dunked, we’d get three different views on our server,” Weiner says. “We’d look at them and post the best one.”By the end of his high school career, Williamson had dunked enough to get a scholarship to Duke, where he spent one season before leaving for the New Orleans Pelicans. Overtime, meanwhile, had created a stealth empire. Whenever Porter ran into an executive from another media company, he got the same question: “How much live sports are you showing?” The answer was invariably confounding: Overtime Elite wasn’t showing any live sports at all. “Our competitors would have crushed us years ago if they actually understood what we were doing,” Porter says now.In effect, Overtime Elite is Zion Williamson writ large, an entire roster of players highlight-reeling their way into the public consciousness, or at least Overtime’s delineated segment of it. But this time, Overtime’s access to these players is virtually unlimited. And because it owns the entire, vertically integrated property, so is the company’s ability to make money from it. When an Overtime Elite player drove to the basket during a scrimmage during the practice session at Core4, then went up for what looked like a layup before suddenly flipping a pass to a teammate in the corner, videographers were there to record not just the move but also the astonished reaction of Lewis, who was sitting out the practice session with an injury. Paying the athletes entices them to sign up, but it also mitigates any guilt Porter might have about profiting from their personal narratives. “We’re going to create media around it,” he says, referring to the league. “Why should it be controversial to pay them? It would be controversial to not pay them. That’s called the N.C.A.A.”Advertising is the easiest way for Overtime Elite to generate revenue. There are plenty of others. The Overtime website, which does a $10 million business selling hoodies, iridescent basketballs, jewelry and other merchandise, has added Overtime Elite apparel. A Jalen Lewis trading card, from a set issued by Topps just a few weeks ago, is listed for $1,200 on the secondary market. Next, why not Overtime Elite workout videos? Or a new Gatorade flavor?“We already have the audience, we already have the brand, we already have many of the relationships,” Weiner says. “So we can go to a company like Gatorade and charge them millions of dollars in Year 1.” When Overtime Elite was unveiled last March, a little more than a year after the Junior Basketball Association sank under the weight of its debts, much of the skepticism concerned whether it could have the economic wherewithal to survive. With the first wave of sponsorships in October, the league announced that it had become self-sufficient into the foreseeable future.ImageCredit...Victor Llorente for The New York TimesFor its blueprint to work, Overtime Elite needs players. And not just any 17-year-olds with smooth moves and silky jump shots. Its targets must have a reasonable enough expectation of reaching the N.B.A. to consider skipping college. They need to be regarded highly enough by recruiting analysts that Overtime’s followers will embrace them as the descendants of Zion.The job of filling the rosters was assigned to Brandon Williams, who played briefly in the N.B.A. before moving into executive roles with the Philadelphia 76ers and Sacramento Kings. Williams had an elite education — Phillips Exeter Academy, Davidson College, law school. He also had grass-roots basketball connections. Still, creating an entire league from a standing start, even one with just three teams, presented a formidable challenge. The six-figure salaries helped entice some families. So did the involvement of Durant, Drake and Bezos. But Williams’s best argument, he felt, was that players who considered themselves headed toward the N.B.A. weren’t improving those prospects by competing against markedly inferior talent. “I’m playing against a guy who is going to be a milkman; I’m playing against a guy who is going to work at U.P.S. — but I’m not playing against a pro,” is how he describes that perspective.Williams also appreciated that many parents were unsettled by the A.A.U. experience, which appeared to be optimized for the convenience of recruiters, not the physical and emotional health of the players. “They’d say things like, ‘It seems weird that my kid played in the 9 p.m. game on Friday, and now he has a 9 a.m. game on Saturday,’” Williams says. “We told them: ‘We’ll have our own building. And in that building, we’ll have great coaches. In fact, here are their résumés.’ And they’d say, ‘I recognize that name — national champion.’ And you start to stack up the offering.”Williams hired a staff of scouts to go to A.A.U. tournaments and find potential recruits. “I couldn’t spend a lot of time talking to the irrationals, the person who really fought against this whole idea,” he says. “I wasn’t trying to be a salesman — ‘I’m better than Duke.’ What I wanted was to find parents who were saying, ‘I’m spending so much of my day doing this, I’ve spent so much money and I’m not even sure of the results.’”In May, Overtime Elite signed its first two players, twins from Florida named Matt and Ryan Bewley. ESPN ranked Matt third and Ryan 12th among players in the graduating class of 2023. Getting them made national news. Another set of twins, the Thompsons, probably helped even more within the A.A.U. subculture. Ausar and Amen Thompson grew up in Oakland, not far from Lewis. They relocated to Florida’s Pine Crest Academy before eighth grade so they could play high school basketball a year early. Like Lewis, they were excellent students, dabbling in coding and taking Advanced Placement classes. To ESPN and the other high-profile websites, they were afterthoughts. But as last spring’s A.A.U. season progressed, they developed into cult favorites. “Some people told us they might be the best players in the entire class,” Williams says. The Thompsons signed at the end of May. That prompted Lewis, among others, to take notice. “They kept telling us, ‘You won’t be able to get the high-level players,’” Williams says. “With every one that we were able to secure, it crushed that argument. And then the kids started talking to each other.”Lewis was the biggest target. Not only was he among the best prep players; he was also an ideal protagonist for the stories the company was trying to create. “He’s a good-looking kid,” Williams says. “Articulate. Courted by everyone. Recognized by everyone. Single dad, so there’s an interesting story.” The scouting staff set out to get to Ahlee and make its pitch. “In this case, the dad was at least receptive,” Williams says. “He was asking very deliberate and very advanced questions.”Ahlee learned all he could about the project. He called Aaron Goodwin, a longtime friend who is a successful agent, and found Goodwin to be enthusiastic. Only then did he approach his son. Lewis had heard stories about his father’s college career. But he’d been “a crazy Warrior fan” since he was 8. The way he saw it, he and his peers were trying to get to the N.B.A. so they could get paid to play basketball. “If you could start getting paid early, and get more work than anyone else, and work with people who were already in the N.B.A., that’s the full package,” he says. At Bishop O’Dowd, and even with his A.A.U. team, he was a 6-foot-9 center, playing with his back to the basket. That made sense, because Lewis could dominate smaller players. He would get the ball, roll to the hoop and score. But if he made it to the N.B.A., it would most likely be as a small forward, playing without handling the ball much, shooting from the corner when he did. Posting up in the foul lane wasn’t going to refine those skills.Ahlee had help from Goodwin, who represented both Durant and LeBron James early in their careers. “The negotiations were not easy,” Williams says. “They knew the value of what Jalen was giving up. Being at home, going to homecoming, maybe going to Cal or U.C.L.A.” The salary was one variable, but Williams asked what he could do to give them a sense of other opportunities. “What lever can we pull?” he said. Was it a meeting with Drake? Access to other investors?In the end, money turned out to be secondary. For 16 years, Ahlee had been trying to orchestrate every aspect of Lewis’s progress while simultaneously earning enough to support them both. Not only was he exhausted; he also wondered if he was making smart decisions. “How do I make sure my son eats right?” he says. “How do I make sure he gets proper rest? How do I drive him all over the Bay Area, so he gets the extra work he needs to get better? With Overtime Elite, so much of that stuff was under one roof. And that was just the basketball part. They also made the academic part relevant. That made me want to turn cartwheels.”On July 9, Lewis announced he was leaving high school to play for Overtime Elite. “The moment we got Jalen, it opened up conversations not just with players but with entities,” Williams says. “Nothing boutique, nothing nuanced, just the stud. Jalen Lewis comes in, and he’s recognized on the national level, the U.S.A. Basketball level. It got easier from there.”ImageCredit...Victor Llorente for The New York TimesI was curious to see what the academic part of Overtime Elite looked like, so I stopped into some classes one morning. A few players were giving presentations, reading scripts off an iPad. They had chosen topics and written speeches. One lobbied for the merits of iPhones, as opposed to Androids. Another warned against recreational drugs. Lewis spoke persuasively about the health benefits of alkaline water.Some of the athletes, like Lewis, are advanced beyond their grade level. Others consider the idea of not studying for exams one of Overtime Elite’s significant benefits. I wondered how it was possible to teach them all in the same classroom.Last February, Overtime Elite hired Maisha Riddlesprigger, who had been a principal in Washington, to solve that problem. In 2010, working under Michelle Rhee, the chancellor of the District of Columbia’s public schools, Riddlesprigger deconstructed and then rebuilt a low-performing elementary school. Later, she did the same in Anacostia, one of the city’s poorest neighborhoods. That makeover involved extensive use of online learning in rotation with a traditional curriculum, a combination that hadn’t often been used in the area. “And then, when the pandemic came, everyone did it,” Riddlesprigger says.For Overtime Elite, she hired facilitators versed in math, English, science and social studies. Then she found an online program flexible enough to integrate sports into its curriculum. That way, history can be taught through the lens of athlete activism, from the 1968 Olympic protests to the Milwaukee Bucks’ refusal to play their N.B.A. game following the shooting of Jacob Blake in Kenosha, Wis. Math might involve free-throw percentages or tracking the parabola of a three-pointer. That would appear to leave out vast areas of knowledge. “But some of our more academically challenged students, when you couch the traditional system in a subject they’re interested in, they apply that interest,” Riddlesprigger says.Each student’s class load depends on the status of his transcripts. Those who have fallen behind grade level take extra classes so they can get on track to graduate — which in this case means earning a degree accredited by a private nonprofit organization, Cognia, that exists for such circumstances. Others might only need two or three classes. Within each subject, the level of the work is tailored to the individual. In May, after the games end, Overtime Elite plans to hold some sort of ceremony for its 12th graders. It all sounded like a reasonable facsimile of high school, except for the parts of high school you actually remember years later.Removing teenagers from a traditional high school experience is only one way that Overtime Elite has caused consternation. Tommy Sheppard, the general manager of the Washington Wizards, said that when he initially heard about the league, it struck him as “somewhere between Amway and a Ponzi scheme.” College coaches competing with Overtime Elite for talent use the rapid demise of the Junior Basketball Association as a cautionary tale; at least one of the Baller players who sacrificed his eligibility claims to have received only a $1,000 payment.ImageCredit...Victor Llorente for The New York TimesLeonard Hamilton, the head coach of the Florida State basketball team, had been courting six or seven of the players who ended up signing with Overtime Elite. He didn’t want to be perceived as dismissive of the league merely because it is new, but the math concerned him. There are only so many spots across N.B.A. rosters. “Making the N.B.A. is extremely hard,” he said. “How many of these kids are really going to get there?” Hamilton also put in a plug for the current system, which enabled him to get a basketball scholarship to the University of Tennessee at Martin in 1969. “Academics has meant a lot to people in America who look like me,” he said. “It changed the whole culture of my family. I don’t have a crystal ball — I can’t see the future. I don’t know the end of the story. But there are 6,000 kids playing Division I basketball every year, and only about 30 kids have a chance to end up in the N.B.A. With that in mind, those others aren’t doing too badly.”Rodney Rice, a guard from DeMatha Catholic, in the Washington suburbs, who recently committed to play at Virginia Tech, was one of Overtime Elite’s initial targets. By remaining in high school, Rice’s chances of making the N.B.A. perhaps declined by a few percentage points. “But at DeMatha,” his coach, Pete Strickland, told me, “he’s going to be told to tuck in his shirt in the hallway. To be in class on time. By teachers who don’t know if our ball is stuffed or blown up. That’s how we grow up. When you mature as a kid, you mature as a player. Those things are connected.”After academics and lunch, the players returned to the van for the ride to practice. They arrived home at 6 p.m., having been out all day. Their apartments, which are paid for by Overtime Elite, include four bedrooms, a kitchen and a living room. One bedroom is kept empty for storage. The players eat dinners prepared in conjunction with the health and performance team — extra-large portions of, say, grilled chicken with pasta, broccoli, a dinner roll, blackberry cobbler — that are stacked on a table in the hallway.One unoccupied suite has been designated for use as a social center. On an evening when I was there, Lewis wandered in. A minute later, Amen Thompson showed up to see who might be around. Soon they were immersed in a game of table tennis. The points were long and intense, and startlingly athletic. When I told Weiner about it later, he used it as an example of yet another potential revenue stream. “What if we set up a Ping-Pong tournament with the players and charged $1 to see it on TikTok or YouTube?” he said.With the score 19-18, and Thompson 2 points from winning, Lewis ranged far to his right and sent a resounding slam across the table. The dinners were piling up in the hallway, but the winning margin had to be 2 points, so I figured they might be there awhile. Instead, Thompson won the next 2 points, the last by magically parrying what appeared to be a sure winner with a flip of his paddle. When it ended, both players were sweating. They bumped fists. It was a perfect moment for social media, but for once there wasn’t a camera in sight.In late October, on what was called Pro Day by Overtime Elite, representatives of N.B.A. teams were invited to visit the facility. That pro scouts would see the players was a major component of the league’s pitch. “How is that not a massive advantage,” Weiner said to me, “if the company you want to work for gives you feedback in real time?” Except that until the scouts actually showed up, nobody knew for sure that they would. The number of talented players involved made Overtime Elite intriguing, but the league was new: an addition to an annual schedule that, for most scouts, had been in place for years.When the doors to the practice court opened at 9:30, scouts from 29 of the 30 N.B.A. teams were there. (Only the Portland Trail Blazers hadn’t sent anyone.) Not surprisingly, the event as staged by a media company had a far different feel than the stripped-down showcases the scouts were accustomed to attending. “To pull up and see that new facility shining bright like a diamond — we were all blown away,” Ryan Hoover, the vice president of global scouting for the Milwaukee Bucks, said.Over the course of the four-hour session, the stock of some prospects rose. Others’ fell. But the judgments didn’t need to be conclusive. N.B.A. rules stipulate that scouts can attend only a limited number of high school and A.A.U. games annually, but Overtime Elite is a professional league. That meant the scouts could return whenever they wanted. Tommy Sheppard told me that the possibility of seeing so many prospective pros in one place would pull scouts for the Wizards away from games around the region. “Most college games, there’s only one or two prospects, to be honest,” he said. “The name Overtime Elite — I mean, not even every N.B.A. player is truly elite, so I don’t know about that. But that Pro Day convinced us that there’s definitely a lot of talent. We’ll be following these kids.”ImageCredit...Victor Llorente for The New York TimesThe following Friday, the Overtime Elite teams started playing games with an opening-night tripleheader. Each faced an opponent that had flown in for the weekend. Lewis’s team was matched against Vertical Academy, which everyone called Team Mikey. It had been created as a showcase for Mikey Williams, a solidly built, 6-foot-2 point guard who has become the most famous prep basketball player in America. Williams had been heavily recruited by Overtime Elite. Instead, he moved from San Ysidro High School in San Diego to North Carolina, where his father and uncle had established a relationship with Lake Norman Christian School outside Charlotte. Vertical’s players attend classes at Lake Norman Christian, but they compete as an independent team.In July, Williams became the first high school athlete to sign with a major sports management firm. Two days before the Overtime Elite game, he announced that he had agreed to a sneaker deal with Puma. By then, he had amassed 3.3 million Instagram followers. He had made millions of dollars. And because he wasn’t getting paid directly for basketball, he would still be eligible to play in college.It seems logical that Overtime Elite’s players may eventually be able to do the same. If their contracts are restructured so that they’re playing basketball unpaid but selling Overtime Elite the same name, image and likeness rights that college players now control, the N.C.A.A. might be persuaded to amend its rules. Those who find that unlikely should consider that many of the Overtime Elite players will have huge followings by the time their classes graduate. Would the sponsors that underwrite March Madness prefer that they play in college at that point, or somewhere else?Each of the three Overtime Elite teams will soon have its own name and logo. Until then, they are differentiated by the names of their coaches. Lewis’s team is named for Dave Leitao, who won the A.C.C.’s coach of the year award while at Virginia. The atmosphere before its game with Vertical Academy was intentionally raucous. “You walk in, there’s cameras everywhere, it’s loud, you’re walking through the smoke,” says Abdul Beyah Jr., a Vertical Academy guard. “It took time to adjust.” Lewis needed time, too. He missed his first seven shots. At halftime, Team Leitao had a 39-37 lead. Lewis had scored a single basket. Watching from the stands, Ahlee was philosophical. “This is like a show,” he said. “The boys are thinking performance rather than basketball.”When he came out to warm up for the second half, Lewis caught his father’s eye. Then he scored 16 points in the third quarter, ending it with a fadeaway jumper from well beyond the 3-point arc. He was hit as he shot, and the force of the contact sent him sliding backward past midcourt. He made the foul shot for a 4-pointer. After three quarters, Team Leitao had a 17-point lead.A scripted reality show couldn’t have been more dramatic than the way the game played out. Vertical Academy rallied. Late in the fourth quarter, Williams banked home a drive and hit a foul shot. With seconds left, his team led by 3. Then Overtime Elite’s Bryce Griggs sank a long 3-pointer at the buzzer. The cameras positioned around the court had recorded the shot from various angles, and all those Overtime employees jumped into action. By the time Team Leitao won in overtime, helped by another thrilling 3-pointer, the highlights had been viewed by thousands of fans. By Sunday, the number of views across all of Overtime’s accounts approached four million. “OTE vs MIKEY was one of the best games I’ve ever seen omgggg” was the caption on @ote’s soundtrack-backed TikTok post.Overtime Elite versus Mikey may prove to be foundational in the annals of guerrilla basketball history. It was surely the most visible game ever played outside the purview of a major network — or any network. It validated Overtime Elite’s credibility. As for Williams, his 4-for-21 shooting meant little in a virtual universe that prioritizes three-second highlights, like his baseline drive in the final minute. Some portion of Overtime’s 55 million followers had caught a glimpse of his artistry, which could only enhance his reputation. His team had lost, but Williams didn’t seem too troubled by the outcome. Sitting on the training table in one of the spacious locker rooms, he couldn’t hide a smile.ImageCredit...Victor Llorente for The New York TimesBruce Schoenfeld is a frequent contributor to the magazine. He last wrote about the Big Ten’s football season in 2020. Victor Llorente is a portrait and documentary photographer based in Queens who was born and raised in Spain. He was selected in The 30: New and Emerging Photographers to Watch in 2020.</description>
      <pubDate>30 Nov 21 22:21 EST</pubDate>
      <guid>https://www.nytimes.com/2021/11/30/magazine/overtime-elite-basketball-nba.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://medium.com/better-marketing/10-skills-to-becoming-a-millionaire-in-5-years-or-less-e16b8b20500c</link>
      <description>&lt;a href=&#34;https://medium.com/better-marketing/10-skills-to-becoming-a-millionaire-in-5-years-or-less-e16b8b20500c&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;HomeNotificationsListsStoriesWriteError410The author deleted this Medium story.</description>
      <pubDate>16 Apr 20 14:51 EDT</pubDate>
      <guid>https://medium.com/better-marketing/10-skills-to-becoming-a-millionaire-in-5-years-or-less-e16b8b20500c</guid>
    </item>
    <item>
      <title></title>
      <link>https://arxiv.org/abs/2102.06171</link>
      <description>&lt;a href=&#34;https://arxiv.org/abs/2102.06171&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Title:High-Performance Large-Scale Image Recognition Without Normalization Download PDF Abstract: Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. Our code is available at this https URL deepmind-research/tree/master/nfnets Submission history From: Andrew Brock [view email] [v1] Thu, 11 Feb 2021 18:23:20 UTC (241 KB)</description>
      <pubDate>14 Feb 21 12:44 EST</pubDate>
      <guid>https://arxiv.org/abs/2102.06171</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.thenewatlantis.com/publications/the-case-against-stem</link>
      <description>&lt;a href=&#34;https://www.thenewatlantis.com/publications/the-case-against-stem&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Among the more influential truisms about science today is that it is essential for technological — and thus economic — progress. It is fitting, then, that the apparent slowing of American innovation has fueled a debate about the importance of science and the need for the federal government to support it. Indeed, there is growing interest across the political spectrum in revitalizing American innovation, raising questions about how best to allocate scarce resources. What kinds of research should we support? Who should decide — government or industry or the scientific community? Should we emphasize science or technology? Should we steer research toward solving practical problems or simply leave science free to pursue its own aims? When asking these questions, we typically take for granted that scientific research is necessary for innovation. But while it may be a truism today, this contention is in fact a modern one, best known from the writings of Francis Bacon. And it rests on an important claim about — and, too often, a misunderstanding of — the relationship between science and technology. Bacon was among the first thinkers to argue that scientific knowledge should not be pursued as an end in itself but rather as a means to an end — the improvement of the human condition. Science, in other words, is essentially useful, especially by enabling the technological mastery of nature. Such Baconian rhetoric is so familiar to us today that it likely passes unnoticed. Scientists have long invoked the practical fruits of their trade, especially when seeking public recognition or funding for science. As historian of science Peter Dear wrote in The Intelligibility of Nature (2006): The authority of science in the modern world rests to a considerable extent upon the idea that science is powerful; that science can do things. Television sets, or nuclear explosions, can act as icons of science because it is taken for granted that they somehow legitimately represent what science really is. In this way, what Dear calls the “instrumentality” of science “stands for the whole of science.” Science, we might say, gets absorbed into technology. When thinking about the technological fruits that we expect from science, we are now all Baconians. A critical examination of this inheritance, then, may help illuminate today’s debates about science, technology, and innovation. As we shall see, Bacon’s contention that scientific knowledge is useful — even essential — for technological innovation was ultimately vindicated by history. But the story of how this came to be is more complicated than we typically assume. Even though science and technology have developed into overlapping and mutually reinforcing fields, they were and remain distinct. The paradox — what I call the Baconian paradox — is that as science becomes more useful for technology (and vice versa), technology tends to overshadow science. As a result, we fail to recognize any meaningful distinction between the two. But this is dangerous for both enterprises. When science is constrained by technological usefulness, scientific knowledge pursued for its own sake falls by the wayside; but besides being intrinsically valuable, this type of knowledge often bears technological fruit. If we want both science and technology to flourish, we may well need to temper our Baconian rhetoric — to promote science as an end in itself, rather than as a means to technological innovation only — precisely because science has become so useful, just as Bacon predicted. What Is Science For? The year 2020 marked the four hundredth anniversary of Francis Bacon’s New Organon, in which he sought to reform science by modeling it on the mechanical arts — what today we would call technology. Bacon’s 1620 treatise argued that science, like the mechanical arts, should produce “useful works.” In contrast to ancient and medieval science, which sought knowledge for its own sake, science — or natural philosophy, as it was then called — should “be judged of by its fruits, and pronounced frivolous if it be barren.” Science is not to be pursued for the sake of wisdom, in other words, but — as Bacon famously put it in the The Advancement of Learning — “for the glory of the Creator and the relief of man’s estate.” This reformation of knowledge, which Bacon calls the “Great Instauration,” would be accomplished with the help of a new kind of logic, a new “organon.” Organon, meaning “tool” or “instrument” in ancient Greek, is the name conventionally given to Aristotle’s collection of writings on logic. But the difference between the old organon and the new is often misunderstood. One popular misconception is that Bacon proposed induction, reasoning from particulars to universals (say, from observations to natural laws) whereas Aristotle and his medieval followers practiced only deduction, reasoning from universals to particulars (for example, from general concepts to observations of particular instances of those concepts). This mistake is closely related to another canard: that pre-moderns did not make any observations, instead relying entirely on syllogistic reasoning. Bacon himself was under no such illusions: He wanted to replace Aristotle’s method of induction with a new and improved one. The problem with Aristotelian induction, Bacon charges, is that it moves too blithely from sensory observations to universal statements — with these universals then serving as premises in syllogistic arguments. Human reason, Bacon warns, has a natural tendency to fly off into abstraction, and so, unless weighted down by experience, it will fail to extend our knowledge, even while creating the illusion of doing so. Like spiders, says Bacon in a memorable passage, the scholastic philosophers of the Middle Ages, following Aristotle, spin intricate webs in the air out of their own thoughts, leaving behind experience and producing only vacuous and irresolvable disputations. These rationalists, as Bacon called them, are not the only victims of his ire. He also excoriated those who, he believed, relied blindly on empirical experience, such as the practical craftsmen and natural historians of his day. Like ants, they pile up stores from sensory observations without understanding them. This “Empirical school” thus winds up giving “birth to dogmas more deformed and monstrous than the Sophistical or Rational school.” This point is often overlooked in cartoonish portrayals of Baconian induction as the mere accumulation of facts. Rather than just relying on plain observation, Bacon “sought … to provide helps for the sense — substitutes to supply its failures, rectifications to correct its errors … by experiments” (emphasis added). Experiments would not only tether the intellect to experience; they would also discipline the senses. With this mediating role for experiment, Bacon believed that he had “established forever a true and lawful marriage between the empirical and the rational faculty.” Unlike the ant that merely collects details (the empirical), and also unlike the spider that spins webs in the air (the rational), the scientist should be like the bee, which “gathers its material from the flowers of the garden and of the field, but transforms and digests it by a power of its own.” It’s worth spelling out further how this vision of science departs from classical, Aristotelian notions. Contrary to some caricatures, the ancients and medievals did make observations — the slogan “there is nothing in the mind that was not first in the senses” was an Aristotelian principle revered by the scholastics. Some even performed experiments, notably Archimedes in third-century-b.c. Greece and Roger Bacon in thirteenth-century England. But the prevailing view was that we obtain knowledge of nature by observing things in their natural settings. Bacon, by contrast, proposed examining “nature under constraint and vexed; that is to say, when by art and the hand of man she is forced out of her natural state, and squeezed and moulded.” Nature could be forced to give up her secrets, but only if pressed with instruments and artificial techniques. Possessed with such knowledge, the natural philosopher would gain not disinterested knowledge but power. In allying knowledge with power, Bacon did not, unlike many of his followers, seek to abolish the distinction between science and technology — between natural philosophy and mechanical art. He certainly wanted natural philosophy to bear fruit and thus to contribute to improving the human condition. But such utilitarian benefits were the result of knowledge gained through the experimental study of nature. As historian of science Paolo Rossi notes, the practical results of science for Bacon were not “artefacts” — particular tools or inventions like the printing press — but rather knowledge of nature’s underlying causes. Equipped with such knowledge, natural philosophers would have the power to harness nature for their own purposes; scientific knowledge would enable technological power. In this sense, Bacon declared, “those twin objects, human knowledge and human power, do really meet in one.” Bacon recognized that such knowledge could be of great value to the state. Accordingly, he endeavored (unsuccessfully) to secure political support for his grand proposal for a new science. In his utopian story New Atlantis [from which this journal takes its name –Ed.], he envisioned a large-scale, organized research enterprise, overseen by scientists but supported by public funds and geared toward the betterment of society. It would take over three centuries for something resembling this vision to become reality — perhaps nowhere more clearly than in the American effort to establish scientific and technological preeminence during and after World War II. Bacon himself made no real scientific contributions, although he did perform some of his own experiments. In one, intended to study the effect of snow on a dead chicken, he reportedly fell ill and died of pneumonia. But while Bacon was primarily a statesman, jurist, and philosopher — rather than a practicing scientist — he is undoubtedly among the most important theorists of modern science, which is why he is often referred to as the “father of modern science.” Scholars continue to debate the extent of Bacon’s influence on the actual practice of science in the centuries after his death, for example how well his logic of induction captures the methods of modern science. But there is no dispute that Bacon’s understanding of science — especially his proposal that it “be judged of by its fruits” — continues to influence our hopes and fears about the nature of science and its place in society. The Great Conflation Bacon’s idea that science is about gaining power rather than acquiring wisdom has attracted critics of modernity no less than its cheerleaders. Writing in 1944, Theodor Adorno and Max Horkheimer singled out Bacon as representative of the Enlightenment, which sought to conform everything to “the standard of calculability and utility.” In Baconian science, “what human beings seek to learn from nature is how to use it to dominate wholly both it and human beings.” This is not unlike magic, which, although it is “bloody untruth,” is similarly “concerned with ends,” with domination. Thus did Adorno and Horkheimer unknowingly echo C. S. Lewis, who, only a year earlier in The Abolition of Man, had pointed to Bacon when arguing that “the serious magical endeavour and the serious scientific endeavour are twins…. born of the same impulse.” The “true object” of Baconian science, as of magic, is “to extend Man’s power to the performance of all things possible.” Less dramatic, though no less Baconian, is the popular association of the word “science” with such patently technological enterprises as the building of computers, robots, and advanced weapons, or the manufacturing of medicines. As Peter Dear puts it: “The popular image of a scientist is of someone in a white coat who inventssomething — a vaccine, a satellite, or a bomb.” Science, in this sense, is “a form of engineering, whether that engineering be mechanical, genetic, computational, or any other sort of practical intervention in the world.” And so we get the prevalent terms that simply lump science and technology together — “science &amp; technology” (S&amp;T), “science and innovation,” “research &amp; development” (R&amp;D), or that great linguistic barbarism “science, technology, engineering, and mathematics” (STEM). All this is more than semantic sloppiness; it affects public policy. Thus we hear proposals to boost federal R&amp;D spending that pay lip service to science but focus almost exclusively on technology — whether “tech hubs” that aim to imitate Silicon Valley or fashionable areas of applied research such as artificial intelligence, robotics, quantum computing, data analytics, and alternative energy technologies. Similarly, politicians and educators promote STEM, with a particular focus on “coding.” Or they want to maintain America’s competitive advantage in “science and technology,” especially by bridging the “STEM talent gap.” And during the Covid-19 pandemic, politicians and pundits have routinely characterized the efforts of private corporations to invent a vaccine by talking of a hope that “science” will come to our rescue. One likely explanation for this ready conflation of science and technology is simply sociological. This talk of “science” unceremoniously bags together epidemiology, virology, and clinical medicine; atmospheric physics, marine biology, and organic chemistry; civil engineering, industrial design, and materials science, to name only a few. Most policymakers have no firsthand acquaintance with the inner workings of these various professions and divergent practices, and so do not recognize them as distinct fields of inquiry. Here we have, perhaps, a contemporary version of C. P. Snow’s famous “two cultures,” with an important difference: Where the technical illiteracy of humanists fueled dismissive condescension toward the practitioners of science, technology, and industry, the technical illiteracy of our media and political elites often fuels uncritical admiration. Yet behind this possible sociological explanation lies something more fundamental: Science has indeed become deeply intertwined with technological development; it is even indispensable to it — just as Bacon envisioned four centuries ago. But the story of how this relationship came to be — and what it means for us today — is not the one that Baconian rhetoric, past or present, would have us believe. The Baconian Story To tell the story of this relationship between science and technology, it will be helpful first to consider an influential version of it — what I will call the Baconian story. It goes something like this. In ancient times, knowledge was thought to be sharply distinct from utility, just as natural philosophy was from manual labor. “In Greek thought as a whole,” writes historian of technology David E. Nye in Technology Matters, “work with the hands was decidedly inferior to philosophical speculation.” This opposition between knowledge and use reflects a pre-modern and pre-scientific view of knowledge, as well as a class structure, dependent on slavery, that associated utility with the “servile arts.” As political scientist Donald E. Stokes puts it in Pasteur’s Quadrant, “severing inquiry from use was strongly reinforced in Greek civilization by the consignment of the practical arts to people of lesser station — and manual labor increasingly to slaves.” In contrast to the practical arts, ancient science aimed at knowledge for its own sake and was therefore “useless.” As such, it was the privilege of the elite, for “only free men,” Peter Dear writes, “such as the citizens of the city-state, had the leisure to devote their time to philosophizing, while practical abilities were the province of servants and slaves.” The ancient edifice built on these ideas remained in place through the Greco-Roman period and into the Middle Ages. Cracks began to appear during the Renaissance, and the structure would crumble under the weight of the Scientific Revolution in the seventeenth and eighteenth centuries. Francis Bacon had introduced the idea of a science aimed toward practical use. No longer the privilege of a class that disdains practicality, science, at Bacon’s direction, was pressed into service for utilitarian ends. At the same time, those very ends came to be of growing interest to the upper classes. The story culminates in the Industrial Revolution. In an influential work on that period, English economic historian T. S. Ashton credited modern science with the technological innovations that drove industrialization in England. More recently — and more plausibly, if not entirely convincingly — another economic historian, Joel Mokyr, credits Bacon for promoting an experimental culture of “useful knowledge” that was conducive to innovation. Science, through innovation, thus becomes the engine of economic growth. At this stage the ancient divide between knowledge and use collapsed, and so too did the pre-modern class structure on which it depended. Rather than a privilege made possible by the labor of others, science, through technology, becomes the creator of wealth. If natural philosophy is thereby brought down from the lofty pedestal upon which the Greeks had placed it, technology now comes to occupy a privileged place as the foundation of economic progress. According to the Baconian story, these historical changes mean that any hard and fast distinction between science and technology is archaic, resting on an outmoded and elitist opposition between knowledge and use. It is now misleading, perhaps even meaningless, to speak of “science” versus “technology,” of “discovery” versus “invention,” of “basic” research versus “applied” research or “development.” Some versions of the story, following Bruno Latour, do away with the distinction altogether: there is only technoscience. What we have is more or less useful knowledge, offering us increasingly effective techniques with which to manipulate nature. We see the main thrust of this story implicit in some of the most influential critiques of modern science and technology. Consider Martin Heidegger’s famous 1954 essay “The Question Concerning Technology,” where he describes how modern technology transforms nature into a “standing-reserve” to be exploited. The development of this technological framework was made possible by “the rise of modern physics as an exact science.” Note that Heidegger does not simply lump the scientific and industrial revolutions together, as in the crudest version of the Baconian story. On the contrary, he points out that “mathematical science arose almost two centuries before [modern] technology.” “How, then,” he asks, could modern science “have already been … placed in [technology’s] service?” His answer: Modern mathematical science presents nature to us as a “calculable coherence of forces” and a “storehouse of the standing energy reserve.” In so doing, science “prepares the way” — like a “herald” — for modern technological exploitation. Similarly, Herbert Marcuse — an early student of Heidegger associated with Marxism and the Frankfurt School — writes in One-Dimensional Man that “the principles of modern science were a priori structured in such a way that they could serve as conceptual instruments for a universe of self-propelling, productive control.” His point is not that science when applied technologically leads to domination, but rather that “science, by virtue of its own method and concepts, has projected and promoted a universe in which the domination of nature has remained linked to the domination of man.” Today, our own techno-pessimists follow Heidegger and Marcuse in blaming modern science for the technological exploitation of nature and humanity. We can also hear echoes of the Baconian story in the prognostications of those techno-optimists who, at their most unrestrained, hope for a posthuman future in which science and technology have allowed us to transcend all limits imposed by nature. Both modernity’s champions and its judges accept the moral of the Baconian story: that the essence of modern science lies in its technological power. Liberal vs. Servile Arts However influential, this Baconian story is wrong in several crucial respects. Start with the opposition between knowledge and use allegedly promoted by the ancients and medievals. It is true that Plato and Aristotle distinguished between science (episteme) and technology or craft (techne) as well as between theory (theoria) and practice (praxis). And they certainly promoted the pursuit of knowledge for its own sake. But Plato also criticized the pre-Socratic natural philosophers for reducing philosophy to the mere study of nature. The Socratic innovation was to orient philosophy toward something much less theoretical and more practical: the moral and political question of how to lead a flourishing life. Aristotle followed his teacher in this respect, taking practical wisdom to be the highest virtue, and devoted considerable attention to the practical sciences of ethics, politics, and rhetoric. As for the Greek words episteme and techne, these do not track well our own distinction between “science” and “technology.” Any comparison between ancient and modern ideas on these subjects is more complicated than the Baconian story suggests. Plato often uses episteme and techne interchangeably, and even thinks of techne as a paradigmatic form of rational knowledge. In the dialogue Ion, for instance, Socrates concludes that his interlocutor, who is a kind of poetic performer, does not possess a techne because he is unable to explain the nature of his craft. Tellingly, Socrates contrasts Ion’s lack of “art and knowledge” not with the philosopher or scientist but with the carpenter, the charioteer, the fisherman, as well as the doctor and the arithmetician. In some places, Aristotle appears to draw a sharper distinction between episteme and techne. But, as Alasdair MacIntyre points out, Aristotle in his Metaphysics also takes the master-craftsman to be “the model of the person with sophia,” or wisdom. (Among other things, the master-craftsman knows what is the right thing to do in the right situation.) And, far from seeing episteme and techne as mutually exclusive, in the Nicomachean Ethics Aristotle characterizes the pursuit of wisdom as a science that is a master-craft. Thus while Aristotle, like Plato, takes the pursuit of knowledge for its own sake to be nobler than any other pursuit, that is because the object of this particular craft is wisdom — and wisdom is the precondition for human flourishing. Similarly, Aristotle believed the theoretical sciences were “higher” than the practical sciences. But this ranking is based on what these sciencesstudy, not on whether they are practical or require the use of one’s hands. As he put it in On the Soul, a science can be said to be “more honourable and precious than another” either because of its “greater exactness” or because of the “higher dignity and greater wonderfulness” of its “objects.” This is why physics and mathematics are higher than practical sciences like politics, which are less exact; but metaphysics is higher still, because it studies not particular beings like plants and stars, but being as such. Our modern distinctions between knowledge and use, or between science and technology, simply fail to capture these intricate webs of meaning. There is no question that both Plato and Aristotle had disparaging things to say about those who had to work for a living. Aristotle writes that shepherds are the “laziest” of all men, and infamously argued that slavery was not contrary to nature. But we should not interpret such claims, morally repugnant and factually unfounded though they are, through the lens of our own notions about knowledge and use. Aristotle does not denigrate the use of one’s hands so much as the condition of people who must concern themselves with survival — with the “servile arts,” such as cooking, blacksmithing, or shepherding. By contrast, he writes in Politics, “those who are in a position which places them above toil,” which is to say, those who have slaves to attend to the necessities of life for them, “occupy themselves with philosophy or with politics.” What makes the servile arts inferior is that they are directed entirely toward instrumental goods rather than being ends in themselves. Thus Aristotle also says — perhaps counterintuitively for us moderns — that the inventors of those “arts” directed to “recreation” are “naturally always regarded as wiser” than the inventors of arts “directed to the necessities of life.” The Middle Ages inherited many of these ancient ideas — and in this era too, the relationship between knowledge and use is more complicated than the Baconian story suggests. For instance, the curricula of medieval universities were structured around the seven “liberal arts”: the trivium (grammar, logic, and rhetoric) and the quadrivium (arithmetic, geometry, music, and astronomy). All seven were considered crafts — artes in Latin, which is just a translation of the Greek techne. To be sure, these liberal arts were separate from the servile arts, but not because the servile arts were associated with practicality so much as with professional vocation. The liberal arts, by contrast, were those crafts that could be practiced only by those who were free of the need to pursue such vocations. (Modern universities have enshrined this conception in the separation of colleges of liberal arts from those of engineering and other professions.) Notably, while the medievals retained the classical concept of servility, the society in which they lived was not dependent on slavery in the same way as ancient Greece and Rome. Medieval thinkers like Thomas Aquinas agreed with Plato and Aristotle that a life devoted to the pursuit of knowledge for its own sake was a privilege, in the sense that it required wealth, time, and leisure — a point that is no less true today. But at the height of the Middle Ages, the socioeconomic system that afforded a small minority of men the freedom to devote themselves to such study was one in which slavery had been greatly reduced, and at least in England all but abolished. Though the agrarian society of the Middle Ages depended on a feudal hierarchy, many peasants were not serfs, but had rights (if often limited) to property, tools, and the surplus of what they produced for their lords. And skilled artisans — masons, weavers, blacksmiths — formed a growing middle class bound together in craft guilds that both transmitted artisans’ skills and protected their economic interests. The contrast in the socio-economic standing between ancient and medieval craftsmen may be brought out by an infamous line from Aristotle’s Politics: “If … the shuttle would weave … without a hand to guide [it], chief workmen would not want servants, nor masters slaves.” At first glance, this appears to be a prophetic statement about automation and emancipation. It is all the more striking when one considers that the nineteenth-century Luddites — who violently protested automation by smashing the machines that threatened their way of life — were weavers. And yet, the Luddites were not servants or slaves, but rather skilled artisans who were members of a craft guild — a medieval institution that had flourished up through the Renaissance and until the Industrial Revolution. Aristotle could more readily imagine the automation of mechanical art than he could a society organized in such a way that its artisans were not slaves. But it was precisely such a society that confronted — and, as we shall see, may have even helped to facilitate — the era of industrialization. Technology’s Breath of Life The Baconian story oversimplifies the ancient and medieval conception of science to the point of falsehood. But there is no question that the new science marked a significant departure from classical and medieval natural philosophy, nor that science and technology became increasingly intertwined. We should not overlook Bacon’s role in these historical changes. The Baconian story should be taken seriously — but not therefore literally. Really, it should not even be taken literally as an interpretation of Bacon. He believed that science needed to reform itself precisely to become more like the mechanical arts, which were “continually thriving and growing, as having in them a breath of life.” In other words, Bacon recognized that progress in technology had been taking place quite independently of science. Indeed, many of history’s impressive technological achievements — the water wheel, the mechanical clock, the mariner’s compass, eyeglasses, gunpowder and many others — long predated modern science. Thanks to figures such as Archimedes and Hero of Alexandria, the Hellenistic period could boast inventions such as the water screw, the early steam engine, and all manner of mechanical devices and weaponry. The Romans, who did little to advance science, made numerous technological improvements — for example in plumbing, construction (notably cement), and hydropower. Similarly, the Middle Ages, despite today’s prejudices, were not a dark age for innovation. “In technology, at least,” writes historian Lynn Townsend White, “the Dark Ages mark a steady and uninterrupted advance over the Roman period.” What is known as the Renaissance of the twelfth century — that is, not the more familiar, later Renaissance — saw countless technical improvements, including in windmills, manufacturing, and architecture. White enthuses: The chief glory of the later Middle Ages was not its cathedrals or its epics or its scholasticism: it was the building for the first time in history of a complex civilization which rested not on the backs of sweating slaves or coolies but primarily on non-human power. The historian Jean Gimpel has gone so far as to refer to an “Industrial Revolution of the Middle Ages.” And finally, there is the Renaissance of the fourteenth through sixteenth centuries, which pre-dated the Scientific Revolution but saw unprecedented levels of innovation in fine and mechanical arts, with Leonardo da Vinci as perhaps the best-known exemplar. It is likely no coincidence that it was immediately following this period that Bacon set out to reform natural philosophy. Of course, technology later came to rely on scientific knowledge to a degree unimaginable to ancient, medieval, or Renaissance inventors. And science, in turn, came to rely on experiments and technology — science became instrumental — just as Bacon envisioned. But for most of history, technology advanced quite independently of science — a dynamic that remained true even during that most significant of technological transformations, the Industrial Revolution. Plain Englishmen Without Science The Industrial Revolution marks the most fundamental transformation of human life in the history of the world recorded in written documents,” wrote the historian Eric Hobsbawm. Whether or not one entirely agrees with this, one can hardly deny that the Industrial Revolution, even if measured only in terms of the quality and quantity of inventions, brooks no comparison. But was this momentous transformation due to modern science? Historians remain divided on this vexed question of causality. But one thing is clear: With a few notable exceptions, most inventions of the Industrial Revolution required little, if any, formal knowledge of the cutting-edge science of the day. And most of the inventors were not educated scientists but skilled artisans and craftsmen. As Hobsbawm puts it, for the most part the technology of the Industrial Revolution “required little scientific knowledge or technical skill beyond the scope of a practical mechanic of the early eighteenth century.” This is hardly to denigrate their achievements, nor to suggest that inventors were working haphazardly, but rather to point to two interrelated historical realities. First, most of the inventions associated with industrialization (at least in its initial phase) were not applications of scientific theory. The flying shuttle, the spinning jenny, the spinning mule, the puddling furnace — these did not require, and did not stem from, any deep knowledge of Copernican astronomy, Newtonian mechanics or optics, or even the chemistry of Boyle or Lavoisier. Whatever role scientific knowledge played, it did not directly drive industrialization. In fact, perhaps the most iconic invention of industrialization, the steam engine, is one of the clearest examples of technology driving scientific understanding rather than being driven by it. The second historical reality worth noting is that the natural sciences and the mechanical arts were at the time professionally and sociologically distinct enterprises. Many of the artisans and craftsmen were of humble birth and lacked the formal education that would have acquainted them with the science of the day. A prime example is Henry Cort, whose puddling and rolling techniques for converting pig iron to wrought iron are among the most important innovations of the Industrial Revolution. Cort was of modest pedigree and education — not much is known about his early life — leading the chemist Joseph Black to describe him as “a plain Englishman without Science.” Most of the inventors of the industrial era were not natural philosophers but engineers, clock makers, miners, and iron masters whose skills were transmitted through apprenticeship rather than university education. The historian Joel Mokyr suggests that this sociological reality may even help explain why industrialization first took hold in Great Britain, for “in an age in which dexterity and experience could still substitute for a formal training in mathematics and physics,” the country was “fortunate to possess a class of able and skilled people, larger and more effective than anywhere else.” Some historians, such as Jane Humphries, have credited the British guild system for preserving these skills that proved so instrumental to industrialization. The idea that scientific knowledge bears technological fruit, however central to the self-understanding of modern science, remained at this point more a reflection of Baconian rhetoric than of historical reality. Classical vs. Baconian Sciences If the role of science in technological change up until the nineteenth century is often greatly exaggerated, so too is the role of technology in science. Even in the Scientific Revolution, technology did not play as significant a role as is commonly believed. The modern sciences that first came to maturity during that time were those least dependent on either technology or experiment. Astronomy — arguably the science that inaugurated the Scientific Revolution — was an observational rather than an experimental science, and it still depended on naked-eye observations when Copernicus placed the Sun at the center of the cosmos. Even the physics of Galileo was more mathematical than experimental. The historian of science Alexandre Koyré has argued that Galileo likely did not carry out many of his most famous experiments, using them instead as what we might call thought experiments or rhetorical devices. This is not to deny any role for instruments in the early days of the Scientific Revolution, especially in making observations — Galileo’s telescope and pendulum come to mind — or to deny that experiments were important, for example in Newton’s research on optics. But even at their most technological, the scientific advances characteristic of the early Scientific Revolution shared with medieval and ancient science a desire to understand nature — albeit increasingly with the aid of instruments and experiments — rather than to dominate nature or furnish technologies for manipulating it. Put simply, the classical sciences with which the Scientific Revolution began were not especially Baconian. If any early modern sciences approximated the Baconian ideal, it was not the classical sciences of astronomy and physics, but the experimental traditions that laid the groundwork for the scientific understanding centuries later of such natural phenomena as magnetism, electricity, and the chemical elements. Unlike the classical sciences, these “Baconian sciences,” as historian Thomas Kuhn calls them, aimed explicitly at the experimental manipulation of nature. But they grew out of an altogether different set of practices and traditions, with roots in Hermeticism — a school of thought that flourished during the Renaissance. Hermeticism emphasized the wisdom of ancient esoteric knowledge — especially of alchemy and pharmacology — and the power of occult forces. Perhaps the most prominent Hermetic thinker of the time was the Swiss Theophrastus von Hohenheim, known as Paracelsus, whose philosophy combined Neoplatonism, Pythagoreanism, Christianity, and paganism, and straddled the line between science and magic. Although Bacon was critical of Paracelsus, he was clearly influenced by his tradition. For Baconian science, as for Hermeticism, the ideal practitioner was not the natural philosopher but, as Kuhn puts it, the “Faustian figure of the magus, concerned to manipulate and control nature, often with the aid of ingenious contrivances, instruments, and machines.” Some key figures of the Scientific Revolution were practitioners of both the classical and Baconian traditions. Newton, for instance, was as devoted to alchemy as he was to physics. Yet, the majority of these experimentalists were not natural philosophers but craftsmen and pharmacists. Their research would not be fully integrated into the scientific mainstream until much later, with the rise of modern chemistry and electromagnetism in the eighteenth and nineteenth centuries — helping to forge the bond between science and technology that has become so familiar to us today. Bacon Vindicated? The relationship between science and technology underwent a substantial transformation over the course of the nineteenth century. As we have seen, science played no direct role in the Industrial Revolution. But by 1898, William John McGee, president of the American Association for the Advancement of Science, could proclaim that “scientific progress … is so closely interwoven with industrial and social progress that the advance of one cannot be traced without constant reference to the other.” What had changed? American companies had begun to recognize the economic significance of scientific knowledge in the second half of the nineteenth century, following the lead of the German chemical industry. The Germans had pioneered the concept of industrial research — the first systematic attempt by industry to exploit science for commercial ends. American entrepreneurs soon followed suit. Thomas Edison opened a research laboratory in Menlo Park in 1876. Around the turn of the century, both General Electric and DuPont did the same, and American chemist Arthur D. Little launched an independent commercial laboratory. Academically trained physicists and chemists were in high demand and started to get hired in large numbers by the private sector. For example, Frank B. Jewett, who had received his doctorate in physics at the University of Chicago, oversaw research and development at AT&amp;T and Western Electric, which later became Bell Telephone Laboratories, where he became president. So intertwined became science and technology that when the Great Depression hit, humanists, religious figures, and ordinary citizens alike were ready to blame science. Science, it was said, not only produced wondrous technologies, like telephony, but also led to automation, unemployment, and rising levels of inequality. As William Wickenden, president of the Case School of Applied Science (now part of Case Western), put it in a 1933 Science magazine article: John Doe isn’t quite so cock-sure as he used to be that all this science is a good thing. This business of getting more bread with less sweat is all right in a way, but when it begins to destroy jobs, to produce more than folks can buy and to make your wife’s relatives dependent on you for a living, it is getting a little too thick. This new attitude was a reaction to the economic crisis but also to the excessive optimism about the social and economic benefits that many believed would come from the application of science to industry. Scientific knowledge had, for better or worse, become indispensable for technological advance — a reality that affected much more than just industry and economics. During World War I, science came to be seen as vital to the public interest for its use in refining the design of machine guns, tanks, hand grenades, submarines, and (infamously) chemical weapons. And World War II finally solidified the Baconian notion that technological advance required sustained federal support for large-scale scientific research — most prominently in the Manhattan Project. All this is to say that the Baconian story about the technological usefulness of science had finally come to reflect historical fact. So was the distinction between science and technology thereby dissolved? If yes, then the Baconian story would appear to have been wrong mainly in its chronology, not its conclusion. That is, while science and technology remained distinct through the Industrial Revolution (roughly 1750 to 1850), by the beginning of the 1900s they had become a single, uniform enterprise. If this were the case, our public discourse, and our public policies, would be right to conflate the two. But this would be a hasty conclusion, because science and technology had in fact not become a single, uniform enterprise. Although deeply interdependent and mutually reinforcing, they remained sociologically and institutionally distinct, with divergent professional identities. ‘Mirror-Image Twins’ Rather than becoming one enterprise, science and technology underwent parallel transformations during the nineteenth century. Science became both more professionalized and more specialized. Universities sought to employ scientists from the new specialties as professors, while scientists founded a growing number of professional societies and journals. Around this time, in 1834, William Whewell coined the term “scientist” — a more apt description of the professional scientist than the more classical “natural philosopher” or the patrician “man of science.” At the same time, technology underwent what historian Edwin Layton has termed the “scientific revolution in technology.” Just as scientists increasingly relied on technology, so too did engineers increasingly rely on science, and the engineering disciplines began to model themselves on the sciences. As Layton describes: The artisan was replaced in the vanguard of technological progress by a new breed of scientific practitioner. For the oral traditions passed from master to apprentice, the new technologist substituted a college education, a professional organization, and a technical literature patterned on those of science…. As a result, by the end of the 19th century, technological problems could be treated as scientific ones. And just as the word “scientist” originated in this period, so too did our current understanding of the word “technology.” The term previously referred to the study of practical and mechanical arts. As historian Eric Schatzberg explains, we owe the newer sense of “technology” — referring broadly to industrialization as well as the machinery and outputs of the industrial process — largely to the economist and social theorist Thorstein Veblen, who used it to translate the German Technik in the early 1900s. The parallel transformations of science and technology and their growing interdependence nevertheless left their institutional and professional separation largely intact. For instance, during the mid-nineteenth century, America’s leading colleges and universities began expanding their course offerings in mathematics and the natural sciences. But science and mathematics departments were (and in many respects still remain) institutionally distinct from the more practically oriented engineering departments and technical schools — such as Harvard’s Lawrence Scientific School, Yale’s Sheffield Scientific School, the Massachusetts Institute of Technology, and the new land-grant universities that tended to focus on such “applied” sciences as agriculture. “The difficulty,” writes one scholar, “was finding an educational formula that could join the progress of theoretical science with applications to the practical arts.” The professional self-understandings of scientists and engineers was similarly distinct, even opposed. The leaders of higher education who promoted science — such as Charles William Eliot at Harvard, James McCosh at Princeton, and Noah Porter at Yale — did not do so because of purported practical benefits but rather because science “ennobles and purifies the mind,” as Eliot put it. The term “pure science” came into vogue during this same time, to signify purity from what used to be called the “servile arts” — applied sciences, engineering, and industry. The American physicist Henry Rowland, in an 1883 address to the American Association for the Advancement of Science titled “A Plea for Pure Science,” took umbrage at the use of the word “science” to describe “telegraphs, electric lights, and such conveniences.” I do not wish to underrate the value of all these things: the progress of the world depends on them, and he is to be honored who cultivates them successfully. So also the cook who invents a new and palatable dish for the table benefits the world to a certain degree; yet we do not dignify him by the name of a chemist. And yet it is not an uncommon thing, especially in American newspapers, to have the applications of science confounded with pure science. Rather than fusing together two preexisting enterprises — science and technology — their parallel transformations helped to produce their ambivalent but fruitful relationship. Science and technology had not become a single, uniform enterprise; as Layton writes, they grew up as “mirror-image twins.” The Baconian Paradox Today, it has become fashionable in policy circles to accept that there is no real distinction between science and technology. Any such distinction is often associated with the much-maligned “linear model” of innovation, according to which scientists conduct their research in a kind of vacuum, disinterested in the use of their findings, while others separately apply their findings to new technologies and products. Not only does this model fail to capture the complicated relationship between science and technology, its critics argue, it also surreptitiously promotes an elitist distinction between knowledge and use — that ancient prejudice that is now outdated thanks to the triumph of Bacon’s ideas. Thus the rejection of the linear model often takes the form of a full embrace of the Baconian story, going so far as to erase altogether the line between science and technology. For instance, Daniel Stokes, in his influential Pasteur’s Quadrant, sets out to criticize the linear model but then goes on to criticize also the continued separation of knowledge and use — of basic and applied science, or of science and technology. Any institutional or professional distinctions of this sort are superficial and outdated, leading to “the perception that basic and applied science are separate ventures.” As we have seen, the linear model that strictly separates science from technology indeed does not hold up; the relationship has always been more complicated. But just because science and technology came to enjoy an intimate and dynamic relationship does not mean that they also became the same enterprise. Rather, the real distinction between the two enterprises has become harder to discern as science has grown more technologically useful. As Thomas Kuhn pointed out in 1971, technologies that are derived from scientific knowledge have become so ubiquitous that they “disguise the still real cleavage between science and technology.” And so “they make it difficult to realize how very recent and decisive the emergence of this kind of interaction has been.” Now, half a century later, technologies borne of science are even more ubiquitous, and the difference between the two is all but invisible to the public eye. We might call this the Baconian paradox: The final vindication of Bacon’s conception of the utility of science has helped to obscure how long it took for this dynamic relationship between science and technology to come off — and that there nonetheless remains a real and lasting distinction between the two. Telling the Difference How can we make sense of this distinction today, given the undeniable overlap between science and technology? The first thing to notice is that the apparent plausibility of the Baconian story is partly due to our rather narrow focus on certain areas of scientific and technological research. In some fields, such as genetic engineering, the boundary between science and technology is indeed quite murky. But few would describe, say, the building of bridges or the designing of air conditioning systems as science, even though they depend on scientific principles. Similarly, it’s hard to imagine anyone mistaking cosmology or entomology for engineering, even though they depend on technology to varying degrees. The restricted focus on a subset of fields in science and technology may be part of the reason why we find it so easy to run the two together. And this restricted focus likely stems from the prestige that some technological domains have come to enjoy in our time. As David E. Nye points out, beginning in the 1990s the word “technology” came to be synonymous with specific types of inventions, especially computers, phones, and related tools. For many of us, “technology” mainly means smartphones and automated systems — the particular products of one broad industry. Not only is the narrow use of the term strikingly different from the classical concept of techne discussed above, it fails even to capture many other types of technologies we take for granted, from automobiles to vacuum cleaners — technologies that do not (or no longer) seem to us especially scientific in nature. In fields where the difference between science and technology has become hard to tell — for examples in high-energy physics and quantum computing — we might be helped by distinguishing between the respective aims of science and technology, even if in practice they are tightly linked. As a rough approximation, we might say that science generally aims at explanation, whereas technology aims at production. A scientific theory is accepted over its rivals because the scientific community deems it a more adequate explanation (however one thinks of this), not because it issues in more useful technologies. Which interpretation of quantum physics should we accept? Is global climate change anthropogenic or not? What is the underlying chemical composition of the stars? By contrast, technologies are deemed successful, at least in part, when engineers are able to produce tools or machines — bridges, HVAC systems, cars, computers, cell phones, nuclear reactors, or whatever — that work. Electric lighting works whether or not we accept the existence of the mechanical ether (as did Maxwell’s theory of electromagnetism); automated doors (which utilize the photoelectric effect) function just fine, even though physicists have yet to agree on the proper interpretation of quantum physics. We can draw a meaningful distinction between science and technology without therefore insisting that they are fully separate: Technology can contribute to scientific knowledge, just as science uses technology to manipulate or produce certain effects. We might think of the two less as separate disciplines than as overlapping but different practices with their own professional and scholarly methods and aims. To be sure, individual scientists and engineers may not always be consciously oriented toward the differing ideals that shape these two enterprises. And exceptional practitioners often participate in and contribute to both at once. But the distinction, as we will see, remains important, even if the relationship is highly interdependent, interactive, symbiotic, and in some places well-nigh invisible. Endangering Science and Technology The risk in conflating science and technology is that this endangers both. As President Roosevelt’s science advisor, Vannevar Bush, pointed out in his famous 1945 report Science, the Endless Frontier, Under the pressure for immediate results, and unless deliberate policies are set up to guard against this, applied research invariably drives out pure. This is problematic because many technological developments depend on advances in so-called basic or pure science. The crowding out of science by technology thus threatens technology by threatening certain areas of science itself. Bush was not the only thinker to sound this alarm. The German cultural critic Friedrich Jünger, in The Failure of Technology (1946), denounced what he called the “subjugation of science”: As technology progresses, the relation between science and technology undergoes a change. Science becomes the servant of technology…. The disciplines of science become auxiliary disciplines of technology, and they fare the better the more willingly they submit to this role. “Pure science” declines because the important thing is no longer an understanding of the laws of nature, but, first of all, the application, the uses, the exploitation, of those laws. And in his classic The Technological Society (1954), the French historian and sociologist Jacques Ellul wrote that “in the twentieth century, this relationship between scientific research and technical invention resulted in the enslavement of science to technique.” Less dramatically, we might simply observe that, especially when budgets are tight, funding for research with overtly utilitarian applications tends to beat out science pursued for its own sake, even if that research might eventually bear fruits. The past several decades of federal R&amp;D policy seems to offer proof of this. Thus, federal funding of basic science, though increasing in absolute terms, has declined over the past half-century as a share of non-defense R&amp;D and as a share of GDP. The ratio is even worse when including defense research, which skews heavily toward applied science and product development. Meanwhile, private actors are even less able to and less interested in bearing the uncertain economic returns of basic science. Unsurprisingly, the vast majority of private R&amp;D budgets goes toward application. And so, as the share of R&amp;D spending funded by the federal government has dwindled, and industry has begun to fund the lion’s share of research, total national spending on applied science has dwarfed basic science. The irony of the Baconian legacy is that the more fruitful science becomes, the more it loses its own identity: Whenever science is technologically useful, the two enterprises tend to appear as “different functions performed by the same community,” as Edwin Layton has put it. But “a fundamental fact is that they constitute different communities, each with its own goals and systems of values.” Almost no society in history, Thomas Kuhn declared, “has managed successfully to nurture both at the same time.” That the United States has been able to do so — so far, at least — has surely contributed to our scientific, technological, and economic preeminence. If we wish science to continue to bear fruit for us, we may well need to refrain from judging it entirely by those fruits, and instead defend science for its own sake. </description>
      <pubDate>14 Feb 21 12:20 EST</pubDate>
      <guid>https://www.thenewatlantis.com/publications/the-case-against-stem</guid>
    </item>
    <item>
      <title></title>
      <link>https://embedded.substack.com/p/instagram-is-facebook-now</link>
      <description>&lt;a href=&#34;https://embedded.substack.com/p/instagram-is-facebook-now&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Embedded is your essential guide to what’s good on the internet, from Kate Lindsay and Nick Catucci.🧩My kingdom for a chronological Instagram—KateFor all its ups and downs, Instagram has been my most consistent social media relationship. I first started using the app in earnest my final year of college, and look back wistfully at how I artfully chronicled my life for the subsequent six years, weaving together pictures of concerts and coffee shops with mirror selfies and delicious meals into an effortless pointillist tableau of my 20s. I still post on the app occasionally, but basically left a little over a month ago in favor of a personal newsletter, where I feel less pressure to be constantly documenting my consciousness. It appears I got out just in time, because sometime last week, the app became completely unrecognizable.Instagram’s push of their TikTok-like feature Reels has been increasing in intensity ever since its launch, but even though Reels have started appearing unprompted on my feed, that’s not actually what bothers me. At least the Reels are from people who I follow, so I’m still consuming content I actually opted into. It’s a different algorithm update that’s converted my feed from “pleasing mix of content from friends and aesthetic creators” into “jarring, inconsistent grab bag of content I follow and wildly misplaced content Instagram erroneously thinks I’ll enjoy.” Here’s an example:Because I like and follow a contestant from the latest season of Love Island, Instagram decided to interrupt my feed with a picture from a UK retailer called B&amp;M (whose top trending product is currently a gin bottle shaped like a high heel). The post it recommends is a blocky, boomer-esque text meme about a man named “Jeff Snowball” that I’d like to see fact checked. It is, put plainly, ugly and out-of-place and I don’t want it on my feed. Not all of the suggested posts are that bad, but they do appear every eight or so images—not including the handful of sponsored posts that show up in between. I’m getting almost as much content that I didn’t choose to see as content I did, and that is not Instagram. That’s Facebook. Instagram and Facebook are of course both part of the same company, Meta, and so I guess it was inevitable that this day would come. As Casey Lewis of After School recently told me, “I feel like [how] when I was just out of college I had to be on Facebook because that&#39;s sort of where people&#39;s birthdays were, Instagram is sort of that for [Gen Z], where they don&#39;t really want to.” And Instagram knows this. Which is I guess why I’m confused that they’ve opted for the same strategy that made Facebook a boomer breeding ground. Mostly, it’s weird to recognize that my time on Instagram is coming to an end, and that what I’m looking at now will be an internet artifact I stumble upon years from now. Or worse, never recover at all. </description>
      <pubDate>01 Dec 21 18:51 EST</pubDate>
      <guid>https://embedded.substack.com/p/instagram-is-facebook-now</guid>
    </item>
    <item>
      <title></title>
      <link>https://blog.codinghorror.com/app-pocalypse-now/</link>
      <description>&lt;a href=&#34;https://blog.codinghorror.com/app-pocalypse-now/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I&#39;m getting pretty sick of being nagged to install your damn apps. XKCD helpfully translates: Yeah, there are smart app banners, which are marginally less annoying, but it&#39;s amazing how quickly we went from &#34;Cool! Phone apps that finally don&#39;t suck!&#34; to this sad, eye rolling, oh-great-of-course-you-have-an-app-too state of affairs. &#34;Would you like to install our free app?!?&#34; is the new &#34;It looks like you&#39;re writing a letter!&#34;— Jeff Atwood (@codinghorror) January 9, 2013 Four years, give or take a few months, if you were counting. So what happened? Millions of pointless apps Your platform now has a million apps? Amazing! Wonderful! What they don&#39;t tell you is that 99% of them are awful junk that nobody would ever want. Let&#39;s start with the basics. How do you know which apps you need? How do you get them installed? How do you keep them updated? How many apps can you reasonably keep track of on a phone? On a tablet? Just the home screen? A few screens? A dozen screens? When you have millions of apps out there, this rapidly becomes less of a &#34;slap a few icons on the page&#34; problem and more of a search problem like the greater web. My son&#39;s iPad has more than 10 pages of apps now, we don&#39;t even bother with the pretense of scrolling through pages of icons, we just go straight to search every time. The more apps out there, the more the app stores are clogged with mediocre junk, the more the overall noise level keeps going up, which leads directly to this profligate nagging. Companies keep asking how can we get people to find and install our amazing app instead of the one question they really should have asked. Why the hell are we building an app in the first place? I want to know who exactly is going to all the trouble of installing the McDonalds app on their device instead of simply visiting the McDonalds website in the browser as needed. What problem does that app solve for french fry enthusiasts that it needs to be permanently installed on your device? Why are they giving away free Big Macs just to get people to install this thing? Fragmentation into parallel and incompatible app worlds It was so much easier when iOS was totally dominant and the iPhone was the only player. Before the iPad and tablets. Before Android got decent in 4.0 and Google standardized the Play store. Now there are, at minimum, four radically different mobile platforms that every serious app player has to support: Android phone iOS phone iOS tablet Android tablet (For extra credit: how many of these are actually &#34;mobile&#34;?) Unless you&#39;re careful to build equivalent apps in all those places, it&#39;s like having multiple parallel Internets. &#34;No, sorry, it&#39;s not available on that Internet, only the iOS phone Internet.&#34; Or even worse, only on the United States iOS phone Internet. If you&#39;re feeling generous, we should technically include Windows 8 and Windows Phone in here too. All with different screen dimensions, development stacks, UI guidelines, and usage patterns. Oh and by the way, that&#39;s assuming no other players emerge as serious contenders in the computing device market. Ever. At the point where you find yourself praying for a duopoly as one of the better possible outcomes, that&#39;s … not a good sign. Paying for apps became a race to the bottom Buying an app is the modern Support Your Favorite Small Software Vendor Day. I was always fine with dropping ten or twenty bucks on software I loved. I&#39;m a software engineer by profession; apps are cheaper so I can buy even more of them. Have you ever noticed that the people complaining about apps that cost $3.99 are the same people dropping five bucks on a cup of fancy coffee without batting an eyelash? Me too, and I&#39;m with the coffee people. $3.99 for your app? Outraaageous! Now, contrast this with your app, Mr. Developer. I don’t know you from Adam. You’re pitching digital Instant Refresher Juice 1.0 to me in the form of a new app. The return I’m going to get is questionable at best. I already have 30 apps on my phone, some of them very good. Do I need another one? I don’t use the 30 I have. The experience I’m going to get from adding one more app is not trustable. I’m assured of nothing. Last week I bought an app for 99 cents and it was terrible. I used it once, for 15 seconds. I could be shoving $1 straight down the toilet again for all I know. Your app, good sir, is a total gamble. Sure, it’s only a $1 gamble… but it’s a gamble and that fact matters more than any price you might place on it. For some reason I don&#39;t completely understand, mobile app review systems are frequently of questionable value, so all you really have to go on are the screenshots and a bit of text provided by the developer. Imagine you bought your coffee, only to open the lid and find it was only half full, or that it wasn&#39;t coffee at all but lemonade. If only 1 in 5 cups of coffee you bought actually contained coffee, a $3.99 price for that coffee starts to seem unreasonably high. When you buy an app, you don&#39;t really know what you&#39;re going to get. Turns out, the precious resource here isn&#39;t the money after all. It&#39;s your time. In a world of millions of apps, free is the correct and only price for most apps except those rare few of extreme, easily demonstrable value – probably from well known brands of websites you already use daily. So hey, everything is free! Awesome! Right? Well… When apps are free, you&#39;re the product I know, I know, I&#39;m sick of this trite phrase too. But if the market is emphatically proving that free is the only sustainable model for apps, then this is the new reality we have to acknowledge. Nothing terrifies me more than an app with no moral conscience in the desperate pursuit of revenue that has full access to everything on my phone: contacts, address book, pictures, email, auth tokens, you name it. I&#39;m not excited by the prospect of installing an app on my phone these days. It&#39;s more like a vague sense of impending dread, with my finger shakily hovering over the uninstall button the whole time. All I can think is what shitty thing is this &#34;free&#34; app going to do to me so they can satisfy their investors? For the sake of argument, let&#39;s say the app is free, and the developers are ethical, so you trust that they won&#39;t do anything sketchy with the personal information on your device to make ends meet. Great! But they still have to make a living, don&#39;t they? Which means doing anything useful in the app requires buying three &#34;optional&#34; add-ons that cost $2.99 each. Or there are special fees for performing certain actions. Isn&#39;t this stuff you would want to know before installing the app? You betcha. Maybe the app is properly tagged as &#34;offering in-app purchases&#34; but the entire burden of discovering exactly what &#34;in-app purchases&#34; means, and how much the app will ultimately cost you, is placed completely on your shoulders. You, the poor, bedraggled user. The app user experience is wildly inconsistent Have you ever tried actually using the Amazon app on iOS, Android, and Windows? iOS does the best, mostly because it&#39;s been an app platform for longer than the others, but even there, the Amazon app is a frustrating morass of missing and incomplete functions from the website. Sure, maybe you don&#39;t need the full breadth of Amazon functions on your phone, though that&#39;s debatable on a tablet. But natural web conveniences like opening links in new tabs, sharing links, the back button, searching within the page, and zooming in and out are available inconsistently, if at all. The minute you begin switching between platforms – say you use an iOS tablet and an Android phone and a Windows 8 touch laptop, like I do – you&#39;ll find there are massive differences between the Amazon apps (and the eBay apps, and the Netflix apps, and the..) on these different platforms. At some point, you just get fed up with all the inconsistencies and oddities and quirks and say to hell with these apps, can I please just use the website instead? Now, if your website is an awful calcified throwback to 2003, like eBay, then the mobile apps can be a valuable opportunity to reinvent your user interface without alienating all your existing users. If there&#39;s one thing I love about tablet and phone design it&#39;s that their small screens and touch interfaces force people to think simpler. This is a good thing. But if you don&#39;t eventually take those improvements home to the mothership, you&#39;re creating two totally different and incompatible UIs for doing the same things. It seems like a fool&#39;s errand to dump millions of dollars of development time into these radically different, siloed app platforms when Amazon could have spent it improving their website and making that experience scale a bit better to every device out there. The World Wide App But that&#39;s not an option, because apparently the web is dead, and mobile apps are the future. I&#39;m doing my best to resist a sudden uncontrollable urge to use my Ledge Finder app to find the nearest ledge to jump from right now. The tablet and phone app ecosystem is slowly, painstakingly reinventing everything I hated about the computer software industry before the web blew it all up. Even fans are concerned: I’m waiting for something that will unify the world of apps and make manually going to an App Store to find a new app as weird as typing in a URL to find a new website. My bet is that this won’t be Facebook. Instead, I would not bet against some young upstart, perhaps one inspired upon reading about a $19 billion deal, to go heads-down and come up with something crazy. I&#39;ll have more to say about this soon, but I expect there to be an explosion of new computing devices all over the world in the next few decades, not a contraction. Sometimes the craziest solution is the one that&#39;s been right there in front of you the whole time. </description>
      <pubDate>24 Mar 20 12:28 EDT</pubDate>
      <guid>https://blog.codinghorror.com/app-pocalypse-now/</guid>
    </item>
    <item>
      <title>On the shoulders of the giants</title>
      <link>https://www.lpalmieri.com/posts/2020-03-08-on-the-shoulders-of-the-giants/</link>
      <description>&lt;a href=&#34;https://www.lpalmieri.com/posts/2020-03-08-on-the-shoulders-of-the-giants/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; March 08, 2020 2062 words 11 min My journey in the world of software has been quite brief. I joined the industry roughly three years ago, as a not-yet-graduated mathematician converted to ML practioner. It took me another two years to find myself in a position where building software was my main occupation. I owe loads to many awesome individuals I have met and worked with along this short walk of life. I owe loads as well to many others who I will probably never meet - the authors of the books I fed upon along the way. First-hand experience is extremely powerful, nonetheless time is finite. Books gave me a chance to tap into the compressed mastery of other practiotioners, a mastery built over thousands and thousands of hours of work. If you could only absorb 10% of that knowledge by reading it would still be a bargain. As I find myself moving from the mentee to the mentor seat in some of my professional relationships, I do happen to share more and more often lists of titles that I found useful on my own journey. Publishing this list as a public blog post will likely increase its reach and prove useful to many others. Themes: Architecture Design Testing Epic Software development lifecycle Management frameworks Architecture Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems by Martin Kleppmann This book is dense. It&#39;s packed with foundational material and it manages to combine a first-principles approach with a focus on the impact of those decisions on the engineering choices behind real-world large-scale distributed systems. I do go back to it from time to time, to re-read a chapter, re-study a set of concepts. I just love it. Cloud Native Patterns: Designing change-tolerant software by Cornelia Davis I never believed the digital native narrative, but it&#39;s indeed true that as a software engineer I am Cloud native. All the systems I have been working with have always been designed, hosted and operated in the Cloud since day 0. Cornelia Davis did a very good job at putting together a primer on the patterns and techniques that you should have in your toolbox when designing increasingly-complex Cloud native applications. I recommend it often as a first read on real-world distributed systems. Design Domain-Driven Design: Tackling Complexity in the Heart of Software by Eric Evans The book on DDD, also known as The Big Blue Book. It&#39;s long but it covers an insane amount of material: if you are in the business of writing enterprise software, it&#39;s a must-read. Translating the rules and the mental model of a complex business domain into sofware is indeed the core of the challenge when writing enterprise software. The techniques and the terminology introduced by Evans will pay dividends as the complexity of the domain you are tackling and the organisation you are working in increases. Different people have suggested me a shorter introduction to the subject, Domain-Driven Design Distilled by Vaughn Vernon, but I haven&#39;t read it first hand. Domain Modeling Made Functional by Scott Wlaschin I discovered this book looking up the author from a talk of his - I loved the talk and I loved the book. It&#39;s on its own a good introduction to DDD as well as to the broader topic of type-driven development. In a nutshell, we can leverage the type system to represent the constraints of our domain, making incorrect state difficult or impossible to represent. The book presents the idea in the context of functional programming, but it&#39;s indeed viable even with non-strictly-functional programming languages as long as they have a rich typesystem (e.g. Rust). If you find the idea interesting, the author&#39;s website is a gold mine. If you want a blog-sized introduction to the topic, check Parse, don&#39;t validate by Alexis King. Testing Test Driven Development: by Example by Kent Beck There is a general appreciation in our industry around Test Driven Development. Nonetheless, I haven&#39;t met many practioners who actually run it by the book, for all sorts of reasons. Before taking a stance on the matter I wanted to see it done religiously. Short on neighbours, I turned to the author himself: Kent Beck is the creator of XP (Xtreme Programming), one the main voices of TDD as well as one of the authours of the Agile manifesto. The book is nothing more nothing less than a long pair programming session with him, as he works his way using TDD through a problem (i.e. writing a testing framework - there is a meta element at play here). You are an observer - as a reader you don&#39;t get to play ping-pong with him; yet, it&#39;s worth reading to actually see what TDD looks and feels like. Working Effectively with Legacy Code by Michael Feathers I have come to appreciate that software is more often read than written. As it often goes, the author has generally moved on (either in another area of the business or somewhere else entirely). Yet the system keeps running in production, hopefully producing value, and it needs to be maintained. As it happens, not all of its behaviour is covered by automated tests - it&#39;s legacy code. And most engineers will spend most of their careers working on such code (that includes code they wrote themselves six or twelve months earlier). I spent a fair share of my short software career doing so already. Feathers put together a series of useful techniques to tame the beast - documenting existing behaviour using tests in order to make it possible to evolve the system itself to satisfy new requirements. Extremely useful as a reference when working on gnarly legacy beasts. xUnit Test Patterns: Refactoring Test Code by Gerard Meszaros As you approach a new project armed with the two books above, you will try to stick to a disciplined testing approach. I sure did when given the chance to start a greenfield service. After a while, our test coverage started to decrease: new code was less thoroughly tested than the code we wrote at the very beginning of the project. Did we do it on purpose? No - if you asked, the whole team would have probably re-stated their faith in the importance of testing. Nonetheless, it was happening. The truth was that our tests had started to slow us a down - it was getting cumbersome to write and maintain them as the codebase evolved. As a result, we were writing less and less tests, without acknowledging it. If we kept at it without changing direction, we would have probably joined the faction of those who see tests as a hindrance more than an asset. Instead, I found this book and a significant refactor of our test suite brought us back to our previous development speed without compromising on our testing practices. The book is a bit dated, but it&#39;s a useful reference for a bunch of important techniques to keep the development tax of your comprehensive test suite under control. It was indeed instrumental for ours. I don&#39;t suggest to read it cover to cover - it&#39;s quite repetitive and way too long. Epic The Soul of A New Machine by Tracy Kidder A novel - what is it doing here? Well, it takes quite the effort to digest the material I linked so far. Why bother? What is it that makes it worthwhile to go through all these hurdles? (Tech money aside, perfectly legit motivation) The Soul of A New Machine resonates. With the part of me that loved reading about Wiles&#39; proof of the Fermat&#39;s Last Theorem. With that part of me that is fascinated by people losing themselves in the quest to solve problems that are bigger than them, almost all-consuming. On that note, I can&#39;t avoid recommending Bryan Cantrill&#39;s review of the The Soul of A New Machine with almost the same fervor of my recommendation for the book itself. Software development lifecycle Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation by Jez Humble and David Farley A deep-dive into the world of CD, Continuous Delivery. The book is 10 years old, but it has withstood the test of time: technologies might have changed, but the principles and the challenges highlighted here are still relevant when designing (and automating) the release pipelines of contemporary systems. Accelerate: The Science of Lean Software and Devops by Nicole Fosgren, Jez Humble and Gene Kim Why should you go through all the trouble of implementing what the previous book details? Accelerate doesn&#39;t necessarily introduce a set of revolutionary metholodogies in software delivery, but it provides solid datapoints and robust research proving that some of those methodologies (Lean, DevOps, etc.) have indeed a measurable impact on the business performance of an organisation. The four key metrics are extremely useful to measure the health of an engineering team. The yearly State of DevOps report complements Accelerate and provides updates on the state of the industry. Management frameworks Turn The Ship Around! The Manager&#39;s Path An Elegant Puzzle: Systems of Engineering Management There is more to software than code. Code is often the easy bit - people are the tough nut to crack. Core skills and leadership are often neglected when describing the minimal toolset needed by a software engineer to be effective. While competency is key, it&#39;s being capable to work with others that makes it or breaks it. The jury is out on the existence of 10x engineers, but I am sure of the existence of 10x (and 0.1x!) teams. It goes beyond the individuals - it&#39;s a mix of practices, processes, vision, values. You can freestyle it as a team of 5 or 6, but it will soon spiral out of control when the organisation grows. These three books are the one I found the most interesting among the many I touched in the &#34;management&#34; section - they are principled, clear-written and insightful. They deserve a spot in an engineering curriculum as much as the fundamentals of testing and domain design. Turn The Ship Around!, in particular, is a written account of the wonders of decentralised decision-making and high mutual-trust in an environment used to a strict Command/Control management-style (the US Navy!). Empowering every single individual to channel their best version of themselves should be the goal of every (engineering) organisation. </description>
      <pubDate>23 Apr 20 10:11 EDT</pubDate>
      <guid>https://www.lpalmieri.com/posts/2020-03-08-on-the-shoulders-of-the-giants/</guid>
    </item>
    <item>
      <title>Why You Need To Start Using A Decision Journal</title>
      <link>https://blog.trello.com/decision-journal</link>
      <description>&lt;a href=&#34;https://blog.trello.com/decision-journal&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Do you remember those “choose your own adventure” books from your childhood? Where the pages were filled with different options and the choices you made advanced the plot?  Yeah, real life feels a lot like that—except, it’s never-ending and not always as thrilling. That’s because we’re faced with a lot of decisions each and every day. And I mean a lot of them.  So instead of throwing your hands up in the air, what should you do instead in the face of an endless list of decisions?  Decisions, Decisions: Are We All Burnt Out On Making Choices? Some estimates go as far as to say that you need to make upwards of 35,000 remotely conscious choices on a daily basis. Even funnier? A study out of Cornell found that you make over 200 decisions a day just related to what you’ll eat or drink. It’s no wonder that so many of us feel like our decision-making muscles are worn out. In fact, the feeling of exhaustion over needing to choose between option A and option B is so relatable that it’s even been given a name: decision fatigue.  Plenty of research demonstrates that the quality of your decisions decreases as you make more choices, simply because you get plain ol’ tired of making them. And, it seems like nobody is immune. One study of judges (who we’d all like to think of as sound decision-makers) found that they were way more likely to grant parole in the morning, compared with later in the day.  Decisions can be exhausting, but they’re also an essential part of your daily life. From the small ones (what should you eat for lunch?) to the giant ones (should you change careers?), you’re in the driver’s seat. So, how can you get better at making choices—particularly the big ones that have potentially major consequences? Should you engage in a rousing game of eenie, meenie, miney, moe? Throw a dart at a board? As it turns out, a decision journal might just be the tool you need.  So...What Is A Decision Journal?  I can’t blame you if the term “decision journal” inspires visions of writing a bunch of heartfelt “dear diary” entries in a locked notebook.  But, this concept is actually a pretty straightforward journaling exercise. In your decision journal (it can be anything from a Google Document to a cheap notebook to even a Trello board), you simply chronicle your bigger decisions and record how you felt when you made them.  As an article for Farnam Street recommends, when you’re faced with a large decision, use your journal to document the following: The choice you’ve made What you expect to happen as a result of that choice Why you expect things to pan out that way How you feel about your decision For example, imagine that you were wrestling with the choice of whether or not to apply for an internal transfer to a different department within your company. Once you’ve actually made your choice (you’re going to go for it and toss your hat into the ring!), you’d use your decision journal to jot down the nuts and bolts of that decision, your assumptions, and your emotional state when you settled on your outcome.  How A Decision Journal Can Help Declutter Your Brain A decision journal isn’t necessarily an in-the-moment tool like a decision matrix or a trusty pros and cons list.  Instead, it’s something you’ll use for reflection. By documenting and periodically reviewing the decisions you make over time, you’ll get a better grasp on your state of mind and identify things like trends or common traps you find yourself falling into.  To put it simply, a decision journal helps to refine your decision-making process as you move forward, rather than being something that helps you actually make a choice in the moment. If that seems like an unnecessary formality, I promise it’s not—because it’ll help you overcome something called the hindsight bias.  “Research shows that we selectively recall information that confirms what we know to be true and we try to create a narrative that makes sense out of the information we have,” explains an article for the Association for Psychological Science. “When this narrative is easy to generate, we interpret that to mean that the outcome must have been foreseeable.” Basically, when you stroke your ego by telling yourself that you’re a fortune teller, you create major blindspots and lose out on opportunities to improve. You need to be able to clearly see where you make mistakes and why they happen. Ultimately, that information helps you make better choices moving forward.  Maybe your decision journal will illuminate the fact that you have the tendency to make irrational choices when you’re stressed and under the gun. Knowing this, you can move through future decisions by giving yourself some space to breathe and mull things over a little more.  Or, perhaps every time you marked down that you felt wary of a decision, it turned out poorly. That’s a sign that maybe you need to start trusting your gut.  How To Make The Most Of Your Decision Journal The process of decision journaling itself is pretty cut and dried: you write down your decision, your assumptions, and your emotions. But, there are a few other tips you’ll want to keep in mind to really make the most of this practice.  1. Don’t Use It For Everything I know what you’re thinking: Journaling about your decisions is just another thing to add to your to-do list—which most of us don’t need, especially since 60% of workers reportedly feel stressed more than three work days per week.  Your decision journal shouldn’t be a burdensome activity that slows down the process of making every single decision.  Reserve it exclusively for the larger decisions that have potentially major consequences and require some serious thought and deliberation. After all, there’s no need to journal about whether you should order a turkey club or a chicken burrito for lunch that day.   2. Keep It Simple Your decision journal should be used to evaluate your more complex decisions, but that doesn’t mean that the journal itself should be complicated. Remember, you want this to be something that’s easy to refer back to and reflect on. Having pages and pages about every option you considered and every detail about your emotional state will only make this a cumbersome resource for you (meaning it’ll probably just collect dust in your desk drawer). Use simple language, short sentences, and be as straightforward as possible when documenting your decisions and emotions. That will allow you to look back and get the information you really need—without wading through paragraphs of flowery language and unnecessary details.  3. Create A Simple Template For Yourself One of the best ways to keep things simple is to create a template that you can use time and time again. It’ll prompt you to stay focused on the need-to-know information and remove a lot of the guesswork and ambiguity from the decision journaling process. Whether you want to create a templated Trello card or start a simple document that you can keep copying, make sure that your decision journal template touches on the basics. Here’s what this could look like: Date I made this decision: ___________________________ The decision I made was: ____________________________ I believe this decision will lead to: __________________ Why I believe this decision will pan out this way: _____________________________________________________ How I feel about the decision I’ve made: ______________________________________________________ See? Not so difficult after all. Of course, you’re welcome to add more to your own template if it helps you, but the point is to at least get a basic process in place. That’ll make the process of journaling less daunting—and make you way more likely to stick to it.  4. Review Your Journal Frequently A decision journal isn’t about helping you make choices in the heat of the moment. It’s a record that you can refer back to in order to understand your blind spots and make better decisions moving forward. So, that means you need to actually look back at it—and you should plan to do so frequently (aim for every quarter, at the very least). Research shows that we all tend to have an inflated view of our own performance. In one study, engineers were asked if they believed they were in the top 5% of the engineers at their company, and a whopping 40% of them said “yes.” And, even further, our own self-ratings aren’t correlated with positive performance. A separate study of physicians found that things like supervisor and peer ratings of surgical residents were fairly accurate in predicting whether or not residents would perform well on their board exams, but there was zero relationship between self-ratings and their exam success. “We apply a lot of positive spin to evidence we get about ourselves,” explains David Dunning, a Professor of Psychology at Cornell University, in an interview with NPR about both studies. “People obviously want to think pleasant things about themselves. They want to avoid thinking threatening things about themselves.” In short, we aren’t great at honestly evaluating ourselves, which means we probably won’t be able to spot our decision-making weaknesses and pitfalls on our own.  If you answer honestly and follow your prompts, your decision journal will serve as an unbiased third party that will equip you with valuable feedback—provided you make the time to lean on it frequently, of course.  When it’s time for you to review your entries, give yourself some quiet, focused time to reflect: Are there mistakes you see yourself making again and again? Are there certain types of decisions that make you feel more anxious than others? What about the types of decisions that inspire a lot of confidence? This time for self-reflection is more than just a feel-good exercise, as you’ll quickly identify areas of improvement. And, the more you do that, the better you’ll get at making choices—which will help you kick that pesky decision fatigue we mentioned earlier to the curb.  Flex Those Decision-Making Muscles And Put Pen To Paper  The “choose your own adventure” books of your childhood were fun. But, in real life, needing to make decision after decision can be draining and ultimately lead to some lackluster choices. That’s why a decision journal should be your not-so-secret weapon. It’ll give you some helpful insight into your own decision-making process, so you can improve your selections moving forward.  While it’s not designed to help you pick between that turkey sandwich or burrito, it will help you approach your larger, real-life decisions with as much certainty as you had when you were a kid choosing which page to flip to.  Good or bad, we&#39;d love to hear your thoughts. Find us on Twitter (@trello) or write in to support@trello.com  Next: Feeling Uncertain? How To Move Forward Without A Plan </description>
      <pubDate>27 Mar 20 17:16 EDT</pubDate>
      <guid>https://blog.trello.com/decision-journal</guid>
    </item>
    <item>
      <title>Uncle Bob is Ruining Software</title>
      <link>https://www.hillelwayne.com/post/10x/</link>
      <description>&lt;a href=&#34;https://www.hillelwayne.com/post/10x/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Let’s start with a couple of premises: There are 10x programmers out there, Some programming techniques are better than others. I don’t think either of these is particularly controversial. The first is pretty widely accepted and borne out by studies. And we see examples of the second everywhere: see for example all of the pissfights about static vs dynamic typing. We may not agree on which is better, but a lot of people think one of them is better, which means that a lot of people believe that some techniques are better than others. Let’s add one more premise: 10x programmers might not use the best programming techniques. This might be a little less obvious, but I think it’s a pretty straightforward argument. It’s crazy to expect anyone to always use the best tools. And it’s easy to find two software giants who argue about something; see again typing wars. Now for the crazy leap in logic: 10x programmers are bad for the industry. I like to think I’m a pretty decent programmer, but every programmer thinks that, so statistically I’m about average. If I want to improve, I’d look into using better languages and developing better techniques. How do I found out what are ‘better techniques’? I can’t look into the research because there’s almost no software engineering research out there. I could do a ton of experiments, but that takes time and runs into the blub problem. The fastest and simplest way is to look into what the community thinks are the best techniques and trust that they’re correct. And the community gets its best techniques from the people who are most successful, which are the 10x programmers. Here’s the thing, though: if the technique is some 10% detrimental, then the 10x becomes a 9x- which is still far better than the average programmer. The 10x would still be incredibly successful despite the bad technique. That means the 10x can still act as an authority on best practices, even if it’s not a best practice! We have no a priori way to determine whether a given practice is good or bad purely based on what the best programmers do, because the best programmers using poor techniques are still the best programmers. Take Robert Martin. He’s a hugely influential software engineer and has played a large role in popularizing several practices, such as TDD and SOLID. But if he instead fell in love with APL and property testing he’d likely be just as effective a programmer and just as effective in advocating for them instead. Uncle Bob is an untrustworthy authority because he’s such a good programmer! Some strawman counterarguments I can easily attack: “He’s speaking about things he knows work.” He knows what he prefers doing. Is he the best judge on whether or not that’s effective? There are a lot of cases where people think they’re better at something but actually worse. “He’s speaking from experience.” So are COBOL programmers. “Are you arguing that we shouldn’t listen to experts because they’re too good? Because that’s really dumb.” Yeah, it’s silly and riddled with holes, I just like the idea that something can be so good at programming that we shouldn’t listen to anything they say about it. So can we do instead? The only thing we can trust is hard scientific research, but that’s incredibly expensive and a lot of people think it’s impossible to study programmers. Also, we can’t trust research either. We could also invest a lot of time in our own exploration, but that’s boring. We could force 10x programmers to argue topics, which I’d like to see happen more- it isn’t great but it helps a little, and it’s entertaining to watch experts shout at each other. I don’t think the experts will be very happy doing that, though. I think the main takeaway is we should start using gotos again, because Dijsktra was an expert so we can’t trust him. </description>
      <pubDate>16 Feb 21 13:24 EST</pubDate>
      <guid>https://www.hillelwayne.com/post/10x/</guid>
    </item>
    <item>
      <title></title>
      <link>http://www.catb.org/esr/faqs/hacker-howto.html</link>
      <description>&lt;a href=&#34;http://www.catb.org/esr/faqs/hacker-howto.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;How To Become A HackerCopyright © 2001 Eric S. RaymondRevision HistoryRevision 1.5203 Jasnuary 2020esr Go makes a place as a plausible learning language, displacing Java. Revision 1.5106 October 2017esr Link to &#34;Things Every Hacker Once Knew.&#34; Mention USB-stick distros. Many updated translation links. Revision 1.5019 July 2015esr Added link to &#34;Let&#39;s Go Larval&#34;. Revision 1.4921 November 2014esr Added link to &#34;How To Learn Hacking&#34;. Revision 1.4819 June 2014esr freshmeat/freecode is dead, alas. Revision 1.4720 May 2014esr Fix up various stale links. Join a hackerspace! Revision 1.4625 Sep 2013esr Add micropatronage explanation and gittip link. Why you should not ask me for advice on how to get started. Revision 1.4512 May 2013esr Open Solaris isn&#39;t, and Unity screwed the pooch. Revision 1.4420 May 2012esr Updated the critique of Java. Revision 1.4307 Feb 2011esr Python passed Perl in popularity in 2010. Revision 1.4222 Oct 2010esr Added &#34;Historical note&#34;. Revision 1.403 Nov 2008esr Link fixes. Revision 1.3914 Aug 2008esr Link fixes. Revision 1.388 Jan 2008esr Deprecate Java as a language to learn early. Revision 1.374 Oct 2007esr Recommend Ubuntu as a Unix distro for newbies. Why This Document?As editor of the Jargon File and author of a few other well-known documents of similar nature, I often get email requests from enthusiastic network newbies asking (in effect) &#34;how can I learn to be a wizardly hacker?&#34;. Back in 1996 I noticed that there didn&#39;t seem to be any other FAQs or web documents that addressed this vital question, so I started this one. A lot of hackers now consider it definitive, and I suppose that means it is. Still, I don&#39;t claim to be the exclusive authority on this topic; if you don&#39;t like what you read here, write your own.If you are reading a snapshot of this document offline, the current version lives at http://catb.org/~esr/faqs/hacker-howto.html.Note: there is a list of Frequently Asked Questions at the end of this document. Please read these—twice—before mailing me any questions about this document.Numerous translations of this document are available: Arabic Belorussian Bulgarian Chinese, Czech. Danish Dutch Estonian French German, Greek Hungarian, Italian Hebrew, Japanese Lithuanian Norwegian, Persian Polish Portuguese (Brazilian), Romanian Spanish, Turkish, and Swedish. Note that since this document changes occasionally, they may be out of date to varying degrees. The five-dots-in-nine-squares diagram that decorates this document is called a glider. It is a simple pattern with some surprising properties in a mathematical simulation called Life that has fascinated hackers for many years. I think it makes a good visual emblem for what hackers are like — abstract, at first a bit mysterious-seeming, but a gateway to a whole world with an intricate logic of its own. Read more about the glider emblem here.If you find this document valuable, please support me on Patreon or SubscribeStar. And consider also supporting other hackers who have produced code that you use and value via Loadsharers. Lots of small but continuing donations add up quickly, and can free the people who have given you gifts of their labor to create more value.What Is a Hacker?The Jargon File contains a bunch of definitions of the term ‘hacker’, most having to do with technical adeptness and a delight in solving problems and overcoming limits. If you want to know how to become a hacker, though, only two are really relevant.There is a community, a shared culture, of expert programmers and networking wizards that traces its history back through decades to the first time-sharing minicomputers and the earliest ARPAnet experiments. The members of this culture originated the term ‘hacker’. Hackers built the Internet. Hackers made the Unix operating system what it is today. Hackers make the World Wide Web work. If you are part of this culture, if you have contributed to it and other people in it know who you are and call you a hacker, you&#39;re a hacker.The hacker mind-set is not confined to this software-hacker culture. There are people who apply the hacker attitude to other things, like electronics or music — actually, you can find it at the highest levels of any science or art. Software hackers recognize these kindred spirits elsewhere and may call them ‘hackers’ too — and some claim that the hacker nature is really independent of the particular medium the hacker works in. But in the rest of this document we will focus on the skills and attitudes of software hackers, and the traditions of the shared culture that originated the term ‘hacker’.There is another group of people who loudly call themselves hackers, but aren&#39;t. These are people (mainly adolescent males) who get a kick out of breaking into computers and phreaking the phone system. Real hackers call these people ‘crackers’ and want nothing to do with them. Real hackers mostly think crackers are lazy, irresponsible, and not very bright, and object that being able to break security doesn&#39;t make you a hacker any more than being able to hotwire cars makes you an automotive engineer. Unfortunately, many journalists and writers have been fooled into using the word ‘hacker’ to describe crackers; this irritates real hackers no end.The basic difference is this: hackers build things, crackers break them.If you want to be a hacker, keep reading. If you want to be a cracker, go read the alt.2600 newsgroup and get ready to do five to ten in the slammer after finding out you aren&#39;t as smart as you think you are. And that&#39;s all I&#39;m going to say about crackers.The Hacker AttitudeHackers solve problems and build things, and they believe in freedom and voluntary mutual help. To be accepted as a hacker, you have to behave as though you have this kind of attitude yourself. And to behave as though you have the attitude, you have to really believe the attitude.But if you think of cultivating hacker attitudes as just a way to gain acceptance in the culture, you&#39;ll miss the point. Becoming the kind of person who believes these things is important for you — for helping you learn and keeping you motivated. As with all creative arts, the most effective way to become a master is to imitate the mind-set of masters — not just intellectually but emotionally as well.Or, as the following modern Zen poem has it:     To follow the path:     look to the master,     follow the master,     walk with the master,     see through the master,     become the master. So, if you want to be a hacker, repeat the following things until you believe them:1. The world is full of fascinating problems waiting to be solved.Being a hacker is lots of fun, but it&#39;s a kind of fun that takes lots of effort. The effort takes motivation. Successful athletes get their motivation from a kind of physical delight in making their bodies perform, in pushing themselves past their own physical limits. Similarly, to be a hacker you have to get a basic thrill from solving problems, sharpening your skills, and exercising your intelligence.If you aren&#39;t the kind of person that feels this way naturally, you&#39;ll need to become one in order to make it as a hacker. Otherwise you&#39;ll find your hacking energy is sapped by distractions like sex, money, and social approval.(You also have to develop a kind of faith in your own learning capacity — a belief that even though you may not know all of what you need to solve a problem, if you tackle just a piece of it and learn from that, you&#39;ll learn enough to solve the next piece — and so on, until you&#39;re done.)2. No problem should ever have to be solved twice.Creative brains are a valuable, limited resource. They shouldn&#39;t be wasted on re-inventing the wheel when there are so many fascinating new problems waiting out there.To behave like a hacker, you have to believe that the thinking time of other hackers is precious — so much so that it&#39;s almost a moral duty for you to share information, solve problems and then give the solutions away just so other hackers can solve new problems instead of having to perpetually re-address old ones.Note, however, that &#34;No problem should ever have to be solved twice.&#34; does not imply that you have to consider all existing solutions sacred, or that there is only one right solution to any given problem. Often, we learn a lot about the problem that we didn&#39;t know before by studying the first cut at a solution. It&#39;s OK, and often necessary, to decide that we can do better. What&#39;s not OK is artificial technical, legal, or institutional barriers (like closed-source code) that prevent a good solution from being re-used and force people to re-invent wheels.(You don&#39;t have to believe that you&#39;re obligated to give all your creative product away, though the hackers that do are the ones that get most respect from other hackers. It&#39;s consistent with hacker values to sell enough of it to keep you in food and rent and computers. It&#39;s fine to use your hacking skills to support a family or even get rich, as long as you don&#39;t forget your loyalty to your art and your fellow hackers while doing it.)3. Boredom and drudgery are evil.Hackers (and creative people in general) should never be bored or have to drudge at stupid repetitive work, because when this happens it means they aren&#39;t doing what only they can do — solve new problems. This wastefulness hurts everybody. Therefore boredom and drudgery are not just unpleasant but actually evil.To behave like a hacker, you have to believe this enough to want to automate away the boring bits as much as possible, not just for yourself but for everybody else (especially other hackers).(There is one apparent exception to this. Hackers will sometimes do things that may seem repetitive or boring to an observer as a mind-clearing exercise, or in order to acquire a skill or have some particular kind of experience you can&#39;t have otherwise. But this is by choice — nobody who can think should ever be forced into a situation that bores them.)4. Freedom is good.Hackers are naturally anti-authoritarian. Anyone who can give you orders can stop you from solving whatever problem you&#39;re being fascinated by — and, given the way authoritarian minds work, will generally find some appallingly stupid reason to do so. So the authoritarian attitude has to be fought wherever you find it, lest it smother you and other hackers.(This isn&#39;t the same as fighting all authority. Children need to be guided and criminals restrained. A hacker may agree to accept some kinds of authority in order to get something he wants more than the time he spends following orders. But that&#39;s a limited, conscious bargain; the kind of personal surrender authoritarians want is not on offer.)Authoritarians thrive on censorship and secrecy. And they distrust voluntary cooperation and information-sharing — they only like ‘cooperation’ that they control. So to behave like a hacker, you have to develop an instinctive hostility to censorship, secrecy, and the use of force or deception to compel responsible adults. And you have to be willing to act on that belief.5. Attitude is no substitute for competence.To be a hacker, you have to develop some of these attitudes. But copping an attitude alone won&#39;t make you a hacker, any more than it will make you a champion athlete or a rock star. Becoming a hacker will take intelligence, practice, dedication, and hard work.Therefore, you have to learn to distrust attitude and respect competence of every kind. Hackers won&#39;t let posers waste their time, but they worship competence — especially competence at hacking, but competence at anything is valued. Competence at demanding skills that few can master is especially good, and competence at demanding skills that involve mental acuteness, craft, and concentration is best.If you revere competence, you&#39;ll enjoy developing it in yourself — the hard work and dedication will become a kind of intense play rather than drudgery. That attitude is vital to becoming a hacker.Basic Hacking SkillsThe hacker attitude is vital, but skills are even more vital. Attitude is no substitute for competence, and there&#39;s a certain basic toolkit of skills which you have to have before any hacker will dream of calling you one.This toolkit changes slowly over time as technology creates new skills and makes old ones obsolete. For example, it used to include programming in machine language, and didn&#39;t until recently involve HTML. But right now it pretty clearly includes the following:1. Learn how to program.This, of course, is the fundamental hacking skill. If you don&#39;t know any computer languages, I recommend starting with Python. It is cleanly designed, well documented, and relatively kind to beginners. Despite being a good first language, it is not just a toy; it is very powerful and flexible and well suited for large projects. I have written a more detailed evaluation of Python. Good tutorials are available at the Python web site; there&#39;s an excellent third-party one at Computer Science Circles.I used to recommend Java as a good language to learn early, but this critique has changed my mind (search for “The Pitfalls of Java as a First Programming Language” within it). A hacker cannot, as they devastatingly put it “approach problem-solving like a plumber in a hardware store”; you have to know what the components actually do. Now I think it is probably best to learn C and Lisp first, then Java.There is perhaps a more general point here. If a language does too much for you, it may be simultaneously a good tool for production and a bad one for learning. It&#39;s not only languages that have this problem; web application frameworks like RubyOnRails, CakePHP, Django may make it too easy to reach a superficial sort of understanding that will leave you without resources when you have to tackle a hard problem, or even just debug the solution to an easy one.A better alternative to Java is to learn Go. This relatively new language is pretty easy to move to from Python, and learning it give you a serious leg up on the possible next step, which is learning C. Additionally, one of the unknowns about the next few years is to what extent Go might actually displace C as a systems-programming language. There is a possible future in which that happens over much of C&#39;s traditional range.If you get into serious programming, you will eventually have to learn C, the core language of Unix. C++ is very closely related to C; if you know one, learning the other will not be difficult. Neither language is a good one to try learning as your first, however. And, actually, the more you can avoid programming in C the more productive you will be.C is very efficient, and very sparing of your machine&#39;s resources. Unfortunately, C gets that efficiency by requiring you to do a lot of low-level management of resources (like memory) by hand. All that low-level code is complex and bug-prone, and will soak up huge amounts of your time on debugging. With today&#39;s machines as powerful as they are, this is usually a bad tradeoff — it&#39;s smarter to use a language that uses the machine&#39;s time less efficiently, but your time much more efficiently. Thus, Python.Other languages of particular importance to hackers include Perl and LISP. Perl is worth learning for practical reasons; it&#39;s very widely used for active web pages and system administration, so that even if you never write Perl you should learn to read it. Many people use Perl in the way I suggest you should use Python, to avoid C programming on jobs that don&#39;t require C&#39;s machine efficiency. You will need to be able to understand their code.LISP is worth learning for a different reason — the profound enlightenment experience you will have when you finally get it. That experience will make you a better programmer for the rest of your days, even if you never actually use LISP itself a lot. (You can get some beginning experience with LISP fairly easily by writing and modifying editing modes for the Emacs text editor, or Script-Fu plugins for the GIMP.)It&#39;s best, actually, to learn all five of Python, C/C++, Perl, and LISP. Besides being the most important hacking languages, they represent very different approaches to programming, and each will educate you in valuable ways. Go is not quite to the point where it can be included among the most important hacking languages, but it seems headed for that status.But be aware that you won&#39;t reach the skill level of a hacker or even merely a programmer simply by accumulating languages — you need to learn how to think about programming problems in a general way, independent of any one language. To be a real hacker, you need to get to the point where you can learn a new language in days by relating what&#39;s in the manual to what you already know. This means you should learn several very different languages.I can&#39;t give complete instructions on how to learn to program here — it&#39;s a complex skill. But I can tell you that books and courses won&#39;t do it — many, maybe most of the best hackers are self-taught. You can learn language features — bits of knowledge — from books, but the mind-set that makes that knowledge into living skill can be learned only by practice and apprenticeship. What will do it is (a) reading code and (b) writing code.Peter Norvig, who is one of Google&#39;s top hackers and the co-author of the most widely used textbook on AI, has written an excellent essay called Teach Yourself Programming in Ten Years. His &#34;recipe for programming success&#34; is worth careful attention.Learning to program is like learning to write good natural language. The best way to do it is to read some stuff written by masters of the form, write some things yourself, read a lot more, write a little more, read a lot more, write some more ... and repeat until your writing begins to develop the kind of strength and economy you see in your models.I have had more to say about this learning process in How To Learn Hacking. It&#39;s a simple set of instructions, but not an easy one.Finding good code to read used to be hard, because there were few large programs available in source for fledgeling hackers to read and tinker with. This has changed dramatically; open-source software, programming tools, and operating systems (all built by hackers) are now widely available. Which brings me neatly to our next topic...2. Get one of the open-source Unixes and learn to use and run it.I&#39;ll assume you have a personal computer or can get access to one. (Take a moment to appreciate how much that means. The hacker culture originally evolved back when computers were so expensive that individuals could not own them.) The single most important step any newbie can take toward acquiring hacker skills is to get a copy of Linux or one of the BSD-Unixes, install it on a personal machine, and run it.Yes, there are other operating systems in the world besides Unix. But they&#39;re distributed in binary — you can&#39;t read the code, and you can&#39;t modify it. Trying to learn to hack on a Microsoft Windows machine or under any other closed-source system is like trying to learn to dance while wearing a body cast.Under Mac OS X it&#39;s possible, but only part of the system is open source — you&#39;re likely to hit a lot of walls, and you have to be careful not to develop the bad habit of depending on Apple&#39;s proprietary code. If you concentrate on the Unix under the hood you can learn some useful things.Unix is the operating system of the Internet. While you can learn to use the Internet without knowing Unix, you can&#39;t be an Internet hacker without understanding Unix. For this reason, the hacker culture today is pretty strongly Unix-centered. (This wasn&#39;t always true, and some old-time hackers still aren&#39;t happy about it, but the symbiosis between Unix and the Internet has become strong enough that even Microsoft&#39;s muscle doesn&#39;t seem able to seriously dent it.)So, bring up a Unix — I like Linux myself but there are other ways (and yes, you can run both Linux and Microsoft Windows on the same machine). Learn it. Run it. Tinker with it. Talk to the Internet with it. Read the code. Modify the code. You&#39;ll get better programming tools (including C, LISP, Python, and Perl) than any Microsoft operating system can dream of hosting, you&#39;ll have fun, and you&#39;ll soak up more knowledge than you realize you&#39;re learning until you look back on it as a master hacker.For more about learning Unix, see The Loginataka. You might also want to have a look at The Art Of Unix Programming.The blog Let&#39;s Go Larval! is a window on the learning process of a new Linux user that I think is unusually lucid and helpful. The post How I Learned Linux makes a good starting point.To get your hands on a Linux, see the Linux Online! site; you can download from there or (better idea) find a local Linux user group to help you with installation.During the first ten years of this HOWTO&#39;s life, I reported that from a new user&#39;s point of view, all Linux distributions are almost equivalent. But in 2006-2007, an actual best choice emerged: Ubuntu. While other distros have their own areas of strength, Ubuntu is far and away the most accessible to Linux newbies. Beware, though, of the hideous and nigh-unusable &#34;Unity&#34; desktop interface that Ubuntu introduced as a default a few years later; the Xubuntu or Kubuntu variants are better.You can find BSD Unix help and resources at www.bsd.org.A good way to dip your toes in the water is to boot up what Linux fans call a live CD, a distribution that runs entirely off a CD or USB stick without having to modify your hard disk. This may be slow, because CDs are slow, but it&#39;s a way to get a look at the possibilities without having to do anything drastic.I have written a primer on the basics of Unix and the Internet.I used to recommend against installing either Linux or BSD as a solo project if you&#39;re a newbie. Nowadays the installers have gotten good enough that doing it entirely on your own is possible, even for a newbie. Nevertheless, I still recommend making contact with your local Linux user&#39;s group and asking for help. It can&#39;t hurt, and may smooth the process.3. Learn how to use the World Wide Web and write HTML.Most of the things the hacker culture has built do their work out of sight, helping run factories and offices and universities without any obvious impact on how non-hackers live. The Web is the one big exception, the huge shiny hacker toy that even politicians admit has changed the world. For this reason alone (and a lot of other good ones as well) you need to learn how to work the Web.This doesn&#39;t just mean learning how to drive a browser (anyone can do that), but learning how to write HTML, the Web&#39;s markup language. If you don&#39;t know how to program, writing HTML will teach you some mental habits that will help you learn. So build a home page.But just having a home page isn&#39;t anywhere near good enough to make you a hacker. The Web is full of home pages. Most of them are pointless, zero-content sludge — very snazzy-looking sludge, mind you, but sludge all the same (for more on this see The HTML Hell Page).To be worthwhile, your page must have content — it must be interesting and/or useful to other hackers. And that brings us to the next topic...4. If you don&#39;t have functional English, learn it.As an American and native English-speaker myself, I have previously been reluctant to suggest this, lest it be taken as a sort of cultural imperialism. But several native speakers of other languages have urged me to point out that English is the working language of the hacker culture and the Internet, and that you will need to know it to function in the hacker community.Back around 1991 I learned that many hackers who have English as a second language use it in technical discussions even when they share a birth tongue; it was reported to me at the time that English has a richer technical vocabulary than any other language and is therefore simply a better tool for the job. For similar reasons, translations of technical books written in English are often unsatisfactory (when they get done at all).Linus Torvalds, a Finn, comments his code in English (it apparently never occurred to him to do otherwise). His fluency in English has been an important factor in his ability to recruit a worldwide community of developers for Linux. It&#39;s an example worth following.Being a native English-speaker does not guarantee that you have language skills good enough to function as a hacker. If your writing is semi-literate, ungrammatical, and riddled with misspellings, many hackers (including myself) will tend to ignore you. While sloppy writing does not invariably mean sloppy thinking, we&#39;ve generally found the correlation to be strong — and we have no use for sloppy thinkers. If you can&#39;t yet write competently, learn to.Status in the Hacker CultureLike most cultures without a money economy, hackerdom runs on reputation. You&#39;re trying to solve interesting problems, but how interesting they are, and whether your solutions are really good, is something that only your technical peers or superiors are normally equipped to judge.Accordingly, when you play the hacker game, you learn to keep score primarily by what other hackers think of your skill (this is why you aren&#39;t really a hacker until other hackers consistently call you one). This fact is obscured by the image of hacking as solitary work; also by a hacker-cultural taboo (gradually decaying since the late 1990s but still potent) against admitting that ego or external validation are involved in one&#39;s motivation at all.Specifically, hackerdom is what anthropologists call a gift culture. You gain status and reputation in it not by dominating other people, nor by being beautiful, nor by having things other people want, but rather by giving things away. Specifically, by giving away your time, your creativity, and the results of your skill.There are basically five kinds of things you can do to be respected by hackers:1. Write open-source softwareThe first (the most central and most traditional) is to write programs that other hackers think are fun or useful, and give the program sources away to the whole hacker culture to use.(We used to call these works “free software”, but this confused too many people who weren&#39;t sure exactly what “free” was supposed to mean. Most of us now prefer the term “open-source” software).Hackerdom&#39;s most revered demigods are people who have written large, capable programs that met a widespread need and given them away, so that now everyone uses them.But there&#39;s a bit of a fine historical point here. While hackers have always looked up to the open-source developers among them as our community&#39;s hardest core, before the mid-1990s most hackers most of the time worked on closed source. This was still true when I wrote the first version of this HOWTO in 1996; it took the mainstreaming of open-source software after 1997 to change things. Today, &#34;the hacker community&#34; and &#34;open-source developers&#34; are two descriptions for what is essentially the same culture and population — but it is worth remembering that this was not always so. (For more on this, see the section called “Historical Note: Hacking, Open Source, and Free Software”.)2. Help test and debug open-source softwareThey also serve who stand and debug open-source software. In this imperfect world, we will inevitably spend most of our software development time in the debugging phase. That&#39;s why any open-source author who&#39;s thinking will tell you that good beta-testers (who know how to describe symptoms clearly, localize problems well, can tolerate bugs in a quickie release, and are willing to apply a few simple diagnostic routines) are worth their weight in rubies. Even one of these can make the difference between a debugging phase that&#39;s a protracted, exhausting nightmare and one that&#39;s merely a salutary nuisance.If you&#39;re a newbie, try to find a program under development that you&#39;re interested in and be a good beta-tester. There&#39;s a natural progression from helping test programs to helping debug them to helping modify them. You&#39;ll learn a lot this way, and generate good karma with people who will help you later on.3. Publish useful informationAnother good thing is to collect and filter useful and interesting information into web pages or documents like Frequently Asked Questions (FAQ) lists, and make those generally available.Maintainers of major technical FAQs get almost as much respect as open-source authors.4. Help keep the infrastructure workingThe hacker culture (and the engineering development of the Internet, for that matter) is run by volunteers. There&#39;s a lot of necessary but unglamorous work that needs done to keep it going — administering mailing lists, moderating newsgroups, maintaining large software archive sites, developing RFCs and other technical standards.People who do this sort of thing well get a lot of respect, because everybody knows these jobs are huge time sinks and not as much fun as playing with code. Doing them shows dedication.5. Serve the hacker culture itselfFinally, you can serve and propagate the culture itself (by, for example, writing an accurate primer on how to become a hacker :-)). This is not something you&#39;ll be positioned to do until you&#39;ve been around for while and become well-known for one of the first four things.The hacker culture doesn&#39;t have leaders, exactly, but it does have culture heroes and tribal elders and historians and spokespeople. When you&#39;ve been in the trenches long enough, you may grow into one of these. Beware: hackers distrust blatant ego in their tribal elders, so visibly reaching for this kind of fame is dangerous. Rather than striving for it, you have to sort of position yourself so it drops in your lap, and then be modest and gracious about your status.The Hacker/Nerd ConnectionContrary to popular myth, you don&#39;t have to be a nerd to be a hacker. It does help, however, and many hackers are in fact nerds. Being something of a social outcast helps you stay concentrated on the really important things, like thinking and hacking.For this reason, many hackers have adopted the label ‘geek’ as a badge of pride — it&#39;s a way of declaring their independence from normal social expectations (as well as a fondness for other things like science fiction and strategy games that often go with being a hacker). The term &#39;nerd&#39; used to be used this way back in the 1990s, back when &#39;nerd&#39; was a mild pejorative and &#39;geek&#39; a rather harsher one; sometime after 2000 they switched places, at least in U.S. popular culture, and there is now even a significant geek-pride culture among people who aren&#39;t techies.If you can manage to concentrate enough on hacking to be good at it and still have a life, that&#39;s fine. This is a lot easier today than it was when I was a newbie in the 1970s; mainstream culture is much friendlier to techno-nerds now. There are even growing numbers of people who realize that hackers are often high-quality lover and spouse material.If you&#39;re attracted to hacking because you don&#39;t have a life, that&#39;s OK too — at least you won&#39;t have trouble concentrating. Maybe you&#39;ll get a life later on.Points For StyleAgain, to be a hacker, you have to enter the hacker mindset. There are some things you can do when you&#39;re not at a computer that seem to help. They&#39;re not substitutes for hacking (nothing is) but many hackers do them, and feel that they connect in some basic way with the essence of hacking.Learn to write your native language well. Though it&#39;s a common stereotype that programmers can&#39;t write, a surprising number of hackers (including all the most accomplished ones I know of) are very able writers.Read science fiction. Go to science fiction conventions (a good way to meet hackers and proto-hackers). Join a hackerspace and make things (another good way to meet hackers and proto-hackers). Train in a martial-arts form. The kind of mental discipline required for martial arts seems to be similar in important ways to what hackers do. The most popular forms among hackers are definitely Asian empty-hand arts such as Tae Kwon Do, various forms of Karate, Kung Fu, Aikido, or Ju Jitsu. Western fencing and Asian sword arts also have visible followings. In places where it&#39;s legal, pistol shooting has been rising in popularity since the late 1990s. The most hackerly martial arts are those which emphasize mental discipline, relaxed awareness, and precise control, rather than raw strength, athleticism, or physical toughness.Study an actual meditation discipline. The perennial favorite among hackers is Zen (importantly, it is possible to benefit from Zen without acquiring a religion or discarding one you already have). Other styles may work as well, but be careful to choose one that doesn&#39;t require you to believe crazy things. Develop an analytical ear for music. Learn to appreciate peculiar kinds of music. Learn to play some musical instrument well, or how to sing. Develop your appreciation of puns and wordplay.The more of these things you already do, the more likely it is that you are natural hacker material. Why these things in particular is not completely clear, but they&#39;re connected with a mix of left- and right-brain skills that seems to be important; hackers need to be able to both reason logically and step outside the apparent logic of a problem at a moment&#39;s notice.Work as intensely as you play and play as intensely as you work. For true hackers, the boundaries between &#34;play&#34;, &#34;work&#34;, &#34;science&#34; and &#34;art&#34; all tend to disappear, or to merge into a high-level creative playfulness. Also, don&#39;t be content with a narrow range of skills. Though most hackers self-describe as programmers, they are very likely to be more than competent in several related skills — system administration, web design, and PC hardware troubleshooting are common ones. A hacker who&#39;s a system administrator, on the other hand, is likely to be quite skilled at script programming and web design. Hackers don&#39;t do things by halves; if they invest in a skill at all, they tend to get very good at it.Finally, a few things not to do. Don&#39;t use a silly, grandiose user ID or screen name. Don&#39;t get in flame wars on Usenet (or anywhere else). Don&#39;t call yourself a ‘cyberpunk’, and don&#39;t waste your time on anybody who does. Don&#39;t post or email writing that&#39;s full of spelling errors and bad grammar.The only reputation you&#39;ll make doing any of these things is as a twit. Hackers have long memories — it could take you years to live your early blunders down enough to be accepted.The problem with screen names or handles deserves some amplification. Concealing your identity behind a handle is a juvenile and silly behavior characteristic of crackers, warez d00dz, and other lower life forms. Hackers don&#39;t do this; they&#39;re proud of what they do and want it associated with their real names. So if you have a handle, drop it. In the hacker culture it will only mark you as a loser.Historical Note: Hacking, Open Source, and Free SoftwareWhen I originally wrote this how-to in late 1996, some of the conditions around it were very different from the way they look today. A few words about these changes may help clarify matters for people who are confused about the relationship of open source, free software, and Linux to the hacker community. If you are not curious about this, you can skip straight to the FAQ and bibliography from here.The hacker ethos and community as I have described it here long predates the emergence of Linux after 1990; I first became involved with it around 1976, and, its roots are readily traceable back to the early 1960s. But before Linux, most hacking was done on either proprietary operating systems or a handful of quasi-experimental homegrown systems like MIT&#39;s ITS that were never deployed outside of their original academic niches. While there had been some earlier (pre-Linux) attempts to change this situation, their impact was at best very marginal and confined to communities of dedicated true believers which were tiny minorities even within the hacker community, let alone with respect to the larger world of software in general.What is now called &#34;open source&#34; goes back as far as the hacker community does, but until 1985 it was an unnamed folk practice rather than a conscious movement with theories and manifestos attached to it. This prehistory ended when, in 1985, arch-hacker Richard Stallman (&#34;RMS&#34;) tried to give it a name — &#34;free software&#34;. But his act of naming was also an act of claiming; he attached ideological baggage to the &#34;free software&#34; label which much of the existing hacker community never accepted. As a result, the &#34;free software&#34; label was loudly rejected by a substantial minority of the hacker community (especially among those associated with BSD Unix), and used with serious but silent reservations by a majority of the remainder (including myself).Despite these reservations, RMS&#39;s claim to define and lead the hacker community under the &#34;free software&#34; banner broadly held until the mid-1990s. It was seriously challenged only by the rise of Linux. Linux gave open-source development a natural home. Many projects issued under terms we would now call open-source migrated from proprietary Unixes to Linux. The community around Linux grew explosively, becoming far larger and more heterogenous than the pre-Linux hacker culture. RMS determinedly attempted to co-opt all this activity into his &#34;free software&#34; movement, but was thwarted by both the exploding diversity of the Linux community and the public skepticism of its founder, Linus Torvalds. Torvalds continued to use the term &#34;free software&#34; for lack of any alternative, but publicly rejected RMS&#39;s ideological baggage. Many younger hackers followed suit.In 1996, when I first published this Hacker HOWTO, the hacker community was rapidly reorganizing around Linux and a handful of other open-source operating systems (notably those descended from BSD Unix). Community memory of the fact that most of us had spent decades developing closed-source software on closed-source operating systems had not yet begun to fade, but that fact was already beginning to seem like part of a dead past; hackers were, increasingly, defining themselves as hackers by their attachments to open-source projects such as Linux or Apache.The term &#34;open source&#34;, however, had not yet emerged; it would not do so until early 1998. When it did, most of the hacker community adopted it within the following six months; the exceptions were a minority ideologically attached to the term &#34;free software&#34;. Since 1998, and especially after about 2003, the identification of &#39;hacking&#39; with &#39;open-source (and free software) development&#39; has become extremely close. Today there is little point in attempting to distinguish between these categories, and it seems unlikely that will change in the future.It is worth remembering, however, that this was not always so.Frequently Asked QuestionsQ: How do I tell if I am already a hacker?Q: Will you teach me how to hack?Q: How can I get started, then?Q: When do you have to start? Is it too late for me to learn?Q: How long will it take me to learn to hack?Q: Is Visual Basic a good language to start with?Q: Would you help me to crack a system, or teach me how to crack?Q: How can I get the password for someone else&#39;s account?Q: How can I break into/read/monitor someone else&#39;s email?Q: How can I steal channel op privileges on IRC?Q: I&#39;ve been cracked. Will you help me fend off further attacks?Q: I&#39;m having problems with my Windows software. Will you help me?Q: Where can I find some real hackers to talk with?Q: Can you recommend useful books about hacking-related subjects?Q: Do I need to be good at math to become a hacker?Q: What language should I learn first?Q: What kind of hardware do I need?Q: I want to contribute. Can you help me pick a problem to work on?Q: Do I need to hate and bash Microsoft?Q: But won&#39;t open-source software leave programmers unable to make a living?Q: Where can I get a free Unix?Q:How do I tell if I am already a hacker?A:Ask yourself the following three questions:Do you speak code, fluently?Do you identify with the goals and values of the hacker community?Has a well-established member of the hacker community ever called you a hacker?If you can answer yes to all three of these questions, you are already a hacker. No two alone are sufficient.The first test is about skills. You probably pass it if you have the minimum technical skills described earlier in this document. You blow right through it if you have had a substantial amount of code accepted by an open-source development project.The second test is about attitude. If the five principles of the hacker mindset seemed obvious to you, more like a description of the way you already live than anything novel, you are already halfway to passing it. That&#39;s the inward half; the other, outward half is the degree to which you identify with the hacker community&#39;s long-term projects.Here is an incomplete but indicative list of some of those projects: Does it matter to you that Linux improve and spread? Are you passionate about software freedom? Hostile to monopolies? Do you act on the belief that computers can be instruments of empowerment that make the world a richer and more humane place?But a note of caution is in order here. The hacker community has some specific, primarily defensive political interests — two of them are defending free-speech rights and fending off &#34;intellectual-property&#34; power grabs that would make open source illegal. Some of those long-term projects are civil-liberties organizations like the Electronic Frontier Foundation, and the outward attitude properly includes support of them. But beyond that, most hackers view attempts to systematize the hacker attitude into an explicit political program with suspicion; we&#39;ve learned, the hard way, that these attempts are divisive and distracting. If someone tries to recruit you to march on your capitol in the name of the hacker attitude, they&#39;ve missed the point. The right response is probably “Shut up and show them the code.”The third test has a tricky element of recursiveness about it. I observed in the section called “What Is a Hacker?” that being a hacker is partly a matter of belonging to a particular subculture or social network with a shared history, an inside and an outside. In the far past, hackers were a much less cohesive and self-aware group than they are today. But the importance of the social-network aspect has increased over the last thirty years as the Internet has made connections with the core of the hacker subculture easier to develop and maintain. One easy behavioral index of the change is that, in this century, we have our own T-shirts.Sociologists, who study networks like those of the hacker culture under the general rubric of &#34;invisible colleges&#34;, have noted that one characteristic of such networks is that they have gatekeepers — core members with the social authority to endorse new members into the network. Because the &#34;invisible college&#34; that is hacker culture is a loose and informal one, the role of gatekeeper is informal too. But one thing that all hackers understand in their bones is that not every hacker is a gatekeeper. Gatekeepers have to have a certain degree of seniority and accomplishment before they can bestow the title. How much is hard to quantify, but every hacker knows it when they see it.Q:Will you teach me how to hack?A:Since first publishing this page, I&#39;ve gotten several requests a week (often several a day) from people to &#34;teach me all about hacking&#34;. Unfortunately, I don&#39;t have the time or energy to do this; my own hacking projects, and working as an open-source advocate, take up 110% of my time.Even if I did, hacking is an attitude and skill you basically have to teach yourself. You&#39;ll find that while real hackers want to help you, they won&#39;t respect you if you beg to be spoon-fed everything they know.Learn a few things first. Show that you&#39;re trying, that you&#39;re capable of learning on your own. Then go to the hackers you meet with specific questions.If you do email a hacker asking for advice, here are two things to know up front. First, we&#39;ve found that people who are lazy or careless in their writing are usually too lazy and careless in their thinking to make good hackers — so take care to spell correctly, and use good grammar and punctuation, otherwise you&#39;ll probably be ignored. Secondly, don&#39;t dare ask for a reply to an ISP account that&#39;s different from the account you&#39;re sending from; we find people who do that are usually thieves using stolen accounts, and we have no interest in rewarding or assisting thievery.Q:How can I get started, then?A:The best way for you to get started would probably be to go to a LUG (Linux user group) meeting. You can find such groups on the LDP General Linux Information Page; there is probably one near you, possibly associated with a college or university. LUG members will probably give you a Linux if you ask, and will certainly help you install one and get started.Your next step (and your first step if you can&#39;t find a LUG nearby) should be to find an open-source project that interests you. Start reading code and reviewing bugs. Learn to contribute, and work your way in.The only way in is by working to improve your skills. If you ask me personally for advice on how to get started, I will tell you these exact same things, because I don&#39;t have any magic shortcuts for you. I will also mentally write you off as a probable loser - because if you lacked the stamina to read this FAQ and the intelligence to understand from it that the only way in is by working to improve your skills, you&#39;re hopeless.Another interesting possibility is to go visit a hackerspace. There is a burgeoning movement of people creating physical locations - maker&#39;s clubs - where they can hang out to work on hardware and software projects together, or work solo in a cogenial atmosphere. Hackerspaces often collect tools and specialized equipment that would be too expensive or logistically inconvenient for individuals to own. Hackerspaces are easy to find on the Internet; one may be located near you.Q:When do you have to start? Is it too late for me to learn?A:Any age at which you are motivated to start is a good age. Most people seem to get interested between ages 15 and 20, but I know of exceptions in both directions.Q:How long will it take me to learn to hack?A:That depends on how talented you are and how hard you work at it. Most people who try can acquire a respectable skill set in eighteen months to two years, if they concentrate. Don&#39;t think it ends there, though; in hacking (as in many other fields) it takes about ten years to achieve mastery. And if you are a real hacker, you will spend the rest of your life learning and perfecting your craft.Q:Is Visual Basic a good language to start with?A:If you&#39;re asking this question, it almost certainly means you&#39;re thinking about trying to hack under Microsoft Windows. This is a bad idea in itself. When I compared trying to learn to hack under Windows to trying to learn to dance while wearing a body cast, I wasn&#39;t kidding. Don&#39;t go there. It&#39;s ugly, and it never stops being ugly.There is a specific problem with Visual Basic; mainly that it&#39;s not portable. Though there is a prototype open-source implementations of Visual Basic, the applicable ECMA standards don&#39;t cover more than a small set of its programming interfaces. On Windows most of its library support is proprietary to a single vendor (Microsoft); if you aren&#39;t extremely careful about which features you use — more careful than any newbie is really capable of being — you&#39;ll end up locked into only those platforms Microsoft chooses to support. If you&#39;re starting on a Unix, much better languages with better libraries are available. Python, for example.Also, like other Basics, Visual Basic is a poorly-designed language that will teach you bad programming habits. No, don&#39;t ask me to describe them in detail; that explanation would fill a book. Learn a well-designed language instead.One of those bad habits is becoming dependent on a single vendor&#39;s libraries, widgets, and development tools. In general, any language that isn&#39;t fully supported under at least Linux or one of the BSDs, and/or at least three different vendors&#39; operating systems, is a poor one to learn to hack in.Q:Would you help me to crack a system, or teach me how to crack?A:No. Anyone who can still ask such a question after reading this FAQ is too stupid to be educable even if I had the time for tutoring. Any emailed requests of this kind that I get will be ignored or answered with extreme rudeness.Q:How can I get the password for someone else&#39;s account?A:This is cracking. Go away, idiot.Q:How can I break into/read/monitor someone else&#39;s email?A:This is cracking. Get lost, moron.Q:How can I steal channel op privileges on IRC?A:This is cracking. Begone, cretin.Q:I&#39;ve been cracked. Will you help me fend off further attacks?A:No. Every time I&#39;ve been asked this question so far, it&#39;s been from some poor sap running Microsoft Windows. It is not possible to effectively secure Windows systems against crack attacks; the code and architecture simply have too many flaws, which makes securing Windows like trying to bail out a boat with a sieve. The only reliable prevention starts with switching to Linux or some other operating system that is designed to at least be capable of security.Q:I&#39;m having problems with my Windows software. Will you help me?A:Yes. Go to a DOS prompt and type &#34;format c:&#34;. Any problems you are experiencing will cease within a few minutes.Q:Where can I find some real hackers to talk with?A:The best way is to find a Unix or Linux user&#39;s group local to you and go to their meetings (you can find links to several lists of user groups on the LDP site at ibiblio).(I used to say here that you wouldn&#39;t find any real hackers on IRC, but I&#39;m given to understand this is changing. Apparently some real hacker communities, attached to things like GIMP and Perl, have IRC channels now.)Q:Can you recommend useful books about hacking-related subjects?A:I maintain a Linux Reading List HOWTO that you may find helpful. The Loginataka may also be interesting.For an introduction to Python, see the tutorial on the Python site.Q:Do I need to be good at math to become a hacker?A:No. Hacking uses very little formal mathematics or arithmetic. In particular, you won&#39;t usually need trigonometry, calculus or analysis (there are exceptions to this in a handful of specific application areas like 3-D computer graphics). Knowing some formal logic and Boolean algebra is good. Some grounding in finite mathematics (including finite-set theory, combinatorics, and graph theory) can be helpful.Much more importantly: you need to be able to think logically and follow chains of exact reasoning, the way mathematicians do. While the content of most mathematics won&#39;t help you, you will need the discipline and intelligence to handle mathematics. If you lack the intelligence, there is little hope for you as a hacker; if you lack the discipline, you&#39;d better grow it.I think a good way to find out if you have what it takes is to pick up a copy of Raymond Smullyan&#39;s book What Is The Name Of This Book?. Smullyan&#39;s playful logical conundrums are very much in the hacker spirit. Being able to solve them is a good sign; enjoying solving them is an even better one.Q:What language should I learn first?A:HTML if you don&#39;t already know it. There are a lot of glossy, hype-intensive bad HTML books out there, and distressingly few good ones. The one I like best is HTML: The Definitive Guide.But HTML is not a full programming language. When you&#39;re ready to start programming, I would recommend starting with Python. You will hear a lot of people recommending Perl, but it&#39;s harder to learn and (in my opinion) less well designed.C is really important, but it&#39;s also much more difficult than either Python or Perl. Don&#39;t try to learn it first.Windows users, do not settle for Visual Basic. It will teach you bad habits, and it&#39;s not portable off Windows. Avoid.Q:What kind of hardware do I need?A:It used to be that personal computers were rather underpowered and memory-poor, enough so that they placed artificial limits on a hacker&#39;s learning process. This stopped being true in the mid-1990s; any machine from an Intel 486DX50 up is more than powerful enough for development work, X, and Internet communications, and the smallest disks you can buy today are plenty big enough.The important thing in choosing a machine on which to learn is whether its hardware is Linux-compatible (or BSD-compatible, should you choose to go that route). Again, this will be true for almost all modern machines. The only really sticky areas are modems and wireless cards; some machines have Windows-specific hardware that won&#39;t work with Linux.There&#39;s a FAQ on hardware compatibility; the latest version is here.Q:I want to contribute. Can you help me pick a problem to work on?A:No, because I don&#39;t know your talents or interests. You have to be self-motivated or you won&#39;t stick, which is why having other people choose your direction almost never works.Q:Do I need to hate and bash Microsoft?A:No, you don&#39;t. Not that Microsoft isn&#39;t loathsome, but there was a hacker culture long before Microsoft and there will still be one long after Microsoft is history. Any energy you spend hating Microsoft would be better spent on loving your craft. Write good code — that will bash Microsoft quite sufficiently without polluting your karma.Q:But won&#39;t open-source software leave programmers unable to make a living?A:This seems unlikely — so far, the open-source software industry seems to be creating jobs rather than taking them away. If having a program written is a net economic gain over not having it written, a programmer will get paid whether or not the program is going to be open-source after it&#39;s done. And, no matter how much &#34;free&#34; software gets written, there always seems to be more demand for new and customized applications. I&#39;ve written more about this at the Open Source pages.Q:Where can I get a free Unix?A:If you don&#39;t have a Unix installed on your machine yet, elsewhere on this page I include pointers to where to get the most commonly used free Unix. To be a hacker you need motivation and initiative and the ability to educate yourself. Start now...</description>
      <pubDate>24 Mar 20 22:14 EDT</pubDate>
      <guid>http://www.catb.org/esr/faqs/hacker-howto.html</guid>
    </item>
    <item>
      <title>Learning Ruby: 2 Things I Like, 2 Things I Miss From Python</title>
      <link>https://medium.com/workpath-thewaywework/learning-ruby-2-things-i-like-2-things-i-miss-from-python-6f60af8ed16c</link>
      <description>&lt;a href=&#34;https://medium.com/workpath-thewaywework/learning-ruby-2-things-i-like-2-things-i-miss-from-python-6f60af8ed16c&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I’ve recently had the fortune to join Workpath as VP of Engineering. Workpath’s backend runs on Ruby on Rails, and while coding won’t be a part of my day-to-day duties, I still need and want to learn the stack.Since most of my experience is with Python, let me tell you about 2 things I’ve liked in Ruby, and 2 that made me want to start up PyCharm again!→ Follow us on 🐦 Twitter @WorkpathEngPython vs. RubyPython and Ruby are really similar. They’re both interpreted, dynamically typed, general-purpose languages. There are similarities in the syntax, and they even share implementation details:You can write extensions in C.They’re compiled to bytecode and then both Python and Ruby interpreters are basically a giant fascinating switch-statement in C.Their interpreters are both plagued by a global interpreter lock, which limits parallelism … the similarities are eerie!But Python and Ruby occupy different spaces today: Python’s stronghold is data science, whereas Ruby is inseparable from the Rails framework. However, I posit that this might as well be accidental; the language designs are so similar that I could just as well imagine a world where Python is the web development lingua franca, and Ruby has all the machine learning libraries.Me at PyCon 2016 with … yeah, a real Python 🐍 #noanimalswereharmedNow, my thoughts on learning Ruby:I Like: The SyntaxRuby’s syntax is concise, and the language is opinionated in areas where Python is not:OOP all the way. In Ruby, you just structure your code with classes and methods. In Python, OOP is optional, and it’s a matter of debate if it is actually “Pythonic”. OOP did not get much love from Python language designers, so you’ll have to make do without a private keyword and similar syntactic sugar. And the deeper you look at its type system, the more likely you are to find leaky abstractions shining through from its C interpreter.Blocks. Ruby has a reputation of permitting multiple ways of doing the same thing (more on that below). But I find that blocks hold such a privileged position in the language syntax that you’re just nudged to use them in your interface design.When leading a team around a large codebase, I appreciate a language that encourages a certain design. Fewer technical decisions, more time to focus on creating value for customers!I Dislike: … Some Of The SyntaxOn the other hand, Ruby takes the “concise” part a bit far. There are nearly always different ways of expressing basic syntactic concepts. And Rubyists have a knack for loaded one-liners — take this Rails scope:scope :registration_on, -&gt;(date) { where created_at: date.beginning_of_day..date.end_of_day }Sure, if you just read the English words from left to right, you grasp what this does. But to my beginner’s eyes, this is much more readable with a few more parentheses.And it seems that Ruby is determined to keep adding syntax. Ruby 3.0 comes with rightward assignment, which makes these gems (heh) possible:I Miss: GeneratorsThey say a programming language is worth learning if it changes the way you think.From Python, the thing that’ll stick with me is generators. Especially for processing data, it just becomes so natural (and memory-efficient) to express generation or transformation of data through generators.In fact, I wrote a Python script to generate different Ruby representations of the Rails scope I mentioned as an example above. I would struggle to crank out this code as quickly in Ruby or any other language:Write a comment if you want to see the output of this thing.Generators gradually evolved to become the basis of async coroutines in modern Python versions. And Ruby 3.0 comes with Fibers, a very similar concept, which means maybe I won’t have to miss generators for long.I Like: The EcosystemAn open-source language lives and dies with its community and ecosystem. And Ruby seems at a disadvantage here, since it’s a well known language, but rather niche in the workplace, and often absent from top 10 rankings.However, my limited experience has been that tooling and libraries are very mature. Ruby seems to be focused around a few key projects, but these are rock solid. If you want to write a web application in Python, or write a BDD test suite etc. etc. you’ll almost always be faced with a myriad of choices.There’s another upside to this focus around a few key open source libraries: While Ruby developers are still few and far between, when you do get to interview one, they tend to have experience in at least 80% of the tech that we use.</description>
      <pubDate>16 Feb 21 09:40 EST</pubDate>
      <guid>https://medium.com/workpath-thewaywework/learning-ruby-2-things-i-like-2-things-i-miss-from-python-6f60af8ed16c</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.drmaciver.com/2009/01/writing-things-right/</link>
      <description>&lt;a href=&#34;https://www.drmaciver.com/2009/01/writing-things-right/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; OO has contributed many big and important innovations to programming. Among these, the foremost is that you write functions after rather than before their argument. No, really. It’s not just OO languages of course. Concatenative languages do the same thing. There’s a long history of mathematicians doing it as well (though we don’t like to talk about them. The cool mathematicians all write their functions on the left). It’s funny how attached people get to this fact though. Consider the following piece of Scala code: object StringUtils{ /** * Trims whitespace from the end of s. */ def rtrim(s : String) = ... } We can invoke this as StringUtils.rtrim(myString). Or if we import StringUtils, just rtrim(myString); People get very upset if you ask them to do so though, and they go to all sorts of lengths to avoid it. Consider the following three examples from different languages: Scala: object StringUtils{ implicit def string2RTrim(s : String) = new { def rtrim = ...; } } Ruby: class String def rtrim ... end end C#: class StringUtils{ public static String rtrim(this String s) {  ... } } What do these achieve over the previous version? Simple: You can write myString.rtrim instead of rtrim(myString). That’s it. (Actually the Ruby and Scala versions both *can* allow you to do different things than that. It’s just that here and in 90% of the use cases they aren’t used for anything else. The C# version literally doesn’t do anything else). The thing is, while I’m making fun of this to a certain degree, it’s actually a perfectly reasonable thing to want to do. Designing things in noun-verb order is a good principle of UI design, and it works for programming as well. Things chain better – when you want to add new functions to a pipeline you add them at the point your cursor is naturally at and it matches well with thinking of it as a pipeline of “take this thing, do this to it, do that to it, do this other thing to it, get this value out”. Also you write far fewer brackets. :-) (compare Haskell’s foo . bar . baz $ thing idiom for a similar bracket avoidance tool). Of these, I’d say that the Ruby solution is the most obvious (it just uses the fact that classes are open to add a new method to String), but it comes with the possibility of amusingly non-obvious runtime errors when someone else defines a conflicting method. The C# solution seems the best to me – it’s relatively little overhead over writing the utility method as you would otherwise and comes with the option to invoke it either as myString.rtrim or StringUtils.rtrim(myString), so when namespacing conflicts inevitably occur you have an easy fallback. But of course it uses a language feature specifically added to do this, while the other two are functions of more general language features. The Scala solution is, to my mind, decidedly the worst of the three.It’s syntactically noisy and comes with a significant additional runtime overhead. But honestly I’m not particularly happy with any of these solutions. The Scala and Ruby solutions come with disproportionate costs to the benefit they give and the C# solution requires an additional language feature. Moreoever, each of these solutions requires effort at each definition site in order to make something available that you always want at the use site. Wouldn’t it be better if for every utility function you automatically had the option to write it on the right? Let’s take a digression. What language is the following (rather pointless) code written in? [1, 2, 3].sort.length Ruby, right? Actually, no. It’s Haskell. Wait, what? Well, it’s Haskell if you do something slightly evil and redefine the (.) operator (which normally means composition): Prelude Data.List&gt; let (.) x f = f x Prelude Data.List&gt; [1, 2, 3].sort.length 3 I saw this trick a while ago (the author was amusingly apologetic for it). It’s evil Haskell code because of the way it redefines an operator that normally means something else (this is totally typesafe of course – existing code will continue to use the old operator definition). But it’s a perfectly valid operator definition, and a rather nice one. It works well with additional arguments to functions too: Prelude Data.List&gt; [1, 2, 3].sortBy(compare).length 3 The reason this works is that sortBy takes the list argument curried as its last argument, so sortBy(compare) gives something of type [Int] -&gt; [Int] which we can then apply as above (Haskell’s precedence rules make this work). So this is a nice trick, but how is it useful to you? Well, it’s probably not. I can’t think of any low noise way of making it work in any of the other languages mentioned so far (the best I can come up with is an evil evil hack in Ruby that would make god go on a kitten killing spree and a mildly nasty hack with operators and implicit conversions in Scala that’s much too noisy to really use), and using it in Haskell will make other Haskell programmers very unhappy with you. But it’s an interesting trick, and I’ll be sure to bear it in mind if I ever get around to creating DRMacIverLang. </description>
      <pubDate>27 Apr 20 15:15 EDT</pubDate>
      <guid>https://www.drmaciver.com/2009/01/writing-things-right/</guid>
    </item>
    <item>
      <title>The Business of Extracting Knowledge from Academic Publications</title>
      <link>https://markusstrasser.org/extracting-knowledge-from-literature/</link>
      <description>&lt;a href=&#34;https://markusstrasser.org/extracting-knowledge-from-literature/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; I ment to post this much earlier but have been fighting with neurological longhaul covid symptoms for most of the year. TL;DR: I worked on biomedical literature search, discovery and recommender web applications for many months and concluded that extracting, structuring or synthesizing &#34;insights&#34; from academic publications (papers) or building knowledge bases from a domain corpus of literature has negligible value in industry. Close to nothing of what makes science actually work is published as text on the web UIs serving up “insights” from machine-reading millions of papers through specialized search engines, paper recommenders, citation analysis, correlation plots of mentions, hierarchical clusterings of topics, interaction networks (relation extraction), causation graphs [...]Here’s the outline:(If you can&#39;t see a ToC, try reloading the page). You can comment on this post on Twitter or HN Psychoanalysis of a Troubled Industry Atop the published biomedical literature is an evolved industry  around the extracting, semantic structuring and synthesizing of research papers into search, discovery and knowledge graph software applications (table of example companies). The usual sales pitch goes something like this:Generate and Validate Drug Discovery Hypotheses Faster Using our Knowledge GraphKeep up with the scientific literature, search for concepts and not papers, and make informed discovery decisions[tellic]Find the relevant knowledge for your next breakthrough in the X million documents and Y million extracted causal interactions between genes, chemicals, drugs, cells… [biorelate - Galaxy]Our insight prediction pipeline and dynamic knowledge map is like a digital scientist [OccamzRazor]Try our sentence-level, context-aware, and linguistically informed extractive search system [AllenAI’s Spike]Or from a grant application of yours truly:a high-level semantic search engine and user interface for answering complex, quantitative research questions in biomedicine You get the idea. All the projects above spring from similar premises:The right piece of information is “out there”If only research outputs were more machine interpretable, searchable and discoverable then “research” could be incrementally automated and progress would go through the roofNo mainstream public platform (eg. arxiv, pubmed, google scholar) provides features that leverage the last decade’s advances in natural language processing (NLP) and they do not improve on tasks mentioned in 2.Which leads them to conclude: Let’s semantically annotate every piece of literature and incorporate it into a structured knowledge base that enables complex research queries, revealing undiscovered public knowledge by connecting causal chains (swanson linking) or generating new hypotheses altogether.Two sentences semantically with multiple layers of semantic annotationAn example of a generic biomedical entity-relation schema that might power a knowledge base on topIt really does sound exciting and VCs and governments like to fund the promise of it. Companies get started, often get a few pivot customers, end up as software consultancies, or silently fail. In rare cases they get acquired for cheap or survive as a modest “request a demo” B2B SaaS with 70% of the company in sales with unit economics that lock them in to only sell to enterprises and never to individuals 0.Meanwhile big players with all the means to provide a category leading platform have either stopped developing such an effort altogether (Google Scholar), shut it down and nobody cares (ie. Microsoft Academic Search) or are unable to commercialize any of it (AllenAI with SemanticScholar, Spike etc.).Let’s take AllenAI as an example. AllenAI is a world-renowned AI research organization with a focus on NLP and automated reasoning. They sponsor a startup incubator to spin-off and commercialize their technologies. The AllenNLP platform and SemanticScholar is used by millions of engineers and researchers. And unlike most big AI research groups they also build domain-focused apps for biomedicine (like supp.ai and Spike).And yet, despite all of that momentum, they can’t commercialize any of it:We have been looking at opportunities to commercialize the technologies we developed in Semantic Scholar, Spike, bioNLP, etc. The challenge for us so far is in finding significant market demand. I&#39;m curious if you have found an angle on this ? [...]This was an email reply I received from a technical director at the AllenAI institute. At that point, I had already spent months building and trying to sell biomedical NLP applications (bioNLP) to biotechs which had made me cynical about the entire space. Sure, I might just be bad at sales but if the AllenAI with 10/10 engineering, distribution and industry credibility can’t sell their offerings, then literature review, search and knowledge discovery are just not that useful…When I then looked at the SUM of all combined funding, valuations, exits of _NLP_ companies in the semantic search / biomedical literature search / biomedical relation extraction space I could find it was less than the valuation of any single bigger _bioinformatic_ drug discovery platform ( ie. Atomwise, Insitro, nFerence). Assuming the market is efficient and not lagging, it shows how little of a problem biomedical literature search and discovery actually is. Just to clarify: This post is about the issues with semantic intelligence platforms that predominantly leverage the published academic literature. Bioinformatic knowledge apps that integrate biological omics data or clinical datasets are actually very valuable (ie. Data4cure), as is information extraction from Electronic Health Records [EHRs] (like nFerence, scienceIO) and patent databases.Back in March 2020 when the first covid lockdowns started I evacuated San Francisco and moved back to Austria. I decided to use the time to get a deeper understanding of molecular biology and began reading textbooks.Before biology, I had worked on text mining, natural language processing and knowledge graphs and it got me thinking ... could you build something that can reason at the level of a mediocre biology graduate?After a few weeks of reading papers in ontology learning, information extraction and reasoning systems and experimenting with toy programs, I had some idea of what&#39;s technologically possible. I figured that a domain-focused search engine would be much simpler to build than a reasoner/chatbot and that the basic building blocks are similar in any case 0.5.I wrote up what I had in mind, applied to Emergent Ventures and was given a grant to continue working on the idea.At that time I also moved to London as an Entrepreneur First (EF) fellow. I made good friends in the program and teamed up with Rico. We worked together for the entirety of the three month program. During that time we prototyped:a search engine with entity, relation and quantitative options: A user could go detailed, expressive queries like:&lt; studies that used gene engineering [excluding selective evolution] in yeast [species A and B only] and have achieved at least [250%] in yield increase &gt;&lt; studies with women over age 40, with comorbidity_A that had an upward change in biomarker_B &gt;They&#39;d tell us in English and we&#39;d translate it into our query syntaxThis collapses a lot of searches and filtering into one query that isn&#39;t possible with known public search engines but we found out that it&#39;s not often in a biotechs lifecycle that questions like this need researching. To get a notion of ratios: one search like this could return enough ideas for weeks of lab worka query builder (by demonstration): A user would upvote study abstracts or sentences that fit their requirements and it would iteratively build configuration. In a sense it was a no-code way for biologists to write interpretable labeling functions while also training a classifier (a constrained case of program synthesis)We built this because almost no biologist could encode or tell us exactly which research they wanted to see but they &#34;know it when I see it&#34;A paper recommender system (and later claim/sentence/statement level) that, in addition to academic literature, included tweets, company websites and other non-scholarly sources (lots of scientific discourse is happening on Twitter these days)A claim explorer that takes a sentence/paragraph and lets you browse through similar claimsA claim verifier that showed sentences that confirm or contradict an entered claim sentenceIt worked ok-ish but the tech is not there to make fact checking work reliably, even in constraint domainsAnd a few more Wizard of Oz experiments to test different variants and combinations of the features aboveAt that point, lots of postdocs had told us that some of the apps would have saved them months during their PhD, but actual viable customers were only moderately excited. It became clear that this would have to turn into another B2B SaaS specialized tool with a lot of software consulting and ongoing efforts from a sales team ...We definitely did not want to go down that route. We wanted to make a consumer product for consumers, startups, academics or independent researchers and had tested good-enough proxies for most ideas we thought of as useful w.r.t. the biomedical literature. We also knew that pivoting to Electronic Health Records (EHRs) or patents instead of research text was potentially a great business, but neither of us was excited to spend years working on that, even if successful.So we were stuck. Rico, who didn&#39;t have exposure to biotech before we teamed up, understandably wanted to branch out and so we decided to build a discovery tool that we could use and evaluate the merits of ourselves.And so we loosened up and spent three weeks playing around with knowledge search and discovery outside of biomed. We built:a browser extension that finds similar paragraphs (in articles/essays from your browser history) to the text you&#39;re currently highlighting (when reading on the web)A third of the time the suggestions were insightful, but the download-retrain-upload loop for the embedding vectors every few days was tedious and we didn&#39;t like it enough to spend the time automatingsimilar: an app that pops up serendipitous connections between a corpus (previous writings, saved articles, bookmarks ...) and the active writing session or paragraph. The corpus, preferably your own, could be from folders, text files, blog archive, a Roam Research graph or a Notion/Evernote database. This was a surprisingly high signal to noise ratio from the get go and I still use it sometimesa semantic podcast search (one version for the Artificial Intelligence Podcast is still live)an extension that after a few days of development was turning into Ampie and was shelvedan extension that after a few days of development was turning into Twemex and was shelvedSome of the above had promise for journalists, VCs, essayists or people that read, cite and tweet all day, but that group is too heterogeneous and fragmented and we couldn&#39;t trust our intuition building for them.To us, these experiments felt mostly like gimmicks that, with more love and industry focus 1.5, could become useful but probably not essential.Now it was almost Christmas and the EF program was over. Rico and I decided to split up as a team because we didn&#39;t have any clear next step in mind 1. I flew to the south of Portugal for a few weeks to escape London&#39;s food, bad weather and the upcoming covid surge.There, I returned to biomed and tinkered with interactive, extractive search interfaces and no-code data programming UIs (users can design labeling functions to bootstrap datasets without coding expertise)Ironically, I got covid in Portugal and developed scary neurological long-hauler symptoms after the acute infection. Except for a handful of ‘good days&#39;, spring and summer came and went without me being able to do meaningful cognitive work. Fortunately, this month symptoms have improved enough to finish this essay.Fundamental Issues with Structuring Academic Literature as a BusinessAs I said in the beginning:extracting, structuring or synthesizing &#34;insights&#34; from academic publications (papers) or building knowledge bases from a domain corpus of literature has negligible value in industryTo me the reasons feel elusive and trite simultaneously. All are blatantly obvious in hindsight.Just a Paper, an Idea, an Insight Does Not Get You InnovationSystems, teams and researchers matter much more than ideas. 2 It wasn’t always like that. In the 19th century ideas (inventions) were actually the main mechanism for innovation. From Notes on The changing structure of American innovation:​​The period from 1850-1900 could be described as the age of the inventor. During this period inventors were the main mechanism for creating innovations. These folks would usually sell these patents directly to large companies like the telegraph operators, railroads, or large chemical companies. The companies themselves did little R&amp;D and primarily operated labs to test the patents that inventors brought themBut the complexity threshold kept rising and now we need to grow companies around inventions to actually make them happen:DuPont bought the patent for creating viscose rayon (processed cellulose used for creating artificial silk and other fibers.) However, Dupont was unable to replicate the process successfully and eventually had to partner with the original inventors to get it to work[...] making systems exponentially more valuable than single technologiesThat’s why incumbents increasingly acqui-hire instead of just buying the IP and most successful companies that spin out of labs have someone who did the research as a cofounder. Technological utopians and ideologists like my former self underrate how important context and tacit knowledge is. And even if you had that context and were perfectly set up to make use of new literature ... a significant share of actionable, relevant research findings aren’t published when they’re hot but after the authors milk them and the datasets for a sequence of derivative publications or patent them before publishing the accompanying paper (half the time behind paywalls) months later.In the market of ideas information asymmetries turn into knowledge monopolies. As mentioned in Market Failures in Science:Scientists are incentivized to, and often do, withhold as much information as possible about their innovations in their publications to maintain a monopoly over future innovations. This slows the overall progress of science.For example, a chemist who synthesizes a new molecule will publish that they have done so in order to be rewarded for their work with a publication. But in the publication they will describe their synthesis method in as minimal detail as they can while still making it through peer review. This forces other scientists to invest time and effort to reproduce their work, giving them a head-start in developing the next, better synthesis.All that is to say: discovering relevant literature, compiling evidence, finding mechanisms turns out to be a tiny percentage of actual, real life R&amp;DContextual, Tacit Knowledge is not Digital, not Encoded or just not Machine-Interpretable yetMost knowledge necessary to make scientific progress is not online and not encoded. All tools on top of that bias the exploration towards encoded knowledge only (drunkard&#39;s search), which is vanishingly small compared to what scientists need to embody to make progress. Expert and crowd-curated world knowledge graphs (eg. ConceptNet) can partially ground a machine learning model with some context and common sense but that is light years away from an expert’s ability to understand implicature and weigh studies and claims appropriately. ML systems are fine for pattern matching, recommendations and generating variants but in the end great meta-research, including literature reviews, comes down to formulating an incisive research question, selecting the right studies and defining adequate evaluation criteria (metrics) 2.5.Besides, accurately and programmatically transforming an entire piece of literature into a computer-interpretable, complete and actionable knowledge artifact remains a pipe dream.Automatically generating complex schemas like micropubs (above) from text is many years away. Micropubs (2014), nanopubs, BEL (Biological Expression Language) and many more are all dead in my view (without admitting it)Experts have well defined, internalized maps of their fieldIn industry, open literature reviews are a sporadic, non-standardized task. Professionals spend years building internal schemas of a domain and have social networks for sourcing (ie. discovery) and vetting relevant information if needed. That&#39;s why the most excited of our initial user cohorts were graduate and PhD students and life science VCs, not professionals who specialized and actively work in biotech or pharma companies.The VC use case of doing due diligence was predominantly around legal and IP concerns rather than anything that a literature review might produce. The conceptual clearance or feedback on the idea itself was done by sending it to a referred expert in their network, not by searching the literature for contradictions or feasibility.Scientific Publishing comes with Signaling, Status Games, Fraud and often Very Little InformationMany published papers have methodical or statistical errors, are derivative and don&#39;t add anything to the discourse, are misleading or obfuscated, sometimes even fraudulent or were just bad research to begin with. Papers are first and foremost career instruments. A naive system will weigh the insights extracted from a useless publication equally to a seminal work. You can correct for that by normalizing on citations and other tricks but that will just mimic and propagate existing biases and issues with current scientific publishing.For example, if I design a system that mimics the practices of an expert reader, it would result in biases towards institutions, countries, the spelling of names, clout of authors and so on. That’s either unfair, biased and unequal or it is efficient, resourceful and a reasonable response based on the reader&#39;s priors. In the end, there is no technological solution. To push this point: if you believe in the great man theory of scientific progress, which has more merit than most want to admit, then why waste time making the other 99%+ of &#34;unimportant&#34; publications more searchable and discoverable? The &#34;greats&#34; (of your niche) will show up in your feed anyway, right? Shouldn&#39;t you just follow the best institutions and ~50 top individuals and be done with your research feed?Well in fact that&#39;s what most people do.From Bad WritingNon-technical Life Science Labor Is CheapWhy purchase access to a 3rd party AI reading engine or a knowledge graph when you can just hire hundreds of postdocs in Hyderabad to parse papers into JSON? (at a $6,000 yearly salary)Would you invest in automation if you have billions of disposable income and access to cheap labor? After talking with employees of huge companies like GSK, AZ and Medscape the answer is a clear no.Life science grads work for cheap, even at the post-grad level. In the UK a good bioinformatician can make 2-4 times of what non-technical lab technicians or early career biologists make. In the US the gap is even larger. The Oxford Chemistry and Biology postdocs I met during bus rides to the science park (from my time at Oxford Nanopore) earned £35k at AstraZeneca 3. That&#39;s half of what someone slightly competent earns after four months of youtubing Javascript tutorials 🤷‍♂️.When we gave our pilot customers (biologists) spreadsheets with relations, mentions and paper recommendations that fit their exact requirements they were quite happy and said it saved them many hours per week. But it wasn’t a burning pain for them since their hourly wage is low either way.I suspect the inconsequential costs of lab labor is a reason why computer aided biology (CAB) and lab automation, including cloud labs, are slow on the uptake …it can’t just be the clogging of liquid handling robots, right?The Literature is Implicitly Reified in Public Structured Knowledge BasesThere are a ton of biomedical knowledge bases, databases, ontologies that are updated regularly 4. They are high signal because groups of experts curate and denoise them. They&#39;re tremendously useful and will eventually make biomedical intelligent systems and reasoners easier to build. Bayer, AZ, GSK all integrate them into their production knowledge graphs and their existence makes any additional commercial attempts to extract relations from the literature less needed.Advanced Interactives and Visualizations are Surprisingly Unsatisfying to ConsumeAs an artist this was the most painful lesson: Interactive graphs, trees, cluster visualizations, dendrograms, causal diagrams and what have you are much less satisfying than just lists, and most often lists will do.On the other hand figures, plots and tables are at least as valuable as the actual text content of a paper but programs can&#39;t intelligibly process and extract their contents (too much implied context).That alone cuts the value of tools based on extracting insights from text in half (even with perfect extraction).I realized that when I sat next to a pro &#34;reading&#34; a paper. It goes like this:Scan abstract. Read last sentence of introduction. Scan figures. Check results that discuss the figures....which was entirely different from my non-expert approach to reading papers. It makes sense: If you don&#39;t have a strong domain model, you have no map that guides your scan, so you have to read it all top to bottom … like a computer.Is this actually useful to experts?This skip-and-scan selective processing also explains why agglomerative, auto-generated and compiled visualizations that incorporate a large corpus of papers are not that valuable: most of the sources would’ve been discarded up front. Research contexts are so multiplicitous that every compiled report or visualization has huge amounts of noise and no user interface ever can be expressive enough to perfectly parameterize that context.Unlike other Business Software, Domain Knowledge Bases of Research Companies are Maximally IdiosyncraticUnlike other SaaS products that have standardized components (auth, storage, messaging, ...), research knowledge graphs are always designed around the domain and approach of a company and are tightly integrated with their infrastructure, proprietary datasets and IP. Two enterprises can encode the same corpus and will produce starkly different knowledge base artifacts. Pharmas all have their own custom knowledge graphs and have entire engineering teams working full time on keeping schemas consistent (AstraZeneca and their setup). Incompatibilities are ontological, not technological. Knowledge representations are design decisions. That&#39;s why 3rd party knowledge graph vendors almost always have to do multi-month ongoing integration work and software consulting to make the sale. “Request a demo” often translates to “hire us for a bespoke data science project”.To put knowledge base interoperability issues into perspective: even small database merges within the same company are still a huge problem in the software industry with no real, verifiable loss-free solution yet. Database incompatibilities have left many database administrators traumatized and knowledge bases have orders of magnitude higher schematic complexity than databases.Divergent Tasks are Hard to Evaluate and Reason AboutBy “divergent” I mean loosely defined tasks where it&#39;s unclear when they&#39;re done. That includes &#34;mapping a domain&#34;, &#34;gathering evidence&#34;, &#34;due diligence&#34; and generally anything without a clear outcome like &#34;book a hotel&#34;, &#34;find a barber&#34;, &#34;run this assay&#34;, &#34;order new vials&#34;...It’s easy to reason about convergent tasks, like getting an answer to a factual question or checking a stock price. It’s hard to reason and quantify a divergent discovery process, like getting a map of a field or exploring a space of questions. You can&#39;t put a price tag on exploration and so it is set very low by default. Companies employ “researchers” not for reading literature, but for lab or coding work and the prestige of the “researcher” title pushes salaries down even further.For example, take a product that offers corpus analytics:What’s the value of getting a topic hierarchy, causal diagram or paper recommendations? It has some value if the user does not already have an operational model in his head but how often is that the case? Most biotech research jobs require a PhD level schema from the get go, so what’s the added value to a noisy AI-generated one?When the components  (claims, descriptions, papers) and tasks (evidence review, due diligence, mapping) are ambiguous it is tough to reason about returns on investment with clients. This continues inside the company: employees can defend hours in the lab much better than hours “researching”.We’re also naturally good at diverging and auto-association which makes an in silico version of that feature less valuable. It often ends up being more effort to parse what those interfaces return than to actually, simply think.Besides, most biotechs (before Series C) don’t have the infrastructure to try out new ideas faster than they can read them. A week in the lab can save you a day in the library 6Public Penance: My Mistakes, Biases and Self-Deceptions Yes, I was raised catholic. How did you know?My biggest mistake was that I didn&#39;t have experience as a biotech researcher or postdoc working in a lab. Sometimes being an outsider is an advantage, but in a field full of smart, creative people the majority of remaining inefficiencies are likely to come from incentives, politics and culture and not bad tooling.I used to be friends with someone I consider exceptional who went on to found a biomedical cause-effect search engine (Yiannis the Co-founder of Causaly). It biased me towards thinking more about this category of products. Also, I had met employees at SciBite, a semantics company that was acquired by Elsevier for £65M, and was so unimpressed by the talent there that I was convinced that there’s lots left to do.I wanted to make this my thing, my speciality. I had all the skills for making augmented interfaces for scientific text: machine learning, UI design and fine arts, frontend development, text mining and knowledge representation...New intuitive query languages, no-code automation tools, new UIs for navigating huge corpi, automated reporting, cluster maps etc. where among the few applications that fit my aesthetics enough to withstand the disgusts of developing production software. I had a good idea of the business model issues after a few weeks of talking to users, but I didn&#39;t want to stop without a first-principles explanation of why that is. Sunk-cost fallacy and a feeling of fiduciary responsibility to Emergent Ventures played a part too, but mainly because I wanted to know decisively why semantic structuring, academic search, discovery and publishing are such low-innovation zones. Looking back, I should’ve been OK with 80% certainty that it’s a lost cause and moved on.I wanted to work on these tools because I could work with scientists and contribute something meaningful without needing to start from scratch teaching myself biology, bioinformatics and actually working in a lab. I started from nothing too many times in my life and had gotten impatient. I wanted to leverage what I already knew. But there aren’t real shortcuts and impatience made me climb a local optima. OnwardsThe initial not so modest proposal I sent to Emergent Ventures was along the lines of:a next-generation scientific search and discovery web interface that can answer complex quantitative questions, built on extracted entities and relations from scientific text, such as causations, effects, biomarkers, quantities, methods and so onI mentioned thatthere were major advances in relevant fields of machine learningthat current interfaces are impoverishedthat an hour of searching could be collapsed into a minute in many casesAll of that is still true but for the reasons I tried to share in this essay nothing of it matters.I had to wrap my head around the fact that close to nothing of what makes science actually work is published as text on the web.Research questions that can be answered logically through just reading papers and connecting the dots don&#39;t require a biotech corp to be formed around them. There&#39;s much less logic and deduction happening than you&#39;d expect in a scientific discipline 5.It&#39;s obvious in retrospect and I likely persisted for too long. I had idealistic notions of how scientific search and knowledge synthesis &#34;should work&#34;.I’ve been flirting with this entire cluster of ideas including open source web annotation, semantic search and semantic web, public knowledge graphs, nano-publications, knowledge maps, interoperable protocols and structured data, serendipitous discovery apps, knowledge organization, communal sense making and academic literature/publishing toolchains for a few years on and off ... nothing of it will go anywhere. Don’t take that as a challenge. Take it as a red flag and run. Run towards better problems. </description>
      <pubDate>09 Dec 21 09:47 EST</pubDate>
      <guid>https://markusstrasser.org/extracting-knowledge-from-literature/</guid>
    </item>
    <item>
      <title></title>
      <link>https://howistart.org/posts/erlang/1/</link>
      <description>&lt;a href=&#34;https://howistart.org/posts/erlang/1/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Written by Fred Hebert on June 22, 2015 Erlang Intro Erlang releases are a bit like magnets. Everyone who thinks about them shares the same thought: f**king releases, how do they work? Fortunately, since the years of Emakefiles, reltool and systools, the Erlang community has stood up and improved its tooling continuously. Rebar has been improving non-stop and keeps getting better for many functions. The newest generation, Rebar3, tries to provide an end-to-end experience to building Erlang projects. Along with installing Erlang (version R16B03-1 at least), getting a hold of Rebar3 is all you’re gonna need. For rebar, just download it and follow the instructions. Rebar3 will basically generate a self-executable that you can store in your repository, or install globally on your computer. This tutorial expects that you have installed it in your system and made it available in your $PATH. Once they’re all installed somewhere in your system, arm yourself with the text editor or IDE of your choice (mine is Vim, because I’m a terrible person) and get ready to write a few things. My Environment Despite you being free to develop on whatever you want, I’m gonna go through whatever my setup is. I use zsh with oh-my-zsh, using a custom theme (depends on hg-prompt as a script), and stuck in vi-mode, because vim vim vim. For vim itself, to work with Erlang, I use these two lines in my .vimrc file: autocmd BufRead,BufNewFile *.erl,*.es.*.hrl,*.yaws,*.xrl set expandtab au BufNewFile,BufRead *.erl,*.es,*.hrl,*.yaws,*.xrl setf erlang And I depend on these two plugins: vimerl erlang-motions I don’t use a lot of material outside of that, and the OS will tend to be my IDE – for projects that I tend to work on a lot, I will use tmux scripts (see this blog post for an example) – to get everything going early. The Project To avoid the usual Hello World stuff, this tutorial will use a somewhat more fun application to get up and running from a basic Erlang app that can be run within a module, to a proper OTP library that can be included by other projects, to a release than can be self-executing and distributed to client’s computer, or on a server. Our project will be the replication of one of the most well-known software programs in popular history, used in life-critical situations: Homer Simpson’s console in the episode where he’s so fat he can work at home. From this episode we can infer the following about the software: When the program boots, it asks you to press any key. The program will ask you questions that can be answered by yes/no, but also Y/N or y/n Most questions can be turned into commands. Each assertion is equivalent to answering a given question positively. For example, Vent radioactive gas? Yes/No can be turned into the Vent gas command. Nothing should go wrong if you keep pressing Y all the time After a given delay, a new question is asked Too many times without venting radioactive gas risks exploding everything Some behaviours aren’t defined by the TV show, so we go somewhat anyway we feel like Out of this, a finite-state machine can be created. The one that follows explains what I understood as possible, but you’ll notice I’m not really good at having a consistent notation for events, states, and so on: [press any key] | (key pressed) | [check core temperature (first)] \________,________/ | (yes/no) | [venting radioactive gases (first)] | | (yes) ,-&lt;-, (no) | | | | [gas blows away crop] | [venting prevents explosions] | | | | | &#39;--&lt;-(yes) (no) \ / \______________,_____________/ V | [wait for command]&lt;--------, / \ | (get data) (timeout) | | | | | [ask question] | | / \ | | (Yes) (No) | | / | | +----&#39; &#39;-----+ | | [show result] --&gt;-------&#39; Based on this, we’ll be able to draw up a first prototype with all the required state transitions. I’ve also looked for transcripts of the show and extracted the following questions and consequences: Check core temperature. yes/no: yes: Core temperature normal. no: – Vent radioactive gas? yes: *gas blows away corn crop* no: venting prevents explosion (allow yes, show only the first time?) Sound alertness horn? yes: *horn sounds in the distance* no: – Decalcify calcium ducts? yes: – no: – Special case: after denying venting too many times, the valve must be disabled manually. The simplest way to write a basic FSM for this one is to use a bunch of function calls. Given Erlang has last call optimization (a call that happens as a return value does not leave a stack trace, and therefore can happen infinitely many times), this is more than adequate. The sequence of states a -&gt; b -&gt; c can be programmed as: a() -&gt; b(). b() -&gt; c(). c() -&gt; done. Of course, there’s going to be more data in our case. The Prototype Our glorious application will be called ‘muumuu’. Whenever I don’t exactly know where I’m going, I decide to prototype stuff. And here I stress the importance of prototype. Despite this fact, it will often end up being in production, but yeah – that’s to be avoided. I decide to start with the basic stuff to prototype, state transitions. I go for them in a fairly simple manner, top-down: -module(muumuu_fsm). -export([start/0]). -define(MAX_NO_VENT, 5). start() -&gt; %% Seed PRNG &lt;&lt;A:32, B:32, C:32&gt;&gt; = crypto:rand_bytes(12), random:seed(A,B,C), wait_any_key(). %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%% States and Transitions %%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% wait_any_key() -&gt; io:get_line(&#34;To Start, Press Any Key.\n&gt; &#34;), first_core_check(). first_core_check() -&gt; case option(&#34;Check core temperature?&#34;) of yes -&gt; core_temperature(); no -&gt; noop() end, first_gas_vent(). first_gas_vent() -&gt; case option(&#34;Vent radioactive gas?&#34;) of yes -&gt; blow_crops_away(); no -&gt; venting_prevents_explosions() end, wait_for_command(). wait_for_command() -&gt; case wait_cmd(10000) of timeout -&gt; {Opt, Yes, No} = random_option(), case option(Opt) of yes -&gt; Yes(); no -&gt; No() end; Cmd -&gt; case match_option(Cmd) of {_, Yes, _} -&gt; Yes(); _ -&gt; noop end end, wait_for_command(). In this bit of code, we can see our 4 main states: wait_any_key first_core_check first_gas_event wait_for_command The rest of the code is more or less going to be events and input management to check the transitions: printing questions and getting responses (option/1) eventually waiting for a command (wait_cmd/1 and match_option/1) or, if it takes too long, generate an option randomly (random_option/1) You can look at the code, find whatever you want about it disgusting. So that’s the general idea I want in the code. Time to add all that option management stuff: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%% Options and Response Handling %%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% option(Prompt) -&gt; show_option(Prompt), Data = io:get_line(&#34;&#34;), case iolist_to_binary(Data) of &lt;&lt;&#34;Y&#34;, _/binary&gt;&gt; -&gt; yes; &lt;&lt;&#34;y&#34;, _/binary&gt;&gt; -&gt; yes; &lt;&lt;&#34;N&#34;, _/binary&gt;&gt; -&gt; no; &lt;&lt;&#34;n&#34;, _/binary&gt;&gt; -&gt; no; _ -&gt; ambiguous end. show_option(Str) -&gt; io:format(&#34;~s (Y/N)~n&gt; &#34;, [Str]). wait_cmd(Timeout) -&gt; Parent = self(), Pid = spawn(fun() -&gt; Parent ! io:get_line(&#34;&#34;) end), receive Data -&gt; Data after Timeout -&gt; exit(Pid, kill), timeout end. random_option() -&gt; Pos = random:uniform(tuple_size(opts())), {_, Val} = element(Pos, opts()), Val. match_option(Data) -&gt; case [Vals || {Pattern, Vals} &lt;- tuple_to_list(opts()), nomatch =/= re:run(Data, Pattern, [caseless])] of [Opt|_] -&gt; Opt; [] -&gt; invalid_opt end. Cool. Not fantastic looking yet. Basically, an option will only fetch a line of text entered by the user, look at the first response, and return what it is. Showing the options just wraps things up so they look like a prompt. Interestingly enough, the command has to be waited for in a different process. The problem with this it that Erlang’s standard library doesn’t support a timeout mode for io operations, which would tell us “wait 10 seconds for input or quit”. Therefore, there is a need to move this to a process. The rest relies on an elusive opts() function that apparently returns all questions and options offered to the user: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%% Defining Options/Events %%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% opts() -&gt; {{&#34;(check|core|temp)&#34;, {&#34;Check core temperature?&#34;, fun core_temperature/0, fun noop/0}}, {&#34;(vent|rad|gas)&#34;, {&#34;Vent radioactive gas?&#34;, fun vent_gas/0, fun no_venting/0}}, {&#34;(sound|alert|horn)&#34;, {&#34;Sound alertness horn?&#34;, fun sound_horn/0, fun noop/0}}, {&#34;(calc|duct)&#34;, {&#34;Decalcify calcium ducts?&#34;, fun noop/0, fun noop/0}}}. This basically is a tuple (I use a tuple because it makes random selection with a fixed position more efficient) of all questions, positive and negative response and consequences, paired up with a regular expression that represents fuzzy matching – for example, someone typing it check temperature should match Check core temperature? as a question, and return both options. The code back in wait_for_command/0 will only execute the core_temperature/0 function. Finally, all actions and consequences can be implemented: noop() -&gt; ok. venting_prevents_explosions() -&gt; case option(&#34;Venting prevents explosion.&#34;) of yes -&gt; blow_crops_away(); no -&gt; noop() end. core_temperature() -&gt; io:format(&#34;Core temperature normal.~n&#34;). blow_crops_away() -&gt; io:format(&#34;*Gas blows away corn crop*~n&#34;). sound_horn() -&gt; io:format(&#34;*horn sounds in the distance*~n&#34;). pressure_too_high() -&gt; io:format(&#34;Pressure too high. Tank must be shut down manually.~n&#34;). vent_gas() -&gt; %% After ?MAX_NO_VENT, pressure has to be shut down %% manually -- unsupported in this here program! case get(missed) of ?MAX_NO_VENT -&gt; pressure_too_high(); _ -&gt; put(missed, 0), blow_crops_away() end. no_venting() -&gt; case get(missed) of undefined -&gt; put(missed, 1); N -&gt; put(missed, N+1) end. Here the two last functions implement the special last requirement: after denying venting too many times, the valve must be disabled manually. Here we use a dirty ugly counter for prototyping’s sake. In fact I had forgotten about that requirement at the time and just bolted it on that way. The prototype helped figure that requirement out, and the final version can now be designed with this in mind. You can run the code and try it from a shell: λ → erlc src/muumuu_fsm.erl &amp;&amp; erl -s muumuu_fsm -noshell To Start, Press Any Key. &gt; . Check core temperature? (Y/N) &gt; N Vent radioactive gas? (Y/N) &gt; No Venting prevents explosion. (Y/N) &gt; yes *Gas blows away corn crop* Sound alertness horn? (Y/N) &gt; Y *horn sounds in the distance* That works. Using -s &lt;module&gt; runs the start/0 function from that module, and using -noshell makes it so that the Erlang VM won’t fight with all the io calls I’m doing for user input ownership. Sadly, the implementation is kind of ugly and shouldn’t go in production. Making it a library There are two ways to make something reach production: distributing yourself, or distributing it as a library other Erlang developers can use. The latter can be a prerequisite for the former, so we’re going to start there. By default, everyone using Erlang in the open source community uses OTP applications. OTP is kind of often treated as a super advanced topic, so what I’m gonna show here is how to take any non-OTP compliant code and turn it into an OTP application. Fun fun. First, the directory structure: src/ - muumuu_fsm.erl That’s all you need in terms of structure if you have rebar3 installed in your system. Add a file in src/ called muumuu.app.src. This file is basically telling Erlang (and rebar3) what the library is: {application, muumuu, [ {description, &#34;Too fat to go to the power plant app&#34;}, {vsn, &#34;0.1.0&#34;}, {registered, []}, {applications, [kernel, stdlib, crypto]}, {mod, {muumuu_app, []}}, {env, []} ]}. The registered entry specifies what processes are going to be globally registered on the node. In this case, none. The applications tuple is a list of all applications we depend on. All applications depend on both kernel and stdlib. These entries have to always be in there. On the other hand, crypto is optional to most apps, but we need it because we use it to seed our pseudo-random number generator in start/0. The env tuple can contain configuration values, but we need none right now. The other option considered here is mod. If your library requires no process to be started and you’re just shipping code around, you’re done. In our case however, we’re starting a process (or we want to), and therefore we specify an application module named muumuu_app. This module is also in src/: -module(muumuu_app). -behaviour(application). -export([start/2, stop/1]). start(_Type, _Args) -&gt; muumuu_sup:start_link(). stop(_) -&gt; ok. That module is basically giving callbacks to the Erlang VM. See it a bit as the main function in C, except you also have to provide a stop function that will clean up once the process exits. In this case we need nothing. What’s the muumuu_sup module? That’s the final step to be glued in OTP. OTP has a concept called supervisors. Supervisors are in charge of checking OTP-compliant processes, to start them, stop them, and provide guarantees regarding their state. Unfortunately, our process isn’t OTP-compliant. The guys at Ericsson have long ago hit that problem and developed a supervisor bridge, which basically acts as a wrapper. This is what we could use if I were not the kind of person to want my OTP processes done correctly everywhere. For the time being, I’ll stick with a regular supervisor and will rewrite the FSM right after: -module(muumuu_sup). -behaviour(supervisor). -export([start_link/0]). -export([init/1]). start_link() -&gt; supervisor:start_link(?MODULE, []). init([]) -&gt; {ok, {{one_for_one, 1, 5}, [{console, {muumuu_fsm, start_link, []}, permanent, 5000, worker, [muumuu_fsm]}]}}. This will start muumuu_fsm as a permanent worker that can die once every 5 seconds before the entire system crashes. I don’t have a good way to pick frequencies, but 1 in 5 seconds sounds like something reasonable for someone to mash keys in ways bad enough it causes errors. So then comes the rewrite from prototype to gen_fsm. This is stuff that has been covered in multiple tutorials before, so I’m going to skip most of it. You can instead look at books and docs for gen_fsm, follow along the final module, muumuu_fsm.erl, and see for yourself. The biggest changes there, outside of providing the gen_fsm callbacks required by the OTP behavior, are related to the general information flow. Rather than being really direct sequences of functions doing whatever they want, the OTP version of the module becomes a lot more declarative. We no longer enter a state function, ask a question, and wait for the response within the same context. The logic has moved so that an event in a state (say first_gas_vent) causes a question to be asked before transitioning to the state that will handle that response. This doesn’t make the code particulalry harder to read, just different: init([]) -&gt; &lt;&lt;A:32, B:32, C:32&gt;&gt; = crypto:rand_bytes(12), random:seed(A,B,C), {ok, wait_any_key, prompt(wait_any_key, #state{})}. %% [...] wait_any_key(_, State) -&gt; {next_state, first_core_check, prompt(first_core_check, State)}. first_core_check(no, State) -&gt; {next_state, first_gas_vent, prompt(first_gas_vent, State)}; first_core_check(yes, State) -&gt; show_core_temperature(), {next_state, first_gas_vent, prompt(first_gas_vent, State)}. first_gas_vent(no, State) -&gt; StateName = venting_prevents_explosions, {next_state, StateName, prompt(StateName, State)}; first_gas_vent(yes, State) -&gt; show_blow_crops_away(), {next_state, wait_for_command, prompt(wait_for_command, State), 10000}. This form, along with the experience gained in the prototype, allows for simpler state management via the State variable, which allows us to be more transparent about our usage of venting limits, for example. We also instantly benefit from everything OTP gives us in terms of transparency: tracing, logging, statistics, and so on (see the sys module) With that code in place, we can compile and run the entire application: λ → rebar3 compile ===&gt; Verifying dependencies... ===&gt; Compiling muumuu With this compiled we can run it, with a funky command: λ → erl -env ERL_LIBS _build/default/lib -eval &#39;application:ensure_all_started(muumuu).&#39; -noshell To Start, Press Any Key. &gt; any Check core temperature? (Y/N) &gt; y Core temperature normal. Vent radioactive gas? (Y/N) &gt; y *Gas blows away corn crop* That’s kind of an ugly command to run the app, but the app is now something other people can use to pull it within their own systems. In order to run it ourselves and actually ship it to customers, we will need to build a release. In any other case, though, you may want to publish your library as a Hex package with the help of the proper rebar3 plugin. Releases The directory structure we’ve been using was for an application and turns out looking like: src/ ebin/ At the simplest level. A release is basically a group of applications put together. For this reason, we’ll change the directory structure a bit: apps/ - muumuu/ - src/ - ebin/ rebar.config All applications you need will go into apps/. Here I just moved src/ to apps/muumuu/. The rebar.config file looks like this: {relx, [ {release, {muumuu, &#34;1.0.0&#34;}, %% list of apps to include [muumuu]}, %% Don&#39;t ship an Erlang VM by default {include_erts, false} ]}. {profiles, [ %% called as `rebar3 as prod &lt;command&gt;` {prod, [ {relx, [ % override relx specifically {include_src, false}, % don&#39;t include source code {include_erts, true} % include the VM in the release ]} ]} ]}. This basically just tells rebar3 what the release-building tool it includes (relx) should do to give us our release. The release will only include our custom Erlang code, and use the currently installed Erlang VM to run things rather than installing a fully self-contianed program. Then the magic happens: λ → rebar3 release ===&gt; Verifying dependencies... ===&gt; Compiling muumuu ===&gt; Starting relx build process ... ===&gt; Resolving OTP Applications from directories: /Users/ferd/code/self/howistart-erlang1-code/release/_build/default/lib /Users/ferd/code/self/howistart-erlang1-code/release/apps /Users/ferd/.kerl/builds/17.4/release_17.4/lib ===&gt; Resolved muumuu-1.0.0 ===&gt; release successfully created! And a release is born! To run it: λ → ./_build/default/rel/muumuu/bin/muumuu -noshell To Start, Press Any Key. &gt; Pretty cool. This can now be shipped and distributed to people. I want to make the release a bit fancier though. As you’ve just seen, we still need to put the -noshell by hand, which is totally unacceptable. To fix this, add a config/ repository, and I open the vm.args file in vim in there: # only show the programmed prompt -noshell # for remote access &amp; debugging -name [email protected] # not needed -smp disable +A 1 Arguments in there I merged into one. A good practice for any Erlang system is to give it a name, which will let you connect to it while it’s running. In this case I could go in and debug the console as the user is maintaining the powerplant. The last arguments (-smp disable +A 1) are basically optimizations for this very app: they remove Erlang parallelism (I’m running a single active process for the thing, so why bother?) and removes the number of asynchronous threads for IO to a single one (for the same reason – one active process, why bother?). In more serious apps, tweaking your VM options can be worthwhile, but outside of this text’s scope. The rebar3 config file needs an update too: {relx, [ {release, {muumuu, &#34;1.0.0&#34;}, %% list of apps to include [muumuu]}, %% Don&#39;t ship an Erlang VM by default {include_erts, false}, {vm_args, &#34;./config/vm.args&#34;} ]}. {profiles, [ %% called as `rebar3 as prod &lt;command&gt;` {prod, [ {relx, [ % override relx specifically {include_src, false}, % don&#39;t include source code {include_erts, true} % include the VM in the release ]} ]} ]}. The last line above the profiles is the new one. Compile again and the arguments should implicitly be passed to the node: λ → rebar3 release ===&gt; Verifying dependencies... ===&gt; Compiling muumuu ===&gt; Starting relx build process ... ===&gt; Resolving OTP Applications from directories: /Users/ferd/code/self/howistart-erlang1-code/release/_build/default/lib /Users/ferd/code/self/howistart-erlang1-code/release/apps /Users/ferd/.kerl/builds/17.4/release_17.4/lib /Users/ferd/code/self/howistart-erlang1-code/release/_build/default/rel ===&gt; Resolved muumuu-1.0.0 ===&gt; release successfully created! λ → ./_build/default/rel/muumuu/bin/muumuu To Start, Press Any Key. &gt; &lt;Tab&gt; Check core temperature? (Y/N) &gt; Cool, everything works. I now have a binary executable I can link to from anywhere in the system and will require no magical arguments to work! Tests As much as I like to try and get testing done ahead of time – it’s the only time it’s not super terrible and crappy – I often end up adding it after the fact when I know I’ll have to maintain it. For this, each app should have its tests, so I’ll have to add a test/ directory in apps/muumuu/. My tool of choice is Common Test, which while it is kind of full of annoying overheads for unit testing and is mostly useless for shell output (you gotta deal with HTML files), it scales fairly well for integration and system tests. The test suite in there is going to be muumuu_SUITE.erl: -module(muumuu_SUITE). -include_lib(&#34;common_test/include/ct.hrl&#34;). -compile(export_all). %% Copy/pasting from the suite -record(state, {no_vent_count=0, pid, yes, no}). all() -&gt; [demo_session]. So at first I’m just gonna make one run-through test. Testing muumuu is going to be hard because it’s purely a side-effectful application. Before going further, I’ll say that the trick to getting this working is to use meck, which is pretty much the best code-mocking application around. Adding meck can be done by declaring rebar.config dependencies: {profiles, [ {test, [ {deps, [ {meck, &#34;0.8.2&#34;} ]} ]}, %% called as `rebar3 as prod &lt;command&gt;` {prod, [ ... ]} ]} ]}. Note that rather than having a top-level deps entry as we usually would, we define this one to be into the test profile. This will allow the dependency to only be fetched and used when running tests, and to avoid bundling it when shipping the application. Rebar3 pulls stuff from a package repository for this one (github dependencies are also an option). Rebar3 will add it to a lock file when it fetches and compiles it later. Now back to muumuu_SUITE. Time to set up the state: init_per_testcase(demo_session, Config) -&gt; mock_io(), {ok, Pid} = muumuu_fsm:start_link(), [{pid, Pid} | Config]. end_per_testcase(_, Config) -&gt; meck:unload(io), Pid = ?config(pid, Config), unlink(Pid), exit(Pid, shutdown), wait_for_death(Pid). Mocking the io system is a fun way to basically take it and make it return messages we can look at. That all takes place in mock_io(), and after that’s in place, we start a muumuu instance directly (no application needed): mock_io() -&gt; %% For this one we mock the IO system so that instead of %% printing messages and getting input to and from the user, %% we instead have a message-passing interface that will %% be inspectable. %% %% Note that because the `io` module is pre-compiled by the %% VM, we have to &#39;unstick&#39; it first, and be careful to keep %% it mocked as little as possible. Parent = self(), code:unstick_dir(filename:dirname(code:where_is_file(&#34;io.beam&#34;))), meck:new(io, [passthrough, no_link]), meck:expect(io, format, fun(Str) -&gt; Parent ! {out, Str}, ok end), meck:expect(io, format, fun(Str, Args) -&gt; Parent ! {out, io_lib:format(Str,Args)}, ok end), meck:expect(io, get_line, fun(_Prompt) -&gt; Parent ! {in, self()}, receive {Parent, In} -&gt; In end end). Ugly. The first step is unstickying the directory for Erlang code. Most modules don’t require that, only those in Erlang’s standard library. Unstickying allows to load new versions of code at run time, which meck dynamically does. Here what I’m doing is mocking the functions io:format/1, io:format/2 and io:get_line/1 to send messages of the form {in, Msg} and {out, Msg} from input and output, respectively. meck:unload(io) will undo that. We also had the wait_for_death/1 call. I’m using these everywhere in tests. Timers are the enemy of good concurrent testing, and if you rely on a timer:sleep(1000) of some sort to make sure everything is clean, you’re doing it wrong. Here the function polls to return ASAP, with a tiny sleep to not heat up your room too much via the CPU: wait_for_death(Pid) -&gt; case is_process_alive(Pid) of true -&gt; timer:sleep(10), wait_for_death(Pid); false -&gt; ok end. With this done, I can start planning more for the test. This here is something I always want to write a library for, and maybe some day I will, but right now I re-do that crap by hand every time: %%%%%%%%%%%%%%%%%% %%% TEST CASES %%% %%%%%%%%%%%%%%%%%% %% Pressing a given key through the message-passing interface %% will yield expected output. There should be a prompt waiting %% for a key. %% All states can be cycled through using only Y/N answers. demo_session(Config) -&gt; Pid = ?config(pid, Config), out(&#34;press.*any.*key.*&gt;&#34;), in(&#34;&lt;tab&gt;&#34;), % the characters shouldn&#39;t matter out(&#34;check.*core.*temp.*&gt;&#34;), in(&#34;Y&#34;), out(&#34;temperature.*normal&#34;), out(&#34;vent.*radioactive.*gas.*&gt;&#34;), in(&#34;no&#34;), out(&#34;venting.*prevents.*explosion.*&gt;&#34;), in(&#34;yES&#34;), out(&#34;gas.*blows.*crop.*&#34;), gen_fsm:send_event(Pid, timeout), % force a timeout faster out(&#34;.*Y/N.*&gt;&#34;), % some question in(&#34;No&#34;), % who cares in(&#34;vent gAs&#34;), % force a command out(&#34;gas.*blows.*crop.*&#34;). I basically just write the test the way I want it to look like. I will start expecting messages that will match the regex &#34;press.*any.*key.*&gt;&#34; being output, after which I’ll insert &lt;tab&gt;. Rinse and repeat. Here, my desire is pretty much to turn the interactions I’d write in the shell into a bunch of function calls and matches. That’s why I planned having a message-passing interface. I can now write functions to wrap that functionality: %%%%%%%%%%%%%%% %%% HELPERS %%% %%%%%%%%%%%%%%% in(Input) -&gt; receive {in, Pid} -&gt; Pid ! {self(), Input} after 1000 -&gt; ct:pal(&#34;MBOX: ~p&#34;, [process_info(self(), messages)]), error({too_long, {in, Input}}) end. If we look back into the mocked function, the mocked function sends us {in, ProcessThatWaitsForInput}. We take the Input argument, and send it back to the mocked function (which runs in its own process). If we never receive the in message, we crash, but printing the debugging information. Interestingly here the function I use is ct:pal. It works exactly like io:format, except: It outputs to both the shell and HTML logs for Common Test It’s not gonna be used in production systems and it’s surely never going to be mocked (unlike io). The out/1 helper is slightly more complex: %% fuzzily match the input string, waiting 1s at most out(Expected) -&gt; receive {out, Prompt} -&gt; ct:pal(&#34;Expected: ~p~nPrompt: ~p&#34;, [Expected, Prompt]), {match, _} = re:run(Prompt, Expected, [dotall, caseless, global]) after 1000 -&gt; ct:pal(&#34;MBOX: ~p&#34;, [process_info(self(), messages)]), error({too_long, {out, Expected}}) end. That one makes an assertion on a regular expression with re:run/3, and the rest is similar to what we did in in/1. We receive the output, match it, and that’s it. And there we go, we can run the tests: λ → rebar3 ct → rebar3 ct ===&gt; Verifying dependencies... ===&gt; Fetching meck ({pkg,&lt;&lt;&#34;meck&#34;&gt;&gt;,&lt;&lt;&#34;0.8.2&#34;&gt;&gt;}) ===&gt; Compiling meck ===&gt; Compiling muumuu ===&gt; Running Common Test suites... &lt;test output omitted&gt; All 1 tests passed. After this, I check in the rebar lock files into version control, and I go do something else because I’m pretty much done. You can see all the code here. </description>
      <pubDate>24 Mar 20 12:29 EDT</pubDate>
      <guid>https://howistart.org/posts/erlang/1/</guid>
    </item>
    <item>
      <title>The Drenching Richness of Andrei Tarkovsky</title>
      <link>https://www.newyorker.com/magazine/2021/02/15/the-drenching-richness-of-andrei-tarkovsky</link>
      <description>&lt;a href=&#34;https://www.newyorker.com/magazine/2021/02/15/the-drenching-richness-of-andrei-tarkovsky&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;While Tarkovsky was pondering his next project, he saw Stanley Kubrick’s “2001: A Space Odyssey,” which he both disliked and envied. He set about making “Solaris” (1972), his own attempt at transcendental science fiction. The source was the eponymous novel by the Polish sci-fi writer Stanisław Lem, in which a sentient ocean planet invades the consciousness of human visitors and drives them mad. Unlike Kubrick, Tarkovsky showed little interest in the mechanics of space travel, dwelling instead on the haunted memories and unresolved conflicts of his protagonist. (Steven Soderbergh’s 2002 remake, also titled “Solaris,” is more faithful to Lem’s text.) Hallmarks of the later Tarkovsky come to the fore, for better or for worse: majestic long takes, rambling philosophical dialogues, extended scrutiny of classic art works, bouts of Bach on the soundtrack. The lead actor, Donatas Banionis, is all too palpably trying to figure out what kind of movie he is in.Tarkovsky was probably right when he named “Solaris” his weakest film, but it is transfixing all the same. As Julia Shpinitskaya points out in “ReFocus,” Tarkovsky almost emulates Kubrick in a nearly five-minute-long sequence that consists largely of highways and tunnels as seen from a moving car. A thick overlay of electronic sound, fashioned by the composer Eduard Artemyev, helps transform the footage into a voyage no less mind-bending than the one at the climax of “2001.” By the end of “Solaris,” Banionis seems to have returned to a country house on Earth, but increasingly lofty vantage points reveal that he is on an island in the seething Solaris ocean. Bach’s chorale prelude “Ich ruf zu dir” gives way to a cataract of noise.“My aim is to place cinema among the other art forms,” Tarkovsky wrote in his diaries. “To put it on a par with music, poetry, prose, etc.” He fulfilled that ambition spectacularly in “Mirror,” which came after “Solaris.” A deeply personal work that re-creates scenes from Tarkovsky’s childhood in fanatical detail, “Mirror” is at the same time a tour-de-force assemblage of stream-of-consciousness memories, dreamscapes, paranormal occurrences, poetry recitations, and grainy newsreel footage. Watching it is like attending a séance of the twentieth-century Russian soul. The first time I saw “Mirror,” I experienced it as a gorgeous, sensuous bewilderment. It was equally rewarding to watch the restored film in conjunction with Johnson and Petrie’s fastidious analysis. “Mirror,” like “Ulysses” or “The Waste Land,” is the kind of work for which you welcome a guide.The cinematographer for “Mirror” was Georgy Rerberg, who had a knack for making drab interiors and dusky landscapes shimmer with unseen forces. From the start, irrational events ensue: a barn bursts into flame, a jug crashes to the floor, ghostly presences materialize, people levitate. Heightening the uncanny atmosphere, the actor Margarita Terekhova plays two distinct characters: one based on Maria Tarkovskaya, Tarkovsky’s mother, and the other based on Irma Raush, his first wife. Tarkovskaya is also cast as herself, in scenes set in the present day. At the end, Tarkovsky creates chronological pandemonium by having his mother share the frame with a representation of her much younger self. The situation is ripe for psychoanalysis, which the filmmaker and historian Evgeny Tsymbal, once Tarkovsky’s assistant, supplies in “ReFocus.” One has the sense that Tarkovsky held his mother partially responsible for his father’s departure, and that this feeling perhaps became a source of his warped attitudes toward women. But the film transcends the director’s misogyny on the strength of Terekhova’s expressively harried performance. She holds fast against the tide of male neurosis rising around her.“Stalker,” Tarkovsky’s final Russian film, has become his most celebrated work, almost a pop-culture phenomenon. It has inspired a brilliant free-associative study by Geoff Dyer—“Zona,” from 2012—as well as a series of first-person-shooter video games. In Tallinn, Estonia, where much of the film was shot, you can take a Tarkovsky-themed bike tour. The cult of “Stalker” is surprising, because, at first encounter, it is the most cryptic of Tarkovsky’s hieroglyphs. Based on Arkady and Boris Strugatsky’s sci-fi novel “Roadside Picnic,” it contrasts an ashen outer world with an eerily verdant place known as the Zone, which appears to have been visited by aliens. Inside the Zone is the Room, where all wishes are said to come true. Although military guards shoot at anyone who tries to enter the Zone, guides known as “stalkers” lead illegal tours. The film follows three men named Stalker, Professor, and Writer, who are played with laconic grit by Alexander Kaidanovsky, Nikolai Grinko, and the hypnotic, hooded-eyed Solonitsyn. Their inching progress across booby-trapped, supernatural terrain unfolds like a slow-motion, hyper-abstract thriller—a zombie apocalypse without zombies.Nothing in Tarkovsky’s work has elicited more awestruck comment than the sequence in which the travellers pass into the Zone. Claire Denis, in conversation with the director Rian Johnson, said of this moment, “I remember I thought I was going to faint. My heart stopped beating for a second.” The first part of the movie, which shows Stalker leaving home and meeting his clients, is shot in desiccated sepia tones. The trio makes it past the guards and travels toward the Zone on railroad tracks, riding a motorized flatcar. A numbing series of shots of irregular length—forty seconds, ninety-six seconds, seven seconds, seventeen seconds, sixty-two seconds—fixate on the sides and backs of the men’s heads, giving only vague glimpses of the surrounding terrain. The clanking of wheels is at first percussively harsh and then fades into an electronic blur. In an abrupt cut, color replaces sepia, and we find ourselves in a landscape of dark-green vegetation, skewed telephone poles, and abandoned vehicles—a leap into a post-human paradise. The flatcar glides to a halt as the men gaze, rapt. It is, Tarkovsky scholars point out, a bleak homage to “The Wizard of Oz.” As with the censer shot in “Rublev,” the sudden absence of motion generates a kind of internal vertigo, accentuated by an onrush of silence.Pontara, in his absorbing study of Tarkovsky’s use of music and sound, shows how much of the spell of “Stalker” depends on its extraordinary audio track. Artemyev, who specialized in electronic composition before collaborating with Tarkovsky, devises a seething soundscape in which otherworldly ditties alternate with upwellings of noise. Tarkovsky throws in some classical selections, but they are alienated from their usual ennobling role. When, in the scenes set in Stalker’s home, trains rumble past, railway sounds intermingle with faintly audible strains of “La Marseillaise,” Wagner’s “Tannhäuser” overture, and Beethoven’s Ninth Symphony. Landmarks of Western music are reduced to technological detritus. Pontara suggests plausibly that Tarkovsky is exposing the catastrophic failure of industrial and cultural progress alike.The final scenes bring tremors of hope. Although the travellers return from their journey without having dared to enter the Room, alterations in the film stock imply that they have smuggled out some essence of the Zone: a touch of color seeps into the sepia wasteland. In a shiver-inducing epilogue, we learn that Stalker’s disabled young daughter, Monkey, has developed occult gifts. Just before the aural train wreck of Beethoven’s Ninth, she telekinetically pushes a glass off a table. Pontara points out the ideological problem underlying this concluding wonder: in place of failed Romantic aesthetics, Tarkovsky substitutes his own heroic gesture of transcendence. “Stalker” ends up reaffirming, in Pontara’s words, “the false promise that we can escape from and step outside of history and civilization.”By the time “Stalker” was released, in 1979, Tarkovsky had become the most internationally celebrated of Soviet filmmakers, but he still faced bureaucratic interference at home. Constraints on his artistic freedom angered him; so did the persecution of Parajanov, a favorite colleague, on anti-gay grounds. (Johnson and Petrie say that Tarkovsky himself was not exclusively straight.) He took up residence in Italy in 1982 and announced his exile two years later. Anticipating this decision, the regime had refused to allow his son Andrei to leave the country. Within two years, perestroika had changed the Soviet cultural atmosphere, but it came too late for Tarkovsky. In 1986, as he was dying of cancer, Mikhail Gorbachev intervened to allow the younger Andrei to go see his father. The French writer and filmmaker Chris Marker was on hand to witness the reunion; heartbreaking footage of a frail Tarkovsky embracing his son appears in Marker’s 1999 documentary, “One Day in the Life of Andrei Arsenevich.”Tarkovsky completed two feature films during his years abroad: “Nostalghia,” made in Italy in 1982 and 1983, and “The Sacrifice,” shot in Sweden in 1985. He enjoyed more creative freedom, but financing was a challenge, and he had lost the network of collaborators who enabled his middle-period masterpieces. Some critics hail these final works as a supreme revolt against cinematic convention; others detect symptoms of mannerism and decline. Both films bewitched me when I first saw them, but I’m now inclined to agree with Dyer, who comments that, after “Stalker,” Tarkovsky fell into self-imitation: “The guru became his own most devoted disciple.”The long takes grow liturgical in manner. At the end of “Nostalghia,” the protagonist, a Russian travelling in Italy, spends nine minutes attempting to carry a lit candle across the length of an empty mineral pool, believing that he will thus avert the end of the world. He then falls dead, and there follows an awesome vision of a Russian dacha nestled within a medieval Italian abbey. “The Sacrifice” stages a similar ritual of world redemption: a Swedish intellectual becomes convinced that if he sleeps with a local witch he will undo an apparent nuclear war. His bargain also involves the burning of his island home—a six-minute take that consummates Tarkovsky’s motif of immolation.These images are as grandly dumbfounding as any that have been put on film, yet the surrounding narratives are thin. The Swedish actor Erland Josephson, a mainstay of Bergman’s troupe, appears in both “Nostalghia” and “The Sacrifice,” and invests his divine-madman roles with emotional conviction. But other actors struggle—especially the women. Domiziana Giordano, in “Nostalghia,” and Susan Fleetwood, in “The Sacrifice,” are obliged to enact prolonged scenes of female hysteria. A dark aspect of Tarkovsky’s critique of industrial modernity manifests itself: the reversion to a pre-modern order brings with it a reinforcement of male dominance. In the Zone of “Stalker,” women disappear entirely, leaving only three men and a dog.Such regressive tendencies have left Tarkovsky open to appropriation by the pseudo-religious illiberal ideology that has asserted itself in Putin’s Russia. The director has attained a canonical position in his homeland; there is a statue of him outside V.G.I.K. and a monument in Suzdal. As Sergey Toymentsev notes, latter-day Russian critics have linked Tarkovsky to Eastern Orthodox theology. Toymentsev counters that, although Tarkovsky was fascinated by religious iconography, he described himself as an agnostic. “The one thing that might save us is a new heresy that could topple all the ideological institutions of our wretched, barbaric world,” he once declared. Nor did he espouse conventional nationalist views. In his diaries, he wrote, “Pushkin is superior to the rest because he did not give Russia an absolute meaning.”</description>
      <pubDate>15 Feb 21 10:53 EST</pubDate>
      <guid>https://www.newyorker.com/magazine/2021/02/15/the-drenching-richness-of-andrei-tarkovsky</guid>
    </item>
    <item>
      <title>Understanding BERT and Search Relevance</title>
      <link>https://opensourceconnections.com/blog/2019/11/05/understanding-bert-and-search-relevance/</link>
      <description>&lt;a href=&#34;https://opensourceconnections.com/blog/2019/11/05/understanding-bert-and-search-relevance/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; There is a growing topic in search these days. The hype of BERT is all around us, and while it is an amazing breakthrough in contextual representation of unstructured text, newcomers to natural language processing (NLP) are left scratching their heads wondering how and why it is changing the field. Many of the examples are tailored for tasks such as text classification, language understanding, multiple choice, and question answering. So what about just plain-old findin’ stuff? This article gives an overview into the opportunities and challenges when applying advanced transformer models such as BERT to search. What’s BERT and why is it important? BERT, which stands for Bidirectional Encoder Representations from Transformers is a deep learning architecture developed by Google for NLP. It is one of several approaches that leverages transformer architecture. Transformers address a gap in previous architectures such as recurrent and long short-term memory neural networks, with the key difference being the focus on maintaining attention during training using a bidirectional encoder. Plenty of articles have been written about this recently. So, I won’t dive into the details of how or why this works, but I’ve added links at the bottom for further reading if you want to learn more. The important part for the practitioner is that pre-trained models and open source libraries have been released to the public for use by anyone. You don’t need to train your own model and you can use these as they are, or for transfer learning (referred to as fine-tuning in BERT). What happens when you use these models is what we’ll focus on here. When you, for example, pass a document’s text through a pre-trained model using a transformer network, you get back a tensor, which is comprised of a vector representation for each token. One pre-trained large uncased model for BERT uses a feature vector of 768 floating point values. 768 features for a token, yielded from such a sophisticated model, contains a highly accurate dense contextual representation of the meaning of that token that can be further used. Also importantly, if the document has 234 words in it, you’ll get a tensor with the dimension of 234×768. If your document has 172 words, your tensor is 172×768, and so on. Traditional search in inverted indexes such as Lucene maintains zero context for each token – as all the words are all usually analyzed in isolation. Relevance engineers spend lots of time working around this problem. Without linguistic context, it is very difficult to associate any meaning to the words, and so search becomes a manually tuned matching system, with statistical tools for ranking. How can we use BERT for search? So what can you do with this tensor information? Well that’s the question at hand for many search engineers these days. We’ve been given this immensely powerful tool, and are trying to figure out how we can apply it to make relevance tuning easier and less prone to silly language issues we commonly face. Before you jump out of your chair shouting this is a solution looking for a problem, remember, the problem is that search in its current form has no connection to language and meaning. So we need to see how these two match up for a better future together. The main area of exploration for search with BERT is similarity. Similarity between documents for recommendations, and similarity between queries and documents for returning and ranking search results. Why? Because search relevance can be phrased as a similarity problem. What documents are most similar to what the user is trying to convey with their query? If you can use similarity to solve this problem with highly accurate results, then you’ve got a pretty great search for your product or application. Commonly, the approach is to use a nearest neighbor algorithm. This takes two or more vectors, and calculates the distance (or similarity) between them in an efficient manner. To use a simple example, let’s say we’ve got two people – Alice, standing 5 meters away from a water fountain, and Bob, standing 7 meters away from the water fountain. Who is standing closest to the fountain? Alice of course. Now Carrie joins the group and stands 8 meters from the fountain. So who is Carrie standing closest to? Alice or Bob? Well, that’s easy – it’s Bob. That’s similarity with a vector of one feature. We can also have a vector of two features, such as latitude and longitude coordinates, and use that calculate who is standing closest to the fountain and who are standing in groups close to each other. When we need to do this with more features, such as 3, 10, or even 768, across thousands or millions of words and documents, things get complicated. So we turn to approximate nearest neighbor algorithms to efficiently calculate this similarity as fast as possible across all these dimensions. And that brings us to where much of the recent practical development has been focusing on: Efforts to apply this type of nearest neighbor calculation to documents that we index in a search engine, and also the queries that people use in the search bar. Because if you can represent all your documents as rich sets of vectors that contain embedded meaning, and cross reference those with a query also represented by a rich set of vectors, you can see which documents are most similar to the query! Making progress, with some obstacles There’s some great stuff happening to make the above nearest neighbor tools available for use inside of search engines. Several approaches are being built to allow arbitrary vectors to be indexed inside of Lucene, and Vespa already supports indexing vectors. However, the sizes noted above are not really practical. For example, vectorizing short (three or four sentence) overview text data from 28000 movie documents balloons the representative size from 5MB to 5GB! That’s a whopping 1000x increase in size! And 28000 really isn’t very many documents. There are investigations underway to distill these huge document tensors into a more maintainable size. Areas of research include fine-tuning by using another model on top of BERT for a smaller representation, or just trying to average some of the dimensions together. The problem with these approaches is as you shrink or compress the size, you lose more of the context that the original model provided. So careful testing for each dataset needs to be performed to ensure the accuracy stays reasonable as the representation is compressed. This is also a very different way of doing things than most search teams are used to. Handling large models that need GPUs for effective speed, and querying across vectors instead of terms, requires a shift in technology, infrastructure, and practice. Also, debugging such a system becomes exceedingly difficult. These models are black boxes, and explainable AI is an open area of research that is out of reach for most practitioners. When you get a strange result from Lucene, you can dig down and see exactly why the result was returned. But when a document or query yields a tensor, knowing why that tensor was produced and what it means is more or less impossible. The problem with queries All the above is wonderful, but there’s another problem – how people search. Think yourself about how you search, when approached with a need to get information from a website or from a web search engine like Google. Do you type an elaborately crafted sentence as if you were asking another person? Of course not. You type one or two words, typically a noun phrase, for what you want to find. Don’t feel bad – everyone does this, even me! People usually don’t give enough context to search engines for most of their queries. But you can’t blame people for this problem – we’ve been doing this for years because that’s how search engines usually work. When you perform a similarity between those short queries and lots of documents, you are faced with the same age-old information retrieval problem: ambiguity. No matter how advanced your technology is, if people don’t provide enough context to search on, your engine is going to have a difficult time returning exactly what they were thinking. You will get documents back that reflect the meaning of the terms better, but they might not be ranked the way you expect. Tradeoffs and next steps Search is full of trade-offs. How much time can you reasonably spend to get search right? How much money is worth improving search for 10% of your queries? Everyone has deadlines. Everyone has budgetary restrictions. Lots of smart people are working hard to make this technology cheaply available and accessible to search teams who can use it to benefit customers. Keep an eye out for more updates, as this represents the biggest shift search has seen in years, so you’re likely to see many more articles, tutorials, software, and training soon. If you’re interested in learning about and potentially applying some of the techniques described above to empower your search team, please contact us! Further reading These links below provide more in depth information for the concepts explained above. Academic papers on BERT and attention models BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Attention Is All You Need The Annotated Transformer Articles explaining BERT in simpler overviews BERT explained Demystifying BERT Understanding searches better than ever before Libraries for using BERT and other transformers Huggingface Transformers Investigations of BERT’s true practicality NLP’s Clever Hans Moment Has Arrived Arbitrarily dense vector search https://github.com/o19s/hangryhttps://github.com/castorini/anserini/blob/master/docs/approximate-nearestneighbor.md https://arxiv.org/abs/1910.10208 https://github.com/jobergum/dense-vector-ranking-performance Tensors in Vespa Attributions Image by Pete Linforth </description>
      <pubDate>22 Apr 20 12:21 EDT</pubDate>
      <guid>https://opensourceconnections.com/blog/2019/11/05/understanding-bert-and-search-relevance/</guid>
    </item>
    <item>
      <title></title>
      <link>https://evjang.com/2021/12/17/lang-generalization.html</link>
      <description>&lt;a href=&#34;https://evjang.com/2021/12/17/lang-generalization.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In my essay “Just ask for Generalization”, I argued that some optimization capabilities, such as reinforcement learning from sub-optimal trajectories, might be better implemented by generalization than by construction. We have to generalize to unseen situations at deployment time anyway, so why not focus on generalization capability as the first class citizen, and then “just ask for optimality” as an unseen case? A corollary to this design philosophy is that we should discard inductive biases that introduce optimization bottlenecks for “the data sponge”: if an inductive bias turns out to be merely “data in disguise”, it may not only cease to provide a benefit in the high data regime, but actually hinder the model on examples where the inductive bias no longer applies. Pushing as much human-written code to “language + deep learning magic” sounds like a lot of fun, but how does a practitioner execute this “just-ask-for-generalization” recipe? If we want to infer optimal behavior via a deep neural net without training explicitly for it, we need to answer hard questions: given a model family and some training data, what kinds of test examples can we expect the model to generalize to? How much is “too much to ask for”? And how do you define generalization, anyway? ML theory gives us some basic definitions like generalization gap and excess risk (i.e. the difference between training and testing losses), but such definitions are not useful for estimating what it takes to achieve a qualitative degree of capability not seen in the training data. For example, if I am training a household robot to be able to wash dishes in any home, how many homes do I need to collect training data in before the learned policy starts to work in any kitchen? This practical question comes in many formal disguises: “What data is out-of-distribution?” “Is my model + data robust to adversarial examples?” “How can we train models to know what they don’t know?” “What is extrapolation?” Like the parable of the blind men and the elephant, computer scientists have come up with different abstract frameworks to describe what it would take to make our machines smarter: equivariance algebra, causal inference, disentangled representations, Bayesian uncertainty, hybrid symbolic-learning systems, explainable predictions, to name a few. I’d like to throw in another take on the elephant: the aforementioned properties of generalization we seek can be understood as nothing more than the structure of human language. Before you think “ew, linguistics” and close this webpage, I promise that I’m not advocating for hard-coding formal grammars as inductive biases into our neural networks (see paragraph 1). To the contrary, I argue that considering generalization as being equivalent to language opens up exciting opportunities to scale up non-NLP models the way we have done for language. Compositionality in Language Hupkes et al. 2020 discusses a few different aspects of “compositionality” in language models. Language is nothing more than the composition of a discrete set of tokens, so what the authors are really doing is specifying a grammar on how the smallest units of discrete meaning (words) fit together to form new meanings, i.e. the structure of language itself. Here is a table in which I’ve paraphrased the definitions and provided some training examples and test-time capabilities. Generalization Type Definition Training Examples Testing Examples Systematicity Recombine constituents that have not been seen together during training {“Bob ate pizza”, “Alice ran home”} “Bob ran home” Productivity Test sequences longer than ones seen during training books with 100k-200k words. books with 200k+ words. Substitutivity Meaning of an expression is unchanged if a constituent is replaced with something of the same meaning “bob ate pizza for lunch” “bob had Dominos at noon” taken to mean (almost) the same thing Localism The meaning of local parts are unchanged by the global context. Arithmetic tasks like {(5)-4, (2+3)} (2+3)-4 : (2+3) locally evaluates to 5, then 5-4 locally evaluates to 1. (2+3) representation not influenced by the presence of -4. Overgeneralization Correctly handle exceptions to rules and patterns {live-&gt;lived, laugh-&gt;laughed, love-&gt;loved} {kill-&gt;killed,break-&gt;broke (not breaked)} Compositionality in Everything Else Hupkes’ categorizations of compositionality can be applied to non-NLP domains as well. In “A Survey of Generalization in Deep Reinforcement Learning”, the authors provide an intuition of how these categorizations can be applied to a robotic block-stacking task: Systematicity - Stack blocks in new configurations not seen in training Productivity - Stacking more blocks than was done in training Substitutivity - Stacking blocks it hasn’t seen before (e.g. understanding that block color does not affect physical properties) Localism - Position of far-away objects do not affect behavior for stacking two blocks that are close together. Overgeneralization - the robot trains on stacking cubes, and knows not to stack a cylindrical block identically to the way it would stack a cube. None of the above tasks involve understanding language, and yet the structures underpinning generalization - systematicity, productivity, substitutivity, localism, overgeneralization - are found here as well. Perhaps we can cast other research around of “improving ML generalization” as special cases of language modeling. Consider “disentangled representations” research, whereby semantic attributes of data can be separately understood as discrete, standalone concepts. Your computer vision model can train on “green grass”, and “red apple”, and ideally would understand what “red grass” means even though it has never encountered that concept in the training data. If a “style” \(\in A\) and “shape” \(\in B\) vector are “disentangled representations”, then \(A \times B\) forms a simple vector space, or toy grammar, that your model ought to understand. We might combine the “red” concept vector with the “grass” concept vector and then decode it into an image with a conditional generative model \(p(\text{image} \vert a, b)\). In a robotic setting. we might train a robot that disentangles objects from skills, and specify goals by providing it with two inputs: a “skill categorical vector” (pick) and an “object categorical vector” (shoe). These sorts of simple two-word grammars are enough if you want to build a Face filtering app (e.g. combine me with “female face”) or a pick-and-place robot, but a logical step to furthering disentangled representations research is to combine “disentangled concepts” in much more open-ended, arbitrary ways beyond orthogonal attributes. Do you know what else are “disentangled, standalone concepts”? Words! If we venture away from toy compositional grammars towards the grammatical structure of natural language, we can now ask a generative model to “draw red grass where the sun is shining and purple grass where it is in shade and a horse eating the red grass”. We can tell a robot to “pick up the leftmost object that is not a cup.” Natural language permits us to do everything we can communicate to another person: embed logical predicates, fuzzy definitions, precise source code, and even supplement knowledge that the model does not know ahead of time (“Blickets are red with spikes, pick up a blicket”). Defining Fuzzy Concepts with Language Models The 5-way categorization introduced by Hupkes plausibly describes the basic structures of language, but there are limits to language-based reasoning. If a robot is trained to “stack many blocks”, and “wash a dish”, and we instruct it to “wash many dishes”, is that a test of systematicity (combining “many” and “dish” concepts from the training data)? Or is it testing productivity (repeated extension of the “wash a dish” task)? Does it really matter? Another ambiguous example: when testing productivity, language comes with a fairly obvious choice for the extrapolation axis: the number of tokens in the input sequence. But one could also imagine productivity being measured as the length of the output sequence (what is the longest story the book could write?), the depth of a parse tree (how many nested parentheses can your NLP model manage the scope for?), or any arbitrary semantic measure (what is the maximum number of characters the model can write a story about)? The distinctions between systematicity and productivity start to break down here again, especially when it comes to compositionality on higher-level concepts beyond the individual token level. As with all semantics, the precise boundaries of anything - even definitions around the basic structures of language itself - become fuzzy if you look too hard. It’s sort of like a Heisenberg’s Uncertainty principle for semantics. Unlike most formal grammars, natural language is capable of handling some fuzziness and ambiguity, insofar as it is good enough for human communication and survival. My analogy to the uncertainty principle (“sort of like…”) is a case in point. The best formal definition of an “image of a cat” we have today is a neural network classifier trained on a lot of human labels - a person simply can’t write down that definition in a sufficiently precise way. If defining cat images is best done from data and machine learning, then it begs the question of whether richer semantic ontologies (especially around generalization) are better defined from data as well. If a model understands human language well enough, then we can use it to venture beyond precise toy grammars into a truly vast, fuzzy space of capabilities like “please imitate another agent pretending to be you”, as I suggested in “Just ask for Generalization”. Bolt-on Generalization When it comes to combining natural language with robots, the obvious take is to use it as an input-output modality for human-robot interaction. The robot would understand human language inputs and potentially converse with the human. But if you accept that “generalization is language”, then language models have a far bigger role to play than just being the “UX layer for robots”. We should regard language capability as a substrate for generalization in any machine learning domain. Linguistic relativists say that language is not only the primary way we communicate to each other, it is also the way we communicate to ourselves when thinking. Language is generalization is cognition. We are still in the early days of imbuing our robots with evidence of linguistic relativity, but there are some exciting promising results in this direction. The paper Pretrained Transformers as Universal Computation Engines (Lu et al 21) showed that the internal structure of pre-trained language models can be frozen, and used as-is to perform a variety of non-language tasks, like numerical computation, protein folding, and vision. The internal representations are only trained on language domains, with the input and output embeddings re-learned for each fine-tuning task. This is wildly exciting, because it suggests that it might be possible to improve generalization simply by scaling up language data, rather than collecting a lot of task-specific data. Perhaps language models can imbue other ML models with systematicity, productivity, substitutivity in an infintiely-composable way, because they already acquired them when training on language datasets. More evidence for this idea comes from our recent BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning research paper, where we use language data in place of real robot data to get the robot to perform new tasks it hasn’t trained on. The robot never saw the bottle and tray in the same scene and was never trained on the “place bottle in the tray” task, and yet it can zero-shot the task. The policy merely conditions on pre-trained sentence embeddings, so the language model is doing the heavy lifting of task-level generalization. Interestingly enough, we also tried to teach the robot to generalize to the same held-out tasks by conditioning on task embeddings of humans performing the task. We struggled for years to get it working, and it wasn’t until we explicitly aligned the video embeddings to match language space that we were able to see held-out task generalization on human videos. There are some ML research tailwinds that make this idea of “bolt-on generalization from language models” increasingly easy to try. Transformers have been the architecture-of-choice for modeling language for a few years now, but in the last year we’ve seen a rapid consolidation of computer vision architectures around transformers as well. Today it is possible to train both state-of-the-art language and state-of-the-art vision models with pretty much the same architecture (e.g. ViT), so I think that we will start to see transformers trained for perception benchmarks start to re-use language datasets as an extra source of data. Here’s a prediction (epistemic confidence 0.7): within the next 5 years, we’ll see a state-of-the-art model for a computer vision benchmark that does not involve natural language (e.g. ImageNet classification), and that model uses knowledge from internet-scale natural language datasets (by either training directly on NLP datasets or indirectly via re-using an existing language model). A sketch of what I think neural architectures of the future will look like, inspired by the Universal Computation Engine and ViT papers: you have a large language model that acts as “bolt-on-generalization layer” for a target task domain, lightweight encoder layers to tokenize the input into something that can capture “word-level” semantics, and lightweight output decoders that transform the “generalization module” output tokens into the right prediction space. Additionally, because the core of the model is a standard transformer, it is simple to pass in additional natural language tokens for goal conditioning or extra human knowledge. Summary Language Models are far from perfect even when restricted to NLP tasks, and I don’t mean to suggest that they are ready today for solving ML once and for all. Rather, I am optimistic that language models will continue to get better, and with improved linguistic capability comes better generalization in other non-NLP domains. The structure of language is the structure of generalization. Formal grammars for language run up against a “semantic uncertainty principle”, so let’s rely on language models to represent language (and therefore, the structure of generalization). Let’s use large language models to “bolt on generalization” to non-NLP domains. Some preliminary evidence (the Universal Computation Engines paper) suggests that it can work. My friend Elijah invited me to give a talk at his company’s algorithms seminar and I shared an early draft of these ideas there. Here is a recording on YouTube. Q/A How does this relate to Chomsky’s ideas of innate language capability and Universal Grammars? Chomsky has a lot more to say than a one-line “language is innate”, but the gist is that humans are born with some innate linguistic capability. At face value, I agree with this since it follows from 1) linguistic relativity 2) equivalence between generalization and language 3) humans are born with some ability to generalize, even if what they are generalizing is their learning ability. Where there is more controversy is how much linguistic capability is innate, and whether learning is distinct from language. If you believe that generalization is language, then maybe it isn’t. Also, the degree to which a capability is innate tells us nothing of our ability to hard-code it correctly. For instance, humans may be innately primed to dislike insects and snakes, but we might have to resort to function approximation from data if we wanted to build such an innate prior. For me, it is less about what is genetically or developmentally innate in animals, but more of whether we want to hard-code with formal structures vs. acquiring the structure via function approximation. If linguistic capability is important for generalization, why not add more hard-coded linguistic rules into our neural networks so that they can perform more robust reasoning? Gary Marcus has an excellent quote from The Next Decade in AI: “The trouble is that GPT-2’s solution is just an approximation to knowledge, and not substitute for knowledge itself. In particular what it acquires is an approximation to the statistics of how words co-occur with one another in large corpora—rather than a clean representation of concepts per se. To put it in a slogan, it is a model of word usage, not a model of ideas, with the former being used as an approximation to the latter. Such approximations are something like shadows to a complex three-dimensional world” Where Marcus sees “meaning = co-occurence statistics” as problematic for the purposes of building robust AI systems, I see this as a preliminary vindication of the Distributional semantics hypothesis. The meaning of words are nothing more than how they are used. Even if there was meaning independent of anthropomorphic usage (e.g. the concepts of life and death probably mean something to most animals), humans lack the ability to implement those concepts formally. That’s not to suggest we should be content with defining everything as is found on the Internet, as some word co-occurences around race and gender and class are problematic for society. But it is helpful to understand that the meaning of words are derived from their usage, and not the other way around. </description>
      <pubDate>18 Dec 21 02:22 EST</pubDate>
      <guid>https://evjang.com/2021/12/17/lang-generalization.html</guid>
    </item>
    <item>
      <title></title>
      <link>https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</link>
      <description>&lt;a href=&#34;https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Fundamental Techniques of Feature Engineering for Machine LearningAll required methods for comprehensive data preprocessing with Pandas examples.IntroductionWhat is a feature and why we need the engineering of it? Basically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for feature engineering arises. I think feature engineering efforts mainly have two goals:Preparing the proper input dataset, compatible with the machine learning algorithm requirements.Improving the performance of machine learning models.The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.— Luca MassaronAccording to a survey in Forbes, data scientists spend 80% of their time on data preparation:Source: https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/This metric is very impressive to show the importance of feature engineering in data science. Thus, I decided to write this article, which summarizes the main techniques of feature engineering with their short descriptions. I also added some basic python scripts for every technique. You need to import Pandas and Numpy library to run them.import pandas as pdimport numpy as npSome techniques above might work better with some algorithms or datasets, while some of them might be beneficial in all cases. This article does not aim to go so much deep in this aspect. Tough, it is possible to write an article for every method above, I tried to keep the explanations brief and informative. I think the best way to achieve expertise in feature engineering is practicing different techniques on various datasets and observing their effect on model performances.List of Techniques1.Imputation2.Handling Outliers3.Binning4.Log Transform5.One-Hot Encoding6.Grouping Operations7.Feature Split8.Scaling9.Extracting Date1.ImputationMissing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.Some machine learning platforms automatically drop the rows which include missing values in the model training phase and it decreases the model performance because of the reduced training size. On the other hand, most of the algorithms do not accept datasets with missing values and gives an error.The most simple solution to the missing values is to drop the rows or the entire column. There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing values with higher than this threshold.threshold = 0.7#Dropping columns with missing value rate higher than thresholddata = data[data.columns[data.isnull().mean() &lt; threshold]]#Dropping rows with missing value rate higher than thresholddata = data.loc[data.isnull().mean(axis=1) &lt; threshold]Numerical ImputationImputation is a more preferable option rather than dropping because it preserves the data size. However, there is an important selection of what you impute to the missing values. I suggest beginning with considering a possible default value of missing values in the column. For example, if you have a column that only has 1 and NA, then it is likely that the NA rows correspond to 0. For another example, if you have a column that shows the “customer visit count in last month”, the missing values might be replaced with 0 as long as you think it is a sensible solution.Another reason for the missing values is joining tables with different sizes and in this case, imputing 0 might be reasonable as well.Except for the case of having a default value for missing values, I think the best imputation way is to use the medians of the columns. As the averages of the columns are sensitive to the outlier values, while medians are more solid in this respect.#Filling all missing values with 0data = data.fillna(0)#Filling missing values with medians of the columnsdata = data.fillna(data.median())Categorical ImputationReplacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection.#Max fill function for categorical columnsdata[&#39;column_name&#39;].fillna(data[&#39;column_name&#39;].value_counts().idxmax(), inplace=True)2.Handling OutliersBefore mentioning how outliers can be handled, I want to state that the best way to detect the outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a decision with high precision. Anyway, I am planning to focus visualization deeply in another article and let’s continue with statistical methodologies.Statistical methodologies are less precise as I mentioned, but on the other hand, they have a superiority, they are fast. Here I will list two different ways of handling outliers. These will detect them using standard deviation, and percentiles.Outlier Detection with Standard DeviationIf a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?There is no trivial solution for x, but usually, a value between 2 and 4 seems practical.#Dropping the outlier rows with standard deviationfactor = 3upper_lim = data[&#39;column&#39;].mean () + data[&#39;column&#39;].std () * factorlower_lim = data[&#39;column&#39;].mean () - data[&#39;column&#39;].std () * factordata = data[(data[&#39;column&#39;] &lt; upper_lim) &amp; (data[&#39;column&#39;] &gt; lower_lim)]In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation.Outlier Detection with PercentilesAnother mathematical method to detect outliers is to use percentiles. You can assume a certain percent of the value from the top or the bottom as an outlier. The key point is here to set the percentage value once again, and this depends on the distribution of your data as mentioned earlier.Additionally, a common mistake is using the percentiles according to the range of the data. In other words, if your data ranges from 0 to 100, your top 5% is not the values between 96 and 100. Top 5% means here the values that are out of the 95th percentile of data.#Dropping the outlier rows with Percentilesupper_lim = data[&#39;column&#39;].quantile(.95)lower_lim = data[&#39;column&#39;].quantile(.05)data = data[(data[&#39;column&#39;] &lt; upper_lim) &amp; (data[&#39;column&#39;] &gt; lower_lim)]An Outlier Dilemma: Drop or CapAnother option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it.#Capping the outlier rows with Percentilesupper_lim = data[&#39;column&#39;].quantile(.95)lower_lim = data[&#39;column&#39;].quantile(.05)data.loc[(df[column] &gt; upper_lim),column] = upper_limdata.loc[(df[column] &lt; lower_lim),column] = lower_lim3.BinningBinning illustration of numerical dataBinning can be applied on both categorical and numerical data:#Numerical Binning ExampleValue Bin 0-30 -&gt; Low 31-70 -&gt; Mid 71-100 -&gt; High#Categorical Binning ExampleValue Bin Spain -&gt; Europe Italy -&gt; Europe Chile -&gt; South AmericaBrazil -&gt; South AmericaThe main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized. (Please see regularization in machine learning)The trade-off between performance and overfitting is the key point of the binning process. In my opinion, for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of algorithms, due to its effect on model performance.However, for categorical columns, the labels with low frequencies probably affect the robustness of statistical models negatively. Thus, assigning a general category to these less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, it might be a good option to unite the labels with a count less than 100 to a new category like “Other”.#Numerical Binning Exampledata[&#39;bin&#39;] = pd.cut(data[&#39;value&#39;], bins=[0,30,70,100], labels=[&#34;Low&#34;, &#34;Mid&#34;, &#34;High&#34;]) value bin0 2 Low1 45 Mid2 7 Low3 85 High4 28 Low#Categorical Binning Example Country0 Spain1 Chile2 Australia3 Italy4 Brazilconditions = [ data[&#39;Country&#39;].str.contains(&#39;Spain&#39;), data[&#39;Country&#39;].str.contains(&#39;Italy&#39;), data[&#39;Country&#39;].str.contains(&#39;Chile&#39;), data[&#39;Country&#39;].str.contains(&#39;Brazil&#39;)]choices = [&#39;Europe&#39;, &#39;Europe&#39;, &#39;South America&#39;, &#39;South America&#39;]data[&#39;Continent&#39;] = np.select(conditions, choices, default=&#39;Other&#39;) Country Continent0 Spain Europe1 Chile South America2 Australia Other3 Italy Europe4 Brazil South America4.Log TransformLogarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. What are the benefits of log transform:It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.A critical note: The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.Log(x+1)#Log Transform Exampledata = pd.DataFrame({&#39;value&#39;:[2,45, -23, 85, 28, 2, 35, -12]})data[&#39;log+1&#39;] = (data[&#39;value&#39;]+1).transform(np.log)#Negative Values Handling#Note that the values are differentdata[&#39;log&#39;] = (data[&#39;value&#39;]-data[&#39;value&#39;].min()+1) .transform(np.log) value log(x+1) log(x-min(x)+1)0 2 1.09861 3.258101 45 3.82864 4.234112 -23 nan 0.000003 85 4.45435 4.691354 28 3.36730 3.951245 2 1.09861 3.258106 35 3.58352 4.077547 -12 nan 2.484915.One-hot encodingOne-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.This method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information. (For details please see the last part of Categorical Column Grouping)One hot encoding example on City columnWhy One-Hot?: If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns. If all the columns in our hand are equal to 0, the missing value must be equal to 1. This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. This function maps all values in a column to multiple columns.encoded_columns = pd.get_dummies(data[&#39;column&#39;])data = data.join(encoded_columns).drop(&#39;column&#39;, axis=1)6.Grouping OperationsIn most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance. This kind of data called “Tidy”.Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.— Hadley WickhamDatasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. In such a case, we group the data by the instances and then every instance is represented by only one row.The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.Categorical Column GroupingI suggest three different ways for aggregating categorical columns:The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.data.groupby(&#39;id&#39;).agg(lambda x: x.value_counts().index[0])Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative.Pivot table example: Sum of Visit Days grouped by Users#Pivot table Pandas Exampledata.pivot_table(index=&#39;column_to_group&#39;, columns=&#39;column_to_encode&#39;, values=&#39;aggregation_column&#39;, aggfunc=np.sum, fill_value = 0)Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. You can check the next section for the explanation of numerical column grouping.Numerical Column GroupingNumerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either.#sum_cols: List of columns to sum#mean_cols: List of columns to averagegrouped = data.groupby(&#39;column_to_group&#39;)sums = grouped[sum_cols].sum().add_suffix(&#39;_sum&#39;)avgs = grouped[mean_cols].mean().add_suffix(&#39;_avg&#39;)new_df = pd.concat([sums, avgs], axis=1)7.Feature SplitPhoto by Jaxon Lott on UnsplashSplitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:We enable machine learning algorithms to comprehend them.Make possible to bin and group them.Improve model performance by uncovering potential information.Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:data.name0 Luther N. Gonzalez1 Charles M. Young2 Terry Lawson3 Kristen White4 Thomas Logsdon#Extracting first namesdata.name.str.split(&#34; &#34;).map(lambda x: x[0])0 Luther1 Charles2 Terry3 Kristen4 Thomas#Extracting last namesdata.name.str.split(&#34; &#34;).map(lambda x: x[-1])0 Gonzalez1 Young2 Lawson3 White4 LogsdonThe example above handles the names longer than two words by taking only the first and last elements and it makes the function robust for corner cases, which should be regarded when manipulating strings like that.Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row.#String extraction exampledata.title.head()0 Toy Story (1995)1 Jumanji (1995)2 Grumpier Old Men (1995)3 Waiting to Exhale (1995)4 Father of the Bride Part II (1995)data.title.str.split(&#34;(&#34;, n=1, expand=True)[1].str.split(&#34;)&#34;, n=1, expand=True)[0]0 19951 19952 19953 19954 19958.ScalingIn most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but it might be still nice to apply. However, the algorithms based on distance calculations such as k-NN or k-Means need to have scaled continuous features as model input.Basically, there are two common ways of scaling:NormalizationNormalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.data = pd.DataFrame({&#39;value&#39;:[2,45, -23, 85, 28, 2, 35, -12]})data[&#39;normalized&#39;] = (data[&#39;value&#39;] - data[&#39;value&#39;].min()) / (data[&#39;value&#39;].max() - data[&#39;value&#39;].min()) value normalized0 2 0.231 45 0.632 -23 0.003 85 1.004 28 0.475 2 0.236 35 0.547 -12 0.10StandardizationStandardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.In the following formula of standardization, the mean is shown as μ and the standard deviation is shown as σ.data = pd.DataFrame({&#39;value&#39;:[2,45, -23, 85, 28, 2, 35, -12]})data[&#39;standardized&#39;] = (data[&#39;value&#39;] - data[&#39;value&#39;].mean()) / data[&#39;value&#39;].std() value standardized0 2 -0.521 45 0.702 -23 -1.233 85 1.844 28 0.225 2 -0.526 35 0.427 -12 -0.929.Extracting DateThough date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like &#34;01–01–2017&#34;.Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. Here, I suggest three types of preprocessing for dates:Extracting the parts of the date into different columns: Year, month, day, etc.Extracting the time period between the current date and columns in terms of years, months, days, etc.Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them.from datetime import datedata = pd.DataFrame({&#39;date&#39;:[&#39;01-01-2017&#39;,&#39;04-12-2008&#39;,&#39;23-06-1988&#39;,&#39;25-08-1999&#39;,&#39;20-02-1993&#39;,]})#Transform string to datedata[&#39;date&#39;] = pd.to_datetime(data.date, format=&#34;%d-%m-%Y&#34;)#Extracting Yeardata[&#39;year&#39;] = data[&#39;date&#39;].dt.year#Extracting Monthdata[&#39;month&#39;] = data[&#39;date&#39;].dt.month#Extracting passed years since the datedata[&#39;passed_years&#39;] = date.today().year - data[&#39;date&#39;].dt.year#Extracting passed months since the datedata[&#39;passed_months&#39;] = (date.today().year - data[&#39;date&#39;].dt.year) * 12 + date.today().month - data[&#39;date&#39;].dt.month#Extracting the weekday name of the datedata[&#39;day_name&#39;] = data[&#39;date&#39;].dt.day_name() date year month passed_years passed_months day_name0 2017-01-01 2017 1 2 26 Sunday1 2008-12-04 2008 12 11 123 Thursday2 1988-06-23 1988 6 31 369 Thursday3 1999-08-25 1999 8 20 235 Wednesday4 1993-02-20 1993 2 26 313 SaturdayConclusionhttps://xkcd.com/1838/I tried to explain fundamental methods that can be beneficial in the feature engineering process. After this article, proceeding with other topics of data preparation such as feature selection, train/test splitting, and sampling might be a good option.You can check my other article about Oversampling.Lastly, I want to conclude the article with a reminder. These techniques are not magical tools. If your data tiny, dirty and useless, feature engineering may remain incapable. Do not forget “garbage in, garbage out!”ReferencesStack Overflow questions are very beneficial for every kind of feature engineering script.I highly recommend Kaggle competitions and their discussion boards.Ways to Detect and Remove the OutliersUnderstanding Feature Engineering (Part 1) — Continuous Numeric DataUnderstanding Feature Engineering (Part 2) — Categorical DataLog Transformations for Skewed and Wide DistributionsTidy dataAbout Feature Scaling and Normalization</description>
      <pubDate>08 Apr 20 11:57 EDT</pubDate>
      <guid>https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</guid>
    </item>
    <item>
      <title></title>
      <link>https://commoncog.com/blog/the-chinese-businessman-paradox/</link>
      <description>&lt;a href=&#34;https://commoncog.com/blog/the-chinese-businessman-paradox/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Also in this series: Let Reality Be The Teacher, Superstition Doesn&#39;t Count, Maybe Strategy Matters, Cash Flow Is King.I met my friend Samuel in the first year of secondary school, way back when we were beginner teenagers. Samuel’s dad was a traditional Chinese businessman. He ran a pharmaceutical import business, supplying a handful of pharmacists around Kuching, population 300,000, the small city on Borneo that we both grew up in.Over the years, Samuel’s father expanded the business into other interests: small real estate holdings, food &amp; beverage outlets, retail shops selling mobile phone kits, clothes, phone data plans and packages. If a business unit wasn’t doing too well, he was quick to shut things down. I observed most of this through my friend’s eyes; Samuel was being groomed to take over the company. Every year when I returned to Kuching, I discovered that Samuel’s father’s business empire had grown, inch by hard-won inch.In Malaysia, where I was born, and in Singapore, where I did university, there is this trope of Chinese business acumen that we grew up with. Group identities are built around tropes, because tropes are the stories that we tell ourselves. This particular trope is a cultural one, born from the history of overseas Chinese.The idea of a savvy business leader emerging from a humble background, thriving despite adversity, is one of the strongest touchstones in the stories that make up who we are. It is in the family histories of all those Chinese who migrated out of China; it is the story of our grandfathers, uncles, and (occasionally) aunties, making a living on distant shores; if we have no direct relationship to a relative in business, it is the stories that we are familiar with through our friends and their families.This idea of a ‘Chinese businessman identity’ is what makes up a race-based superiority complex, as Amy Chua argues in The Triple Package — it is the reason certain racial identities help with certain types of achievement. When a Chinese person feels down-and-out in the education system, he has a path that allows him immediate respect in his society: that of doing business. The ones who succeed are lauded by their families and friends. The ones who don’t do so well are still seen as engaging in a respectable pursuit.Over time, my friends and I referred to those Chinese peers who did well in business as having ‘Chinese businessman wisdom’. We used this phrase in the following way: “He’s not really into startups. He’s into business. He has Chinese businessman wisdom.” and “If even half of the people doing startups today have Chinese businessman wisdom, we would have a lot more successes on our hands.”In other societies, and in other cultures, this might be phrased in a different way: “he’s business savvy”, or “he’s built for business”. But this phrase resonates with us because we are surrounded by positive examples of successful (and not so successful) Chinese businessmen, many of them seem to be built of the same mould. Nearly all the major tycoons in South East Asia are Chinese: Robert Kuok of Malaysia, Ng Teng Fong from Singapore, Mochtar Riady of Indonesia; the list goes on. They are described as thrifty, low-key, savvy, cunning, pragmatic, and respected — even begrudgingly! — by the communities they run businesses in.Which then begs the question: “what exactly is Chinese businessman wisdom?” And how do you gain more of it?‘Business Sense’Traditional Chinese Shophouse frontage.One aspect of the Chinese businessman wisdom trope is the idea that some people are born with business sense, and others are not.Samuel’s dad is fond of saying that his brothers tried their hand at business before he did, and nearly all of them failed. He was the only one from his family who succeeded.My maternal grandfather, who ran a car workshop on the island of Labuan for most of his adult life, was dinged for not being business savvy enough. The stories I heard of him are mostly about how he was kinder than he needed to be, and charged less than he could have charged. He did eventually sell off the business to retire in Kuching, and I’m now experiencing the terror of doing a business on my own, so I respect him tremendously; I think he’s done alright for himself. But the perception of ‘business savvy’ or ‘not business savvy’ as an inborn trait, unchangeable by circumstance, is hardcoded into our culture; an inalienable part of the ‘traditional Chinese businessman’.I reject this notion almost as strongly as I reject the notion of pre-ordained destiny.(This shouldn’t be a surprise to you — this entire blog is built around the idea of a keeping only the ideas that are useful to your future career. Believing in ‘business sense’ as an unchangeable, in-born trait is not actionable in a way that another belief might be; therefore reject it in favour for the actionable belief.)A more acceptable version of this ‘business sense as an inborn trait’ meme is the one articulated by Robert Kuok in his memoirs:To me, talent and instincts in business are inborn and cannot be learned in the classroom. The best comparison I can give you is the game of soccer. The best soccer players are born with great innate talent. Even if a person is not born with the ability to be a top soccer player, he can learn to be a good soccer player through good coaching. But he will never be one of the great stars of the game, such as a Pele or a Messi, who are gifted people with inborn skills augmented by good coaching.This rings true to me, as it gels with my experience for writing and for Judo — the two other skills I am most familiar with. In On Writing, Stephen King makes a similar case:There are no bad dogs, according to the title of a popular training manual, but don’t tell that to the parent of a child mauled by a pit bull or a rottweiler; he or she is apt to bust your beak for you. And no matter how much I want to encourage the man or woman trying for the first time to write seriously, I can’t lie and say there are no bad writers. Sorry, but there are lots of bad writers. Some are on-staff at your local newspaper, usually reviewing little-theater productions or pontificating about the local sports teams. Some have scribbled their way to homes in the Caribbean, leaving a trail of pulsing adverbs, wooden characters, and vile passive-voice constructions behind them. Others hold forth at open-mike poetry slams, wearing black turtlenecks and wrinkled khaki pants; they spout doggerel about “my angry lesbian breasts” and “the tilted alley where I cried my mother’s name.”Writers form themselves into the pyramid we see in all areas of human talent and human creativity. At the bottom are the bad ones. Above them is a group which is slightly smaller but still large and welcoming; these “are the competent writers. They may also be found on the staff of your local newspaper, on the racks at your local bookstore, and at poetry readings on Open Mike Night. These are folks who somehow understand that although a lesbian may be angry, her breasts will remain breasts.The next level is much smaller. These are the really good writers. Above them—above almost all of us—are the Shakespeares, the Faulkners, the Yeatses, Shaws, and Eudora Weltys. They are geniuses, divine accidents, gifted in a way which is beyond our ability to understand, let alone attain. Shit, most geniuses aren’t able to understand themselves, and many of them lead miserable lives, realizing (at least on some level) that they are nothing but fortunate freaks, the intellectual version of runway models who just happen to be born with the right cheekbones and with breasts which fit the image of an age.I’ve long made peace with the idea that I might never become a Steinbeck or a Hemingway, but that with a lot of effort and a bit of luck, I am satisfied with the idea that I might become good. It’s only fair that the same could be said about business.Rationality and BusinessI’ve long suspected that much of business ability may be explained by rationality. This stems from experience: I ran the Vietnam office of a Singaporean company for three years, and I got to see a number of ‘traditional Chinese businessmen’ up close. In some of these cases, they were our clients; in others, we were competitors with opportunistic traditional Chinese businessmen, and received our licks in a race for customers.(We also drove a few of them to bankruptcy, but that’s another story for another day).The best businessmen we dealt with were always relentlessly pragmatic. They were rational in their actions — or at least, they were rational in the sense that they did what was necessary for the survival of their businesses. I remember a client who managed to get a ton of free add-ons out of us, which he did so skilfully none of us realised what was going on for months on end. (He’s over 50 years old and very very good; my old company remains in business with him). I also remember our discovery that a major competitor in our space had been gutted by its management, and turned into a real-estate investment company in Singapore.But rationality alone didn’t explain everything that I saw. The question that most interested me was the following paradox: first, that the most successful Chinese businessmen were often the least educated ones. Second, that Chinese businessmen as a group were more superstitious than most, but that this didn’t seem to have much of an effect on their ability to run their businesses! And finally, that the vast majority of Chinese businesspeople did not think too far ahead. They optimised locally, and by keeping close tabs on the bottom line, shut down lines of business that weren’t performing as well.The paradox was driven by my misguided attempt to figure out what made the successful ones successful, and to see if I could copy them. If intelligence were a deciding factor, then the educated Chinese businessmen were more likely to do better. If rationality were truly a deciding factor, the superstitious Chinese businessmen wouldn’t have done as well as the less superstitious businessmen.  And if strategy didn’t matter, the Chinese businessmen who optimised locally would always end up ahead. None of these things were consistently true. Whatever it is that made them successful is difficult to pin down; it quickly became clear to me that I couldn’t find an answer in simple explanations like ‘intelligence’, ‘rationality’ or ‘strategic thinking’.But I could still try.I suspect the Chinese businessman paradox holds the answer to my original question. I asked, at the top of this post: “what exactly is Chinese businessman wisdom?” And how do you gain more of it?Over the next couple of posts, I’ll describe my current theories. Think of these as practitioner notes — only as true as I have found them in practice. It will take years before I find out if I’m right. I expect to write revisions to these posts over the long term, and I expect to write many of them. I am currently putting some of these theories to the test in my day-to-day pursuit of business; but in all honesty, I’m writing these things down because I’m interested in these questions and have been for a number of years. It’s like catnip for my mind.To my knowledge, I don’t think these sorts of articles have ever been written before. If I succeed in business, later in life, you may point to these posts as a record of what I got right. But if I failed, then these posts would be a record of all the wrong things I believe that ultimately did me in at the end.I look forward to looking backwards. But for now, these posts will serve as anchors to my present, a record to look back onto in the future past.Parts:Let Reality Be The TeacherSuperstition Doesn&#39;t CountMaybe Strategy MattersCash Flow Is King </description>
      <pubDate>16 Feb 21 09:41 EST</pubDate>
      <guid>https://commoncog.com/blog/the-chinese-businessman-paradox/</guid>
    </item>
    <item>
      <title></title>
      <link>https://wiki.nikitavoloboev.xyz/macos</link>
      <description>&lt;a href=&#34;https://wiki.nikitavoloboev.xyz/macos&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;I do wish to expand my horizons and try out Linux more. I doubt I will ever be able to move to another operating system as I have too much invested in optimizing and using macOS but I do want to take the best of all worlds. Linux is open source, has an increasingly large community of users and developers and one thing that I love about UNIX systems is that by using these systems you effectively become a developer. Because otherwise you are simply missing out on the full experience.Clean installYou can clean install by going to Recovery mode (restart with cmd+r pressed). Then Disk Utility &gt; Select disk &gt; Erase (Format it) &gt; Close Disk Utility &gt; Select option Reinstall MacOS (Choose macOS ver. to install).NotesIn save dialogues I can press these keys:I can also press / or ~ to quickly go to some directory from a save dialogue. And I can press ⌘ + ↑ to go to parent directory.Recovery mode: Power off the machine, press the power button and immediately hold Cmd-R.I can appreciate someone wanting to do a clean install if they&#39;ve installed and removed many apps and just want to clear out everything spread around all the system and hidden folders, even if it doesn&#39;t really affect performance and won&#39;t save a ton of disk space. There is something cathartic about a clean install./usr/local/bin is a good place to put raw binaries available in the path, that are not installed with Nix.CodemacOS Defaults# Remove dock animation. https://www.reddit.com/r/apple/comments/6xg9xq/tip_of_the_day_one_thing_i_cant_live_without_in/defaults write com.apple.dock autohide-delay -int 0defaults write com.apple.dock autohide-time-modifier -float 0.4defaults delete com.apple.dock autohide-delaydefaults delete com.apple.dock autohide-time-modifier# Turn internal keyboard off. https://discussions.apple.com/thread/5044946?answerId=26556362022#26556362022sudo kextunload /System/Library/Extensions/AppleUSBTopCase.kext/Contents/PlugIns/AppleUSBTCKeyboard.kext/# Turn internal keyboard onsudo kextload /System/Library/Extensions/AppleUSBTopCase.kext/Contents/PlugIns/AppleUSBTCKeyboard.kext/Links​AppUpdater - Simple app-updater for macOS, checks your GitHub releases for a binary asset once a day and silently updates your app.​Brooklyn - Screensaver inspired by Apple&#39;s Event on October 30, 2018.​PureDarwin - Community project to make Darwin more usable. (HN)​BlockBlock - Continually monitors common persistence locations and displays an alert whenever a persistent component is added to the OS.​Impact - Crash detection and recording library for Apple platforms.​gon - CLI and Go Library for macOS Notarization.​macOS Headers - Consistently maintained dump of most macOS Headers.​AppMover - Framework for moving your application bundle to Applications folder on launch.​Lilu - Arbitrary kext and process patching on macOS.​osx-hid-inspector - Command line tool for macOS for inspecting human input devices (HID).​mas-cli - Simple command line interface for the Mac App Store. Designed for scripting and automation.​apply-user-defaults - Small utility to set macOS user defaults declaratively from a YAML file.​Zero.sh - Radically simple personal bootstrapping tool for macOS.​BlackHole - Modern MacOS virtual audio driver that allows applications to pass audio to other applications with zero additional latency.​xcnotary - Missing macOS app notarization helper, built with Rust. (HN)​skhd - Simple hotkey daemon for macOS.​Proxy Audio Driver - Virtual audio driver for macOS to sends all audio to another output.​Icons.app - App for macOS which is designed to generate consistent sized icons of an existing application in various states, jiggling (shaking) etc.​OpenCore - Open-source, unconventional, first-in-class piece of software designed to intercept kernel loading to insert a highly advanced rootkit, designed to be an alternative to Clover. (Code) (HN)​Syphon - macOS technology to allow applications to share video and still images with one another in realtime, instantly.​netboot.nix - Create full netboot images in 15 seconds.​Swizzle - Extensible tweak to create simple tweaks for any app, from within any app.​AquaticPrime - Mac software licensing code using cryptographically signed license files.​SimpleVM - Sample code for Virtualization framework. (Fork)​Sinter - User-mode application authorization system for MacOS written in Swift. (Article)​fastmac - MacOS instance or Linux shell. (HN)​macOS Simple KVM - Tools to set up a quick macOS VM in QEMU, accelerated by KVM.​Apple Platform Versions - Recent history of platforms developed by Apple, including Apple-managed build tools for these platforms.​Project Mendacius - GUI based virtualization tool for running Linux on macOS Big Sur.​dmgdist - Automate the process of creating, uploading and notarizing the DMG of a Mac app.​MacHack - List of built-in tools in macOS that you probably didn&#39;t know about.​Shield - App to protect against process injection on macOS. (Article)​VMCLI - Set of utilities to help you manage VMs with Virtualization.framework. (HN)​SplitConfigurations - Up the basics of a Big Sur app layout. Includes splitview and toolbarItems.​TinyLinux - Tiny minimum implementation of Virtualization framework to boot Linux.​DyldExtractor - Extract Binaries from Apple&#39;s Dyld Shared Cache.​ipsw - iOS/macOS Research Swiss Army Knife. (Web)​strongarm - Full-featured, cross-platform ARM64 Mach-O analysis library.​macbac - Lists, controls and schedules efficient APFS snapshots for your convenience.​UTM - Virtual machines for Mac.​xhyve - Lightweight macOS virtualization solution.​Lima - Linux-on-Mac (&#34;macOS subsystem for Linux&#34;, &#34;containerd for Mac&#34;). (HN)​MacVM - MacOS VM for Apple Silicon using Virtualization API.​ArchTest - Why does this not compile on Apple Silicon?​Santa - Binary authorization system for macOS. (Docs)​Rudolph - Control server counterpart of Santa, and is used to rapidly deploy configurations to Santa agents.​macOS Orb - Convenient tools and settings for utilizing MacOS on CircleCI.​perfrecord - macOS-only command line CPU profiler that displays the result in the Firefox profiler.​vz - Create virtual machines and run Linux-based operating systems in Go using Apple Virtualization.framework.​diskspace - macOS command line tool to return the available disk space on APFS volumes.​PlayCover - Run iOS apps &amp; games on M1 Mac with mouse, keyboard and controller support.​node-mac-userdefaults - Native Node.js module that provides an interface to the user’s defaults database on macOS.​asitop - Performance monitoring CLI tool for Apple Silicon. (Web)​macOS Optimizer - Shell scripts to speed up your mac boot time, accelerate loading, and prevent unnecessary throttling.​optool - Command Line Tool for interacting with MachO binaries on macOS/iOS.​mkuser - Make user accounts for macOS with many advanced options.​macFUSE - Allows you to extend macOS via third party file systems.</description>
      <pubDate>25 Jan 21 13:47 EST</pubDate>
      <guid>https://wiki.nikitavoloboev.xyz/macos</guid>
    </item>
    <item>
      <title></title>
      <link>https://tedgioia.substack.com/p/you-dont-need-a-mentorfind-a-nemesis</link>
      <description>&lt;a href=&#34;https://tedgioia.substack.com/p/you-dont-need-a-mentorfind-a-nemesis&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;On three different occasions I considered writing a book about sports. Are you surprised? Well, so am I.I’ll be the first to say that these ambitions were ridiculous in almost every way. I’m no expert on sports. I’m not even an especially passionate sports fan. I tune in, and tune out—and sometimes I’ll go weeks without paying any attention to rankings and competitions. But each of these ideas captured my interest because they felt much bigger than sports. They appealed to me in my guise of a cultural critic—which, on certain days of the week, is how I view myself—and especially because they were each so rich with implications for everyday life. The Honest Broker is a reader-supported guide to music, books, and culture. Both free and paid subscriptions are available. If you want to support my work, the best way is by taking out a paid subscription.I’ll never write these books, but in the spirit of George Steiner—who once wrote an entire book about the books he never got to write—I want to share these three concepts, and then explore one of them (what I called my nemesis project) in a little more depth.The first of these unwritten books seemed very simple, at least at first glance. It would have been about sportsmanship. The opening chapter would have focused on an extraordinary moment at the 1935 Davis Cup, when Baron Gottfried von Cramm refused to take a match point that would have given Germany victory over the United States—because he claimed his racket had touched a ball before it went out-of-bounds.It was an error that no one else saw, including the umpire. No one would have ever known about it. But the Baron felt there was something more important than winning. When the German captain denounced him as a “traitor to the nation,” von Cramm responded: &#34;On the contrary, I don&#39;t think that I&#39;ve failed the German people. In fact, I think I&#39;ve honored them.&#34;The story is rich with implication, and would make a great opening gambit for a classroom discussion, and not just about sports. But I was especially interested in the idea that the rules of the game aren’t absolute guides—that sometimes there are even higher rules that must be consulted. Philosophers call this natural law, but it’s usually treated as some abstract theoretical concept with little bearing to everyday life. My inquiry into sportsmanship would have shown how the most powerful laws are those we impose on ourselves, and how each of us understands intuitively—even a youngster in a sporting competition—that there are some standards of behavior that are even more binding than the written rules and guidelines. I never wrote that book, and it’s probably no great loss. I didn’t have the expertise to deal with the subject adequately, without devoting years of research to it. But I still believe that sportsmanship is the single most neglected concept in all of athletics. You can listen to sports talk radio for a year, and not even hear the word mentioned once. Yet it ought to be part of our everyday dialogue about any sphere of life involving intense competition.The second sports book concept actually morphed into music research. I have long been interested in how rituals and performances enact violence symbolically in order to defuse actual bloodshed. Sporting events are the most obvious examples of this, but musical performances achieve very much the same function. That’s why there’s so much symbolic violence—performers destroying their instruments, and then later trashing their hotel rooms, etc.—in popular music. Those who have read my recent book Music: A Subversive History will recognize how deeply I was influenced by this line of thinking.So I also have no regrets that I never wrote that second book on sports. I got more benefit by applying that conceptual framework to my study of music.But then we come to my third sports book concept, and this is one that still preoccupies me. It has to do with what sportswriters and fans call rivalries. But I view it as something different—I refer to this same phenomenon as the Concept of the Nemesis. And I have a hunch that it is one of the most powerful and underrated driving forces in human behavior. I believe that my Nemesis Concept even explains key developments in music and the arts. A recent study of composers during the period 1750-1899 discovered that they were significantly more productive when they lived in close proximity to other composers. The most likely way of accounting for this, to my mind, is the inherent rivalry that arises when creative people encounter each other daily. I’d even guess that the extraordinary impact of New York on the history of American music is partly explained in the same way. New York musicians are highly competitive—did you know that?—much more so than the West Coast players I dealt with during my formative years. Growing up in LA, I found that the geography was so spread out that musicians could operate in quasi-isolation, only encountering their peers at the rehearsal or gig. This wasn’t always a bad thing. The sprawling nature of SoCal urban development allowed for a greater degree of creative freedom and independence—allowing many experimental or avant-garde musicians to develop in Los Angeles (Ornette, Dolphy, Cage, Mingus, etc.) without having to worry about the groupthink or prevailing norms. But that came at the cost of cultural intensity. New York, in contrast, possessed that intensity as an inevitable result of its population density—three times as great as LA’s—which puts musicians in frequent contact with one another. In other words, it’s much easier to find your nemesis in Manhattan.The duel of Georges Clarétie and Léon Daudet on March 4, 1911Even after I gave up on my idea to write a sports book about the nemesis, I pursued at some length the idea of turning it into a novel. I wrote more than 30 thousand words of it, before setting it aside. I often begin a book project by designing an imaginary cover. Here was the cover for my never-finished nemesis novel. I won’t even begin to explain the plot (for which you should be grateful). It’s based on an actual historic figure, who briefly appears as a character in Gabriel García Márquez’s fiction and autobiography, but deserves an entire book of his own. So don’t ask me about all the strange dealings in this book—which begins with everybody in the world spouting halos and ends with a duel in front of Google’s headquarters. But I will describe my ideas about the nemesis, which plays such a key role in the text. The first thing to understand is that your nemesis is not your enemy. Or, put differently, your nemesis is more than just an enemy. Rather, the nemesis is an adversary is who is like your dark twin. Even as you battle with the nemesis, you share a kind of DNA. The gaze at your nemesis is like looking into a mirror, but one of those fun house mirrors at the carnival, where everything is both recognizable and distorted.In every way, this relationship is more powerful and volatile than a battle between enemies. I even sensed that as a child when I read comic books. The Superman stories were shallow because he never faced a true nemesis, he merely dealt with pathetic villains, and even the best of them—Lex Luthor, for example—could never compete with the Man of Steel on his own terms. That’s why they needed to rely on dirty tricks (usually involving Kryptonite) to weaken him, and make him as pathetic as they already were.But Batman was a different story entirely. Batman had a true nemesis in the Joker—and even a child could see the genetic resemblance. They both wear strange outfits. They both had bad childhoods. They both operate outside the law. They are both unhinged. They are both obsessed with gadgets. They both are shameless media hounds. And both of them desperately need to get laid, but it will never happen because they refuse to get out of their costumes. They need each other. They complete each other. They hate each other, but also have a repressed attraction to each other.You can’t find a richer or more multifaceted relationship. Not even between a married couple. I feel sorry for Robin, who will never rise beyond the sad pay grade of sidekick.  The Joker is the real target of Batman’s passions and obsessions. That’s why movies featuring that remarkable nemesis have earned more Oscar nominations than all the other superhero films combined. But this is more than just a matter for filmmakers. All great societies were built on the fruitful conflict between a hero and a nemesis. Think of Cain and Abel in Judeo-Christian culture or Set and Osiris in Egyptian mythology. Look back to the origins of Roman preeminence in the conflict between Romulus and Remus. As mentioned above, artists often benefit from identifying a nemesis. Art history is full of examples—Constable and Turner, Michelangelo and Raphael, Brunelleschi and Ghiberti, Picasso and Matisse, etc. For example, the inspired daub of red paint in Helvoetsluys by J. M. W. Turner was allegedly inspired by the painter’s desire to outshine his rival John Constable when both were participating in the same exhibition.And in science and technology you have just as many examples: Edison and Tesla, Newton and Leibniz, Jobs and Gates. Even now Musk and Bezos are engaged in a ludicrous space race that must reflect some personal animus between these two very similar rivals. Helvoetsluys by J M W Turner,And then we come to sports. As I mentioned above, I’m not a huge sports fan. But that changes when a nemesis is involved. That’s rarely a term used by sportswriters, who talk endlessly of rivalries. But those who see these encounters up close have no doubts about what’s really involved. After the Lakers lost to the Celtics in the 1984 NBA Finals, Magic Johnson stayed in the shower for a full 30 minutes—allegedly weeping—before meeting with the press. His teammate Michael Cooper later tried to explain what had happened: “Not only did we lose to the Boston Celtics, but [Magic] had lost to his nemesis Larry Bird.” Johnson himself also understood that to defeat his nemesis, he needed to become more like him—something mere enemies never feel. “I learned a valuable lesson. Only the strong survive. Talent just doesn’t get it. That’s the first time the [80’s] Lakers ever encountered that, someone stronger minded.” A year later, Magic and the Lakers beat the Celtics—but only because they learned from their nemesis.That’s the fastest way to discover whether someone is an enemy or a nemesis. People always want to stress how different they are from their enemies. You don’t even want to get them started on the subject, because they won’t stop. They will enumerate all of the enemy’s terrible character traits and failings—making sure you know how different they are from that awful person. As I look back at a lifetime of watching sports, the moments I truly cherish always involved this kind of mirror-image relationship between competitors. So I will always prefer watching Magic-versus-Bird over those Michael Jordan highlight films—even though I realize the sheer superiority in athleticism of the latter. Without  a nemesis, Jordan could win championships, but he couldn’t create the kind of Greek tragic drama of the encounter with the other. The first time I experienced this was as a child, when Ali battled Frazier in three titanic boxing matches that will, in my opinion, never be surpassed. Maybe there have been better boxers—I’ll let the aficionados of fisticuffs debate that—but if you’re seeking out the encounter with the nemesis, those two boxers achieved something of almost archetypal dimensions. I got a brief glimpse of the same phenomenon in the extraordinary confrontation of middle-distance racers Sebastian Coe and Steve Ovett. But, sadly, they only met each other on the track seven times. But I know that I’m not the only one who would shell out money for a pay-per-view if they raced again nowadays as old men. No, not because they would be especially fast—that won’t happen again—but because they embody the encounter with the nemesis at the highest possible level.“I was prepared to die with blood in my boots for the 1500 meter,” was Coe’s later description of his mindset. Ovett described their races as his “duel” with someone who was “bullet-proof.” Many years later, the two rivals had dinner together and discovered how much they had in common. Some might be surprised, but no one who has deeply considered the true nature of the nemesis. When else have I experienced the joy of the nemesis in sports? In tennis, Nadal versus Federer is the obvious example. Serena and Venus Williams could have been on that level—and here they actually share the same DNA, not just symbolically but biologically. Yet I don’t think they were willing to view each other as the nemesis, so this never took off as a rivalry. McEnroe might have been able to achieve something similar with Borg, but his character made him incapable of grasping the essence of the nemesis. For McEnroe, all the people against him were just enemies—and with that mindset he could never truly rise to that high destiny. And it’s probably why Borg won Wimbledon five years in a row, and McEnroe could only repeat once. In contrast, the totally dominant athletes—Tiger Woods, Tom Brady, etc.—fail to capture my imagination. They might very well be unsurpassed at what they do, but they’ve never had a nemesis, and so I really can’t be bothered. At best, they treat their own past achievements as benchmarks. But if your only nemesis is your younger self, your life will inevitably follow a downward trajectory.I believe there are useful lessons here for all of us—and not just high-performance athletes. First, you need to take care in deciding who plays the role of nemesis in your life—and if you don’t have someone like that, you might be missing out. Because your nemesis will teach you things that no mentor can. The nemesis can push buttons you didn’t even know were there to be pushed. The nemesis may bring out the worst in you, but if you manage the situation effectively, also the best.Next, be sensitive to the true nature of the people who cause problems in your life. I have reached the realization that some of my fiercest critics have picked me out as their nemesis. They never consulted me about this—in fact, they’re usually people I’ve never met face-to-face, but that’s probably why I’m able to serve this role in their psyches. If we had met, I might have turned into a mentor. At least, that’s how I prefer to view the dynamic at play. This doesn’t necessarily make me want to run off and have a beer with them. But I do grasp that the true nature of our relationship is richer than meets the eye. If I just viewed them as enemies or fools, I would miss out on the opportunities for growth that their confrontations provide me with. Let me make one last point—and in this instance I want to return to the larger issue of sports with which I started this essay. Athletic competitions have tremendous potential for building character, but it is almost entirely misused at the present juncture. I could cite many examples and reasons, but I hardly need to—just read the sports news for a few weeks, and you will see how sadly we fall short. Instead of using competition as a means of self improvement, we have turned it into a platform for every kind of dysfunctional behavior. And, worst of all, this starts in all the youth sports leagues, where a sick kind of Darwinian eat-or-be-eaten mentality predominates. There are exceptions, and they are inspiring. But in a culture that celebrates winning at all costs, and making as much money in the process, they are hidden from view.So I regret not writing my three books, not because I would have done such a good job with them—I probably would have fallen far short of my aspirations, all things considered—but merely because the message was the right one. At the forefront of that first unwritten book was the notion that sportsmanship deserves to be as celebrated as winning. I still believe that passionately. Second was the idea that violence in sports ought to be a way of defusing the violence in society; and that’s also still a core personal value so many years later. But that last unwritten book had the biggest lesson of all, namely that we really don’t need to make enemies in our life—it’s much better to find (and learn from) your nemesis. And even the enemies we think we have might not really be all that different from ourselves.Those aren’t just lessons for athletes, but sports would be a good place to begin putting them to work. Then maybe the rest of us would start paying attention too. </description>
      <pubDate>05 Jan 22 23:34 EST</pubDate>
      <guid>https://tedgioia.substack.com/p/you-dont-need-a-mentorfind-a-nemesis</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.lesswrong.com/posts/FCXCXigp7byv2dM8D/how-to-make-billions-of-dollars-reducing-loneliness</link>
      <description>&lt;a href=&#34;https://www.lesswrong.com/posts/FCXCXigp7byv2dM8D/how-to-make-billions-of-dollars-reducing-loneliness&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Loneliness Is a Big Problem On Facebook, my friend Tyler writes: Lately, I&#39;ve been having an alarming amount of conversations arise about the burdens of loneliness, alienation, rootlessness, and a lack of belonging that many of my peers feel, especially in the Bay Area. I feel it too. Everyone has a gazillion friends and events to attend. But there&#39;s a palpable lack of social fabric. I worry that this atomization is becoming a world-wide phenomenon – that we might be some of the first generations without the sort of community that it&#39;s in human nature to rely on. And that the result is a worsening epidemic of mental illness... Without the framework of a uniting religion, ethnicity, or purpose, it&#39;s hard to get people to truly commit to a given community. Especially when it&#39;s so easy to swipe left and opt for things that offer the fleeting feeling of community without being the real thing: the parties, the once-a-month lecture series, the Facebook threads, the workshops, the New Age ceremonies. We often use these as &#34;community porn&#34; – they&#39;re easier than the real thing and they satisfy enough of the craving. But they don&#39;t make you whole. I&#39;ve had some thoughts about experiments to try. But then I think about how hard it is (especially in this geographic area) to get people to show up to something on at least a weekly basis. Even if it&#39;s for something really great. I see many great attempts at community slowly peter out. Young people are lonely. Old people are lonely. Loneliness is bad for your health. It&#39;s bad for society&#39;s health. Having a smartphone that keeps you entertained all day, and enough money to live by yourself, might sound like first world problems. But they are likely contributors to loneliness. And as developing countries get richer, they&#39;ll start having first world problems too. So I think addressing loneliness could be very high-leverage for the world. People are starting businesses to address loneliness: you can pay someone to call you periodically or take you for a walk. But I&#39;d argue these services are a band-aid in the same sense that parties, workshops, and ceremonies are. They don&#39;t solve the underlying problem: You&#39;re still alone by default instead of together by default. Roommates Could Be a Great Solution Sociologists think there are three conditions necessary for making friends: proximity; repeated, unplanned interactions; and a setting that encourages people to let their guard down and confide in each other. These conditions tend to be present during college for many people, but not afterwards. Why do people find it easier to make friends in college? Maybe it&#39;s because college students don&#39;t usually live alone. Going to events doesn&#39;t work because (a) you don&#39;t typically get repeated interactions with the same person and (b) events take place at a scheduled time. Which may or may not be a time you&#39;re feeling lonely. If you have a lot of roommates, all you have to do is step outside your room and find someone to chat with. No transportation CO2 emissions needed. But more important, you know your roommates are always gonna be around. But I Already Have Roommates Even if you already have roommates, I think there&#39;s a good chance your roommate situation is under-optimized. Given that you spend so much time with them, there&#39;s a lot of value in living with people you really connect with. (Finding great coworkers makes sense for similar reasons.) The layout of your house and the number of roommates you have can also make a big difference. I used to have friends living in a 4-bedroom place where all the bedrooms opened directly into a single large common area. If anyone else was outside their room, you&#39;d immediately know it and have an opportunity for interaction. Later I lived in an 8-bedroom place which felt far lonelier, even with every room occupied. The house was laid out so it was easy to go about your day without ever running into a fellow roommate. I also lived in a house with over 50 bedrooms for a while, which was wild &amp; a lot of fun. But I Don&#39;t Want Roommates One reason you might not want roommates is because you&#39;re worried you might have conflicting preferences for what living together should be like. For example, my philosophy towards dirty dishes is to let them pile up on the counter and periodically stuff them all in the dishwasher, to be as time-efficient as possible. Surprisingly, some people dislike this approach. RoomieMatch.com is a website which tries to solve the roommate compatibility problem. You create a profile by answering questions about dishes, food in the fridge, housecleaning, social events, noise, overnight guests, shared household items, walking around in your underwear, TV, etc. In addition, there are questions to help predict how you well you will connect as people. You Could Make a Lot of Money RoomieMatch has two search options: free and cheap. Cheap costs $20/year. The problem with RoomieMatch is they&#39;re leaving a massive amount of money on the table. A few years ago, a friend of mine was jobless &amp; struggling financially. He was living in a 4-bedroom house at the time, and he was the primary contact with the landlord. My friend took responsibility for vetting folks from Craigslist in order to fill the remaining rooms in the house. He found that folks from Craigslist were willing to pay enough rent for the remaining 3 rooms that he was able to live rent-free until he found a job. I acknowledge this is murky ethical territory, and I&#39;m not condoning my friend&#39;s actions. (I don&#39;t believe anyone ever found out or got upset, for whatever that&#39;s worth.) The point I&#39;m trying to make is that property management is way more lucrative than roommate matching. RoomieMatch makes $20 per user per year at best. My friend was making $100+ per user per month. What I&#39;m suggesting is that you take the full-stack startup playbook which has been successful in Silicon Valley recently, and apply it to online roommate matching + property management. The extreme full-stack approach is to own your own properties. Apparently the US has a surplus of big houses right now. There are already players in this space such as Roam which are proving that people will pay for community. (As if people paying extra to live in hip cities like SF &amp; NYC didn&#39;t prove that already. BTW, I found that the awesome community at the Athena Hotel more than made up for the fact that it&#39;s in a non-hip city.) Anyway, I think existing players are mostly pursuing the extreme full-stack option. I actually think this is the wrong play. You want to be a marketplace, like Airbnb (valued at over $30 billion). The more people who are using your tool, the finer-grained roommate matching services you can provide. It&#39;s hard to achieve massive scale if you have to own every property. You want to be playing matchmaker for individuals with common interests who all happen to be looking for rooms around the same time, plus landlords with empty houses. Maybe you&#39;ll want to undercut RoomieMatch, and provide free matching services for people who live in their properties, in order to achieve the necessary scale. (RoomieMatch&#39;s existing scale is impressive by the way--I quickly got 100+ active, vetted matches in a midsize US city when I tried the tool. If you have the money you might want to just buy it.) So instead of buying properties, maybe you just want to contact people selling large homes &amp; see if you can convince them to let you manage their property. Note that this is a good company to start if a recession happens, since people who currently live alone will be thinking about how to save on rent. This Could Be Really Great Most roommate search tools, like Craigslist, don&#39;t make it easy to figure out if a future roommate is someone you&#39;d actually want to live with. Imagine reaching a scale where you could match people based on factors like: They love to play board games, or pool, or Super Smash Bros. They want a compost pile and a garden in their backyard. One has a pet, and the other likes animals but isn&#39;t yet ready to make a lifetime commitment. They want a squat rack in the basement to save time &amp; money going to the gym. They want to continue partying like college students after graduation. They want to be part of an intentional community devoted to mutual improvement and life optimization, or spirituality, or whatever. They want to share childcare responsibilities. They&#39;re all fans of the same sports team. They enjoy reading and discussing the same genre of novels, or watching the same movies. They&#39;re musicians looking for people to jam with. They want to live near hiking trails and go on group hikes together. They want to do independent study of the same topic. They&#39;re trying to eat a healthier diet. They just moved to a new city and want friends they can explore the city with. They have the same unusual work schedule. One needs a caretaker, and the other wants to make extra money. They like the idea of having a couch or two listed on CouchSurfing. One knows a language the other wants to learn. They work close together in the same expensive metropolitan area and want save on housing. So they live in the outskirts of the city and commute together every day using the diamond lane. One drives and the other pays for gas. I also see opportunities to reduce friction in the current roommate matching process: Automatically find times when everyone is available for a meet &amp; greet video call. Let people take virtual tours of the houses on offer to minimize driving. No need to worry about breaking a lease if someone moves to a different house in your company&#39;s network. Let people try out a few communities &amp; see what works for them. Use machine learning to improve your matching as you gather more data. Provide external mediation in the event of roommate disputes, and have a reputation system to encourage good behavior. You aren&#39;t providing housing as a service (like Airbnb), or companionship as a service (like the people-walking startup). You&#39;re providing community as a service. You could even organize mixers across your houses. Conclusion Technology has been blamed for the loneliness epidemic, but I think we can use technology to cure the loneliness epidemic as well. I&#39;m too busy being obsessed with machine learning to start any company which isn&#39;t mostly about that. But I think this is a product the world needs, and I want you to build it. I encourage you to sign the Founders Pledge and donate the money to effective charities in case you actually end up making billions of dollars as a result of reading this. I apologize if you found the tone of this post overly sales-y. My goal was to light a spark in the right person. (Feel free to steal phrases from this post when pitching investors!) Some folks in the rationalist community might be a little underwhelmed by this idea, since people in the rationalist community have been living together in group houses for a long time. The thing is, finding roommates by connecting based on mutual interests via the internet is still kind of weird in the eyes of the general public. As Paul Graham put it: &#34;Live in the future, then build what&#39;s missing.&#34; The existence of so many lonely people proves that this option is still missing for most people. Anyway, if you&#39;re interested in building/investing in this, please comment below, or send me a private message via my user page with the country you&#39;re in and I&#39;ll put you in contact with others who message me. (Edit: I might be slow to reply, sorry) Cross-posted from the Effective Altruism Forum. See also discussion on Hacker News. </description>
      <pubDate>24 Mar 20 12:22 EDT</pubDate>
      <guid>https://www.lesswrong.com/posts/FCXCXigp7byv2dM8D/how-to-make-billions-of-dollars-reducing-loneliness</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.deeplearningbook.org/</link>
      <description>&lt;a href=&#34;https://www.deeplearningbook.org/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Deep Learning An MIT Press book Ian Goodfellow and Yoshua Bengio and Aaron Courville Exercises   Lectures   External Links   The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free. The deep learning textbook can now be ordered on Amazon. For up to date announcements, join our mailing list. Citing the book To cite this book, please use this bibtex entry: @book{Goodfellow-et-al-2016, title={Deep Learning}, author={Ian Goodfellow and Yoshua Bengio and Aaron Courville}, publisher={MIT Press}, note={\url{http://www.deeplearningbook.org}}, year={2016} } To write your own document using our LaTeX style, math notation, or to copy our notation page, download our template files. Errata in published editions Deep Learning Table of Contents Acknowledgements Notation 1 Introduction Part I: Applied Math and Machine Learning Basics 2 Linear Algebra 3 Probability and Information Theory 4 Numerical Computation 5 Machine Learning Basics Part II: Modern Practical Deep Networks 6 Deep Feedforward Networks 7 Regularization for Deep Learning 8 Optimization for Training Deep Models 9 Convolutional Networks 10 Sequence Modeling: Recurrent and Recursive Nets 11 Practical Methodology 12 Applications Part III: Deep Learning Research 13 Linear Factor Models 14 Autoencoders 15 Representation Learning 16 Structured Probabilistic Models for Deep Learning 17 Monte Carlo Methods 18 Confronting the Partition Function 19 Approximate Inference 20 Deep Generative Models Bibliography Index FAQ Can I get a PDF of this book? No, our contract with MIT Press forbids distribution of too easily copied electronic formats of the book. Why are you using HTML format for the web version of the book? This format is a sort of weak DRM required by our contract with MIT Press. It&#39;s intended to discourage unauthorized copying/editing of the book. What is the best way to print the HTML format? Printing seems to work best printing directly from the browser, using Chrome. Other browsers do not work as well. Can I translate the book into Chinese? Posts and Telecom Press has purchased the rights. If you notice any typos (besides the known issues listed below) or have suggestions for exercises to add to the website, do not hesitate to contact the authors directly by e-mail at: feedback@deeplearningbook.org Since the book is complete and in print, we do not make large changes, only small corrections. Known issues: In outdated versions of the Edge browser, the &#34;does not equal&#34; sign sometimes appears as the &#34;equals&#34; sign. This may be resolved by updating to the latest version. </description>
      <pubDate>26 Apr 20 13:41 EDT</pubDate>
      <guid>https://www.deeplearningbook.org/</guid>
    </item>
    <item>
      <title>Things I Wished More Developers Knew About Databases</title>
      <link>https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78</link>
      <description>&lt;a href=&#34;https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;A large majority of computer systems have some state and are likely to depend on a storage system. My knowledge on databases accumulated over time, but along the way our design mistakes caused data loss and outages. In data-heavy systems, databases are at the core of system design goals and tradeoffs. Even though it is impossible to ignore how databases work, the problems that application developers foresee and experience will often be just the tip of the iceberg. In this series, I’m sharing a few insights I specifically found useful for developers who are not specialized in this domain.You are lucky if 99.999% of the time network is not a problem.ACID has many meanings.Each database has different consistency and isolation capabilities.Optimistic locking is an option when you can’t hold a lock.There are anomalies other than dirty reads and data loss.My database and I don’t always agree on ordering.Application-level sharding can live outside the application.AUTOINCREMENT’ing can be harmful.Stale data can be useful and lock-free.Clock skews happen between any clock sources.Latency has many meanings.Evaluate performance requirements per transaction.Nested transactions can be harmful.Transactions shouldn’t maintain application state.Query planners can tell a lot about databases.Online migrations are complex but possible.Significant database growth introduces unpredictability.Thanks much to Emmanuel Odeke, Rein Henrichs and others for their review and feedback on an earlier version this article.You are lucky if 99.999% of the time network is not a problem.It’s an open debate how reliable today’s networking is and how commonly systems experience downtime because of networking outages. The available research is limited and is often dominated by large organizations who have dedicated networking with custom hardware, as well as specialized staff.With 99.999% service availability, Google cites only 7.6% of Spanner (Google’s globally distributed database) issues are caused by networking even though it keeps crediting its dedicated networking as a core reason behind its availability. Bailis’ and Kingsbury’s survey from 2014 is challenging one of the Fallacies of Distributed Computing coined by Peter Deutsch in 1994. Is network really reliable?We don’t have comprehensive survey outside of giants or over the public Internet. There is also not enough data from major providers how much of their customers issues can be traced back to networking problems. We often experience outages in large cloud provider’s networking stack can take parts of the Internet down for hours but these are only the high-impact events where a large number of visible customers are impacted. Networking outages might be affecting more cases even though not all events are making much noise. Cloud customers don’t necessarily have visibility into their problems either. When there is an outage, identifying it as a networking error caused in the provider is not possible. To them, third-party services are black boxes. Estimating the impact without being a major provider is not possible.In comparison to what major players report on their systems, it might be safe to say you are lucky if networking issues represents a small percentage of your potential problems that cause outage. Networking still suffer from conventional issues such as hardware failures, topology changes, administrative configuration changes and power failures. But I recently learned that newly discovered problems such as SHARK BITES (yes, shark bites) are a reality.ACID has many meanings.ACID stands for atomicity, consistency, isolation, durability. These are the properties database transactions need to guarantee to their users for validity even in the event of crash, error, hardware failures and similar. Without ACID or similar contracts, application developers wouldn’t have a guidance on what’s their responsibility versus what the databases provide. Most relational transactional databases are trying to be ACID-compliant, but new approaches such as NoSQL movement gave birth to many databases without ACID transactions because they are expensive to implement.When I was new in the industry, our tech lead was arguing whether ACID is an obsolete concept or not. It is fair to say ACID is considered a loose description instead of a strict implementation standard. Today, I find it mostly useful because it provides a category of problems (and a category of possible solutions).NOT every database is ACID-compliant and among ACID-compliant databases, ACID can be interpreted differently. One of the reasons why ACID is implemented differently is the number of tradeoffs involved in implementing ACID capabilities. Databases might advertise themselves as ACID but might still have different interpretation in edge cases or how they handle “unlikely” events. Developers can at least learn at a high-level how databases implement things in order to have a proper understanding of fail modes and design tradeoffs.One well-known debate is how ACID MongoDB is even after v4. MongoDB didn’t have journaling support for a long time even though it was not committing data files to disk not more frequently (every 60 seconds) by default. Consider the following scenario, application makes two writes (w1 and w2). MongoDB was able to persist the change for the first write, but it fails to do it for w2 because it crashes due to a hardware failure.An illustration of data loss if MongoDB crashes before it writes to the physical disk.Committing to disk is an expensive process and by avoiding commits, they were claiming to be performant in writes while sacrificing durability. As of today, MongoDB has journaling but dirty writes still can affect the durability of data because they are committing journals at every 100ms by default. The same scenario is still possible for the durability of the journals and changes represented in those logs even though the risk is significantly less.Each database has different consistency and isolation capabilities.Among ACID properties, consistency and isolation have the widest spectrum of different implementation details because the spectrum of tradeoffs is wider. Consistency and isolation are expensive capabilities. They require coordination and are increasing contention in order to keep data consistent. When having to horizontally scale among data centers (especially among different geographic regions), the problems become significantly harder. Providing high levels of consistency can be extremely hard as availability decreases and networking partitions happen more often. See the CAP theorem for a more general explanation of this phenomena. It is worth to also note that applications can handle a bit of inconsistency or programmers might have enough insights about the problem to add additional logic in the application to handle it without heavily relying on their database.Databases often provide a variety of isolation layers so the application developers can pick the most cost effective one based on their tradeoffs. Weaker isolation can be faster but may introduce data races. Stronger isolation eliminates some potential data races but will be slower and might introduce contention that will slow down the database to a point it may cause outages.An overview of the existing concurrency models and the relationships between them.The SQL standard only defines four isolation levels even though there are more levels theoretically and practically available. jepson.io provides a compelling overview of the existing concurrency models if you need further reading. For example, Google’s Spanner guarantee external serializability with clock synchronization and even though this is a stricter isolation layer, it is not defined in the standard isolation layers.The isolation levels mentioned in the SQL standard are:Serializable (most strict, expensive): A serializable execution produces the same effect as some serial execution of those transactions. A serial execution is one in which each transaction executes to completion before the next transaction begins. One note about Serializable level is that it is often implemented as “snapshot isolation” (e.g. Oracle) due to differences in interpretation and “snapshot isolation” is not represented in the SQL standard.Repeatable reads: Uncommitted reads in the current transaction are visible to the current transaction but changes made by other transactions (such as newly inserted rows) won’t be visible.Read committed: Uncommitted reads are not visible to the transactions. Only committed writes are visible but the phantom reads may happen. If another transaction inserts and commits new rows, the current transaction can see them when querying.Read uncommitted (least strict, cheap): Dirty reads are allowed, transactions can see not-yet-committed changes made by other transactions. In practice, this level could be useful to return approximate aggregates, such as COUNT(*) queries on a table.Serializable level allows least opportunities for data races to happen although being the most expensive and introduces the most contention to the system. Other isolation levels are cheaper but increases the possibility of data races. Some databases allow you to set your isolation level, some databases are more opinionated about them and not necessarily supporting all of them.Even though databases advertise their support for these isolation levels, a careful examination of their behavior may provide more insights on what the actually do.An overview of concurrency anomalies at different isolation levels per database.Martin Kleppmann’s hermitage provides an overview of different concurrency anomalies and whether a database is able to handle it at a particular isolation level. Kleppmann’s research shows how isolation levels can be interpreted differently by database designers.Optimistic locking is an option when you can’t hold a lock.Locks can be extremely expensive not only because they introduce more contention in your database but they might require consistent connections from your application servers to the database. Exclusive locks can be effected by network partitions more significantly and cause deadlocks that are hard to identify and resolve. In cases where being able to hold exclusive locks is not easy, optimistic locking is an option.Optimistic locking is a method when you read a row, you take note of a version number, last modified timestamps or its checksum. Then you can check the version hasn’t changed atomically before you mutate the record.UPDATE productsSET name = &#39;Telegraph receiver&#39;, version = 2 WHERE id = 1 AND version = 1Update to products table is going to affect 0 rows if another update has changed this row earlier. If no earlier updates have been done, it will affect 1 row and we can tell our update has succeeded.There are anomalies other than dirty reads and data loss.When we are talking about data consistency, we primarily pay a lot of attention to possible race conditions that can lead to dirty reads and data loss. But anomalies with data are not just limited to them.An example of this type of anomalies is write skews. Write skews are harder to identify because we are not actively looking for them. Write skews are caused not when dirty reads happen on writes or lost but logical constraints on data is compromised.For example, assume a monitoring application that requires one person among their operators to be oncall all the times.BEGIN tx1; BEGIN tx2;SELECT COUNT(*) FROM operatorsWHERE oncall = true;0 SELECT COUNT(*) FROM operators WHERE oncall = TRUE; 0UPDATE operators UPDATE operatorsSET oncall = TRUE SET oncall = TRUEWHERE userId = 4; WHERE userId = 2;COMMIT tx1; COMMIT tx2;In the situation above, there will be a write skew if two of the transactions successfully commit. Even though no dirty read or data loss happened, the integrity of data is lost because there are two people assigned to be oncall.Serializable isolation, schema design or database constraints can be helpful to eliminate write skews. Developers need to be able to identify such anomalies during development to avoid data anomalies in production. Having said that, identifying write skews in code bases can be extremely hard. Especially in large systems, if different teams are responsible for building features based on same tables without talking to each other and examining how they access the data.My database and I don’t always agree on ordering.One of the core capabilities databases offer is the ordering guarantees but ordering may be surprising to the application developer. Databases see transactions in the order they receive them not in the programming order developers see them. The order of the transaction execution is hard to predict especially in high-volume concurrent systems.In development time, especially when working with non-blocking libraries, poor style and readability may contribute to the problem where users think transactions are executed sequentially even though they can arrive at the database at any order. The program below makes it look like T1 and T2 are going to be invoked sequentially, but if these functions are non-blocking and return immediately with a promise, the order of the invocation will be up to the time they have received at the database.result1 = T1() // results are actually promisesresult2 = T2()If atomicity is required (to either fully commit or abort all operations) and the sequence matter, the operations in T1 and T2 should run in a single database transaction.Application-level sharding can live outside the application.Sharding is a way to horizontally partition your database. Even though some databases can automatically partition data horizontally, some don’t or may not be good at it. When data architects/developers can predict how data is going to be accessed, they might create horizontal partitions at the user-land instead of delegating this work to their database. This is called application-level sharding.The name, application-level sharding, often gives the wrong impression that sharding should live in the application services. Sharding capabilities can be implemented as a layer in front of your database. Depending on data growth and schema iterations, sharding requirements might get complicated. Being able to iterate on some strategies without having to redeploy application servers may be useful.An example architecture where application servers are decoupled from the sharding service..Having sharding as a separate service can increase your capabilities on iterating on sharding strategies without having to redeploy your applications. One such example of an application-level sharding system is Vitess. Vitess provides horizontal sharding for MySQL and allows clients to connect to it via the MySQL protocol and it shards the data on various MySQL nodes that don’t know about each other.AUTOINCREMENT’ing can be harmful.AUTOINCREMENT’ing is a common way of generating primary keys. It’s not uncommon to see cases where databases are used as ID generators and there are ID-generation designated tables in a database. There are a few reasons why generating primary keys via auto-incrementing may not be not ideal:In distributed database systems, auto-incrementing is a hard problem. A global lock would be needed to be able to generate an ID. If you can generate a UUID instead, it would not require any collaboration between database nodes. Auto-incrementing with locks may introduce contention and may significantly downgrade the performance for insertions in distributed situations. Some databases like MySQL may require specific configuration and more attention to get things right in master-master replication. The configuration is easy to mess up and can lead to write outages.Some databases have partitioning algorithms based on primary keys. Sequential IDs may cause unpredictable hotspots and may overwhelm some partitions while others stay idle.The fastest way to access to a row in a database is by its primary key. If you have better ways to identify records, sequential IDs may make the most significant column in tables a meaningless value. Please pick a globally unique natural primary key (e.g. a username) where possible.Please consider the impacts of auto-incremented IDs vs UUIDs on indexing, partitioning and sharding before you decide on what works better for you.Stale data can be useful and lock-free.Multi-version concurrency control (MVCC) enables a lot of the consistency features we briefly discussed above. Some databases (e.g. Postgres, Spanner) uses MVCC to allow each transaction to see a snapshot, an older version of the database. Transactions against snapshots still can be serializable for consistency. When reading from an old snapshot, you read stale data.Reading slightly stale data would be useful, for example when you are generating analytics from your data or calculating approximate aggregate values.The first advantage of reading stale data would be latency (especially if your database is distributed among different geographical regions). The second advantage of a MVCC database is that it would allow read-only transactions to to be lock-free. A major advantage in a read-heavy application if the stale data can be tolerated.Application server reads 5-second old stale data from local replica even though the latest version is available on the other side of the Pacific Ocean.Databases sweep the old versions automatically and in some cases, they allow you to do that on demand. For example, Postgres allows users to VACUUM on demand as well as automatically vacuuming once a while, and Spanner runs a garbage collector to get rid of the versions older than an hour.Clock skews happen between any clock sources.The most well-hidden secret in computing is that all time APIs lie. Our machines don’t accurately know what the current time is. Our computers all contain a quartz crystal that produces a signal to tick time. But quartz crystals can’t accurately tick and drift in time, either faster or slower than the actual clock. Drift could be up to 20 seconds a day. The time on our computers need to be synchronized by the actual time every now and then for accuracy.NTP servers are used for synchronization but synchronization itself could be delayed due to network. When synchronizing with an NTP server in the same data center can take time, syncing with a public NTP server may cause more skew.Atomic and GPS clocks are better sources to determine the current time but they are expensive and need complicated setup that they cannot be installed on every machine. Given the limitations, in data centers, a multi-tiered approach is used. While atomic and/or GPS clocks are providing accurate timing, their time is broadcasted to the rest of the machines via secondary servers. This means every machine will be drifted from the actual current time with some magnitude.There is more… Applications and databases often live in different machines (if not in different centers). Not just that database nodes distributed in a few machines won’t be able to agree on what the time is, application server clock and a database node clock won’t agree either.Google’s TrueTime is following a different approach here. Most people think Google’s progress in clocks can be attributed to their use of atomic and GPS clocks, but that’s only the part of the story. This is what TrueTime does:TrueTime uses two different sources: GPS and atomic clocks. These clocks have different fail modes, hence using both of them is increasing the reliability.TrueTime has an unconventional API. It returns the time as an interval. The time could be in fact anywhere between the lower bound and the upper bound. Google’s distributed database Spanner then can wait until it is certain the current time is beyond a particular time. This method adds some latency to the system especially when the uncertainty advertised by masters are high but provides correctness even in a globally distributed situation.Spanner components use TrueTime where TT.now() returns an interval, so Spanner can inject sleeps to ensure the current time has passed a particular timestamp.As the confidence on the current time decreases, it means Spanner operations might take more time. This is why even though having accurate clocks would be impossible, it is still important to keep the confidence high for performance.Latency has many meanings.If you ask ten people in a room what “latency” means, they may all have different answers. In databases, latency is often referred to “database latency” but not the latency client perceives. Client will see a latency of database latency and network latency. Being able to identify client and database latency is critical when debugging escalating problems. When collecting and displaying metrics, always consider having both.Evaluate performance requirements per transaction.Sometimes databases advertise their performance characteristics and limitations in terms of write and read throughput and latency. Although this may give a high level overview of the major blockers, when evaluating a new database for performance, a more comprehensive approach is to evaluate critical operations (per query and/or per transaction) separately. Examples:Write throughput and latency when inserting a new row in to table X (with 50M rows) with given constraints and populating rows in related tables.Latency when querying the friends of friends of a user when average number of friends is 500.Latency of retrieving the top 100 records for the user timeline when user is subscribed to 500 accounts which has X entries per hour.Evaluation and experimentation might contain such critical cases until you are confident that a database will be able to serve your performance requirements. A similar thumb of rule is also considering this breakdown when collecting latency metrics and setting SLOs.Be careful about high cardinality when collecting metrics per operation. Use logs, even collection or distributed tracing if you need high cardinality debugging data. See Want to Debug Latency? for an overview on latency debugging methodologies.Nested transactions can be harmful.Not every database supports nested transactions but when they do, nested transactions may cause surprising programming errors that are not always easy to identify until it becomes clear that you are seeing anomalies.If you’d like to avoid nested transactions, client libraries can do work to detect and avoid nested transactions. If you can’t avoid them, you have to pay attention to avoid ending up surprising situations where committed transactions are accidentally aborted due to a child transaction.Encapsulating transactions in different layers can contribute to surprising nested transaction cases and from a readability point-of-view, it might be hard to understand the intend. Take a look at the following program:with newTransaction(): Accounts.create(&#34;609-543-222&#34;) with newTransaction(): Accounts.create(&#34;775-988-322&#34;) throw Rollback();What’s going to be the result of the code above? Is it going to rollback both of the transactions or only the inner one? What happens if we were relying on multiple layers of libraries that were encapsulating the transaction creation from us. Would we be able to identify and improve such cases?Imagine a data-layer with several operations (e.g. newAccount) already is implemented in their own transactions. What happens when you run them in higher level business logic that runs in it own transaction? What would be the isolation and consistency characteristics would be?function newAccount(id string) { with newTransaction(): Accounts.create(id)}Instead of dealing with such open-ended questions, avoid nested transactions. Your data layer can still implement high level operations without creating their own transactions. Then, business logic can start transactions, run the operations on the transaction, commit or abort.function newAccount(id string) { Accounts.create(id)}// In main application:with newTransaction(): // Read some data from database for configuration. // Generate an ID from the ID service. Accounts.create(id) Uploads.create(id) // create upload queue for the user.Transactions shouldn’t maintain application state.Application developers might want to use application state in transactions to update certain values or tweaks the query parameters. One critical thing to consider is to having the scope right. Clients often retry the transactions when networking issues happen. If a transaction is relying on state that is mutated elsewhere, it might pick the wrong value depending on the possibility of the data races in the problem. Transactions should be careful about in-application data races.var seq int64with newTransaction(): newSeq := atomic.Increment(&amp;seq) Entries.query(newSeq) // Other operations...The transaction above will increase the sequence number each time it runs regardless of its end result. If commit fails due to network, on the second retry, it will query with a different sequence number.Query planners can tell about databases.Query planners determine how your query is going to be executed in the database. They also analyze the queries and optimize them before running. Planners can only provide some possible estimations based on signals it has. How to tell how to find the results for the following query:SELECT * FROM articles where author = &#34;rakyll&#34; order by title;There are two ways to retrieve the results:Full table scan: We can go through every entry on the table and return the articles where author name is matching, then order.Index scan: We can use an index to find the matching IDs, retrieve those rows and then order.Query planner’s role is to determine which strategy is the best option. Query planners have limited signals about what they can predict and might result in poor decisions. DBAs or developers can use them to diagnose and fine tune poorly performing queries. New releases of databases can tweak query planners and self-diagnosing them can help you when upgrading your database if new version introduces performance problems. Reports such as the slow query logs, latency problems, or stats on execution times could be useful to determine the queries to optimize.Some metrics the query planner provides could be noisy, especially when it estimates latency or CPU time. As a supplement to query planners, tracing and execution path tools can be more useful to diagnose these issues even though not every database provides such tools.Online migrations are complex but possible.Online, realtime or live migrations mean migrating from one database to another without downtime and compromising data correctness. Live migrations are easier if you you are migrating to the same database/engine, but can get more complicated when migrating to a new database with different performance characteristics and schema requirements.There are different models when it comes to online migrations, here is one:Start doing dual writes to both databases. At this stage, new database won’t have all the data but will start seeing the new ones. Once you are confident about this step, you can move on to the second.Start enabling the read path to use both databases.Use the new database primarily for reads and writes.Stop writing to the old database although keep reading from the old database. At this point, new database still doesn’t have all the new data and you might need to fallback to the old database for old records.At this point, old database is read-only. Backfill the new database with the missing data from the old database. Once migration is complete, all the read and write paths can use the new database and the old database can be removed from your system.If you need more caste studies, see Stripe‘s comprehensive article on their migration strategy that follows this model.Significant database growth introduces unpredictability.Database growth makes you experience unpredictable scale issues. The more we know about the internals of our databases, the less we might predict how they might scale but there are things we can’t predict.With growth, previous assumptions or expectations on data size and network capacity requirements can become obsolete. This is when large scheme rewrites, large-scale operational improvements, capacity issues, deployment reconsiderations or migrating to other databases happen to avoid outage.Don’t assume knowing a lot about the internals of your current database is the only thing you need, scale will introduce new unknowns. Unpredictable hotspots, uneven distribution of data, unexpected capacity and hardware problems, ever growing traffic and new network partitions will make you reconsider your database, your data model, your deployment model and the size of your deployment.—When I mentioned about potentially publishing this article, I already had five more items on my initial draft. Then, I received an overwhelming amount of new ideas on what else to capture. I tried keep the scope limited to the least obvious problems that need the most attention. This doesn’t mean I won’t get to write more on this topic and won’t keep updating this document.</description>
      <pubDate>22 Apr 20 11:31 EDT</pubDate>
      <guid>https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78</guid>
    </item>
    <item>
      <title></title>
      <link>https://supermemo.guru/wiki/Kill_the_alarm_clock</link>
      <description>&lt;a href=&#34;https://supermemo.guru/wiki/Kill_the_alarm_clock&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;This text is part of: &#34;Science of sleep&#34; by Piotr Wozniak (2017) Kill the alarm clock! Alarm clock epidemic Few upwardly mobile people in the modern rat-race society can live without an alarm clock. With a shot of strong coffee and round-the-clock stress, most people learn to live and survive with an alarm clock. Half of the population wakes up with an alarm, 9% are woken by a partner, 4% by pets, 3% by children, etc. That leaves a minority that wake up naturally. Increasingly, time becomes the most precious commodity in society where achievement is often associated with speed and perfect time-management. However, alarm clocks introduce harmful side effects: stress, sleep debt, and worst of all, disruption of the natural physiological sleep function. At worst, those factors will result in physical damage to the brain (e.g. such sensitive structures as the hippocampus, your memory switchboard, may literally lose neurons as a result of disrupted sleep). The art of time-management makes it possible to live at a high speed with an alarm clock at your side, and still be free from stress. However, the societal damage inflicted by alarm clocks and sleep deprivation is unforgivable. An alarm clock that interrupts your sleep damages your memories, your ability to learn, your creativity, your mood and temper, your relationships with other people, your ability to focus, and your overall intellectual performance! Dr Robert Stickgold has shown that people who learn a skill during the day do not show significant improvement until they get 7-8 hours of good sleep[1]. There was a noticeable correlation between the degree of improvement and the quality of sleep received. My own work with SleepChart also shows that the use of alarm clocks can dramatically reduce memory recall and consolidation. Forgetting is so painless that we rarely notice its effects. In a natural way, forgetting will proceed even if you get as much sleep as you need, and it is difficult to point to specific memories lost as a result of not sleeping enough. Moreover, sleep deprivation may leave your memories intact while their storage will be sub-optimum. The difference may be impossible to spot without measurement. We are more likely to notice sleepiness, reduced mental agility, or bad mood. Disrespect for sleep has reached biblical proportions. This is most noticeable in the US and other highly industrialized nations. Men&#39;s Health&#39;s Dan Vergano writing for ABC News in &#34;No More Rude Awakenings&#34; suggests a seven-day system for fighting sleepiness: &#34;The secret is to fuel that arousal system so it can beat the pants off the sleep system. By creating the kind of feel-good expectations that trigger hormones to wake the brain, you’ll override the need to sleep and be able to jump out of bed like a man on fire&#34;. The article suggests a &#34;fresh&#34; mind method that capitalizes on the fact that stress hormones help keep you alert. However, the only rational remedy for &#34;rude awakenings&#34; is simple: get enough sleep! Jumping like a man on fire is not likely to have a positive effect on your creative potential! You may often notice that waking up with an alarm clock gives you a jumpstart for the day. You may then come to believe that using the alarm clock might help you stay alert later in the day. This is not the case. The alarm signal simply scares your brain into wakefulness, disrupting the carefully planned process of neural optimization that occurs in sleep. As a result, you get an immediate injection of adrenaline and your levels of ACTH and cortisol also increase. This is cortisol that peaks at awakening in natural sleeping rhythm that provides you with the fresh-mind impression. With passing time, this cheaply gained alertness will wear thin unless you continue abusing your physiology with more &#34;remedies&#34;. You may use more scare tactics for keeping yourself alert, abuse caffeine, or even get a more profound effect with modafinil, cocaine, or amphetamines. Alertness should be achieved with the help of sufficient sleep, not despite the lack of sleep! Apart from your reduced ability to learn new things, all unnatural anti-drowsiness methods will produce a great deal of side effects that can be pretty damaging to your health in the long run. All efforts to overcome sleepiness by means other than sleep itself can be likened to a chase of the first high in the use of psychoactive substances. If you drink buckets of coffee, do pushups, pour cold water over your head, or slap your face, you only dip into the last reserves of your alertness hormones that only worsen the effects of deprivation after the effects of the stimulation wear off, which is usually a matter of minutes. Rarely can you get a boost lasting more than an hour, and the more you perk up, the lower you fall in the aftermath. Insomnia trap If your life without an alarm clock may seem like an impossibility, you will probably need to use all methods in the book to be sure you get enough sleep and minimize the damage. If you need to wake up early at the cost of your brain, avoid the insomnia trap! Insomnia trap is a vicious circle of: going to sleep too early to get more sleep, failing to fall asleep in time (or worse, waking up prematurely), feeling even more tired on the next day, and going to sleep even earlier on the next day to catch up with the lost sleep. It is better to go to sleep at a natural hour (i.e. a bit later), wake up early, suffer a degree of sleep deprivation, and hope for a phase reset that will make it possible to continue on the designer schedule. For a solution to the insomnia trap see: Curing DSPS and insomnia. If you cannot reset your phase and still feel tired when getting up early on a regular basis, consider choosing a job that is acceptable for your body, not the other way around. Your long-term health and well-being is at stake. If you absolutely cannot live without an alarm clock, you can at least start from changing your mindset about the importance of sleep and ensure you do not impose wrong habits on your children. Perhaps the young ones will be lucky enough to work in a flex-time system that will make it possible to get sufficient amount of undisturbed sleep. At least, do not set a bad example! Wake up the President President Bill Clinton was woken up twice by telephone during the night of April 22, 2000 before the infamous I.N.S. raid on the home of Miami relatives of the young Cuban exile Elian Gonzales. He was probably the most often disrupted and sleep deprived president in history. Only after a heart surgery did Clinton take diet, sleep and (real) exercise seriously. Those interrupted nights would definitely influence his performance and the quality of his decisions! Has anybody thought of a rule: Do not wake up the president? A rule that could only be revoked in a true national emergency? President G. W. Bush (b. 1946) was woken up when an American spy plane landed in China in 2001. He was also woken up after a suicide bombing in Jerusalem in 2002. George H. W. Bush (b. 1924) and Hilary Clinton made &#34;waking up in the middle of the night&#34; part of their presidential campaign and prowess. It seems that only Ronald Reagan had pretty strong rules for protecting his own sleep. He also famously napped during some cabinet meetings. He slept through a couple of international events without an apparent negative impact on his somewhat delayed decision-making. Critics would say he slept through the entire Iran-Contra affair. Was Reagan so protective of sleep because he understood the role of sleep better, or perhaps he was just a bit lazier than other presidents? I don&#39;t know. However, he sure set a good example. Alarm clock monsters Andrea K. wrote to me with skepticism: &#34;Take the alarm clock away from a typical person and they won&#39;t just wake up on their own at their desired time and they will miss work, school, or whatever. An alarm clock can&#39;t be that bad for you because of the simple fact that most people use it and I never noticed any problem with them :) Everyone in my family has been using one since they were children, and no one suddenly went crazy or began to mutate into a monster (yet)!&#34; When you use an alarm early in the morning in order to get to work or to school, you cut off the later stages of sleep. If the intrusion into natural sleep is not large (e.g. from minutes to an hour), the damage may be limited and hard to notice. Alarm clock will do far more damage if it cuts deep into the middle of the night sleep. You can compare the use of alarm clocks to smoking or eating hot dogs. The harm is not great enough to be instantly noticeable. It took the public many years to largely accept that &#34;smoking is bad&#34; or &#34;fast food is bad&#34;. It is hard to quantify the degree of damage. However, as we move to knowledge society where our intellectual performance becomes increasingly important, the effects of sleep deprivation will come under closer scrutiny and alarm clocks are bound to gradually fall out of favor. Unlike hot dogs, they are already universally hated by their users. Most people are able to somewhat adapt their sleep to their schedules if their routines are regular enough. When those people need to resort to the use of the alarm clock, they cut less of their sleep and the damage is proportionally smaller. Nevertheless, we should always strive at eliminating alarm clocks altogether. Most of all, we should protect our kids from suffering interrupted sleep! References ↑ Stickgold R., &#34;Sleep-dependent memory consolidation,&#34; Nature / Volume 437 (October 27, 2005): 1272-1278 </description>
      <pubDate>17 Feb 21 09:20 EST</pubDate>
      <guid>https://supermemo.guru/wiki/Kill_the_alarm_clock</guid>
    </item>
    <item>
      <title>10 Most(ly dead) Influential Programming Languages</title>
      <link>https://hillelwayne.com/post/influential-dead-languages/</link>
      <description>&lt;a href=&#34;https://hillelwayne.com/post/influential-dead-languages/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; The other day I read 20 most significant programming languages in history, a “preposterous table I just made up.” He certainly got preposterous right: he lists Go as “most significant” but not ALGOL, Smalltalk, or ML. He also leaves off Pascal because it’s “mostly dead”. Preposterous! That defeats the whole point of what “significant in history” means. So let’s talk about some “mostly dead” languages and why they matter so much. Disclaimer: Yeah not all of these are dead and not all of these are forgotten. Like most people have heard of Smalltalk, right? Also there’s probably like a billion mistakes in this, because when you’re doing a survey of 60 years of computing history you’re gonna get some things wrong. Feel free to yell at me if you see anything! Disclaimer 2: Yeah I know some of these are “first to invent” and others are “first to popularize”. History is complicated! Detecting Influence Before we start, a quick primer on finding influence. Just knowing that X was the first language with feature Z doesn’t mean that X actually influenced Z. While Absys was arguably the first logic programming language, almost all of logic programming actually stems from Prolog, which was developed independently. Ultimately there’s only one way to know for certain that X influenced Y: citation. This means one of Y cites X in its reference manual Y cites a paper that cites X The author of Y says “we were influenced by X.” Citations are transitive. Sometimes the language manual for Q lists motivating document R, which cites paper S as an inspiration, which mentions it got the ideas from language T. Then we know that T influenced Q, even if the chain is several steps long. This means digging through many sources to find a signal. To speed this up we use heuristics to decide where to look. One effective heuristic is programming language cognates. It’s very rare for languages to independently come up with the same syntax. So if two languages share some syntax, one likely influenced the other. For example: even without reading design decisions by Matz, we know that Ruby was influenced by Smalltalk, as they both filter a list with a select method. This isn’t conclusive evidence. Maybe Matz came up with it independently, or maybe Ruby and Smalltalk were both influenced by a common ancestor. But it gives us a place to start looking. COBOL Background: CODASYL, 1960. COBOL is shaped by the business/science split in computing. At that time high-level industry languages were either used for engineering computations or managing data. The engineers had all gotten behind FORTRAN while the business world was a mess of COMTRAN, FLOW-MATIC, and others, so the Department of Defense got a committee together to make a single universal business language. That’s COBOL. COBOL was one of the four “mother” languages, along with ALGOL, FORTRAN, and LISP. While we consider it a punchline today, it was once the most popular language in the world. It still runs a lot of our legacy business systems. Significance: In terms of syntax and semantics we don’t see much of COBOL in modern computing. COBOL’s most important addition is the concept of record data. In FORTRAN and ALGOL, your only data structure was the static array. In COBOL, though, you could read in structured files with hierarchical data, and it would automatically destructure them into the representative variables. This was a precursor to modern day structs. Cause of Death: Two factors here. One: COBOL had no overlap with other PLT efforts. Very few people built on COBOL. This meant that second or third generation languages, which built on the lessons of their ancestors, had almost no COBOL DNA. This was less intrinsic problem of COBOL and more because of the academia’s disdain for its creation process. CODASYL was a business group and obviously wasn’t worth paying attention to.1 COBOL was also enormously complex, even for today’s languages. This means that COBOL compilers lagged contemporaries on microcomputers and minicomputers, giving spaces for other languages to flourish and eventually outcompete it. ALGOL Background: The ALGOL committee, 1960. ALGOL-58 came out two years before but was quickly superseded, so I’m wrapping them into each other. The committee wanted to make a good language for researching algorithms. In other words, ALGOL was a formalized “pseudocode”. Of the four mother languages, ALGOL is the most “dead”; Everybody still knows about LISP,2 COBOL still powers tons of legacy systems, and most scientific packages still have some FORTRAN. But I’ve met plenty of programmers who haven’t even heard of ALGOL. You’d think it’d be the least important of the mother languages, but it’s the opposite. Of the four, only LISP comes anywhere close to the pervasive importance of ALGOL. Significance: Let’s see: lexical scoping, structured programming, nested functions, formal language specifications, call-by-name semantics, BNF grammars, block comments… every modern language today is deeply influenced by ALGOL. Cause of Death: ALGOL was a research language, not a commercial language. It was designed for studying algorithms. The spec didn’t define any I/O, which kinda made it impossible to use in practice. Sure, you could write a compiler extension, but then you might as well add other stuff too. And that’s exactly what people did. In 1960 and 70 people made a huge number of ALGOL-likes by extending ALGOL with I/O and extra data structures. This includes JOVIAL, SIMULA, CLU, and CPL. Later languages were then based off these extensions, not ALGOL directly. We call C an “ALGOL-like”, but it’s actually a BCPL-like, which was a CPL-like, which was an ALGOL-like. ALGOL’s children buried it. Eventually the ALGOL people tried to extend it into ALGOL-68, which radically departed from ALGOL-60 and hasn’t had close to the same influence. The ALGOL-60 lineage continues with Niklaus Wirth’s Pascal. APL Background: Ken Iverson, 1962. Originally a hand-written notation for array math, IBM picked it up and used as an programming language. As a language, APL focused on array processing: being able to concisely manipulate large blocks of numbers. If you’ve heard of APL before, you probably know it as “that weird symbol language”. One of the most notorious code snippets is this implementation of the Game of Life: life←{↑1 ⍵∨.∧3 4=+/,¯1 0 1∘.⊖¯1 0 1∘.⌽⊂⍵} You had to write it with a specialized keyboard, like this: An APL keyboard (source) Nonetheless, APL got popular on mainframes for running with very low memory requirements. Significance: Array processing. At a time when adding two lists of numbers meant a map or a loop, APL introduced the idea of operating on the entire array at once. For example: 1 + 1 2 3 4 2 3 4 5 1 2 3 4 + 1 2 3 4 2 4 6 8 2 4 ⍴ ⍳8 1 2 3 4 5 6 7 8 1 2 3 4 +[2] 2 4 ⍴ ⍳8 2 4 6 8 6 8 10 12 This was a really big deal in scientific circles. So much applied math boils down to large-scale operations on large matrices. When you can just take the outer product with ∘.f, it’s really damn easy to take outer products! Through this innovation APL lead to R, numpy, pandas, Matlab, etc. There’s also the direct descendants of APL: J, Dyalog, K, Q. They’ve been less successful but still see lots of use in the finance sector. Cause of Death: Well, the obvious problem is the keyboards. If you can’t write it in ASCII, you’re not going to write very much of it. Iverson fixed this with J, which uses digraphs instead of different symbols. Instead of ≠, you write ~:. This was in 1990, though, which is a bit late to popularize a radically different programming style. The subtler problem is that APL and J only worked on homogeneous data. You can’t store both strings and numbers in the same data structure (unless you use boxes, which is a whole other can of worms) and working with strings is generally a nightmare. So no dataframes, which excludes a lot of modern data science. Further Reading: Notation as a Tool of Thought BASIC Background: John Kemeny, 1964. Originally a simplified FORTRAN-like, intended to help people outside engineering use computers. BASIC really took off in the microcomputer era. The first microcomputers didn’t have enough memory to compile “real” programming languages, whereas you could cram a pared-down BASIC compiler into like 2 kilobytes. BASIC became a lingua franca for early-stage programmers. If you were programming at home in the 1970’s, you were probably writing BASIC on a microcomputer. 10 PRINT &#34;Hello, World!&#34; 20 END Significance: The biggest technical impact is runtime interpretation. BASIC was the first language with a real-time interpreter (the Dartmouth Time Sharing System), beating APL by a year. And that APL system was only available to IBM customers, so really it was BASIC or nothing for a long time.3 BASIC had a bigger social impact. It brought programming to households, kids especially. Many of the influential programmers in the 80’s and 90’s first learned how to program on BASIC. Many enterprise programs were also written in BASIC, which probably helped accelerate the decline of Cobol. BASIC has one more neat trick up its sleeve: Office tooling! Microsoft eventually turned BASIC into Visual Basic, which they used as the Office macro language. This then spread to OpenOffice and LibreOffice, entrenching BASIC in that particular niche. More recently it’s lost ground to JavaScript and is now a legacy macro language. Cause of Death: People saw BASIC as a “lesser” language. You might use it if you were a kid or a small business owner, but real programmers used a real language. Once manufacturers could cheaply make microcomputers with more than 16k of RAM they started depreciating BASIC for languages like Pascal and C. BASIC lived on for a while as a legacy kids teaching language, but seems to have died out of that niche, too. PL/I Background: IBM, 1966. IBM’s business was split into two languages: FORTRAN for scientists and COMTRAN for business folk. Facing competition from COBOL and wanting to streamline their systems, they tried to make a language that was useful for both engineering and business purposes. This ended up looking like a sort of superset of the two languages, with a bunch of additional features stapled on top.4 Now everybody could use the same language and IBM can make a lot more money! Yaaaaaaaay Significance: The authors of ALGOL-68 mockingly called PL/I an obsolete language. But everything ALGOL-68 did, PL/I did earlier and better. While COBOL got structured data first, PL/I was the first language to implement them as a type. In COBOL, reading in a user with a name would give you two global variables, user and name. In PL/I, you’d get one variable with a field, user.name. PL/I was also the first high-level language with pointers for direct memory manipulation, constants, and function overloading.5 Many of these ideas entered mainstream programming via C, which was a mix of both BCPL and PL/I. C even uses PL/I’s comment syntax. Cause of Death: All the FORTRAN programmers thought it was too much like COBOL and all the COBOL programmers thought it was too much like FORTRAN. IBM had tried to take on two established languages with a much more complicated one. It didn’t help that they were the only group with the compiler, meaning everybody else was mistrustful of vendor lock-in. By the time IBM was able to make headway in both of these issues the wider computing world had already moved on to the microcomputer era, where PL/I was out competed by BASIC. Further Reading: The Choice of PL/I SIMULA 67 Background: Ole Dahl and Kristen Nygaard, 1967. They extended ALGOL for doing simulations. First they made SIMULA I, which had dedicated simulation and “activity” syntax. SIMULA I saw some early use, but the two were dissatisfied with how “specialized” the language felt and how much duplicate code they had in their simulations. They wanted to make a more general framework for representing things in general, not simulations only. Their idea was to allow users to define new types called “classes” with polymorphic function resolution. Then users could build the simulation features as a special case of the object system, making it easy to customize how it all worked to their particular needs. Significance: While SIMULA wasn’t the first “true” OOP language, it was the first language with proper objects and laid much of the groundwork that others would build on. This includes the class/object split, subclassing, virtual methods, and protected attributes. It inspired almost all of the academic research into objects after 1967. Both CLU and ML cited SIMULA as a major source of inspiration. Bjarne Stroustroup did his PhD on SIMULA, eventually incorporating a lot of its ideas into C++. Cause of Death: In that same PhD Stroustroup claimed that SIMULA was waaaaaay too slow to use at scale. “Good luck getting anything done if you aren’t on a mainframe” slow. It’s worth noting that Smalltalk-80, which took the same ideas even further, had an extra 13 years of Moore’s law behind it. And even Smalltalk was often mocked as too slow. Everybody went and implemented the ideas in SIMULA that they could integrate into faster, simpler languages. Further Reading: Compiling SIMULA: a historical study of technological genesis, The History of Simula Pascal Background: Niklaus Wirth, 1970. Made to capture the essence of ALGOL-60 after ALGOL-68 got waaaaaay too complicated for Wirth’s liking. It first got big as the “introduction to CS” language, and by the early 80’s was the second-most popular language on the Usenet job boards. Wirth considers the whole family- Pascal, Modula, and Oberon- as a single unified language concept. Significance: Pascal didn’t introduce any completely new ideas. It was an intentionally conservative language that tried to pick the best parts of the past decade and provide them in a unified package. Pascal brought ALGOL syntax outside academia, so much so that ALGOL’s assignment syntax, :=, got called “Pascal style” instead. From this point on most language features that look like ALGOL were more likely inspired by Pascal than directly by ALGOL itself. While Pascal wasn’t very innovative, variants of it were. Wirth also pioneered the idea of “stepwise refinement” as a means of writing rigorous software. This eventually lead to the Modulas, which popularized first class software modules, and Euclid, the first formal verification language to see production use. Cause of Death: I’m calling a mulligan on this one. Unlike most of the other ones on this list, Pascal didn’t have major structural barriers or a sharp competitor. Sure, it competed with C, but it was still doing fine for a very long time. People usually attribute the Why Pascal is not my favorite language essay, but that’s too neat of an answer and history is a lot messier. Also, Delphi is still pretty high-ranked in the TIOBE and PYPA measurements, so it’s not exactly dead in the same way SIMULA is. An accurate analysis of the fall of Pascal would be longer than the rest of this essay. Further Reading: The Programming Language Pascal, Pascal and its Successors CLU Background: Barbara Liskov, 1975. Liskov wanted to mess around with abstract data types. That’s it. That’s the whole reason for CLU. Significance: CLU might be the most influential language that nobody’s ever heard of. Iterators? CLU. Abstract data types? CLU. Generics? CLU. Checked exceptions? CLU. We didn’t adopt the same terminology, so it’s not 100% obvious it all comes from CLU, but still. Every language spec for the next decade would namedrop CLU. CLU did a lot. Cause of Death: CLU was a showcase language; Liskov wanted to get people to adopt her ideas, not her specific language. And they did: almost every language today owes something to CLU. As soon as she completed CLU she moved on to Argus, which was supposed to showcase her ideas on concurrency. That hasn’t seen nearly the same adoption, and there’s still a lot of stuff in it left to mine. Further reading: A History of CLU ML Background: Robin Milner, 1976.6 Milner was building the LCF Prover, one of the first proof assistants. If you wrote a proof in the right format, LCF could check to see if it was correct or not. To assist in writing the proofs, Milner created a metalanguage based on sound mathematical formalisms, which at the time meant strict static types and higher-order functions. Eventually ML was standardized as Standard ML. Significance: ML is arguably the oldest “algebraic programming language”. There’s a lot of stuff we attribute to ML: algebraic data types, modules, typed functional programming. Surprisingly, it was not the first for a lot of these! The first ML was just designed to work with LCF and wasn’t a general purpose language, so lacked a lot of these features. As people started making it more general they pulled ideas from other research languages and incorporated them into ML. One very important idea did start in ML, though: type inference. ML was the first statically-typed language where you didn’t have to write the types out, as the compiler would figure out the types for you. This paved the way for typed FP to escape academia and enter production use. ML also greatly influenced modern theorem provers. The “program” languages for Isabelle, CVC3, and Coq are ML-based. And a lot of type theory was based on ML, though in more recent years the Haskell branch of FP has become more popular.7 Cause of Death: ML had a lot of interesting features, but people paid attention to it for the type inference. At the time ML was still a special purpose language for the theorem provers. SML came out the same year as Haskell, which was a much “purer” example of a typed FP language.6 Smalltalk Background: Alan Kay, 1972, 1976, and 1980. It’s sort of a moving target. Smalltalk-72 was the first, Smalltalk-76 introduced the idea of “object-oriented programming” to the wider world, and Smalltalk-80 was the one that saw widespread adoption. Smalltalk wasn’t the first language with objects but it was the first “object-oriented” one. The difference was that Simula had objects in addition to primitives like numbers and booleans, while in Smalltalk, booleans were also objects. I wrote a bit about this here if you want to learn more. Significance: We sometimes think that Smalltalk is “true” OOP and things like Java and Python aren’t “real” OOP, but that’s not true. OOP is a giant mess of many different influences, just like every other paradigm. But it was certainly the thing that popularized the idea. If you crack open any general theory OOP book from the mid-80’s or early 90’s, they’ll be in Smalltalk. Many will also translate their examples to C++, and a few will use another language, but everybody will use Smalltalk. Smalltalk also spread the idea of objects as shareable data, leading the way to CORBA, and it inspired the computational Actor model. Cause of Death: The common belief is that Smalltalk lost because people used C++ instead. But that’s untrue. Smalltalk did have some issues, specifically its difficulty interoping with other tools and poor runtime performance. But even into the 1990’s Smalltalk was doing respectable business and many people assumed it would be a dominant business language. Then Java happened. (source) Smalltalk wasn’t the only casualty of the “Javapocalypse”: Java also marginalized Eiffel, Ada95, and pretty much everything else in the OOP world. The interesting question isn’t “Why did Smalltalk die”, it’s “Why did C++ survive”. I think it’s because C++ had better C interop so was easier to extend into legacy systems. This is just a small sample of the important dead languages. I didn’t cover ALPHARD, ALTRAN, Argus, Automath, BCPL, COMTRAN, CPL, Eiffel, FLOW-MATIC, HOPE, Hypercard, ISWIM, JOVIAL, MacSyma, Mesa, Miranda, Multics Shell, PLANNER, SMP, Sketchpad, or SNOBOL. All of them contributed in their own way to the modern programming world. History is complicated. Most influential languages never went mainstream. Few people used any one of them. But each one inspired people, who inspired other people, so the DNA of these forgotten languages appear decades after they’re forgotten. But there are also untold languages that didn’t get their ideas out. The Encyclopaedia of Programming Languages lists over 8,000 programming languages. Many of them had ideas that never left their bubble. Consider how much we’d have lost if nobody had heard of SIMULA, or Liskov never shared CLU. That’s one reason I love studying history. To learn what we’ve lost and find it again. The first draft of this was originally shared on my newsletter. If you found this interesting, why not subscribe? Thanks to Miikka Koskinen, Kevlin Henney, Eric Fischer, and Levi Pearson for corrections and feedback. </description>
      <pubDate>26 Mar 20 11:12 EDT</pubDate>
      <guid>https://hillelwayne.com/post/influential-dead-languages/</guid>
    </item>
    <item>
      <title>What&#39;s the Right Tool for the Job?</title>
      <link>https://www.hillelwayne.com/right-tool/</link>
      <description>&lt;a href=&#34;https://www.hillelwayne.com/right-tool/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; “Use the right tool for the job” is a pretty tired cliche. Mostly it’s used to dismiss overengineering and one-size-fits-all solutions to problems, like using microservices for your 10-user app. It isn’t a bad saying, it’s just tautologically true. I don’t think anybody wants to use the wrong tool for the job, unless they’re trying to sabotage it. “Should I use the right tool for the job?” is a rhetorical question. “What’s the right tool for the job?” is a much more fun question. Imagine someone says “Which datastore should I use for my startup: Postgres or MongoDB?” I’d probably say Postgres and give a bunch of reasons why. But what if someone else comes in and says “MongoDB”? They could probably give a bunch of good reasons, too. After all, five years ago it was The Future of Databases! How can I reasonably claim that Postgres is the right tool? And what about the 300 other database engines? To some extent you can satisfice: say Postgres may not be the perfect fit, but it’s good enough. But that doesn’t solve the problem: how do we decide what’s “good enough”? Is MongoDB good enough? Memcache? TinyDB? At some point you need to answer the question. Here’s some answers I’ve seen and some thoughts on them.1 Requirements First things first is the sanity check. Often you can rule out tools (or more rarely, decide on one) solely based on the requirements you have. You are not going to use Mongo for bank accounts, because you need complex transactions. You wouldn’t use Excel or Haskell in embedded hardware. You wouldn’t use regex to parse HTML. Rarely, you find the right tool with requirements alone. Usually you’ll still have a bunch of candidates. Let’s move on. Experience This is probably the simplest answer: “I have experience with Postgres. I know it works.” It’s a pretty fast heuristic and in many cases works just fine. On the other hand, can you really trust yourself? You only have a limited frame of reference and all the squishy meat biases that make us believe dumb things. Personal story: I thought PHP was God’s gift to mankind and brushed off all the arguments against it with “I use the right tool for the job”. In retrospect, the reason I thought it was so great was because the only other languages I knew at the time were C++ and Matlab.2 After I learned Python I realized that PHP was actually pretty cruddy. I know Dunning-Kruger is often overused3 but I think it applies here. We’re bad at objectively judging ourselves. There’s only a weak correlation between how much you believe you understand something and how much you actually do. We need to recognize the limits of our experience, which means finding some other way to decide. Listening to an authority. Also known as “trusting someone else’s experiences”. This is A Bad Idea for all of the same reasons that trusting your own is. If you don’t believe me, here’s Ken Thompson saying safer languages are dumb and here’s Guido Van Rossum saying map and filter are dumb. Most authorities are really, really good at what they do. But they still have the same limits of experiences and biases we do, and we shouldn’t just take them on their word. Abstract Arguments The vast majority of information produced is people using “reason”, aka abstract arguments. From a pro-Mongo article: The majority of ‘operational’ data is non-relational Schema data is hard to refactor You rarely need transactions. On the other hand, from an anti-Mongo article: The majority of ‘business case’ data is relational Schemaless data is hard to refactor You always need transactions. Two sets of abstract arguments saying the exact opposite things. Who do you believe? Without any outside information, we tend to agree with who made the better argument. In practice that’s the better writer. If all you’ve got is “reason”, the Right Tool may be “whichever has the best marketing.” Fortunately, we can do much better than abstract arguments. We can actually support our claims with evidence. Korokithakis brought in a lot more evidence than Asay did. That’s a better path to finding the right tool. “But how can we trust evidence?” I don’t know, I’m not a philosopher. I guess we could say that knowledge isn’t a “thing” so we can’t play the same languages games with–4 “But how can we trust this piece of evidence?” Okay that I can answer. Let’s list some pieces. Examples Examples are self-contained demonstrations of a tool. They require minimal contextual knowledge to understand. Most often they are created for the sole purpose of being evidence, unlike f.ex case studies (which often are byproducts of software development). Kyle Banker uses one to show how easily Mongo handles shopping carts: {&#39;_id&#39;: objectid(&#39;4b980a6dea2c3f4579da141e&#39;), &#39;user_id&#39;: objectid(&#39;4b980a6dea2c3f4579a4f54&#39;), &#39;state&#39;: &#39;cart&#39;, &#39;line_items&#39;: [ {&#39;sku&#39;: &#39;jc-432&#39;, &#39;name&#39;: &#39;John Coltrane: A Love Supreme&#39;, &#39;retail_price&#39;: 1099 }, {&#39;sku&#39;: &#39;ly-211&#39;, &#39;name&#39;: &#39;Larry Young: Unity&#39;, &#39;retail_price&#39;: 1199 }, ], &#39;shipping_address&#39;: { &#39;street&#39;: &#39;3333 Greene Ave.&#39;, &#39;city&#39;: &#39;Brooklyn&#39;, &#39;state&#39;: &#39;NY&#39;, &#39;zip&#39;: &#39;11216&#39; }, &#39;subtotal&#39;: 2199 } Here we’ve got some query that, he claims, would require a bunch of nasty joins across multiple SQL tables, while the Mongo shopping cart is a single object. He argues this means Mongo is easier to work with. I love examples. I spend a lot of time thinking about examples. I think a good example is worth a hundred abstract arguments. But examples are contrived. This makes them easily explainable and good for showcasing things, but they don’t completely reflect reality. The writer can choose which parts of the real world they reproduce and this, unconsciously or not, can rig the evidence. If an example would undermine the writer’s point, they can simple choose not to present it, or tweak it to look better. Let’s go back to the Mongo example above. Banker also talks about how easy aggregation is: map = &#34; function() { emit(this[&#39;shipping_address&#39;][&#39;zip&#39;], {total: this.total}) }&#34; reduce = &#34; function(key, values) { var sum = 0; values.forEach(function(doc) { sum += doc.total; } return {total: sum}; }&#34; db.orders.mapReduce(map, reduce, {out: &#39;order_totals_by_zip&#39;}); Is this really better for reporting than SUM and GROUP BY? What if you needed to add the top three totals for each zip? What if you need to find the best-selling item in each zip, or the item with the highest deviation? By slightly tweaking the example we can make it more pro- or anti-subject. Case Studies Case studies are analyses of real-world systems. Sometimes the system was created for the purpose of providing a case study, more often the case study is a happy byproduct. Case studies are some of the strongest evidence that the average person can produce, because they can faithfully represent all of the tradeoffs in a tool. They also (in theory) can’t be ignored: the downsides of an approach can’t be argued away by ‘reason’. Probably the most famous anti-Mongo article is Sarah Mei’s Diaspora Case Study. She identifies the issues they had with MongoDB: Most of their data ended up being relational Making SQL more performant was easier than making Mongo more constistant Their non-relational data also turned out to be relational Score one for Korokithakis! His reasoning is backed by the evidence of reality. Case studies are great and we should write more. At least, we need to write more good case studies, and it’s easy for even an experienced person to write a bad one. Making a good case study is exhausting. You need to first build the system, then analyze it, and then make sure you have as much detail as possible. Often, you can bias a case study by just leaving stuff out. You can also bias it by not exploring the counterfactuals: showing that “A” is not just better than “nothing”, but also better than “B”. You see this a lot with microservices articles. The writer is really enthusiastic about how microservices worked better for them than a monolith… after they invested a ton of time into transitioning to microservices. What if they instead invested that time into redesigning the monolith? I’ve never seen any comparisons of that; if you have any, send them my way. There’s an uglier, more dangerous issue with case studies. Just as you can tweak examples, you can rig case studies. Business Insider has a case study where they found Mongo a great fit. In the full disclosure, though, they mention that Business Insider and 10gen (the creators of Mongo) share a cofounder. Sure, the author says “I [still] believe it’s the best technology for us”, but Mongo was only officially released just nine months before the BI article. According to Nemil Dalal 10gen consistently used case studies as a marketing tactic, meaning many pro-Mongo case studies should be treated with caution. One way to identify biased case studies is to see how “fair” they are. Mei talks about why Mongo was a good idea at first, then what the workarounds to their initial problems were, and then finally what Mongo’s effective use case is. Even though she’s strongly anti-Mongo, she’s willing to talk about the good aspects. On the other hand, BI lists the flaws as “it lacks transactions, so banks shouldn’t use it”, and “it’s not good if your legacy system relies on SQL”. These ‘flaws’ don’t show balance or a well-rounded case study. The final issue with case studies is they’re pretty hard to find. You ideally want a few so you can see patterns or correct for biases in individual cases, but for niche problems you might only find one or two. I’ve started bookmarking all the good studies I read, regardless of whether it’s actually useful, just in case I ever need it. Formal Research Controlled studies, obsessive benchmarking, experiments on undergrads, all that fun stuff. A full academic study is, in theory, the best we can aspire to. In practice, we’re not quite there yet. First of all, most software engineers don’t really respect research. It’s a common meme that there’s no way to measure software engineering. Second, most papers are inconclusive with many factors of error. If a paper sees no difference between bugs in static and dynamic type systems, was it because there is no difference? Small sample size? Wide variations in programmer abilities? The specific type system? The scope of the exercise? Amount of time they had? Both of these are solvable problems, but they’re both symptoms of the third and biggest problem: software engineering research is hard. Really hard. The Software Engineering Institute is one of the biggest institutes in the USA and we pour 300 million into it every year.5 Sure, they do stuff besides academic research, but still. You have to spend a lot of time and money to get even marginal results. Formal Research is a definite nice to have, but I wouldn’t count on having it for any given question. So where does that leave us? I’m seeing a tradeoff of ease vs rigour here. For most small decisions you’re fine just using experience. Otherwise, in my experience (ha!) case studies are the gold standard. They’re more informative than everything but conclusive research, which is almost never available. Plus, unlike research, “laypeople” can produce useful case studies: you’re presumably doing something at work, and it’s usually possible to turn “here’s what I did this quarter” into something people can learn from. Note to self: write some case studies. One other interesting thing: I feel (but am not certain) that while examples are considerably more rigorous than abstract argument, they aren’t much harder to create. So if you’re writing about a tool and are primarily using abstract arguments, back them all up with examples to make your case stronger. </description>
      <pubDate>16 Feb 21 13:27 EST</pubDate>
      <guid>https://www.hillelwayne.com/right-tool/</guid>
    </item>
    <item>
      <title>Ask Ethan: Is Spacetime Real?</title>
      <link>https://www.forbes.com/sites/startswithabang/2021/02/12/ask-ethan-is-spacetime-real/?sh=3ed8d588fe85</link>
      <description>&lt;a href=&#34;https://www.forbes.com/sites/startswithabang/2021/02/12/ask-ethan-is-spacetime-real/?sh=3ed8d588fe85&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; An illustration of heavily curved spacetime, outside the event horizon of a black hole. As you get ... [+] closer and closer to the mass&#39;s location, space becomes more severely curved, eventually leading to a location from within which even light cannot escape: the event horizon. PIXABAY USER JOHNSONMARTIN When most of us think about the Universe, we think about the material objects that are out there across the great cosmic distances. Matter collapses under its own gravity to form cosmic structures like galaxies, gas clouds contract to form stars and planets; stars emit light by burning their fuel through nuclear fusion; that light travels throughout the Universe, illuminating anything it comes into contact with. But there’s more to the Universe than the objects within it. There’s also the fabric of spacetime, which has its own set of rules that it plays by: General Relativity. The fabric of spacetime is curved by the presence of matter and energy, and curved spacetime itself tells matter and energy how to move through it. But what, exactly, is spacetime, and is it a “real” thing, or just a calculational tool? That’s the inquiry of Dave Drews, who wants to know: “What exactly is spacetime? Is it a real thing like an atom, or just a mathematical construct that is used to describe how mass &#39;generates&#39; gravity?” It’s an excellent question, and a tough one to wrap your head around. Moreover, before Einstein came along, our conception of the Universe was very different from the one we have today. Let’s go way back to the Universe before we even had the concept of spacetime, and then come forward to where we are today. From macroscopic scales down to subatomic ones, the sizes of the fundamental particles play only a ... [+] small role in determining the sizes of composite structures. Whether the building blocks are truly fundamental and/or point-like particles is still not known, but we do understand the Universe from large, cosmic scales down to tiny, subatomic ones. There are nearly 10^28 atoms making up each human body, in total. MAGDALENA KOWALSKA / CERN / ISOLDE TEAM At a fundamental level, we had long supposed that if you took everything that was in the Universe and cut it up into smaller and smaller constituents, you’d eventually reach something that was indivisible. Quite literally, that’s what the word “atom” means: from the Greek ἄτομος: not able to be cut. The first record we have of this idea goes back some 2400 years to Democritus of Abdera, but it’s plausible that it may go back even farther. These “uncuttable” entities do exist; each one is known as a quantum particle. Despite the fact that we took the name “atom” for the elements of the periodic table, it’s actually subatomic particles like quarks, gluons, and electrons (as well as particle that aren’t found in atoms at all) that are truly indivisible. These quanta bind together to build up all the complex structures we know of in the Universe, from protons to atoms to molecules to human beings. And yet, no matter what types of quanta we deal with — matter or antimatter, massive or massless, fundamental or composite structures, on subatomic or cosmic scales — those quanta only exist within the same Universe that we do. If you know all the rules governing how an object will move through spacetime as well as the initial ... [+] conditions and the exertion of forces between the object and the remainder of your system, you should be able to predict how this object will move through both space and time. You cannot describe the position of an object accurately without including a time coordinate in addition to the spatial ones. Tristan Fewings/Getty Images This is important, because if you want the “stuff” in your Universe to do things to one another — interact, bind together, form structures, transfer energy, etc. — there has to be a way for the different things that exist within the Universe to affect one another. It’s similar to having a play where you have all the characters fleshed out, all the actors ready to play them, all the costumes ready to be worn, and all the lines written and memorized. The only thing missing, and yet very necessary for the play to occur, is a stage. What is that stage, then, in physics? Before Einstein came along, the stage was set by Newton. All of the “actors” in the Universe could be described by a set of coordinates: a location in three-dimensional space (a position) as well as a moment in time (an instant). You can envision it like a Cartesian grid: a three-dimensional structure with an x, y and z axis, where every quantum can also have a momentum, describing its motion through space as a function of time. Time itself was assumed to be linear, always passing at the same rate. In Newton’s picture, both space and time were absolute. We often visualize space as a 3D grid, even though this is a frame-dependent oversimplification when ... [+] we consider the concept of spacetime. In reality, spacetime is curved by the presence of matter-and-energy, and distances are not fixed but rather can evolve as the Universe expands or contracts. ReunMedia / Storyblocks However, the discovery of radioactivity in the late 19th century began to throw Newton’s picture into doubt. The fact that atoms could emit subatomic particles moving close to the speed of light taught us something exciting: when a particle moved close to the speed of light, it experienced space and time very differently from something that was either slow-moving or at rest. Unstable particles that would decay very quickly at rest lived longer the closer to the speed of light they moved. Those same particles traveled greater distances than their speeds and lifetimes would indicate before decaying. And if you tried to calculate the energy or momentum of a particle in motion, different observers (i.e., people watching the particle and moving at different speeds relative to it) would calculate values that were inconsistent with one another. Something must be flawed with Newton’s conception of space and time. At speeds close to the speed of light, time dilates, lengths contract, and energy and momentum really are frame-dependent. In short, the way you experience the Universe depends on your motion through it. A light-clock, formed by a photon bouncing between two mirrors, will define time for any observer. ... [+] Although the two observers may not agree with one another on how much time is passing, they will agree on the laws of physics and on the constants of the Universe, such as the speed of light. A stationary observer will see time pass normally, but an observer moving rapidly through space will have their clock run slower relative to the stationary observer. John D. Norton Einstein was responsible for the remarkable breakthrough of the concept of relativity, which identified which quantities were invariant, and didn’t change with the observer’s motion, and which ones were frame-dependent. The speed of light, for example, is the same for all observers, as is the rest mass of any quantum of matter. But the spatial distance you’d perceive between two points depended very strongly on your motion along the direction connecting those points. Similarly, the rate at which your clock ran as you journeyed from one point to another also depended on your motion. Space and time weren’t absolute, as Newton intuited, but were experienced differently by different observers: they were relative, which is where the name “relativity” comes from. Moreover, there was a specific relationship between how any particular observer experienced space and how they experienced time: something that was put together a couple of years after Einstein put forth his special theory of relativity by his former professor, Hermann Minkowski, who laid out a unified mathematical structure encompassing space and time together: spacetime. As Minkowski himself put it, “Henceforth space by itself, and time by itself, are doomed to fade away into mere shadows, and only a kind of union of the two will preserve an independent reality.” Today, this spacetime is still commonly as our stage used whenever we neglect gravity: Minkowski space. An example of a light cone, the three-dimensional surface of all possible light rays arriving at and ... [+] departing from a point in spacetime. The more you move through space, the less you move through time, and vice versa. Only things contained within your past light-cone can affect you today; only things contained within your future light-cone can be perceived by you in the future. This illustrates flat Minkowski space, not the curved space of General Relativity. Wikimedia Commons user MissMJ But in our real Universe, we do have gravitation. Gravity isn’t a force that acts instantly across the far reaches of space, but rather only propagates at the same speed all massless quanta move at: the speed of light. (Yes, the speed of gravity equals the speed of light.) All the rules that were formulated in special relativity still apply to the Universe, but to bring gravity into the fold, something extra was required: the notion that spacetime itself had an intrinsic curvature to it that depended on the presence of matter and energy within it. It’s simple, in a sense: when you put a set of actors on a stage, that stage needs to bear the weight of the actors themselves. If the actors are massive enough and the stage isn’t perfectly rigid, the stage itself will deform owing to the presence of the actors. The same phenomenon is at play with spacetime: the presence of matter and energy curves it, and that curvature affects both distances (space) and the rate at which clocks run (time). Moreover, it affects the two of them in an intricate way, where if you calculate the effects that matter and energy have on spacetime, the “spatial” effect and the “temporal” effects are related. Instead of the three-dimensional grid lines we envisioned in special relativity, those grid lines are now curved in General Relativity. Instead of an empty, blank, 3D grid, putting a mass down causes what would have been &#39;straight&#39; ... [+] lines to instead become curved by a specific amount. Note that they appear to drag towards, rather than away from, the mass in question. CHRISTOPHER VITALE OF NETWORKOLOGIES AND THE PRATT INSTITUTE You can, if you like, conceptualize spacetime as a purely calculational tool and never go any deeper than that. Mathematically, every spacetime can be described by a metric tensor: a formalism that allows you to calculate how any field, line, arc, distance, etc., can exist in a well-defined way. The space can be flat or curved in an arbitrary way; the space can be finite or infinite; the space can be open or closed; the space can contain any number of dimensions. In General Relativity, the metric tensor is four dimensional (with three space dimensions and one time dimension), and the thing that determines the curvature of spacetime is the matter, energy, and stresses present within it. In plain English, the contents of your Universe determine how spacetime is curved. You can then take the spacetime curvature and use it to predict how every quanta of matter and energy will move through and evolve in your Universe. The rules of General Relativity enable us to predict how matter, light, antimatter, neutrinos, and even gravitational waves will move through the Universe, and those predictions line up exquisitely with what we observe and measure. The signal from the gravitational wave event GW190521, as seen in all three detectors. The entire ... [+] signal duration lasted just ~13 milliseconds, but represents the energy equivalent of 8 solar masses converted to pure energy via Einstein&#39;s E = mc^2. R. Abbott et al. (LIGO Scientific Collaboration and Virgo Collaboration), Phys. Rev. Lett. 125, 101102 What we don’t measure, though, is spacetime itself. We can measure distances and we can measure time intervals, but those are only indirect probes of the underlying spacetime. We can measure anything that interacts with us — our bodies, our instruments, our detectors, etc. — but an interaction only occurs when two quanta occupy the same point in spacetime: when they meet at an “event.” We can measure every one of the effects that curved spacetime has on the matter and energy in the Universe, including: the redshifting of radiation due to the Universe’s expansion, the bending of light due to the presence of foreground masses, the effects of frame-dragging on a rotating body, the additional precession of orbits due to gravitational effects that go beyond what Newton predicted, how light gains energy when it falls deeper into a gravitational field and loses energy when it climbs out of it, and many, many others. But the fact that we can only measure the effects of spacetime on the matter and energy in the Universe, and not the spacetime itself, tells us that spacetime behaves indistinguishably from a purely calculational tool. Quantum gravity tries to combine Einstein’s General theory of Relativity with quantum mechanics. ... [+] Quantum corrections to classical gravity are visualized as loop diagrams, as the one shown here in white. If you extend the Standard Model to include gravity, the symmetry that describes CPT (the Lorentz symmetry) may become only an approximate symmetry, allowing for violations. Thus far, however, no such experimental violations have been observed. SLAC National Accelerator Laboratory But that doesn’t mean that spacetime itself isn’t a physically real entity. If you have actors acting out a play, you’d justifiably call the location where the play took place “their stage,” even if it was simply a field, a platform, bare ground, etc. Even if the play took place in the weightlessness of space, you’d simply note that they were using their freely-falling reference frame as a stage. In the physical Universe, at least as we understand it, you cannot have quanta or interactions between them without the spacetime for them to exist in. Wherever spacetime exists, so do the laws of physics, and so do the fundamental quantum fields that underpin all of nature. In a sense, “nothingness” is the vacuum of empty spacetime, and talking about what occurs in the absence of spacetime is as nonsensical — at least from a physics perspective — as talking about a “where” that’s outside of the boundaries of space or a “when” that’s outside of the boundaries of time. Such a thing may exist, but we have no physical conception of it. An animated look at how spacetime responds as a mass moves through it helps showcase exactly how, ... [+] qualitatively, it isn&#39;t merely a sheet of fabric. Instead all of 3D space itself gets curved by the presence and properties of the matter and energy within the Universe. Multiple masses in orbit around one another will cause the emission of gravitational waves. LucasVB Perhaps most interestingly, when it comes to the nature of spacetime, there are so many questions that remain unanswered. Are space and time inherently quantum and discrete, where they themselves are divided up into indivisible “chunks,” or are they continuous? Is gravity inherently quantum in nature like the other known forces, or is it somehow non-quantum: a classical and continuous fabric all the way down to the Planck scale? And if spacetime is anything other than what General Relativity dictates it ought to be, how is it different, and in what way(s) will we be able to detect that? But despite all the things that spacetime enables us to predict and know, it isn’t real in the same way that an atom is real. There’s nothing you can do to “detect” spacetime directly; you can only detect the individual quanta of matter and energy that exist within your spacetime. We’ve found a description of spacetime in the form of Einstein’s General Relativity that can successfully predict and explain every physical phenomenon we’ve ever observed or measured, but as far as exactly what it is — and whether it’s “real” or not — that’s not a question that science has yet discovered the answer to. Send in your Ask Ethan questions to startswithabang at gmail dot com! </description>
      <pubDate>16 Feb 21 09:40 EST</pubDate>
      <guid>https://www.forbes.com/sites/startswithabang/2021/02/12/ask-ethan-is-spacetime-real/?sh=3ed8d588fe85</guid>
    </item>
    <item>
      <title>What You Should Know About The Stock Market</title>
      <link>https://betterexplained.com/articles/what-you-should-know-about-the-stock-market/</link>
      <description>&lt;a href=&#34;https://betterexplained.com/articles/what-you-should-know-about-the-stock-market/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Everyone’s heard of the stock market — but few know why it works. Were you aware that each stock has two prices? That you can’t buy and sell for the same amount? That a “stock market” works better and is more open than a “stock store”? If you’re like most of us, probably not. Here’s why stock markets rock: They match buyers and sellers efficiently All prices are completely transparent and you see what other people have paid/sold for You pick your own price and will get that amount if there’s a willing partner Most explanations jump into the minor details — not here. Today we’ll see why the stock market works as it does. iPhones Ahoy! I’m told iPhones are popular with the 18-35 demographic. A market research firm asked me to find a good selling price, so I’ll pass the question onto you: Me: You, the coveted 18-35 year old demographic, want an iPhone. What’s it worth? You: Dude, just get the price. Duh. Ok hotshot, riddle me this: what is the price, exactly? What you can buy it for? (Your best bid) What you can sell it for? (What you’d ask for it) So which price is the “real one”? Both. You see, buyers and sellers each have prices in mind. When prices match, whablamo, there’s a transaction (no match, no whablamo). The idea of two prices for every item is key to understanding any market, not just stocks. Everything has a bid and an ask, and each shopping model has a different way of handling them. This leads to different advantages for buyers and sellers. Shopping Time Suppose we want to buy an iPhone from Amazon. You see the selling price of \$200 (Amazon’s ask), and personally decide if it’s “worth it” (i.e. less than or equal to your bid): In the store model, Amazon shows a public asking price (\$200). Each buyer has a secret bidding price, some more than others. Buyers willing to bid \$200 or more purchase the iPhone; the rest hold off (\$199 and below). Amazon picks a price that attracts the most bidders yet still keeps a profit. In the store model: Buyer pro: Buyers know the price and can pay less than their internal value Buyer con: Buyers have to visit multiple stores to find the best price Seller con: Sellers don’t know what each buyer is willing to pay; it’s difficult to set the pricing. Do low sales mean a bad price or a bad product? Even though buyers are “in control”, they may have to search around to find a store that meets their bid (if any). That’s inefficient. Onto eBay Now suppose we want to sell our new, unopened gadget (you, the 18-35 demographic, are fickle like that; the survey said so). Sure, we could try to sell it on Amazon — now we’re our own store and need a price we think people will pay. We’re in the same boat as Amazon, and could set the price too low. That’s no fun. Instead, we auction off the new iPhone on eBay to maximize profits: In the eBay model, buyers have public bids and compete for the product. The seller keeps their minimum price secret and hopes to make a profit by having someone “overpay”. In the auction model: Seller pro: Sellers have a secret ask (reserve or minimum price) and can get paid above this. Seller pro: Buyers’ demand is transparent. They can easily see if they are pricing too high. Buyer con: Difficult to buy a product. eBay is great for sellers — you have the chance of making extra profit. For buyers, it’s not so great: you can lose auctions by \$1 (paying 201 when 202 was the highest bid), even though the seller would have been happy with 201. You could enter multiple auctions with \$201 but risk getting two iPhones. Want Ads and Hagglers There’s other trading approaches also: Want ad: Publicly announce your desire for an iPhone and let sellers fight it out. Haggle: Find someone with an iPhone, and without knowing a selling price, make an offer. You both haggle back and forth, trying to eke the other person out of a few bucks. If you’ve gone car shopping you know how fun this is. In want ads, the asks are transparent while the bids (your value) are hidden. When haggling, both prices are hidden which can lead to a stressful situation. It’s About Supply and Demand Each model has similar concepts, namely: Supply: sellers provide asks Demand: buyers provide bids The phrase liquidity refers to how effectively you can trade; how easily cash can flow. When buyers and sellers have to argue or haggle, trading freezes up. In particular, there’s a common problem in the market above: There’s secret prices and a lack of transparency There’s multiple vendors and a lack of consolidation When buyers and sellers need to search to find each other, and haggle when they get there, trading slows down. Enter the Market But hope is not lost! Surprisingly, the very symbol of capitalism is an “open source” model: All prices are transparent Buyers write public bids (buying price) Sellers write public asks (selling price) There’s one location to get a particular stock; there’s no searching Dealers/specialists help match buyers and sellers And here’s what it looks like: Every iPhone seller lists their asking price (210, 205, 201, 200). Every iPhone buyer lists their buying price (190, 195, 199, 200). When prices match, a transaction happens: the buyer who wants to pay 200 gets matched with the seller who wants 200. They’re happy. Eventually the matches cease and we come to a standstill. Drop and Spread ‘em. Trades don’t last forever: there’s a standoff and an awkward pause. The lowest sellers want \$201, and the highest bidder wants \$199; this \$2 gap is called the spread. The last price of a transaction was \$200. Now what happens? Buyers and sellers can do: Limit order: put their bid/ask in the queue. Market order: buy or sell immediately. When you place a limit order (“Buy an iPhone for 195″), your order gets added to the bid queue (similar for asks). If you need to trade right now (“buy it now!” or “sell it now!”), then you use a market order. You’ll get the best price available: Market order to sell: You can unload your iPhone for \$199 (the highest bid). The “last” price is now 199. Market order to buy: You can buy for \$201 (the lowest price). The “last” price is now 201. Now this is interesting. Notice how market orders take items off the queue and change the last price. When people place market orders, the stock price fluctuates. Yes, it’s “just” supply and demand, but it’s pretty cool to know it’s happening real-time in the stock market. If there’s a lot of buyers, they’ll “use up” the ask queue and the price will rise. If there’s a lot of sellers, they’ll “use up” the bid queue and the price will fall. This explains why it’s hard to buy and sell for the same price. If you buy for 201, and no new bids come in, you’ll only be able to sell for 199. In the real world, the list looks like this: You see the bids, asks, quantities, and names. Here the bid is 204.91 (max someone will pay) and the ask is 204.92 (min someone will sell). When a buyer or seller gets restless, they may decide to immediately buy/sell, which moves the price. This detailed data is called a Level II quote. So Who Runs This Popsicle Stand? The NYSE and NASDAQ are the two major American exchanges. There are differences, but at the core they provide: A single market to trade. All stocks for Microsoft (MSFT), are traded on the NASDAQ exchange. All stocks for Ford (F) are on the NYSE. A market maker or “specialist” (not the kind that kills people). These people make the market liquid: they help collect and match bids and asks. The NYSE has one specialst per stock; NASDAQ has several market makers (dealers) who compete on price. How Do They Make Money? Well, often they don’t. In the NYSE, 88% of the trades happen between the public without needing the specialist (remember those guys waving papers and screaming at each other? I wouldn’t want to get involved with them either). But sometimes they are needed. The market makers literally “create a market” by providing liquidity: you can buy and sell stocks to them at the bid and ask prices. Popular stocks have a small spread due to the demand and volume. But how do market makers make money? Well, it’s a bit like a currency exchange at a bank, where’s there’s a different rate for buying and selling. Let’s say Sue has an iPhone to sell, and Bob wants to buy an iPhone. It might go like this: Hey Sue, I’ll take your iPhone. Here’s 199. Hi Bob, I’ll sell you an iPhone. That’ll be 201. See what happened? The market maker bought an iPhone for 199 and sold it for 201: it pocketed the spread of \$2. Dealers constantly change their prices based on the bids and asks; they can even lose money depending on the trades coming in. But usually it’s a pretty good gig. You, the investor, can avoid paying “the spread” by placing limit orders to sell or buy at a certain price. But then you aren’t guaranteed to make a trade. It’s All About Timing Bill Gates has a lot of shares of Microsoft. People naively put this wealth as “shares times price”, but you know that doesn’t really work. If he tried to sell all his shares, he’d use up the bids. Each block of shares would be sold for a lower and lower value — and potential buyers would panic and reduce their bids, thinking something was amiss. Sellers would fear the worst and lower their asks to compete. Pandemonium would ensue. So the actual liquidation value of his shares is really some fraction of the reported amount. But it’s still nothing to sneeze at. Similarly, large institutions must spread their stock trades over time so they don’t disrupt the market (and evaporate their profits). The market has built-in shock absorbers: as you sell more, the price you get is smaller and smaller, so you sell less. As you buy more, the price you pay gets higher and higher, so you buy less. So it makes sense to take things slow. Nifty. There’s Much to Learn I’ve simplified a lot of things and only scratched the glossed-over surface. Each market has its own rules to create a trading-friendly environment. Read more here: Invest-faq on the NASDAQ and NYSE. The NYSE is an “auction market” where bids and asks are public (this is different from eBay auctions, where only bidders compete in a given auction). The NASDAQ is a “dealer market” where you buy/sell from a dealer’s personal inventory. Investopedia on the difference between a market maker and specialist See the current bid/ask for Microsoft or Google (and # of shares at that price) But, my goal wasn’t to fill your head with details. I want to share insight: Markets exist to match supply and demand The stock market is fast, transparent, and efficient Every stock has a bid and ask Buying or selling changes the trading price in a direct, measurable way Want a stock tip? Don’t listen to stock tips. (Stolen from a Charles Schwab ad). This article is about looking at a system as one way to solve a larger problem. Happy investing. Other Posts In This Series The Rule of 72 Understanding Accounting Basics (ALOE and Balance Sheets) Understanding Debt, Risk and Leverage What You Should Know About The Stock Market Understanding the Pareto Principle (The 80/20 Rule) Combining Simplicity and Complexity </description>
      <pubDate>14 Feb 22 12:02 EST</pubDate>
      <guid>https://betterexplained.com/articles/what-you-should-know-about-the-stock-market/</guid>
    </item>
    <item>
      <title></title>
      <link>https://www.2uo.de/myths-about-urandom/</link>
      <description>&lt;a href=&#34;https://www.2uo.de/myths-about-urandom/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Myths about /dev/urandom¶ There are a few things about /dev/urandom and /dev/random that are repeated again and again. Still they are false. By the way I&#39;m mostly talking about reasonably recent Linux systems, not other UNIX-like systems. /dev/urandom is insecure. Always use /dev/random for cryptographic purposes. Fact: /dev/urandom is the preferred source of cryptographic randomness on UNIX-like systems. /dev/urandom is a pseudo random number generator, a PRNG, while /dev/random is a “true” random number generator. Fact: Both /dev/urandom and /dev/random are using the exact same CSPRNG (a cryptographically secure pseudorandom number generator). They only differ in very few ways that have nothing to do with “true” randomness. /dev/random is unambiguously the better choice for cryptography. Even if /dev/urandom were comparably secure, there&#39;s no reason to choose the latter. Fact: /dev/random has a very nasty problem: it blocks. But that&#39;s good! /dev/random gives out exactly as much randomness as it has entropy in its pool. /dev/urandom will give you insecure random numbers, even though it has long run out of entropy. Fact: No. Even disregarding issues like availability and subsequent manipulation by users, the issue of entropy “running low” is a straw man. About 256 bits of entropy are enough to get computationally secure numbers for a long, long time. And the fun only starts here: how does /dev/random know how much entropy there is available to give out? Stay tuned! But cryptographers always talk about constant re-seeding. Doesn&#39;t that contradict your last point? Fact: You got me! Kind of. It is true, the random number generator is constantly re-seeded using whatever entropy the system can lay its hands on. But that has (partly) other reasons. Look, I don&#39;t claim that injecting entropy is bad. It&#39;s good. I just claim that it&#39;s bad to block when the entropy estimate is low. That&#39;s all good and nice, but even the man page for /dev/(u)random contradicts you! Does anyone who knows about this stuff actually agree with you? Fact: No, it really doesn&#39;t. It seems to imply that /dev/urandom is insecure for cryptographic use, unless you really understand all that cryptographic jargon. The man page does recommend the use of /dev/random in some cases (it doesn&#39;t hurt, in my opinion, but is not strictly necessary), but it also recommends /dev/urandom as the device to use for “normal” cryptographic use. And while appeal to authority is usually nothing to be proud of, in cryptographic issues you&#39;re generally right to be careful and try to get the opinion of a domain expert. And yes, quite a few experts share my view that /dev/urandom is the go-to solution for your random number needs in a cryptography context on UNIX-like systems. Obviously, their opinions influenced mine, not the other way around. Hard to believe, right? I must certainly be wrong! Well, read on and let me try to convince you. I tried to keep it out, but I fear there are two preliminaries to be taken care of, before we can really tackle all those points. Namely, what is randomness, or better: what kind of randomness am I talking about here? And, even more important, I&#39;m really not being condescending. I have written this document to have a thing to point to, when this discussion comes up again. More than 140 characters. Without repeating myself again and again. Being able to hone the writing and the arguments itself, benefitting many discussions in many venues. And I&#39;m certainly willing to hear differing opinions. I&#39;m just saying that it won&#39;t be enough to state that /dev/urandom is bad. You need to identify the points you&#39;re disagreeing with and engage them. You&#39;re saying I&#39;m stupid!¶ Emphatically no! Actually, I used to believe that /dev/urandom was insecure myself, a few years ago. And it&#39;s something you and I almost had to believe, because all those highly respected people on Usenet, in web forums and today on Twitter told us. Even the man page seems to say so. Who were we to dismiss their convincing argument about “entropy running low”? This misconception isn&#39;t so rampant because people are stupid, it is because with a little knowledge about cryptography (namely some vague idea what entropy is) it&#39;s very easy to be convinced of it. Intuition almost forces us there. Unfortunately, intuition is often wrong in cryptography. So it is here. True randomness¶ What does it mean for random numbers to be “truly random”? I don&#39;t want to dive into that issue too deep, because it quickly gets philosophical. Discussions have been known to unravel quickly, because everyone can wax about their favorite model of randomness, without paying attention to anyone else. Or even making himself understood. I believe that the “gold standard” for “true randomness” are quantum effects. Observe a photon pass through a semi-transparent mirror. Or not. Observe some radioactive material emit alpha particles. It&#39;s the best idea we have when it comes to randomness in the world. Other people might reasonably believe that those effects aren&#39;t truly random. Or even that there is no randomness in the world at all. Let a million flowers bloom. Cryptographers often circumvent this philosophical debate by disregarding what it means for randomness to be “true”. They care about unpredictability. As long as nobody can get any information about the next random number, we&#39;re fine. And when you&#39;re talking about random numbers as a prerequisite in using cryptography, that&#39;s what you should aim for, in my opinion. Anyway, I don&#39;t care much about those “philosophically secure” random numbers, as I like to think of your “true” random numbers. Two kinds of security, one that matters¶ But let&#39;s assume you&#39;ve obtained those “true” random numbers. What are you going to do with them? You print them out, frame them and hang them on your living-room wall, to revel in the beauty of a quantum universe? That&#39;s great, and I certainly understand. Wait, what? You&#39;re using them? For cryptographic purposes? Well, that spoils everything, because now things get a bit ugly. You see, your truly-random, quantum effect blessed random numbers are put into some less respectable, real-world tarnished algorithms. Because almost all of the cryptographic algorithms we use do not hold up to information-theoretic security. They can “only” offer computational security. The two exceptions that come to my mind are Shamir&#39;s Secret Sharing and the One-time pad. And while the first one may be a valid counterpoint (if you actually intend to use it), the latter is utterly impractical. But all those algorithms you know about, AES, RSA, Diffie-Hellman, Elliptic curves, and all those crypto packages you&#39;re using, OpenSSL, GnuTLS, Keyczar, your operating system&#39;s crypto API, these are only computationally secure. What&#39;s the difference? While information-theoretically secure algorithms are secure, period, those other algorithms cannot guarantee security against an adversary with unlimited computational power who&#39;s trying all possibilities for keys. We still use them because it would take all the computers in the world taken together longer than the universe has existed, so far. That&#39;s the level of “insecurity” we&#39;re talking about here. Unless some clever guy breaks the algorithm itself, using much less computational power. Even computational power achievable today. That&#39;s the big prize every cryptanalyst dreams about: breaking AES itself, breaking RSA itself and so on. So now we&#39;re at the point where you don&#39;t trust the inner building blocks of the random number generator, insisting on “true randomness” instead of “pseudo randomness”. But then you&#39;re using those “true” random numbers in algorithms that you so despise that you didn&#39;t want them near your random number generator in the first place! Truth is, when state-of-the-art hash algorithms are broken, or when state-of-the-art block ciphers are broken, it doesn&#39;t matter that you get “philosophically insecure” random numbers because of them. You&#39;ve got nothing left to securely use them for anyway. So just use those computationally-secure random numbers for your computationally-secure algorithms. In other words: use /dev/urandom. Structure of Linux&#39;s random number generator¶ An incorrect view¶ Chances are, your idea of the kernel&#39;s random number generator is something similar to this: “True randomness”, albeit possibly skewed and biased, enters the system and its entropy is precisely counted and immediately added to an internal entropy counter. After de-biasing and whitening it&#39;s entering the kernel&#39;s entropy pool, where both /dev/random and /dev/urandom get their random numbers from. The “true” random number generator, /dev/random, takes those random numbers straight out of the pool, if the entropy count is sufficient for the number of requested numbers, decreasing the entropy counter, of course. If not, it blocks until new entropy has entered the system. The important thing in this narrative is that /dev/random basically yields the numbers that have been input by those randomness sources outside, after only the necessary whitening. Nothing more, just pure randomness. /dev/urandom, so the story goes, is doing the same thing. Except when there isn&#39;t sufficient entropy in the system. In contrast to /dev/random, it does not block, but gets “low quality random” numbers from a pseudorandom number generator (conceded, a cryptographically secure one) that is running alongside the rest of the random number machinery. This CSPRNG is just seeded once (or maybe every now and then, it doesn&#39;t matter) with “true randomness” from the randomness pool, but you can&#39;t really trust it. In this view, that seems to be in a lot of people&#39;s minds when they&#39;re talking about random numbers on Linux, avoiding /dev/urandom is plausible. Because either there is enough entropy left, then you get the same you&#39;d have gotten from /dev/random. Or there isn&#39;t, then you get those low-quality random numbers from a CSPRNG that almost never saw high-entropy input. Devilish, right? Unfortunately, also utterly wrong. In reality, the internal structure of the random number generator looks like this. A better simplification¶ Before Linux 4.8¶ See the big difference? The CSPRNG is not running alongside the random number generator, filling in for those times when /dev/urandom wants to output something, but has nothing good to output. The CSPRNG is an integral part of the random number generation process. There is no /dev/random handing out “good and pure” random numbers straight from the whitener. Every randomness source&#39;s input is thoroughly mixed and hashed inside the CSPRNG, before it emerges as random numbers, either via /dev/urandom or /dev/random. By the way This is a pretty rough simplification. In fact, there isn&#39;t just one, but three pools filled with entropy. One primary pool, and one for /dev/random and /dev/urandom each, feeding off the primary pool. Those three pools all have their own entropy counts, but the counts of the secondary pools (for /dev/random and /dev/urandom) are mostly close to zero, and “fresh” entropy flows from the primary pool when needed, decreasing its entropy count. Also there is a lot of mixing and re-injecting outputs back into the system going on. All of this is far more detail than is necessary for this document. Another important difference is that there is no entropy counting going on here, but estimation. The amount of entropy some source is giving you isn&#39;t something obvious that you just get, along with the data. It has to be estimated. Please note that when your estimate is too optimistic, the dearly held property of /dev/random, that it&#39;s only giving out as many random numbers as available entropy allows, is gone. Unfortunately, it&#39;s hard to estimate the amount of entropy. The Linux kernel uses only the arrival times of events to estimate their entropy. It does that by interpolating polynomials of those arrival times, to calculate “how surprising” the actual arrival time was, according to the model. Whether this polynomial interpolation model is the best way to estimate entropy is an interesting question. There is also the problem that internal hardware restrictions might influence those arrival times. The sampling rates of all kinds of hardware components may also play a role, because they directly influence the values and the granularity of those event arrival times. In the end, to the best of our knowledge, the kernel&#39;s entropy estimate is pretty good. Which means it&#39;s conservative. People argue about how good it really is, but that issue is far above my head. Still, if you insist on never handing out random numbers that are not “backed” by sufficient entropy, you might be nervous here. I&#39;m sleeping sound because I don&#39;t care about the entropy estimate. So to make one thing crystal clear: both /dev/random and /dev/urandom are fed by the same CSPRNG. Only the behavior when their respective pool runs out of entropy, according to some estimate, differs: /dev/random blocks, while /dev/urandom does not. From Linux 4.8 onward¶ In Linux 4.8 the equivalency between /dev/urandom and /dev/random was given up. Now /dev/urandom output does not come from an entropy pool, but directly from a CSPRNG. We will see shortly why that is not a security problem. What&#39;s wrong with blocking?¶ Have you ever waited for /dev/random to give you more random numbers? Generating a PGP key inside a virtual machine maybe? Connecting to a web server that&#39;s waiting for more random numbers to create an ephemeral session key? That&#39;s the problem. It inherently runs counter to availability. So your system is not working. It&#39;s not doing what you built it to do. Obviously, that&#39;s bad. You wouldn&#39;t have built it if you didn&#39;t need it. But the problem runs even deeper: people don&#39;t like to be stopped in their ways. They will devise workarounds, concoct bizarre machinations to just get it running. People who don&#39;t know anything about cryptography. Normal people. By the way I&#39;m working on safety-related systems in factory automation. Can you guess what the main reason for failures of safety systems is? Manipulation. Simple as that. Something about the safety measure bugged the worker. It took too much time, was too inconvenient, whatever. People are very resourceful when it comes to finding “inofficial solutions”. Why not patching out the call to random()? Why not having some guy in a web forum tell you how to use some strange ioctl to increase the entropy counter? Why not switch off SSL altogether? In the end you just educate your users to do foolish things that compromise your system&#39;s security without you ever knowing about it. It&#39;s easy to disregard availability, usability or other nice properties. Security trumps everything, right? So better be inconvenient, unavailable or unusable than feign security. But that&#39;s a false dichotomy. Blocking is not necessary for security. As we saw, /dev/urandom gives you the same kind of random numbers as /dev/random, straight out of a CSPRNG. Use it! The CSPRNGs are alright¶ But now everything sounds really bleak. If even the high-quality random numbers from /dev/random are coming out of a CSPRNG, how can we use them for high-security purposes? It turns out, that “looking random” is the basic requirement for a lot of our cryptographic building blocks. If you take the output of a cryptographic hash, it has to be indistinguishable from a random string so that cryptographers will accept it. If you take a block cipher, its output (without knowing the key) must also be indistinguishable from random data. If anyone could gain an advantage over brute force breaking of cryptographic building blocks, using some perceived weakness of those CSPRNGs over “true” randomness, then it&#39;s the same old story: you don&#39;t have anything left. Block ciphers, hashes, everything is based on the same mathematical fundament as CSPRNGs. So don&#39;t be afraid. What about entropy running low?¶ It doesn&#39;t matter. The underlying cryptographic building blocks are designed such that an attacker cannot predict the outcome, as long as there was enough randomness (a.k.a. entropy) in the beginning. A usual lower limit for “enough” may be 256 bits. No more. Considering that we were pretty hand-wavey about the term “entropy” in the first place, it feels right. As we saw, the kernel&#39;s random number generator cannot even precisely know the amount of entropy entering the system. Only an estimate. And whether the model that&#39;s the basis for the estimate is good enough is pretty unclear, too. Re-seeding¶ But if entropy is so unimportant, why is fresh entropy constantly being injected into the random number generator? First, it cannot hurt. If you&#39;ve got more randomness just lying around, by all means use it! By the way djb remarked that more entropy actually can hurt. There is another reason why re-seeding the random number generator every now and then is important: Imagine an attacker knows everything about your random number generator&#39;s internal state. That&#39;s the most severe security compromise you can imagine, the attacker has full access to the system. You&#39;ve totally lost now, because the attacker can compute all future outputs from this point on. But over time, with more and more fresh entropy being mixed into it, the internal state gets more and more random again. So that such a random number generator&#39;s design is kind of self-healing. But this is injecting entropy into the generator&#39;s internal state, it has nothing to do with blocking its output. The random and urandom man page¶ Update! There has actually been an updated version of the Linux kernel man page for /dev/random and /​dev/​urandom. Unfortunately,​ a simple web search still turns up the old, deficient version I&#39;m describing here in the top results. Furthermore,​ many Linux distributions still ship the old man pages. So unfortunately this section needs to stay a bit longer in the essay. I&#39;m so looking forward to deleting it! The man page for /dev/random and /dev/urandom is pretty effective when it comes to instilling fear into the gullible programmer&#39;s mind: Quote A read from the /dev/urandom device will not block waiting for more entropy. As a result, if there is not sufficient entropy in the entropy pool, the returned values are theoretically vulnerable to a cryptographic attack on the algorithms used by the driver. Knowledge of how to do this is not available in the current unclassified literature, but it is theoretically possible that such an attack may exist. If this is a concern in your application, use /dev/random instead. Such an attack is not known in “unclassified literature”, but the NSA certainly has one in store, right? And if you&#39;re really concerned about this (you should!), please use /dev/random, and all your problems are solved. The truth is, while there may be such an attack available to secret services, evil hackers or the Bogeyman, it&#39;s just not rational to just take it as a given. And even if you need that peace of mind, let me tell you a secret: no practical attacks on AES, SHA-3 or other solid ciphers and hashes are known in the “unclassified” literature, either. Are you going to stop using those, as well? Of course not! Now the fun part: “use /dev/random instead”. While /dev/urandom does not block, its random number output comes from the very same CSPRNG as /dev/random&#39;s. If you really need information-theoretically secure random numbers (you don&#39;t!), and that&#39;s about the only reason why the entropy of the CSPRNG&#39;s input matters, you can&#39;t use /dev/random, either! The man page is silly, that&#39;s all. At least it tries to redeem itself with this: Quote If you are unsure about whether you should use /dev/random or /dev/urandom, then probably you want to use the latter. As a general rule, /dev/urandom should be used for everything except long-lived GPG/SSL/SSH keys. By the way The current,​ updated version of the man page says in no uncertain terms: The /dev/random interface is considered a legacy interface, and /​dev/​urandom is preferred and sufficient in all use cases, with the exception of applications which require randomness during early boot time; for these applications, getrandom(2) must be used instead, because it will block until the entropy pool is initialized. Fine. I think it&#39;s unnecessary, but if you want to use /dev/random for your “long-lived keys”, by all means, do so! You&#39;ll be waiting a few seconds typing stuff on your keyboard, that&#39;s no problem. But please don&#39;t make connections to a mail server hang forever, just because you “wanted to be safe”. Orthodoxy¶ The view espoused here is certainly a tiny minority&#39;s opinions on the Internet. But ask a real cryptographer, you&#39;ll be hard pressed to find someone who sympathizes much with that blocking /dev/random. Let&#39;s take Daniel Bernstein, better known as djb: Quote Cryptographers are certainly not responsible for this superstitious nonsense. Think about this for a moment: whoever wrote the /dev/random manual page seems to simultaneously believe that (1) we can&#39;t figure out how to deterministically expand one 256-bit /dev/random output into an endless stream of unpredictable keys (this is what we need from urandom), but (2) we can figure out how to use a single key to safely encrypt many messages (this is what we need from SSL, PGP, etc.). For a cryptographer this doesn&#39;t even pass the laugh test. Or Thomas Pornin, who is probably one of the most helpful persons I&#39;ve ever encountered on the Stackexchange sites: Quote The short answer is yes. The long answer is also yes. /dev/urandom yields data which is indistinguishable from true randomness, given existing technology. Getting “better” randomness than what /dev/urandom provides is meaningless, unless you are using one of the few &#34;information theoretic&#34; cryptographic algorithm, which is not your case (you would know it). The man page for urandom is somewhat misleading, arguably downright wrong, when it suggests that /dev/urandom may &#34;run out of entropy&#34; and /dev/random should be preferred; Or maybe Thomas Ptacek, who is not a real cryptographer in the sense of designing cryptographic algorithms or building cryptographic systems, but still the founder of a well-reputed security consultancy that&#39;s doing a lot of penetration testing and breaking bad cryptography: Quote Use urandom. Use urandom. Use urandom. Use urandom. Use urandom. Use urandom. Not everything is perfect¶ /dev/urandom isn&#39;t perfect. The problems are twofold: On Linux, unlike FreeBSD, /dev/urandom never blocks. Remember that the whole security rested on some starting randomness, a seed? Linux&#39;s /dev/urandom happily gives you not-so-random numbers before the kernel even had the chance to gather entropy. When is that? At system start, booting the computer. FreeBSD does the right thing: they don&#39;t have the distinction between /dev/random and /dev/urandom, both are the same device. At startup /dev/random blocks once until enough starting entropy has been gathered. Then it won&#39;t block ever again. On Linux it isn&#39;t too bad, because Linux distributions save some random numbers when booting up the system (but after they have gathered some entropy, since the startup script doesn&#39;t run immediately after switching on the machine) into a seed file that is read next time the machine is booting. So you carry over the randomness from the last running of the machine. Obviously that isn&#39;t as good as if you let the shutdown scripts write out the seed, because in that case there would have been much more time to gather entropy. The advantage is obviously that this does not depend on a proper shutdown with execution of the shutdown scripts (in case the computer crashes, for example). And it doesn&#39;t help you the very first time a machine is running, but the Linux distributions usually do the same saving into a seed file when running the installer. So that&#39;s mostly okay. Virtual machines are the other problem. Because people like to clone them, or rewind them to a previously saved check point, this seed file doesn&#39;t help you. But the solution still isn&#39;t using /dev/random everywhere, but properly seeding each and every virtual machine after cloning, restoring a checkpoint, whatever. Update! In the meantime, Linux has implemented a new syscall, originally introduced by OpenBSD as getentropy(2): getrandom(2). This syscall does the right thing: blocking until it has gathered enough initial entropy, and never blocking after that point. Of course, it is a syscall, not a character device, so it isn&#39;t as easily accessible from shell or script languages. It is available from Linux 3.17 onward. tldr;¶ Just use /dev/urandom! </description>
      <pubDate>25 Mar 20 13:42 EDT</pubDate>
      <guid>https://www.2uo.de/myths-about-urandom/</guid>
    </item>
    <item>
      <title>How Do We Trust Our Science Code?</title>
      <link>https://www.hillelwayne.com/how-do-we-trust-science-code/</link>
      <description>&lt;a href=&#34;https://www.hillelwayne.com/how-do-we-trust-science-code/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; In 2010 Carmen Reinhart and Kenneth Rogoff published Growth in a Time of Debt. It’s arguably one of the most influential economics papers of the decade, convincing the IMF to push austerity measures in the European debt crisis. It was a very, very big deal. In 2013 they shared their code with another team, who quickly found a bug. Once corrected, the results disappeared. Greece took on austerity because of a software bug. That’s pretty fucked up. Programming is now essential to scientific research. Pretty much every policy, econ, and sociology grad student I know has used Python or Excel at least once, to say nothing of the hard scientists out there. For most them coding is not interesting. Their job is research, not fighting weird computer bullshit. Knowing shell arcana is as little a part of their world as grantwriting is to me. They want to hammer out some code, get some results, and get back to their work. In 2015 a well-known fintech company accidentally introduced hundreds of mock applicants into their production database. For months their credit models were completely wrong and nobody noticed. This was a disciplined engineering team who documented carefully, practiced TDD, and ran rigorous code reviews. Even with all of that, serious bugs can slip through. Software engineers know this. How many bugs happen when you don’t use any of that? Hell, how many bugs happen when you don’t use functions, descriptive variables, or comments? So how do we fix this? No clue. Obviously we have the technical tools: tests, code reviews, etc. But how do you convince everyone to make a boring, frustrating part of their work more boring and frustrating for such a nebulous benefit? Most researchers don’t worry about this. Neither did Reinhart. This is a social problem, not a technical one. Which is why I don’t have a good answer. Watching engineers try to solve social problems is either tragic or hilarious, depending on how far you are from the explosion. The only thing I can think of is to have all papers include their source code. That way other people can help you find any bugs. Not a great solution, but at least it’s something. This is something a lot of researchers want and many are quite happy to share their code. There’s no way to do it simply and at scale, though. Every journal has different means of distributing code, if they require open code at all. I don’t think this would be an easy fix, or even a painless one.1 If you have any ideas, feel free tell me about them. Being paranoid about science isn’t fun. Thanks to Daiyi, Elliot Abrams, and Sasha Ayvazov for their feedback. </description>
      <pubDate>16 Feb 21 13:34 EST</pubDate>
      <guid>https://www.hillelwayne.com/how-do-we-trust-science-code/</guid>
    </item>
    <item>
      <title></title>
      <link>https://jdan.github.io/98.css/</link>
      <description>&lt;a href=&#34;https://jdan.github.io/98.css/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; 98.css A design system for building faithful recreations of old UIs. Intro 98.css is a CSS library for building interfaces that look like Windows 98. See more on GitHub. This library relies on the usage of semantic HTML. To make a button, you&#39;ll need to use a &lt;button&gt;. Input elements require labels. Icon buttons rely on aria-label. This page will guide you through that process, but accessibility is a primary goal of this project. You can override many of the styles of your elements while maintaining the appearance provided by this library. Need more padding on your buttons? Go for it. Need to add some color to your input labels? Be our guest. This library does not contain any JavaScript, it merely styles your HTML with some CSS. This means 98.css is compatible with your frontend framework of choice. Here is an example of 98.css used with React, and an example with vanilla JavaScript. The fastest way to use 98.css is to import it from unpkg. &lt;link rel=&#34;stylesheet&#34; href=&#34;https://unpkg.com/98.css&#34; &gt; You can install 98.css from the GitHub releases page, or from npm. npm install 98.css Components Button A command button, also referred to as a push button, is a control that causes the application to perform some action when the user clicks it. A standard button measures 75px wide and 23px tall, with a raised outer and inner border. They are given 12px of horizontal padding by default. Show code &lt;button&gt;Click me&lt;/button&gt; When buttons are clicked, the raised borders become sunken. The following button is simulated to be in the pressed (active) state. Show code &lt;button&gt;I am being pressed&lt;/button&gt; Disabled buttons maintain the same raised border, but have a &#34;washed out&#34; appearance in their label. Show code &lt;button disabled&gt;I cannot be clicked&lt;/button&gt; Button focus is communicated with a dotted border, set 4px within the contents of the button. The following example is simulated to be focused. Show code &lt;button&gt;I am focused&lt;/button&gt; Checkbox A check box represents an independent or non-exclusive choice. Checkboxes are represented with a sunken panel, populated with a &#34;check&#34; icon when selected, next to a label indicating the choice. Note: You must include a corresponding label after your checkbox, using the &lt;label&gt; element with a for attribute pointed at the id of your input. This ensures the checkbox is easy to use with assistive technologies, on top of ensuring a good user experience for all (navigating with the tab key, being able to click the entire label to select the box). This is a checkbox Show code &lt;input type=&#34;checkbox&#34; id=&#34;example1&#34;&gt; &lt;label for=&#34;example1&#34;&gt;This is a checkbox&lt;/label&gt; Checkboxes can be selected and disabled with the standard checked and disabled attributes. When grouping inputs, wrap each input in a container with the field-row class. This ensures a consistent spacing between inputs. I am checked I am inactive I am inactive but still checked Show code &lt;div class=&#34;field-row&#34;&gt; &lt;input checked type=&#34;checkbox&#34; id=&#34;example2&#34;&gt; &lt;label for=&#34;example2&#34;&gt;I am checked&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input disabled type=&#34;checkbox&#34; id=&#34;example3&#34;&gt; &lt;label for=&#34;example3&#34;&gt;I am inactive&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input checked disabled type=&#34;checkbox&#34; id=&#34;example4&#34;&gt; &lt;label for=&#34;example4&#34;&gt;I am inactive but still checked&lt;/label&gt; &lt;/div&gt; OptionButton An option button, also referred to as a radio button, represents a single choice within a limited set of mutually exclusive choices. That is, the user can choose only one set of options. Option buttons can be used via the radio type on an input element. Option buttons can be grouped by specifying a shared name attribute on each input. Just as before: when grouping inputs, wrap each input in a container with the field-row class to ensure a consistent spacing between inputs. Yes No Show code &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio5&#34; type=&#34;radio&#34; name=&#34;first-example&#34;&gt; &lt;label for=&#34;radio5&#34;&gt;Yes&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio6&#34; type=&#34;radio&#34; name=&#34;first-example&#34;&gt; &lt;label for=&#34;radio6&#34;&gt;No&lt;/label&gt; &lt;/div&gt; Option buttons can also be checked and disabled with their corresponding HTML attributes. Peanut butter should be smooth I understand why people like crunchy peanut butter Crunchy peanut butter is good Show code &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio7&#34; type=&#34;radio&#34; name=&#34;second-example&#34;&gt; &lt;label for=&#34;radio7&#34;&gt;Peanut butter should be smooth&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input checked disabled id=&#34;radio8&#34; type=&#34;radio&#34; name=&#34;second-example&#34;&gt; &lt;label for=&#34;radio8&#34;&gt;I understand why people like crunchy peanut butter&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input disabled id=&#34;radio9&#34; type=&#34;radio&#34; name=&#34;second-example&#34;&gt; &lt;label for=&#34;radio9&#34;&gt;Crunchy peanut butter is good&lt;/label&gt; &lt;/div&gt; GroupBox A group box is a special control you can use to organize a set of controls. A group box is a rectangular frame with an optional label that surrounds a set of controls. A group box can be used by wrapping your elements with the fieldset tag. It contains a sunken outer border and a raised inner border, resembling an engraved box around your controls. Show code &lt;fieldset&gt; &lt;div class=&#34;field-row&#34;&gt;Select one:&lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio10&#34; type=&#34;radio&#34; name=&#34;fieldset-example&#34;&gt; &lt;label for=&#34;radio10&#34;&gt;Diners&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio11&#34; type=&#34;radio&#34; name=&#34;fieldset-example&#34;&gt; &lt;label for=&#34;radio11&#34;&gt;Drive-Ins&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio12&#34; type=&#34;radio&#34; name=&#34;fieldset-example&#34;&gt; &lt;label for=&#34;radio12&#34;&gt;Dives&lt;/label&gt; &lt;/div&gt; &lt;/fieldset&gt; You can provide your group with a label by placing a legend element within the fieldset. Show code &lt;fieldset&gt; &lt;legend&gt;Today&#39;s mood&lt;/legend&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio13&#34; type=&#34;radio&#34; name=&#34;fieldset-example2&#34;&gt; &lt;label for=&#34;radio13&#34;&gt;Claire Saffitz&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio14&#34; type=&#34;radio&#34; name=&#34;fieldset-example2&#34;&gt; &lt;label for=&#34;radio14&#34;&gt;Brad Leone&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio15&#34; type=&#34;radio&#34; name=&#34;fieldset-example2&#34;&gt; &lt;label for=&#34;radio15&#34;&gt;Chris Morocco&lt;/label&gt; &lt;/div&gt; &lt;div class=&#34;field-row&#34;&gt; &lt;input id=&#34;radio16&#34; type=&#34;radio&#34; name=&#34;fieldset-example2&#34;&gt; &lt;label for=&#34;radio16&#34;&gt;Carla Lalli Music&lt;/label&gt; &lt;/div&gt; &lt;/fieldset&gt; TextBox A text box (also referred to as an edit control) is a rectangular control where the user enters or edits text. It can be defined to support a single line or multiple lines of text. Text boxes can rendered by specifying a text type on an input element. As with checkboxes and radio buttons, you should provide a corresponding label with a properly set for attribute, and wrap both in a container with the field-row class. Occupation Show code &lt;div class=&#34;field-row&#34;&gt; &lt;label for=&#34;text17&#34;&gt;Occupation&lt;/label&gt; &lt;input id=&#34;text17&#34; type=&#34;text&#34; /&gt; &lt;/div&gt; Additionally, you can make use of the field-row-stacked class to position your label above the input instead of beside it. Address (Line 1) Address (Line 2) Show code &lt;div class=&#34;field-row-stacked&#34; style=&#34;width: 200px&#34;&gt; &lt;label for=&#34;text18&#34;&gt;Address (Line 1)&lt;/label&gt; &lt;input id=&#34;text18&#34; type=&#34;text&#34; /&gt; &lt;/div&gt; &lt;div class=&#34;field-row-stacked&#34; style=&#34;width: 200px&#34;&gt; &lt;label for=&#34;text19&#34;&gt;Address (Line 2)&lt;/label&gt; &lt;input id=&#34;text19&#34; type=&#34;text&#34; /&gt; &lt;/div&gt; To support multiple lines in the user&#39;s input, use the textarea element instead. Additional notes Show code &lt;div class=&#34;field-row-stacked&#34; style=&#34;width: 200px&#34;&gt; &lt;label for=&#34;text20&#34;&gt;Additional notes&lt;/label&gt; &lt;textarea id=&#34;text20&#34; rows=&#34;8&#34;&gt;&lt;/textarea&gt; &lt;/div&gt; Text boxes can also be disabled and have value with their corresponding HTML attributes. Favorite color Show code &lt;div class=&#34;field-row&#34;&gt; &lt;label for=&#34;text21&#34;&gt;Favorite color&lt;/label&gt; &lt;input id=&#34;text21&#34; disabled type=&#34;text&#34; value=&#34;Windows Green&#34;/&gt; &lt;/div&gt; Slider A slider, sometimes called a trackbar control, consists of a bar that defines the extent or range of the adjustment and an indicator that shows the current value for the control... Sliders can rendered by specifying a range type on an input element. Volume: Low High Show code &lt;div class=&#34;field-row&#34; style=&#34;width: 300px&#34;&gt; &lt;label for=&#34;range22&#34;&gt;Volume:&lt;/label&gt; &lt;label for=&#34;range23&#34;&gt;Low&lt;/label&gt; &lt;input id=&#34;range23&#34; type=&#34;range&#34; min=&#34;1&#34; max=&#34;11&#34; value=&#34;5&#34; /&gt; &lt;label for=&#34;range24&#34;&gt;High&lt;/label&gt; &lt;/div&gt; You can make use of the has-box-indicator class replace the default indicator with a box indicator, furthermore the slider can be wrapped with a div using is-vertical to display the input vertically. Note: To change the length of a vertical slider, the input width and div height. Show code &lt;div class=&#34;field-row&#34;&gt; &lt;label for=&#34;range25&#34;&gt;Cowbell&lt;/label&gt; &lt;div class=&#34;is-vertical&#34;&gt; &lt;input id=&#34;range25&#34; class=&#34;has-box-indicator&#34; type=&#34;range&#34; min=&#34;1&#34; max=&#34;3&#34; step=&#34;1&#34; value=&#34;2&#34; /&gt; &lt;/div&gt; &lt;/div&gt; Dropdown A drop-down list box allows the selection of only a single item from a list. In its closed state, the control displays the current value for the control. The user opens the list to change the value. Dropdowns can be rendered by using the select and option elements. Show code &lt;select&gt; &lt;option&gt;5 - Incredible!&lt;/option&gt; &lt;option&gt;4 - Great!&lt;/option&gt; &lt;option&gt;3 - Pretty good&lt;/option&gt; &lt;option&gt;2 - Not so great&lt;/option&gt; &lt;option&gt;1 - Unfortunate&lt;/option&gt; &lt;/select&gt; By default, the first option will be selected. You can change this by giving one of your option elements the selected attribute. Show code &lt;select&gt; &lt;option&gt;5 - Incredible!&lt;/option&gt; &lt;option&gt;4 - Great!&lt;/option&gt; &lt;option selected&gt;3 - Pretty good&lt;/option&gt; &lt;option&gt;2 - Not so great&lt;/option&gt; &lt;option&gt;1 - Unfortunate&lt;/option&gt; &lt;/select&gt; Window The following components illustrate how to build complete windows using 98.css. Title Bar At the top edge of the window, inside its border, is the title bar (also reffered to as the caption or caption bar), which extends across the width of the window. The title bar identifies the contents of the window. Include command buttons associated with the common commands of the primary window in the title bar. These buttons act as shortcuts to specific window commands. You can build a complete title bar by making use of three classes, title-bar, title-bar-text, and title-bar-controls. Show code &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Title Bar&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; We make use of aria-label to render the Close button, to let assistive technologies know the intent of this button. You may also use &#34;Minimize&#34;, &#34;Maximize&#34;, &#34;Restore&#34; and &#34;Help&#34; like so: Show code &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Title Bar&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Minimize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Maximize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;br /&gt; &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Maximized Title Bar&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Minimize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Restore&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;br /&gt; &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Helpful Bar&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Help&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; You can make a title bar &#34;inactive&#34; by adding inactive class, useful when making more than one window. Show code &lt;div class=&#34;title-bar inactive&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;An inactive title bar&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; Window contents Every window has a boundary that defines its shape. To give our title bar a home, we make use of the window class. This provides a raised outer and inner border, as well as some padding. We can freely resize the window by specifying a width in the container style. Show code &lt;div class=&#34;window&#34; style=&#34;width: 300px&#34;&gt; &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Complete Window&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Minimize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Maximize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; To draw the contents of the window, we use the window-body class under the title bar. A Window With Stuff In It There&#39;s so much room for activities! Show code &lt;div class=&#34;window&#34; style=&#34;width: 300px&#34;&gt; &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Window With Stuff In It&lt;/div&gt; &lt;div class=&#34;title-bar-controls&#34;&gt; &lt;button aria-label=&#34;Minimize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Maximize&#34;&gt;&lt;/button&gt; &lt;button aria-label=&#34;Close&#34;&gt;&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&#34;window-body&#34;&gt; &lt;p&gt;There&#39;s so much room for activities!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; Status Bar A status bar is a special area within a window, typically the bottom, that displays information about the current state of what is being viewed in the window or any other contextual information, such as keyboard state. You can render a status bar with the status-bar class, and status-bar-field for every child text element. A Window With A Status Bar There are just so many possibilities: A Task Manager A Notepad Or even a File Explorer! Press F1 for help Slide 1 CPU Usage: 14% Show code &lt;div class=&#34;window&#34; style=&#34;width: 320px&#34;&gt; &lt;div class=&#34;title-bar&#34;&gt; &lt;div class=&#34;title-bar-text&#34;&gt;A Window With A Status Bar&lt;/div&gt; &lt;/div&gt; &lt;div class=&#34;window-body&#34;&gt; &lt;p&gt; There are just so many possibilities:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Task Manager&lt;/li&gt; &lt;li&gt;A Notepad&lt;/li&gt; &lt;li&gt;Or even a File Explorer!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class=&#34;status-bar&#34;&gt; &lt;p class=&#34;status-bar-field&#34;&gt;Press F1 for help&lt;/p&gt; &lt;p class=&#34;status-bar-field&#34;&gt;Slide 1&lt;/p&gt; &lt;p class=&#34;status-bar-field&#34;&gt;CPU Usage: 14%&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; TreeView A tree view control is a special list box control that displays a set of objects as an indented outline based on their logical hierarchical relationship. To render a tree view, use an ul element with the tree-view class. The children of this list (li elements), can contain whatever you&#39;d like. We can put ✨ Whatever ✨ We want in here Show code &lt;ul class=&#34;tree-view&#34;&gt; &lt;li&gt;We can put&lt;/li&gt; &lt;li&gt;&lt;strong style=&#34;color: purple&#34;&gt;✨ Whatever ✨&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;We want in here&lt;/li&gt; &lt;/ul&gt; To make this a tree, we can nest further ul elements (no class needed on these). This will provide them with a nice dotted border and indentation to illustrate the structure of the tree. To create expandable sections, wrap child lists inside of details elements. Table of Contents What is web development? CSS Selectors Specificity Properties JavaScript Avoid at all costs Unless Avoid At Avoid At All Cost All Cost HTML Special Thanks Show code &lt;ul class=&#34;tree-view&#34;&gt; &lt;li&gt;Table of Contents&lt;/li&gt; &lt;li&gt;What is web development?&lt;/li&gt; &lt;li&gt; CSS &lt;ul&gt; &lt;li&gt;Selectors&lt;/li&gt; &lt;li&gt;Specificity&lt;/li&gt; &lt;li&gt;Properties&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;details open&gt; &lt;summary&gt;JavaScript&lt;/summary&gt; &lt;ul&gt; &lt;li&gt;Avoid at all costs&lt;/li&gt; &lt;li&gt; &lt;details&gt; &lt;summary&gt;Unless&lt;/summary&gt; &lt;ul&gt; &lt;li&gt;Avoid&lt;/li&gt; &lt;li&gt; &lt;details&gt; &lt;summary&gt;At&lt;/summary&gt; &lt;ul&gt; &lt;li&gt;Avoid&lt;/li&gt; &lt;li&gt;At&lt;/li&gt; &lt;li&gt;All&lt;/li&gt; &lt;li&gt;Cost&lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;/li&gt; &lt;li&gt;All&lt;/li&gt; &lt;li&gt;Cost&lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/details&gt; &lt;/li&gt; &lt;li&gt;HTML&lt;/li&gt; &lt;li&gt;Special Thanks&lt;/li&gt; &lt;/ul&gt; Issues, Contributing, etc. 98.css is MIT licensed. Refer to the GitHub issues page to see bugs in my CSS or report new ones. I&#39;d really like to see your pull requests (especially those new to open-source!) and will happily provide code review. 98.css is a fun, silly project and I&#39;d like to make it a fun place to build your open-source muscle. Thank you for checking my little project out, I hope it brought you some joy today. Consider starring/following along on GitHub and maybe subscribing to more fun things on my twitter. 👋 </description>
      <pubDate>22 Apr 20 12:25 EDT</pubDate>
      <guid>https://jdan.github.io/98.css/</guid>
    </item>
    <item>
      <title></title>
      <link>https://towardsdatascience.com/where-you-should-drop-deep-learning-in-favor-of-constraint-solvers-eaab9f11ef45</link>
      <description>&lt;a href=&#34;https://towardsdatascience.com/where-you-should-drop-deep-learning-in-favor-of-constraint-solvers-eaab9f11ef45&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;When you should use Constraint Solvers instead of Machine LearningA practical application for a constraint solver. Studying other technologies can save you several days of data cleansing and model training.A Rubik’s cube could be modeled as a constraint satisfaction problem [1], Photo by NeONBRAND on UnsplashMachine Learning and Deep Learning are ongoing buzzwords in the industry. Branding ahead of functionalities led to Deep Learning being overused in many artificial intelligence applications.This post will provide a quick grasp at constraint satisfaction, a powerful yet underused approach which can tackle a large number of problems in AI and other areas of computer science, from logistics and scheduling to temporal reasoning and graph problems.Solving a real-world problemLet’s consider a factual and highly topical problem.A pandemic is rising. Hospitals must organize quickly to treat ill people.The world needs an algorithm which matches infected people and hospitals together given multiple criteria such as the severity of illness, patient age and location, hospital capacity and equipment, etc.Fig. 1: Map of the pandemic, reduced to 3 parameters: location of patients and hospitals, severit of patientsMany would say that a neural network would be the perfect fit for it: different configurations from a broad range of parameters that need to be reduced to a unique solution.However, there are downsides which would undermine such an approach:The model needs to be trained, hence the need for historical data on previous cases,A lot of time would be wasted cleaning and consolidating the dataset,A variety of architectures would need to be tested with lengthy training sessions.On the other hand, if formulated in terms of a boolean satisfiability problem, the situation wouldn’t have any of the aforementioned downsides while still giving a sub-optimal solution in nondeterministic polynomial time (NP-complete problem), and without the need for any historical data.Disclaimer: the purpose of this post is to deliver a quick look at CSPs. Theory and problem formulation will be much overlooked. For a more rigorous approach, please refer to [2][3][4].Abstracting the problemThis post will provide a gentle introduction to constraint programming, aiming to resolve this case study. This map of the pandemic (1) illustrates the output of our algorithm which will match infected people with hospitals.There are several frameworks for constraint solving. Google Optimization Tools (a.k.a., OR-Tools) is an open-source software suite for solving combinatorial optimization problems. Our problem will be modeled using this framework in Python.from ortools.sat.python import cp_modelParametersFor now, let’s simplify the problem to 4 parameters (1):Location of infected peopleSeverity of infected peopleLocation of hospitalsNumber of beds in each hospitalLet’s define those parameters in python:# Number of hospitalsn_hospitals = 3# Number of infected peoplen_patients = 200# Number of beds in every hospitaln_beds_in_hospitals = [30,50,20]# Location of infected people -- random integer tuple (x,y)patients_loc = [(randint(0, 100), randint(0, 100)) for _ in range(n_patients)]# Location of hospitals -- random integer tuple (x,y)hospitals_loc = [(randint(0, 100), randint(0, 100)) for _ in range(n_hospitals)] # Illness severity -- 1 = mild -&gt; 5 = severepatients_severity = [randint(1, 5) for _ in range(n_patients)]VariablesA constraint satisfaction problem consists of a set of variables that must be assigned values in such a way that a set of constraints is satisfied.Let I be the set of hospitalsLet Jᵢ be the set of beds in hospital iLet K be the set of patients.Let define as our indexed family of variables :If in the hospital i, the bed j is taken by the person k then xᵢⱼₖ = 1. In order to associate each bed of an hospital to an ill person, the goal is to find a set of variables that satisfies all of our constraints.We can add those variables to our model:model = cp_model.CpModel()x = {}for i in range(n_hospitals): for j in range(n_beds_in_hospitals[i]): for k in range(n_patients): x[(i,j,k)] = model.NewBoolVar(&#34;x(%d,%d,%d)&#34; % (i,j,k))Hard constraintsHard constraints define a goal for our model. They are essential, if they are not resolved then the problem can’t be tackled:There must be at most a single person in every bed,There must be at most a single bed assigned to every person.Let’s focus on the first hard constraint. For each bed j in every hospital i:Either there is a unique patient k,Either the bed is empty.Hence, it can be expressed in the following way:Our solver is a combinatorial optimization solver, it can process integer constraints only. Hence, must be turned into an integer equation:This inequality can then be added to our model.# Each bed must host at most one personfor i in range(n_hospitals): for j in range(n_beds_in_hospitals[i]): model.Add(sum(x[(i,j,k)] for k in range(n_patients)) &lt;= 1)Next, the second hard constraint: for every patient k:Either he is in a unique bed j in a unique hospital i,Either he is at home.In the same way, can be translated into an integer inequality:Finally, this constraint can be added to the model.# Each person must be placed in at most one bedfor k in range(n_patients): inner_sum = [] for i in range(n_hospitals): inner_sum.append(sum(x[(i,j,k)] for j in range(n_beds_in_hospitals[i]))) model.Add(sum(inner_sum) &lt;= 1)Soft constraintsNext, there are soft constraints. Those are highly desired: our solution must try to satisfy them as much as possible, yet they are not essential to find a solution:Every sick person should be placed into a bed,Every person should be handled by the nearest hospital,Sick persons in severe condition should be handled first when there are not enough beds.While hard constraints are modeled as equalities or inequalities, soft constraints are expressions to minimize or maximize.Let Ω be the set of all solutions that satisfy the hard constraints.Every sick person should be placed into a bed means to maximize the number of occupied beds.Every person should be handled by the nearest hospital means to minimize the distance between every patient and his assigned hospital.Sick persons in a severe condition should be handled first when there are not enough beds means to maximize the total severity of all handled patients. By denoting sev(k) the severity of the patient k:Then we can reduce all the soft constraints into a single objective:One needs to be careful then: these soft constraints don’t have the same domain.Patients maximization constraint ranges from 0 to n, with n the number of patients,Severity constraint ranges from 0 to 5nDistance constraint ranges from 0 to the maximum euclidian distance for all i and k.Given that all of these constraints share the same priority, we must define penalty factors to equilibrate the different constraints.Here is the corresponding code:# Integer distance functionidist = lambda xy1, xy2: int(((xy1[0]-xy2[0])**2 + (xy1[1]-xy2[1])**2)**0.5)# Gain factors (1/penalty factors)gain_max_patients = 140gain_severity = int(140/5)gain_distance = -1# Maximization objectivesoft_csts = []for i in range(n_hospitals): for j in range(n_beds_in_hospitals[i]): for k in range(n_patients): factor = \ gain_max_patients \ + gain_distance * idist(hospitals_loc[i], patients_loc[k]) \ + gain_severity * patients_severity[k] soft_csts.append(factor * x[(i,j,k)])model.Maximize(sum(soft_csts))SolverNow we can launch the solver. It will try to find the optimal solution within a specified time limit. If it can’t manage to find the optimal solution, it will return the closest sub-optimal solution.solver = cp_model.CpSolver()solver.parameters.max_time_in_seconds = 60.0status = solver.Solve(model)In our case, the solver returns an optimal solution in 2.5 seconds (2).Fig. 2: Solution returned by the solverConclusionTo create this solution, all it takes is 1 hour of research and 30 minutes of programming.For a Deep Learning counterpart, one can predict a few days of data cleansing, at least a day to test different architectures and another day for training.Moreover, a CP-SAT model is very robust if well modelized. Below are the results with different simulation parameters (3). Results are still coherent in many different cases, and with increased simulation parameters (3000 patients, 1000 beds), solution inference took a little less than 3 minutes.Fig. 3: Different simulation parametersOf course, CSPs hardly apply to topics like computer vision and NLP, where Deep Learning is sometimes the best approach. However, in logistics, scheduling and planning, it is often the way to go.The Deep Learning hype inspired some people to try some crazy moves to gain recognition. Sometimes, it is better to return to the basics by reading several survey papers on the issue you are working on.Antoine Champion, Apr. 1st 2020References[1] Jingchao Chen, Solving Rubik’s Cube Using SAT Solvers, arXiv:1105.1436, 2011.[2] Biere, A., Heule, M., and van Maaren, H. Handbook of satisfiability, volume 185. IOS press, 2009a[3] Knuth, D. E., The art of computer programming, Volume 4, Fascicle 6: Satisfiability. Addison-Wesley Professional, 2015[4] Vipin Kumar, Algorithms for constraint-satisfaction problems: a survey, AI Magazine Volume 13, Issue 1, 1992.</description>
      <pubDate>16 Feb 22 16:00 EST</pubDate>
      <guid>https://towardsdatascience.com/where-you-should-drop-deep-learning-in-favor-of-constraint-solvers-eaab9f11ef45</guid>
    </item>
    <item>
      <title>The Devastating Decline of a Brilliant Young Coder</title>
      <link>https://www.wired.com/story/lee-holloway-devastating-decline-brilliant-young-coder/</link>
      <description>&lt;a href=&#34;https://www.wired.com/story/lee-holloway-devastating-decline-brilliant-young-coder/&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;On Friday, September 13, 2019, Matthew Prince and Michelle Zatlyn, cofounders of the San Francisco internet security firm Cloudflare, stood on a slim marble balcony overlooking the floor of the New York Stock Exchange. A cluster of the company&#39;s executives stood near Prince, ready to shout out a countdown. “Louder! Loud!” Prince urged them. “Five! Four! Three! …” At 9:30 am sharp, the founders reached down to ring the exchange&#39;s famous bell, kicking off the day&#39;s trading and offering their 10-year-old company on the public market. It was a rite of passage and also their payday, a moment that unlocked many millions of dollars in newfound wealth.More than 100 employees and investors cheered from the trading floor below, their phones held high to capture the scene. Kristin Holloway, employee number 11, looked up at the balcony and snapped photos, then popped them into a text to her husband, Lee Holloway, the company&#39;s third cofounder. He was home in California. Every so often, a familiar face pushed through the throng to say to her, “Lee should be here.”In Cloudflare&#39;s early years, Lee Holloway had been the resident genius, the guy who could focus for hours, code pouring from his fingertips while death metal blasted in his headphones. He was the master architect whose vision had guided what began as a literal sketch on a napkin into a tech giant with some 1,200 employees and 83,000 paying customers. He laid the groundwork for a system that now handles more than 10 percent of all internet requests and blocks billions of cyberthreats per day. Much of the architecture he dreamed up is still in place.But some years before the IPO, his behavior began to change. He lost interest in his projects and coworkers. He stopped paying attention in meetings. His colleagues noticed he was growing increasingly rigid and belligerent, resisting others&#39; ideas, and ignoring their feedback.Lee&#39;s rudeness perplexed his old friends. He had built his life around Cloudflare, once vowing to not cut his hair until the startup&#39;s web traffic surpassed that of Yahoo. (It took a few short months, or about 4 inches of hair.) He had always been easygoing, happy to mentor his colleagues or hang out over lunch. At a birthday party for Zatlyn, he enchanted some children, regaling them with stories about the joys of coding. The idea of Lee picking fights simply didn&#39;t compute.He was becoming erratic in other ways too. Some of his colleagues were surprised when Lee separated from his first wife and soon after paired up with a coworker. They figured his enormous success and wealth must have gone to his head. “All of us were just thinking he made a bunch of money, married his new girl,” Prince says. “He kind of reassessed his life and had just become a jerk.”The people close to Lee felt tossed aside. They thought he&#39;d chosen to shed his old life. In fact, it was anything but a choice. Over the next few years, Lee&#39;s personality would warp and twist even more, until he became almost unrecognizable to the people who knew him best. Rooting out the cause took years of detective work—and forced his family to confront the trickiest questions of selfhood.On the floor of the stock exchange that September morning, Lee&#39;s younger brother Alaric weathered the morning in a state of low-grade panic. He snapped selfies with early employees and fired them off in texts to his brother. Alaric had never worked at Cloudflare, and he knew barely anyone there. But his dark hair flopped over his forehead with the same distinctive swoop as his brother&#39;s, and his long, tapering face had the same dark eyes and olive skin. “It was surreal,” Alaric says. “People kept looking at me like they knew me.”At home with his parents in San Jose, Lee, 38, was restless. He paced the rooms and hallways of the 1,550-square-foot house, a loop he&#39;d been tracing since he&#39;d moved in with them two years earlier. He didn&#39;t speak. His parents had the TV on, and they called him over whenever Prince or Zatlyn appeared onscreen.Later, he paused at the family&#39;s Roku to search YouTube for videos of Cloudflare. Then he resumed his circuit: walking the halls, buzzing his lips, snacking on cashews.Lee Holloway spends time with his youngest son at home on California&#39;s Central Coast.Artwork by Amy Friend; Photograph by Jack BoolWhat makes you you? The question cuts to the core of who we are, the things that make us special in this universe. The converse of the question raises another kind of philosophical dilemma: If a person isn&#39;t himself, who is he?Countless philosophers have taken a swing at this elusive piñata. In the 17th century, John Locke pinned selfhood on memory, using recollections as the thread connecting a person&#39;s past with their present. That holds some intuitive appeal: Memory, after all, is how most of us register our continued existence. But memory is unreliable. Writing in the 1970s, renowned philosopher Derek Parfit recast Locke&#39;s idea to argue that personhood emerges from a more complex view of psychological connectedness across time. He suggested that a host of mental phenomena—memories, intentions, beliefs, and so on—forge chains that bind us to our past selves. A person today has many of the same psychological states as that person a day ago. Yesterday&#39;s human enjoys similar overlap with an individual of two days prior. Each memory or belief is a chain that stretches back through time, holding a person together in the face of inevitable flux.The gist, then, is that someone is “himself” because countless mental artifacts stay firm from one day to the next, anchoring that person&#39;s character over time. It&#39;s a less crisp definition than the old idea of a soul, offering no firm threshold where selfhood breaks down. It doesn&#39;t pinpoint, for example, how many psychological chains you can lose before you stop being yourself. Neuroscience also offers only a partial answer to the question of what makes you you.Neural networks encode our mental artifacts, which together form the foundation of behavior. A stimulus enters the brain, and electrochemical signals swoosh through your neurons, culminating in an action: Hug a friend. Sit and brood. Tilt your head up at the sun and smile. Losing some brain cells here or there is no big deal; the networks are resilient enough to keep a person&#39;s behaviors and sense of self consistent.But not always. Mess with the biological Jell-O in just the right ways and the structure of the self reveals its fragility.Lee&#39;s personality had been consistent for decades—until it wasn&#39;t.From an early age, he was a person who could visualize sprawling structures in his mind. Growing up in the 1990s in Cupertino, where his dad worked at Apple, Lee had early access to the latest computers, and he and his brother grew up bingeing on videogames. As a gamer, he was legendary among his friends for being able to read a complex situation, rapidly adjust strategies, and win match after match. And it wasn&#39;t just videogames. His childhood friend Justin Powell remembers Lee strolling into a middle school chess club tournament cold. He wasn&#39;t a member of the club, but he won the tournament anyway. Lee avoided becoming insufferable by channeling his wit into snarky commentary. “Watching a movie with him was like a version of Mystery Science Theater 3000,” Powell says. “His very presence challenged you to keep up with him.”Lee and his friends would cart their computers to each other&#39;s houses to play games together. He became curious about the machines themselves and started learning computer science, first in high school, then at a local community college and UC Santa Cruz, where an unlikely set of circumstances connected him with Matthew Prince.Then a young entrepreneur, Prince was pursuing an idea for an antispam software tool when he encountered Arthur Keller, a UC Santa Cruz computer science professor. Keller and his students had already worked out a very similar concept. Prince and Keller agreed to share a patent, along with Keller&#39;s students. One of those students was Lee, and Prince hired him on the spot. “I had no idea this school project would turn into something much bigger,” Lee later said in a video interview with a group called Founderly.Prince set up the company, Unspam Technologies, in Park City, Utah, about a mile from a cluster of slopes where he could indulge his passion for skiing. Lee moved into Prince&#39;s basement, at first working for free in exchange for food and housing. But Lee and the other Unspam engineers grew restless, and they started spinning up side projects, including one called Project Honey Pot, which tracked spammers as they crawled the web. That&#39;s all it did—it collected and published data on spammers, but it didn&#39;t do anything to stop them. Still, the project quickly amassed a loyal following.In 2007, Prince left Utah to start business school at Harvard, and Lee moved to California to live with his girlfriend, Alexandra Carey. They&#39;d known each other as undergrads, when she was a teaching assistant in his computer architecture class. Lee had goofed off in that class, once pranking the professor by scrawling childish notes on the transparencies of an overhead projector. Alexandra had been amused, but it wasn&#39;t until after college that a relationship bloomed. Living in different cities, they fell for each other while playing and chatting within a multiplayer videogame called Savage. Now, with Prince leaving Utah, it seemed a natural time for Lee to join Alexandra. They married in 2008.Everything you ever wanted to know about Equifax, Mariott, and the problem with social security numbers.Lee and Prince kept working at Unspam from their respective cities, but as Prince was wrapping up business school, Lee called to tell him he was considering other job offers. Prince countered with a new and rather audacious pitch: He and a classmate, Michelle Zatlyn, had hit on a startup idea they thought had potential. What if they expanded Project Honey Pot to not just recognize spammers and hackers but also fight back against them? The plan was to build out massive networks of servers around the world, convince website owners to route their traffic through those servers, and gather enough data to detect malicious requests amid the good ones. That might give them the tools they needed to stop even the world&#39;s biggest denial of service attack. But Prince needed a technical cofounder, and his about-to-defect employee was his top choice.Prince talked for an hour straight. At the end of this spiel, Lee&#39;s side of the line was quiet. “I was like, ‘Are you still on the phone?’ ” Prince recalls. “Then he said, ‘Yeah, that&#39;ll work, let&#39;s do that.’ ” And that was it.They whipped together a demo and in late 2009 raised a little over $2 million from two venture capital firms. It was enough to rent a converted two-bedroom apartment above a nail salon in Palo Alto, where they could start building their idea in earnest. Lee would show up every day wearing the same Calvin Klein jeans, leather jacket, and beanie on his head, and lugging a giant ThinkPad laptop nicknamed the Beast. “We had this shared vision,” Zatlyn says. “And Lee was the architect behind it. He just obsessed over it.”The following year, Prince talked his way into TechCrunch Disrupt, an onstage competition for startups that can lead to big funding rounds. As Disrupt approached, Prince and Zatlyn grew nervous. Lee had missed a lot of days of work due to migraines. He didn&#39;t seem anywhere close to finishing a demo. When the day of the event arrived, Prince and Zatlyn walked onstage praying that the software they were presenting would actually work.Prince started his pitch. “I&#39;m Matthew Prince, this is Michelle Zatlyn, Lee Holloway is in the back of the room. We&#39;re the three cofounders of Cloudflare,” he boomed, stabbing the air with his finger as he spoke. In fact, Lee was backstage furiously fixing a long list of bugs. Prince held his breath when he ran the software, and, perhaps miraculously, it worked. It really worked. In the hour after he walked onstage, Cloudflare got 1,000 new customers, doubling in size.They earned second place at Disrupt. “In the next couple of weeks, all these somewhat mythical VCs that we&#39;d heard of and read about all called us,” Prince says. Under the onslaught of attention, Prince, Holloway, and one early hire, Sri Rao, rolled out constant fixes to hold the system together. “We launched in September, and in a month we had 10,000 websites on us,” Lee said in the Founderly interview. “If I&#39;d known, we would have had eight data centers instead of five.”With customers now multiplying, Ian Pye, another early engineer, hollowed out a toaster, tucked an Arduino board inside, and hooked it up to the network. Whenever a website signed up for Cloudflare services, the toaster sang a computerized tune Pye had composed. “It was horribly insecure,” Pye says. “But what were they going to do, hack our toaster?” The toaster lasted two weeks before the singing became too frequent and annoying and they unplugged it.Cloudflare was growing fast, and Lee worked long days, often from home in Santa Cruz. He and Alexandra now had an infant son. During the first few months of the baby&#39;s life, Lee and Alexandra still made time to play videogames together. Alexandra remembers cracking up when Lee co-opted a nursing pillow to support his neck while he sat at his computer. Several of his old friends came over once a week to play the board game version of Game of Thrones or the multiplayer videogame Team Fortress 2. Alexandra focused on childcare, but she made sure the players had food. “I was doing it for him,” she says.But around 2011 she started noticing that Lee was growing distant and forming some odd new habits. He spent a lot more time asleep, for one. After long workdays, she recalls, he&#39;d walk in the door, take off his shoes, and immediately pass out on the floor. Their cat sometimes curled up and napped on his chest. His son, not yet 2, would clamber over him, trying and failing to rouse him to play.When people invited them to parties, Lee refused to go. Alexandra started attending her friends&#39; weddings by herself. It hurt her to see everyone else there as a couple, while the chair next to her sat empty. At home she&#39;d cook dinner, and he&#39;d look at it and say he was ordering pizza. On a weeklong family trip to France, he spent three days sleeping in the hotel room. “I&#39;d say, ‘What&#39;s going on, we&#39;re going to these places—are you coming?’ ” Alexandra says. He&#39;d insist he was too tired. She was finishing up a master&#39;s degree and shouldering the bulk of childcare; she, too, was tired. Alexandra begged him to go to therapy and cajoled him to play with their son, but he didn&#39;t engage. “After a while you think, well, this is the person I&#39;m with,” she says.In 2012, Alexandra told him she was taking an internship in Southern California, at NASA, and she was planning to take their son with her. She says his response was to calmly ask her to file for divorce before she left. “I was crushed. I said, ‘Maybe it doesn&#39;t have to be that way,’ ” she recalls. “He said, ‘No, no, it does.’ ”When Lee told Prince and Zatlyn about his divorce, they both expressed their shock and gave their condolences, but Lee seemed to barely acknowledge the change. Prince and Zatlyn found his behavior tremendously odd. Still, they could rationalize it away. Relationships end for many reasons. Alexandra and Lee had married young, and both had worked long hours; perhaps they had grown apart. Besides, Lee was thriving at the company, so they didn&#39;t press.Lee and his Cloudflare cofounders, Michelle Zatlyn and Matthew Prince, attend a holiday party in 2011.Artwork by Amy Friend; Photograph courtesy of CloudflareSome months after Alexandra moved away, Lee was sitting at a table with a couple of coworkers, including Kristin Tarr, who ran communications at Cloudflare. She&#39;d just published a blog post describing how customers could enable two-factor authentication on their accounts. He turned to her and said, “I read your blog post. It was really good.” A friend saw the interaction and teased her: Lee&#39;s flirting with you!Lee and Kristin started spending time together. On one of their first dates, Lee took her to see his favorite metal band, the Swedish group Opeth. He revved up her interest in basketball, and they became Golden State Warriors junkies, watching every game. Kristin brought her own interests and energy into the relationship. She convinced him to trade in his old jeans-and-leather-jacket uniform for nicer shirts from Rag &amp; Bone. He still wore beanies and hoodies, but now they came from Lululemon, where Kristin, a running freak, had a weekend gig as a brand ambassador. Sometimes he refused to get out of bed or retreated with a migraine; Kristin responded by signing him up for 5K races and coaxing him into training for them. Their coworkers marveled that their lead engineer had become so athletic.Within a few months they had moved in together. She whisked him off on adventures, pulling him away from his computer and his videogames. They went tubing on the Truckee River. They played endless rounds of Bang! and Settlers of Catan with board-game-loving coworkers. Both nearsighted, they pretended they were moles, snuggled up in their burrow of a home. As their fortunes grew, they upgraded their digs, moving from Mole Hole to Mole Tower to Mole Terrace. They gave their friends animal identities too; Prince was a mongoose, while another executive was a swan. In May 2014, Kristin quit Cloudflare, and the next day they left for a vacation in Italy. They got engaged in Rome.At work, Lee was still the star engineer. At the end of the summer of 2014, he took on a project that earned Cloudflare its first bout of internet fame: The company would help websites become encrypted for free. (It was not yet standard for company websites to be encrypted.)Lee agreed to build the necessary software by the end of September. As the date approached, Prince asked for updates, but Lee blew him off. Then, on the day before the new system was supposed to go live, he pulled his hoodie down low on his head, put on his headphones, and sat down to bang out the code.It was a Sunday, but the office was packed with people writing up the pending announcement or delivering coffee and food. Lee&#39;s coding, though, was the main event. “And he is typing, typing, and I don&#39;t think anyone dared to interrupt,” says John Graham-Cumming, then an engineer and now Cloudflare&#39;s chief technology officer. “His hoodie is on, he&#39;s in the zone, he&#39;s doing brain surgery on this thing.”Then, late in the night, Lee stood up. He announced that he&#39;d finished, and he wandered away. “It was like, bzhzhzhzh, type-type-type, ‘I&#39;m done!’” Graham-Cumming says.The other engineers immediately started reviewing his code. By the morning, the debugging process began for real. The gambit worked, and all of their existing customers suddenly got encryption. It was a proud moment. Says Graham-Cumming: “The size of the encrypted web doubled overnight.”Lee and his wife, Kristin Holloway, on vacation in Rome in 2014. He proposed to her hours after this photo was taken.Artwork by Amy Friend; Photograph courtesy of Kristin HollowayAs Lee and Kristin planned their wedding, he decided to address a health problem he&#39;d long ignored. Lee had been born with a heart issue, a leaky aortic valve, and some doctors thought it might be contributing to his migraines. “If you put your head on his chest, you could hear it,” Kristin says. “We called it his squishy heart.” Doctors were split on how serious his condition was, but in January 2015 a surgeon at Stanford insisted he get surgery right away. Lee went in for the six-hour procedure. As he lay on his hospital bed, he recorded a video to his son: “I love you! I&#39;ll see you soon with a brand-new heart.” He signed off with a smile and a wave.Kristin now sees the surgery as a grim turning point. Lee&#39;s heart came out of the procedure stronger than ever, but mentally he never seemed to recover. He slept all the time. He&#39;d taken a leave from work to have the surgery, but he extended his leave by a month, and then another, until he finally returned to the office in the late spring.In June they got married, in Hawaii, in front of a crowd of friends and family. Kristin noticed that he seemed subdued. It was as if someone had washed the color out of his personality. Prince noticed too but chalked it up to a slow recovery from the surgery.Not long after, Lee and Kristin took a trip to Europe, spending a few days in France, just as Lee and Alexandra had years earlier. Kristin had never been to Paris, and she was excited to explore the city. She ended up doing that on her own, while Lee again spent days asleep in their hotel room. “This is so weird,” Kristin remembers thinking. On their trip to Italy, he&#39;d been eager to jump out of bed and visit museums and cafés, and walk around. She was puzzled, but between his migraines and his heart issue, there was always an explanation at hand.At the office, he was becoming impossible to work with. He would lash out at people, and then in meetings he would zone out, openly playing games on his phone. During one meeting, Prince texted him: “Are you playing a game? People are noticing.” Then: “Not a great leadership signal.”Prince and Zatlyn confronted him about his behavior, and Lee promised to do better. But his responses seemed rote. “I was like, why is he so disengaged? Why doesn&#39;t he seem to care?” Zatlyn recalls. They figured he must be burned out. Still, it hurt; it felt as if Lee was breaking up with them. She&#39;d paid attention to the stories of startup founders who split up, the mess of their breakups sometimes dragging their companies down with them. “So I&#39;m thinking, well, I guess that&#39;s what that feels like.”They put their friend on an official performance-improvement plan. Over many weekly lunches, Zatlyn and Prince tried to get through to him. Nothing seemed to stick. “For several years,” Prince says, “the thing that was causing me just incredible anxiety was that I had all this loyalty to this person, but they&#39;re becoming a jerk.”Eventually, in 2016, they decided Lee had to leave the company. “He kind of just said, yup, that sounds about right,” Prince says. They threw him a going-away party that July. Prince thanked him in a speech with tears streaming down his cheeks. Lee stood beside him with a beer in hand, a thin smile on his face.Lee (center) gathered with his family for Thanksgiving in 2016, including (from left) his brother Alaric; his wife, Kristin; his older son; his mother, Kathy; his younger son; and his father, Rendon.Artwork by Amy Friend; Photograph courtesy of Kristin HollowayNow that he wasn&#39;t working, Lee napped constantly. Kristin was seven months pregnant, and they agreed that after the baby&#39;s birth, Lee would be a stay-at-home dad, at least until he figured out what to do next. In the meantime, they would live off their savings and Kristin&#39;s salary from a new job at an ad tech firm.Lee&#39;s actions, however, only grew more bizarre. He watched Home Alone several nights a week. He wore his beanie all day, every day, pulling it lower and lower. When Kristin went into labor, he slept through most of the two-day ordeal, first slumbering at home and then resuming his nap at the hospital. When he woke up, he insisted, against Kristin&#39;s wishes, that she not get an epidural, which provoked a heated argument with one of the doctors. After their son was born, Kristin&#39;s mom says the doctor pulled her aside and commented that she&#39;d never seen an expectant dad react that way. Kristin confronted him about his behavior later, and he promised her, “I&#39;ll do better.”In those heady first months of parenthood, he failed. He took copious naps. Sometimes she&#39;d cook him dinner, and he&#39;d reject it and order a burrito. “I was like, what is happening?” Kristin says. “Everything felt so strange and out of control.”Distraught at his lack of interest in their son, she decided to stage a moment of parenting normalcy. If she couldn&#39;t coax him into engaging with their child, she&#39;d settle for its illusion. While Lee lay on the couch, she handed him the infant and grabbed her phone to record the scene. “You&#39;re standing, and you&#39;re so cute!” he coos as he props up the baby on his chest. “You&#39;re smiling and making a sound!” He dotes on his baby for less than a minute before handing him back to Kristin.She kept trying to probe what was on his mind, and he kept replying, “I&#39;ll do better.” The repetitiveness of his answers struck her as robotic. It seemed of a piece with the way he now touched every tree he passed on their walks. “I think deep down I knew something was wrong,” Kristin says. She thought maybe he&#39;d developed PTSD after the surgery or was struggling with a bout of depression. She&#39;d been asking him to see a counselor with her. Finally, as she prepared to return to work, she threatened to leave him if he didn&#39;t. Lee agreed.In the couples&#39; therapy session, Kristin cried openly and talked about how her husband didn&#39;t seem to care about their new baby. “Lee was just blank,” she recalls, and she wondered why he wasn&#39;t reaching out to comfort her. Suddenly he stood up, announced he&#39;d forgotten to return the therapist&#39;s office bathroom key, and wandered out of the room to put it back, returning a few minutes later.When her maternity leave came to an end, Kristin hired a nanny and went back to work, but her alarm was mounting. She started booking appointments with every specialist she could think of while Lee spent his days in bed. “So I&#39;m cajoling him out of bed, getting him into the car, making sure my son is out with his nanny, covering my own work somehow,” and then shuttling him from appointment to appointment. “It was like that for three months.”In mid-March of 2017, Kristin and Lee went to a neurologist to get the results of an MRI. To Kristin, it seemed that the neurologist had initially been skeptical of her concerns. Lee was young, healthy, and communicative.The MRI told a different story: There was atrophy in the brain inconsistent with the age of the patient, the neurologist reported to them. When Kristin asked her what that meant, she said Lee had a neurodegenerative disease of some kind, but they&#39;d need to do more tests to get a specific diagnosis. One of their doctors suggested they go to the Memory and Aging Center at UC San Francisco.That evening, Kristin started Googling. She pulled up the website of the Memory and Aging Center and started reading the descriptions of brain atrophy diseases. She knew immediately the neurologist was right. And in that moment she glimpsed the future: This was going to kill her husband.She remembers sitting with her son that night. “Until that point, I&#39;d held out hope. We have the resources, the best doctors, I can fly him to get him the best care,” she says. “But to be in this position where nothing can be done is just … It&#39;s so awful.” She quit her job the next day.A few weeks later, Kristin and Lee, their parents, and Alaric all gathered in a conference room on the UCSF campus with a panel of experts. “Do you know why you&#39;re here?” the lead neurologist asked Lee. He replied, “My wife organized this.”“Do you know that you&#39;re sick?”“I get migraines a lot,” he said. “And I had heart surgery.”The neurologists delivered their verdict: He appeared to have a textbook case of frontotemporal dementia—known by the shorthand FTD—specifically, the behavioral variant of that disease. It targets a network of brain regions sometimes described as underpinning one&#39;s sense of self. As the pathological process advanced, it was carving a different person out of Lee&#39;s raw substance.The term frontotemporal dementia refers to a cluster of neurodegenerative diseases that affect a person&#39;s behavior or speech while leaving memory largely intact, at least early on. Unlike Alzheimer&#39;s disease, FTD isn&#39;t well known. It is a rare disease, affecting roughly one in 5,000 people, though many of the neurologists who study it believe it is underdiagnosed. What is known is that for people under the age of 60, it is the most common form of dementia. Still, as a man in his thirties, Lee was unusually young to be afflicted. For some patients, one of several genetic mutations turns out to be the likely cause, and a subset of patients have a family history of neurodegenerative diseases. But nothing in the neurologists&#39; investigations turned up even a hint as to why Lee had been struck down.Regardless of cause, the prognosis is grim. There&#39;s no treatment. Lee&#39;s doctors warned that his symptoms would grow worse, and that over time he would likely stop talking, become immobile, and struggle to swallow, until eventually an infection or injury would likely turn fatal. The best the doctors could recommend was eating a balanced diet and getting exercise.The family sat stunned at the neurologist&#39;s words. The brain scans were undeniable. On a wall-mounted screen the doctors showed a cross-section of the four lobes of Lee&#39;s brain. In a healthy brain, the familiar, loopy folds of tissue appear white or gray and push up against the edges of the cranium, filling every available space. Lee&#39;s brain looked nothing like that.Black voids pocked his frontal lobe, areas where brain tissue had gone dead. Seeing it, Kristin gasped. “There were huge dark spots in his brain,” Alaric says. “That&#39;s what … that made it concrete.”Lee received his death sentence with pure calm. While his family cried beside him, he complimented a doctor for having a nice wedding ring. At that, Alaric looked at him and realized for the first time the depths of his brother&#39;s transformation.Lee still takes part in some activities with his wife and children, including working on jigsaw puzzles.Artwork by Amy Friend; Photograph by Jack BoolFew disorders ravage their victims&#39; selfhood with the intensity of the behavioral variant of FTD. It takes all the things that define a person—hobbies and interests, the desire to connect with others, everyday habits—and shreds them. Over time, the disease transforms its victims into someone unrecognizable, a person with all the same memories but an alarming new set of behaviors. Then it hollows them out and shaves away their mobility, language, and recollections.Because it is relatively unknown and can resemble Alzheimer&#39;s or a psychiatric disorder, FTD is often hard to diagnose. As in Lee&#39;s case, the early stages can be misinterpreted as signs of nothing more serious than a midlife crisis. Patients can spend years shuttling to marriage counselors, human resources departments, therapists, and psychologists. By the time patients learn the name of their disorder, they are often unable to grasp the gravity of their situation.Depending on where in the brain the disease first strikes, the symptoms can be jarring. Some sufferers become deeply religious, undergo wild shifts in political identity, or have a sharp change in interests or style of dress. One stockbroker, for example, started wearing all-lavender clothes and developed a sudden obsession with painting. As his disease progressed, he engaged in petty theft and swam nude in public pools.The loss of embarrassment is common among some FTD patients, leading them to act in ways that might have horrified their former selves. Urinating in public, shoplifting, running red lights, making inappropriate sexual advances, digging through trash cans for food—all can be symptoms. Patients can lose the ability to evaluate social situations too, making them hard to interact with. In one extreme case, a patient&#39;s wife nearly severed her finger while using a pair of borrowed gardening shears. She shrieked to her husband, who had FTD, that she needed to go to the hospital. He replied by saying they had to first return the shears to their neighbor.These behaviors all arise because neurons are dying off in the frontal and temporal lobes, two large areas of the brain. Particularly vulnerable within these broad continents is a dispersed set of regions known as the salience network, which sifts through a barrage of sensations, memories, and emotions to focus a person&#39;s attention on what matters most in that moment. When this network breaks down, people may fail to grasp the emotional impact of their actions on others. “Emotions drive most choices in life, so if you don&#39;t have those systems, you&#39;re not the same person,” says Virginia Sturm, a neuropsychologist and neuroscientist at UCSF. “There are no tight anchors to your sense of self anymore, and the boundaries of self become loose.”Eventually, many FTD patients end up as apathetic as Lee, the light of their personhood dimmed to a pale flicker. Apathy also leads to incontinence, as patients lose the desire to take even basic care of themselves.In the months after Lee&#39;s diagnosis, Kristin spent as much time with her husband as she could. His decline had been steady so far, and she realized he would only slip further away. They spent the summer of 2017 going on long walks together. They took family trips. She found herself scrutinizing every interaction: Was that his last joke? His last laugh? His last hug? She never knew. He started leaving the apartment without saying anything, and she&#39;d have to grab the baby and chase him down San Francisco&#39;s busy streets.Lee was quickly becoming unmanageable. Once the baby learned to crawl, Kristin installed a gate at the top of the stairs to keep him from falling down the steps. But whenever Lee walked past the gate, he&#39;d reach down and unlatch it. He started blasting music videos in the living room at 11 o&#39;clock at night, despite the small child asleep in an adjacent room. Sometimes he&#39;d stay up all night, walking around in circles. Kristin struggled to take care of her son while making sure her husband didn&#39;t duck out the door unnoticed.She and Lee&#39;s parents grew increasingly worried he could get lost or mugged or wander into traffic. His parents, who are in their sixties, volunteered to take over Lee&#39;s care, and in the fall of 2017, Kristin agreed it was time for him to move in with them in San Jose while they figured out a long-term plan. “It&#39;s too hard to keep him safe in San Francisco,” his father, Rendon Holloway, says. “He has to have his walks.” Kristin was working full-time in San Francisco; she and their son stayed behind. Lee would visit them a few days a month.Kristin and their son spent many of their weekends in San Jose. In the first year, his mother, Kathy Holloway, recalls, when Lee saw the two of them arrive, “he always ran to his bedroom and grabbed his suitcase.” He would say, “I want to go back to San Francisco.”Lee often tried to leave the house. His parents eventually added an alarm that chimed loudly whenever the front door opened. They hid his shoes. He&#39;d hunt for them, and if he found them he&#39;d lace up and bolt out the door.When he wasn&#39;t trying to escape, Lee settled into a rhythm of scrolling through family photos on his phone, playing Mario Kart, or watching YouTube videos, all in roughly 30-second spurts. He&#39;d search YouTube for “Cloudflare,” “Kristin Holloway,” or his favorite bands and watch snippets of their music videos. Then he&#39;d pace heavily around the house, loud footfalls thudding at all hours of the day. Kathy lined the floors with rubber mats to deaden the sound.As the months passed, he spoke less and less. In one video from July 2018, Lee has his arm wrapped around his son while he reads him a bedtime book. Lee mumbles the words unevenly, without inflection, and hurries through the paperboard pages.From behind her phone&#39;s camera lens, Kristin saw that this might be the last bedtime story he read their son. Still, she kept recording, and she ended it with a “Good job!” to them both.Conversations soon became impossible. Lee started chattering in repetitive, unceasing loops. He would tell Kristin: “We met at Cloudflare. We got engaged in Rome. We got married in Maui, Hawaii.” He repeated it hundreds of times a day. Then the loops got shorter, more cryptic. He spoke fewer sentences, instead muttering sequences of numbers or letters.In September 2018, Prince and Zatlyn went to visit him while he was on one of his trips to San Francisco. Seeing Lee for the first time in many months, they thought he looked like a zombie, trooping aimlessly from room to room with empty eyes. At intervals during their visit he&#39;d sit down in the living room, turn on the TV, and flip through the channels, never watching any one thing for more than a minute. Then he&#39;d wander off again, all the while whispering numbers: 1 2 3 4 5 6 7. 1 2 3 4 5 6 7.He was both present and absent, a combination that kept his family on edge. When I visited his parents&#39; house in April 2019, Kristin and Alaric were also there for the day. We were clustered in the front hallway while his mother slipped into the kitchen to make tea. Lee, dressed in a Henley shirt and sweatpants, emerged from the back of the house. He stood tall and silent, and his arms hung heavily at his sides. He looked at Kristin, expressionless, as she introduced me and explained I&#39;d come to write a story about his life. He turned to wander into the living room and kitchen, where he leaned his elbows on the counter and reached a hand out to his mother, wordlessly requesting a snack. Then Kristin and Alaric went out with him for a walk, while I sat down with his parents.As we sat in the family&#39;s living room, Kathy described caring for her son, even as he grew increasingly distant. She misses the warmth in their daily interactions. “He used to come give me a hug and say, ‘I love you, Mom,’ ” she says. “No more.”Kathy is not the only one struggling to accept Lee for who he is—whoever he is. Managing his decline has strained the family, and his relatives sometimes clash over who should take care of him and how he should live. Kristin has spent many hours in therapy working through her grief and her feelings of guilt over deciding to live apart from Lee. She says she has felt alone in their relationship for years, and she&#39;s determined to give her son a relatively normal childhood. Alexandra, Lee&#39;s first wife, wonders whether her marriage fell apart because of the disease or their incompatibility. Was Lee simply someone who could sleep through European vacations and reject a homemade meal, or were those early incidents symptoms?There&#39;s no way to know for sure. Who was he then? Who is he now? How tightly knit is any person&#39;s selfhood across time? The philosopher Derek Parfit might have approached the issue by asking how many psychological chains bind Lee today to Lee in the past. His links are more tenuous than most people&#39;s. But they persist.In January 2019, Kristin was driving in a grocery store parking lot when her phone rang. She glimpsed the screen and froze. Lee was calling. There on the screen was his face, an old photo from when they had just started dating. She hadn&#39;t seen the photo in almost two years—it had been that long since he had called her.She answered, and the words tumbled out of her. “Baby, I love you so much, I miss you,” she cried. “Are you OK? Do you need anything?” He didn&#39;t say anything, but she could hear his breathing on the other end.He hung up.In that instant she realized how desperately she missed hearing his voice. “I&#39;d been in this process of losing him, then to have this moment of him reaching out from wherever he is,” she says. “It blew my mind.”The Cloudflare IPO in September raised $525 million. Lee, as one of the founders, suddenly became a whole lot richer. With his financial future now secure, Kristin set in motion the plan for his long-term care. She bought a 5,000-square-foot house on an acre of California&#39;s Central Coast, a spot they chose in the hope that his father, Rendon, could walk with him along the shore. She worked with a landscape architect to tailor the outdoor space to Lee&#39;s needs. There are zigzagging paths on which Lee can roam and a fence to keep him safely inside. Nontoxic plants only. No nut or fruit trees allowed; those could be choking hazards once he develops difficulty swallowing, as his doctors anticipate he will.Lee and his parents have moved there, and he has full-time care assistance too. Kristin shipped some of the furniture they&#39;d bought together to make the house feel more familiar to him, and she blanketed a wall in family photos. She, Alexandra, and their sons visit occasionally.Kristin hopes she has designed the perfect environment. Most FTD patients aren&#39;t so fortunate, if you can call it that, to wind down their lives on a personalized estate with a staff dedicated to keeping them safe and calm. Their families don&#39;t always have a choice in how involved they want to be. Still, all the money in the world can&#39;t answer the question of who, really, is living in that house.On rare occasions, Lee still surprises his parents with an affectionate pat on the back. He calls people from time to time, even if he never speaks a word. An old colleague recently saw that he&#39;d liked a post on LinkedIn. However diminished, a person lingers in the shattered roadways of his mind.Some months ago, Lee sent Kristin a series of text messages. In them were photos she&#39;d shared with him earlier: she and their son on Halloween, a trip to the park, Christmastime. At the end, he&#39;d typed the words: “the love.”SANDRA UPSON (@sandraupson) is a senior editor at WIRED. This is her first feature story for the magazine.This article appears in the May issue. Subscribe now.Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.More Great WIRED StoriesBuild cities for bikes, buses, and feet—not carsA one-time poultry farmer invents the future of refrigerationOK, Zoomer! How to become a videoconferencing power userDisney+ should offer the Star Wars original cuts—all of themWhy don’t we just ban targeted advertising?👁 Why can&#39;t AI grasp cause and effect? Plus: Get the latest AI news🏃🏽‍♀️ Want the best tools to get healthy? Check out our Gear team’s picks for the best fitness trackers, running gear (including shoes and socks), and best headphones</description>
      <pubDate>15 Apr 20 11:36 EDT</pubDate>
      <guid>https://www.wired.com/story/lee-holloway-devastating-decline-brilliant-young-coder/</guid>
    </item>
    <item>
      <title>Monetize Your Content, Maximize Your Reach</title>
      <link>https://paygo.media/p/25171</link>
      <description>&lt;a href=&#34;https://paygo.media/p/25171&#34;&gt;Source&lt;/a&gt;&lt;br/&gt;Turn site visitors into readersCasual visitors aren’t ready to subscribe yet; but they’re here and ready to pay. Let them Pay-Per-Read.Increase your market potentialOnly 8% of Americans have 2 or more subscriptions; now monetize the rest. Give consumers what they wantQuit selling out to aggregators. Paygo offers visitors frictionless access to your content but on your site, with your brand, at your price.Own Your ExperienceAt Paygo, we believe that publishers should control their content, their brand, on their site. We streamline the user experience while getting out of the way, so your brand and content can take center stage.Paywall BrandingArticle StatusArticle PricingArticle MetaPaygo lets you monetize your content on your terms: you set the price and the rules -- we do the rest. With Paygo, charge several cents for Pay-Per-Read access profitably, while still earning 10X revenue per article compared to running ads.Checkout ModuleTipping ModuleApple PayGoogle PayCredit CardWorks AnywhereReady to monetize your international visitors? Social media traffic? AMP sessions? Paygo gives you audience-control of the Pay-per-Read option so you can offer it to specific, low conversion cohorts to minimize risk and maximize new revenue.Embedded CheckoutExpress CheckoutHosted CheckoutProtect Your ContentPaygo offers a turnkey SDK that guards your content and communicates everything from user authentication to content purchased with your website. Simple setup, existing traffic, new revenue.User AuthenticationContent OwnershipContent PurchasedContent UnlockedKeep TrackPaygo tracks everything from site visits to funnel activity and article conversions so your team can analyze performance and adapt quickly.Site VisitsFunnel ActivityConversion RatesRevenueUsersPaygo Checkout DemoThank You TeachersDan RatherThere has been so much upheaval, so much sadness, so many reasons to be dejected and deterred. And yet every day, across this vast and diverse country, women and men get up and set about on one of the most important tasks facing our nation...View ArticlePaygo Tip DemoThe Press and the Party of NoDan RatherThe Biden Administration is finding a familiar answer to everything it is trying to do from the Republicans on Capitol Hill. It is the same answer that Biden saw up close when he was Vice President in the last Democratic administration.View Article</description>
      <pubDate>17 Feb 21 09:18 EST</pubDate>
      <guid>https://paygo.media/p/25171</guid>
    </item>
    <item>
      <title></title>
      <link>https://3fx.ch/typing-is-hard.html</link>
      <description>&lt;a href=&#34;https://3fx.ch/typing-is-hard.html&#34;&gt;Source&lt;/a&gt;&lt;br/&gt; Typing is Hard Type Checking and Type Inference Common terms Completeness Soundness Decidability Hindley-Milner Type System Dependent types System F How Hard is Type-Checking Your Favorite Language? C++ C# Elm F# Go Haskell Idris Java OCaml ML Rust Scala Swift TypeScript Zig FAQ Where are Python, Bash, etc.? What about unsafe casts? I’ve spotted a mistake/imprecision, what should I do? Type Checking and Type Inference Type checking is the process of taking a given program in some programming language and working out whether all variables and expressions have the correct types, i.e. strings are assigned to strings, arithmetic expressions involve only numbers, etc. Some languages also offer type inference, tasking the compiler with the task of figuring out correct types on its own. Depending on a language’s features the type checking and type inference problems range from trivial to undecidable. Common terms Completeness A type checker is complete if it can check every correctly typed program. Soundness A type checker is sound if it only accepts correctly typed programs. Decidability A decision problem is decidable if for any input we can compute whether the input satifies the problem in finite time. Examples of decidable problems include primality testing and boolean satisfiability. The halting problem for example is undecidable: We cannot check whether a program runs infinitely long in finite time. We are interested in the type checking and type inference problems for programming languages: Given some input program, does it type check? And given some program, which types should we assign to the expression such that it typechecks? Hindley-Milner Type System The Hindley-Milner (HM) type system is a type system for the simple typed lambda calculus with parametric polymorphism, which is used as a base for many functional languages. Type-checking for HM is decidable and efficient, its complexity is linear in the size of the program. Type inference is more difficult than type checking, since the compiler has to solve the type constraints incurred for expressions in the program. It is decidable for the HM type system, however the problem itself is PSPACE-hard and EXPTIME-complete, meaning that in the worst case it needs at least a polynomial amount of extra space and exponential time relative to the input size. Fortunately, the type inference algorithm is linear when the nesting depth of polymorphic variables is bounded, as is the case for most applications. There exist many type inference algorithms, the best known one is the so-called Algorithm W. Many functional programming languages implement variants of the HM type system. Dependent types In simple terms dependent types allow types to depend not only on other types, but on values. This is best understood by an example: Normally we can only encode very coarse information, such as “x is of integer type”. Dependent types allow us to define more detailed types. For example we could then create a type “even integer”, whose only inhabitants are even integers. This is strictly more powerful than the previous setting: Dependent types in general make type inference undecidable as can be shown by reduction to the Post Correspondence Problem. System F System F is an extension of the simply typed lambda calculus which allows quantification over types via type variables. An example is the identity function (in pseudo-Haskell): id :: a -&gt; a, which is shorthand for id :: forall a. a -&gt; a. In this example forall binds the type variable a, and hence id is quantified over all types. Type inference in System F is always undecidable, while the question whether type-checking is decidable depends on lower-level details. How Hard is Type-Checking Your Favorite Language? Below I’ve compiled a list of languages and how hard type checking/type inference is in these languages. If you find a mistake or are missing a language please file an issue with your language, its type checking complexity and optimally a resource that supports your claim. I do not claim completeness nor correctness of the properties shown here, it is mainly an amalgamation of blog posts I’ve been collecting. C++ undecidable, C++ Templates are Turing Complete, Todd L. Veldhuizen (2003), even its grammar is undecidable C# unsound, undecidable, there is an excellent SO answer by Eric Lippert on this topic. Other fun things include a SAT solver using the C# type-checker Elm decidable, uses Hindley-Milner, but currently unsound, due to an interesting compiler bug: (String.length &#34; &#34;) ^ (-1) : Int F# undecidable, GitHub user cannorin implemented the untyped lambda calculus in F# Go decidable, since type inference is only used for variable initialization Haskell 1998 standard, no extensions: decidable, variant of HM 2010 standard, no extensions: decidable, restriction of System F with sufficient1 extensions: undecidable, as there exist Turing Machines in Haskell types. A simpler variant is to implement the SKI calculus. Idris decidable, surprisingly. Idris has dependent types, which in general have undecidable type-checking, but at compile time it will only evaluate expressions which it knows to be total (terminating and covering all inputs). Java undecidable, because Java Generics are Turing complete. Java 5 or later is unsound, as shown by Amin and Tate (2015) OCaml undecidable, since we can encode an undecidable problem in OCaml modules ML decidable, uses Hindley-Milner Rust undecidable, unsound, both type inference (since Rust has rank-3-types) and type checking, as shown by this Smallfuck interpreter implemented using traits. There is a long-standing (open since 2015) bug about an unsoundness issue with traits. Scala undecidable, since it admits a type-level SKI calculus, unsound, as shown by Amin and Tate (2015). Scala 2.13.3 (newest as of writing this) also exhibits the same problem. Swift undecidable, as proven here by reduction to the word problem for finitely generated groups2 TypeScript undecidable, unsound. The TypeScript documentation mentions its unsoundness and the motivations behind them. Undecidability: TypeScript’s type system was proven Turing complete until they disallowed self-referential types. However Robbie Ostrow wrote a program checking the Collatz conjecture, and as the generalized form of the Collatz conjecture is undecidable3, the TypeScript type system is undecidable as well. Zig undecidable, since evaluation of recursive functions at compile time is possible, thus requiring the compiler to solve the halting problem. FAQ Where are Python, Bash, etc.? Type checking and type inference work primarily for statically typed languages. While there exist extensions to some dynamic languages imbuing them with static type checking these are not part of the language, and the complexity depends not on the language but the extension. What about unsafe casts? Some languages offer explicit unchecked casts, which are accepted by the type checker and may potentially fail at runtime. Examples are casting from Object to some subclass in C# and Java, casting values of type interface{} in Go or using unsafeCoerce in Haskell. I’ve chosen not to account for such casts/coercions since unchecked downcasting is an inherently unsafe operation not covered by type checker guarantees. I’ve spotted a mistake/imprecision, what should I do? Great work! Please report it on the official issue tracker detailing what is wrong and I will try to fix it as soon as possible. Using only RankNTypes suffices to stump inference. Interestingly enough this is only undecidable for N &gt;= 3.↩ I think a reduction to the PCP-problem is also possible.↩ Thanks to reddit user /u/nckl for pointing that out.↩ </description>
      <pubDate>16 Feb 21 19:47 EST</pubDate>
      <guid>https://3fx.ch/typing-is-hard.html</guid>
    </item>
  </channel>
</rss>